<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Visualise Crypto with SQL and FastAPI</title>
      <link href="/Hexo-Blog/2021/03/15/2021-03-15-visualise-crypto-with-sql-and-fastapi/"/>
      <url>/Hexo-Blog/2021/03/15/2021-03-15-visualise-crypto-with-sql-and-fastapi/</url>
      
        <content type="html"><![CDATA[<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>In this article, first, I’ll populate crypto database using Python and SQL. I retrieve the list of crypto coin assets, verify the data, and tackle any errors I encounter along the way. Second, I’ll talk about how to keep the database up to date with the latest prices, and retrieve daily data from the yahoo finance API. Finally, I’ll set up to build a web UI usin FastAPI.</p><h1 id="Initialise-Database"><a href="#Initialise-Database" class="headerlink" title="Initialise Database"></a>Initialise Database</h1><p>At the very begining, I create a database file called “crypto.db”, by typing <code>touch crypto.db</code> in the command line. I want to crawl the crypto coin data on a daily basis, so I create a list which contains the crypto coin I want to keep up to date.</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">symbols <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'BTC-USD'</span><span class="token punctuation">,</span> <span class="token string">'ETH-USD'</span><span class="token punctuation">,</span> <span class="token string">'BNB-USD'</span><span class="token punctuation">,</span> <span class="token string">'DOGE-USD'</span><span class="token punctuation">,</span> <span class="token string">'USDT-USD'</span><span class="token punctuation">,</span> <span class="token string">'LTC-USD'</span><span class="token punctuation">,</span> <span class="token string">'LINK-USD'</span><span class="token punctuation">,</span> <span class="token string">'USDC-USD'</span><span class="token punctuation">,</span> <span class="token string">'THETA-USD'</span><span class="token punctuation">,</span> <span class="token string">'XMR-USD'</span><span class="token punctuation">]</span>cryptocurrencies <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'Bitcoin USD'</span><span class="token punctuation">,</span> <span class="token string">'Ethereum USD'</span><span class="token punctuation">,</span> <span class="token string">'Binance USD'</span><span class="token punctuation">,</span> <span class="token string">'Dogecoin USD'</span><span class="token punctuation">,</span> <span class="token string">'Tether USD'</span><span class="token punctuation">,</span> <span class="token string">'Litecoin USD'</span><span class="token punctuation">,</span> <span class="token string">'Chainlink USD'</span><span class="token punctuation">,</span> <span class="token string">'USDCoin USD'</span><span class="token punctuation">,</span> <span class="token string">'ThETA USD'</span><span class="token punctuation">,</span> <span class="token string">'Monero USD'</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="Create-Table"><a href="#Create-Table" class="headerlink" title="Create Table"></a>Create Table</h2><p>Next, I will create two tables, one for symbol mapping to name of cryptocurrency, another for prices information.</p><img src="/Hexo-Blog/2021/03/15/2021-03-15-visualise-crypto-with-sql-and-fastapi/database.png" class=""><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> sqlite3<span class="token keyword">def</span> <span class="token function">create_table</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>connection <span class="token operator">=</span> sqlite3<span class="token punctuation">.</span>connect<span class="token punctuation">(</span><span class="token string">'crypto.db'</span><span class="token punctuation">)</span>cursor <span class="token operator">=</span> connection<span class="token punctuation">.</span>cursor<span class="token punctuation">(</span><span class="token punctuation">)</span>cursor<span class="token punctuation">.</span>execute<span class="token punctuation">(</span><span class="token triple-quoted-string string">"""CREATE TABLE IF NOT EXISTS crypto (id INTEGER PRIMARY KEY, symbol TEXT NOT NULL, cryptocurrency TEXT NOT NULL);"""</span><span class="token punctuation">)</span>cursor<span class="token punctuation">.</span>execute<span class="token punctuation">(</span><span class="token triple-quoted-string string">"""CREATE TABLE IF NOT EXISTS crypto_price (id INTEGER PRIMARY KEY, crypto_id INTEGER, date NOT NULL, open NOT NULL, high NOT NULL, low NOT NULL, close NOT NULL, volume NOT NULL, FOREIGN KEY (crypto_id) REFERENCES crypto (id));"""</span><span class="token punctuation">)</span>connection<span class="token punctuation">.</span>commit<span class="token punctuation">(</span><span class="token punctuation">)</span>create_table<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="Insert-Symbols-into-Table"><a href="#Insert-Symbols-into-Table" class="headerlink" title="Insert Symbols into Table"></a>Insert Symbols into Table</h2><p>After creating the two tables, I insert the symbol information into the table <code>crypto</code>.</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">insert_into_table</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>connection <span class="token operator">=</span> sqlite3<span class="token punctuation">.</span>connect<span class="token punctuation">(</span><span class="token string">'crypto.db'</span><span class="token punctuation">)</span>cursor <span class="token operator">=</span> connection<span class="token punctuation">.</span>cursor<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">for</span> symbol<span class="token punctuation">,</span> cryptocurrency <span class="token keyword">in</span> <span class="token builtin">zip</span><span class="token punctuation">(</span>symbols<span class="token punctuation">,</span> cryptocurrencies<span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token comment"># Insert values into table only if they don't exist in a row.</span>cursor<span class="token punctuation">.</span>execute<span class="token punctuation">(</span><span class="token triple-quoted-string string">"""INSERT INTO crypto (symbol, cryptocurrency)SELECT ?, ?WHERE NOT EXISTS (SELECT symbol, cryptocurrency FROM crypto WHERE symbol=? AND cryptocurrency=?);"""</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>symbol<span class="token punctuation">,</span> cryptocurrency<span class="token punctuation">,</span> symbol<span class="token punctuation">,</span> cryptocurrency<span class="token punctuation">)</span><span class="token punctuation">)</span>connection<span class="token punctuation">.</span>commit<span class="token punctuation">(</span><span class="token punctuation">)</span>insert_into_table<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="Drop-Table"><a href="#Drop-Table" class="headerlink" title="Drop Table"></a>Drop Table</h2><p>(Optional) If you want to drop the table and re-create a new list of different cryptocurrencies, then you can use the function below.</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">drop_table</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>connection <span class="token operator">=</span> sqlite3<span class="token punctuation">.</span>connect<span class="token punctuation">(</span><span class="token string">"crypto.db"</span><span class="token punctuation">)</span>cursor <span class="token operator">=</span> connection<span class="token punctuation">.</span>cursor<span class="token punctuation">(</span><span class="token punctuation">)</span>cursor<span class="token punctuation">.</span>execute<span class="token punctuation">(</span><span class="token triple-quoted-string string">"""DROP TABLE crypto_price;"""</span><span class="token punctuation">)</span>cursor<span class="token punctuation">.</span>execute<span class="token punctuation">(</span><span class="token triple-quoted-string string">"""DROP TABLE crypto;"""</span><span class="token punctuation">)</span>connection<span class="token punctuation">.</span>commit<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="Populate-Prices"><a href="#Populate-Prices" class="headerlink" title="Populate Prices"></a>Populate Prices</h1><p>I will retrieve daily price data from yahoo finance API and populate it into the pre-build table <code>crypto_price</code> in the database. This sets up to begin building a web UI in the next part.</p><h2 id="Insert-Prices-into-Table"><a href="#Insert-Prices-into-Table" class="headerlink" title="Insert Prices into Table"></a>Insert Prices into Table</h2><p>In this section, I insert the prices information, which I get it from yahoo finance API, into the table <code>crypto_price</code>.</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> yfinance <span class="token keyword">as</span> yf<span class="token keyword">def</span> <span class="token function">populate_prices</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>connection <span class="token operator">=</span> sqlite3<span class="token punctuation">.</span>connect<span class="token punctuation">(</span><span class="token string">"crypto.db"</span><span class="token punctuation">)</span>connection<span class="token punctuation">.</span>row_factory <span class="token operator">=</span> sqlite3<span class="token punctuation">.</span>Rowcursor <span class="token operator">=</span> connection<span class="token punctuation">.</span>cursor<span class="token punctuation">(</span><span class="token punctuation">)</span>cursor<span class="token punctuation">.</span>execute<span class="token punctuation">(</span><span class="token triple-quoted-string string">"""SELECT * FROM crypto;"""</span><span class="token punctuation">)</span>rows <span class="token operator">=</span> cursor<span class="token punctuation">.</span>fetchall<span class="token punctuation">(</span><span class="token punctuation">)</span>symbols <span class="token operator">=</span> <span class="token punctuation">[</span>row<span class="token punctuation">[</span><span class="token string">'symbol'</span><span class="token punctuation">]</span> <span class="token keyword">for</span> row <span class="token keyword">in</span> rows<span class="token punctuation">]</span>cryptocurrencies <span class="token operator">=</span> <span class="token punctuation">[</span>row<span class="token punctuation">[</span><span class="token string">'cryptocurrency'</span><span class="token punctuation">]</span> <span class="token keyword">for</span> row <span class="token keyword">in</span> rows<span class="token punctuation">]</span>crypto_dict <span class="token operator">=</span> <span class="token punctuation">&#123;</span>row<span class="token punctuation">[</span><span class="token string">'symbol'</span><span class="token punctuation">]</span><span class="token punctuation">:</span> row<span class="token punctuation">[</span><span class="token string">'id'</span><span class="token punctuation">]</span> <span class="token keyword">for</span> row <span class="token keyword">in</span> rows<span class="token punctuation">&#125;</span><span class="token keyword">for</span> symbol<span class="token punctuation">,</span> cryptocurrency <span class="token keyword">in</span> <span class="token builtin">zip</span><span class="token punctuation">(</span>symbols<span class="token punctuation">,</span> cryptocurrencies<span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Processing symbol </span><span class="token interpolation"><span class="token punctuation">&#123;</span>cryptocurrency<span class="token punctuation">&#125;</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>df <span class="token operator">=</span> yf<span class="token punctuation">.</span>download<span class="token punctuation">(</span>symbol<span class="token punctuation">,</span> progress<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>df <span class="token operator">=</span> df<span class="token punctuation">.</span>reset_index<span class="token punctuation">(</span>drop<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>df<span class="token punctuation">.</span>columns <span class="token operator">=</span> df<span class="token punctuation">.</span>columns<span class="token punctuation">.</span><span class="token builtin">str</span><span class="token punctuation">.</span>lower<span class="token punctuation">(</span><span class="token punctuation">)</span>crypto_id <span class="token operator">=</span> crypto_dict<span class="token punctuation">[</span>symbol<span class="token punctuation">]</span><span class="token keyword">for</span> idx<span class="token punctuation">,</span> row <span class="token keyword">in</span> df<span class="token punctuation">.</span>iterrows<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>d <span class="token operator">=</span> <span class="token builtin">str</span><span class="token punctuation">(</span>row<span class="token punctuation">[</span><span class="token string">"date"</span><span class="token punctuation">]</span><span class="token punctuation">)</span>o <span class="token operator">=</span> row<span class="token punctuation">[</span><span class="token string">"open"</span><span class="token punctuation">]</span>h <span class="token operator">=</span> row<span class="token punctuation">[</span><span class="token string">"high"</span><span class="token punctuation">]</span>l <span class="token operator">=</span> row<span class="token punctuation">[</span><span class="token string">"low"</span><span class="token punctuation">]</span>c <span class="token operator">=</span> row<span class="token punctuation">[</span><span class="token string">"close"</span><span class="token punctuation">]</span>v <span class="token operator">=</span> row<span class="token punctuation">[</span><span class="token string">"volume"</span><span class="token punctuation">]</span>cursor<span class="token punctuation">.</span>execute<span class="token punctuation">(</span><span class="token triple-quoted-string string">"""INSERT INTO crypto_price (crypto_id, date, open, high, low, close, volume)SELECT ?, ?, ?, ?, ?, ?, ?WHERE NOT EXISTS (SELECT * FROM crypto_price WHERE crypto_id=? AND date=?);"""</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>crypto_id<span class="token punctuation">,</span> d<span class="token punctuation">,</span> o<span class="token punctuation">,</span> h<span class="token punctuation">,</span> l<span class="token punctuation">,</span> c<span class="token punctuation">,</span> v<span class="token punctuation">,</span> crypto_id<span class="token punctuation">,</span> d<span class="token punctuation">)</span><span class="token punctuation">)</span>connection<span class="token punctuation">.</span>commit<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>You can use DB Browser to check whether prices data is stored correctly.</p><img src="/Hexo-Blog/2021/03/15/2021-03-15-visualise-crypto-with-sql-and-fastapi/db-browser.jpg" class=""><p>Or input <code>sqlite3 crypto.db</code> in command line , and check it through some SQL command.</p><pre class="line-numbers language-sql" data-language="sql"><code class="language-sql"><span class="token keyword">SELECT</span> <span class="token operator">*</span> <span class="token keyword">FROM</span> crypto_price<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h1 id="Web-UI"><a href="#Web-UI" class="headerlink" title="Web UI"></a>Web UI</h1><p>In this section, I begin building a web frontend for the crypto coin price database. I cover the basics of <a href="https://fastapi.tiangolo.com/">FastAPI</a>, a lightweight Python framework for buildings API’s and web applications. I use <a href="https://semantic-ui.com/introduction/getting-started.html">Semantic UI</a> and <a href="https://jinja.palletsprojects.com/en/2.11.x/">jinja2</a> templates to quickly create a simple, but nice looking UI for our database.</p><h2 id="FastAPI"><a href="#FastAPI" class="headerlink" title="FastAPI"></a>FastAPI</h2><p>The current folder and files should look like this:</p><pre class="line-numbers language-console" data-language="console"><code class="language-console">G:.|   app.py|   crypto.db|+---templates|       crypto_detail.html|       index.html|       layout.html<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>Create a file <code>app.py</code> with:</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> sqlite3<span class="token keyword">from</span> fastapi <span class="token keyword">import</span> FastAPI<span class="token punctuation">,</span> Request<span class="token keyword">from</span> fastapi<span class="token punctuation">.</span>templating <span class="token keyword">import</span> Jinja2Templates app <span class="token operator">=</span> FastAPI<span class="token punctuation">(</span><span class="token punctuation">)</span>templates <span class="token operator">=</span> Jinja2Templates<span class="token punctuation">(</span>directory<span class="token operator">=</span><span class="token string">"templates"</span><span class="token punctuation">)</span><span class="token decorator annotation punctuation">@app<span class="token punctuation">.</span>get</span><span class="token punctuation">(</span><span class="token string">"/"</span><span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">index</span><span class="token punctuation">(</span>request<span class="token punctuation">:</span> Request<span class="token punctuation">)</span><span class="token punctuation">:</span>    connection <span class="token operator">=</span> sqlite3<span class="token punctuation">.</span>connect<span class="token punctuation">(</span><span class="token string">"crypto.db"</span><span class="token punctuation">)</span>    connection<span class="token punctuation">.</span>row_factory <span class="token operator">=</span> sqlite3<span class="token punctuation">.</span>Row    cursor <span class="token operator">=</span> connection<span class="token punctuation">.</span>cursor<span class="token punctuation">(</span><span class="token punctuation">)</span>    cursor<span class="token punctuation">.</span>execute<span class="token punctuation">(</span><span class="token triple-quoted-string string">"""        SELECT * FROM crypto ORDER BY symbol;    """</span><span class="token punctuation">)</span>    rows <span class="token operator">=</span> cursor<span class="token punctuation">.</span>fetchall<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> templates<span class="token punctuation">.</span>TemplateResponse<span class="token punctuation">(</span>        <span class="token string">"index.html"</span><span class="token punctuation">,</span>         <span class="token punctuation">&#123;</span><span class="token string">"request"</span><span class="token punctuation">:</span> request<span class="token punctuation">,</span> <span class="token string">"rows"</span><span class="token punctuation">:</span> rows<span class="token punctuation">&#125;</span><span class="token punctuation">)</span><span class="token decorator annotation punctuation">@app<span class="token punctuation">.</span>get</span><span class="token punctuation">(</span><span class="token string">"/crypto/&#123;symbol&#125;"</span><span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">crypto_detail</span><span class="token punctuation">(</span>request<span class="token punctuation">:</span> Request<span class="token punctuation">,</span> symbol<span class="token punctuation">)</span><span class="token punctuation">:</span>    connection <span class="token operator">=</span> sqlite3<span class="token punctuation">.</span>connect<span class="token punctuation">(</span><span class="token string">"crypto.db"</span><span class="token punctuation">)</span>    connection<span class="token punctuation">.</span>row_factory <span class="token operator">=</span> sqlite3<span class="token punctuation">.</span>Row    cursor <span class="token operator">=</span> connection<span class="token punctuation">.</span>cursor<span class="token punctuation">(</span><span class="token punctuation">)</span>    cursor<span class="token punctuation">.</span>execute<span class="token punctuation">(</span><span class="token triple-quoted-string string">"""        SELECT * FROM crypto WHERE symbol=?;    """</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>symbol<span class="token punctuation">,</span> <span class="token punctuation">)</span><span class="token punctuation">)</span>    row <span class="token operator">=</span> cursor<span class="token punctuation">.</span>fetchone<span class="token punctuation">(</span><span class="token punctuation">)</span>    cursor<span class="token punctuation">.</span>execute<span class="token punctuation">(</span><span class="token triple-quoted-string string">"""        SELECT * FROM crypto_price WHERE crypto_id=? ORDER BY date DESC    """</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>row<span class="token punctuation">[</span><span class="token string">'id'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">)</span><span class="token punctuation">)</span>    bars <span class="token operator">=</span> cursor<span class="token punctuation">.</span>fetchall<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> templates<span class="token punctuation">.</span>TemplateResponse<span class="token punctuation">(</span>        <span class="token string">"crypto_detail.html"</span><span class="token punctuation">,</span>         <span class="token punctuation">&#123;</span><span class="token string">"request"</span><span class="token punctuation">:</span> request<span class="token punctuation">,</span> <span class="token string">"row"</span><span class="token punctuation">:</span> row<span class="token punctuation">,</span> <span class="token string">"bars"</span><span class="token punctuation">:</span> bars<span class="token punctuation">&#125;</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>where <code>layout.html</code> is like the following</p><pre class="line-numbers language-html" data-language="html"><code class="language-html"><span class="token doctype"><span class="token punctuation">&lt;!</span><span class="token doctype-tag">DOCTYPE</span> <span class="token name">html</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>html</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>head</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>title</span><span class="token punctuation">></span></span>crypto<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>title</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>link</span> <span class="token attr-name">rel</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>stylesheet<span class="token punctuation">"</span></span> <span class="token attr-name">href</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>https://cdn.jsdelivr.net/npm/semantic-ui@2.4.2/dist/semantic.min.css<span class="token punctuation">"</span></span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>script</span> <span class="token attr-name">src</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>https://cdn.jsdelivr.net/npm/semantic-ui@2.4.2/dist/semantic.min.js<span class="token punctuation">"</span></span><span class="token punctuation">></span></span><span class="token script"></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>script</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>head</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>body</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>div</span> <span class="token attr-name">class</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>ui container<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>&#123;% block content %&#125;&#123;% endblock %&#125;<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>div</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>body</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>html</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>and <code>index.html</code> is like the following</p><pre class="line-numbers language-html" data-language="html"><code class="language-html"><span class="token doctype"><span class="token punctuation">&lt;!</span><span class="token doctype-tag">DOCTYPE</span> <span class="token name">html</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>html</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>head</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>title</span><span class="token punctuation">></span></span>crypto<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>title</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>link</span> <span class="token attr-name">rel</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>stylesheet<span class="token punctuation">"</span></span> <span class="token attr-name">href</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>https://cdn.jsdelivr.net/npm/semantic-ui@2.4.2/dist/semantic.min.css<span class="token punctuation">"</span></span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>script</span> <span class="token attr-name">src</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>https://cdn.jsdelivr.net/npm/semantic-ui@2.4.2/dist/semantic.min.js<span class="token punctuation">"</span></span><span class="token punctuation">></span></span><span class="token script"></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>script</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>head</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>body</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>div</span> <span class="token attr-name">class</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>ui container<span class="token punctuation">"</span></span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>h1</span><span class="token punctuation">></span></span>Crypto List<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>h1</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>table</span> <span class="token attr-name">class</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>ui selectable striped table<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>thead</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>tr</span><span class="token punctuation">></span></span>      <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>th</span><span class="token punctuation">></span></span>Symbol<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>th</span><span class="token punctuation">></span></span>      <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>th</span><span class="token punctuation">></span></span>Coin<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>th</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>tr</span><span class="token punctuation">></span></span>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>thead</span><span class="token punctuation">></span></span>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>tbody</span><span class="token punctuation">></span></span>  &#123;% for row in rows %&#125;    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>tr</span><span class="token punctuation">></span></span>      <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>td</span><span class="token punctuation">></span></span>&#123;&#123; row.symbol &#125;&#125;<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>td</span><span class="token punctuation">></span></span>      <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>td</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>a</span> <span class="token attr-name">href</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>/crypto/&#123;&#123; row.symbol &#125;&#125;<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>&#123;&#123; row.cryptocurrency &#125;&#125;<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>a</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>td</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>tr</span><span class="token punctuation">></span></span>    &#123;% endfor %&#125;  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>tbody</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>table</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>div</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>body</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>html</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>and <code>crypto_detail.html</code> is like the following</p><pre class="line-numbers language-html" data-language="html"><code class="language-html">&#123;% extends "layout.html" %&#125;&#123;% block content %&#125;<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>h1</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>a</span> <span class="token attr-name">href</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>/<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>Crypto <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>a</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>i</span> <span class="token attr-name">class</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>angle double right icon<span class="token punctuation">"</span></span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>i</span><span class="token punctuation">></span></span>&#123;&#123; row.cryptocurrency &#125;&#125; (&#123;&#123; row.symbol &#125;&#125;)<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>h1</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>table</span> <span class="token attr-name">class</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>ui selectable striped table<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>thead</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>tr</span><span class="token punctuation">></span></span>      <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>th</span><span class="token punctuation">></span></span>Date<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>th</span><span class="token punctuation">></span></span>      <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>th</span><span class="token punctuation">></span></span>Open<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>th</span><span class="token punctuation">></span></span>      <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>th</span><span class="token punctuation">></span></span>High<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>th</span><span class="token punctuation">></span></span>      <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>th</span><span class="token punctuation">></span></span>Low<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>th</span><span class="token punctuation">></span></span>      <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>th</span><span class="token punctuation">></span></span>Close<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>th</span><span class="token punctuation">></span></span>      <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>th</span><span class="token punctuation">></span></span>Volume<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>th</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>tr</span><span class="token punctuation">></span></span>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>thead</span><span class="token punctuation">></span></span>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>tbody</span><span class="token punctuation">></span></span>  &#123;% for bar in bars %&#125;    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>tr</span><span class="token punctuation">></span></span>      <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>td</span><span class="token punctuation">></span></span>&#123;&#123; bar.date &#125;&#125;<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>td</span><span class="token punctuation">></span></span>      <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>td</span><span class="token punctuation">></span></span>&#123;&#123; bar.open &#125;&#125;<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>td</span><span class="token punctuation">></span></span>      <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>td</span><span class="token punctuation">></span></span>&#123;&#123; bar.high &#125;&#125;<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>td</span><span class="token punctuation">></span></span>      <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>td</span><span class="token punctuation">></span></span>&#123;&#123; bar.low &#125;&#125;<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>td</span><span class="token punctuation">></span></span>      <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>td</span><span class="token punctuation">></span></span>&#123;&#123; bar.close &#125;&#125;<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>td</span><span class="token punctuation">></span></span>      <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>td</span><span class="token punctuation">></span></span>&#123;&#123; bar.volume &#125;&#125;<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>td</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>tr</span><span class="token punctuation">></span></span>    &#123;% endfor %&#125;  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>tbody</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>table</span><span class="token punctuation">></span></span>&#123;% endblock %&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>Type in <code>uvicorn app:app --reload</code>, and open your browser at <code>http://127.0.0.1:8000</code>, then you can see the nice looking web UI.</p><img src="/Hexo-Blog/2021/03/15/2021-03-15-visualise-crypto-with-sql-and-fastapi/ui.jpg" class=""><p>If you click on any symbol under the ‘Coin’ column, you can get to its detailed page.</p><img src="/Hexo-Blog/2021/03/15/2021-03-15-visualise-crypto-with-sql-and-fastapi/detail.jpg" class=""><h2 id="Add-Tradingview-Chart"><a href="#Add-Tradingview-Chart" class="headerlink" title="Add Tradingview Chart"></a>Add Tradingview Chart</h2><p>TradingView Advanced Chart Widget is a free and powerful charting solution that easily embeds into any website. Simply adjust the settings and click Apply to see a preview, then copy the embed code and paste it into your site code. You can personalize the chart by modifying the default symbol, watchlist, adding tools for technical analysis and a lot more. You can even add hotlists or an economic calendar to make the widget into an entire analytics platform.</p><p>You can get the embedding code over <a href="https://www.tradingview.com/widget/advanced-chart/">here</a>. Next, put this into <code>crypto_detail.html</code>.</p><pre class="line-numbers language-html" data-language="html"><code class="language-html">&#123;% extends "layout.html" %&#125;&#123;% block content %&#125;<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>h1</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>a</span> <span class="token attr-name">href</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>/<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>Crypto <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>a</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>i</span> <span class="token attr-name">class</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>angle double right icon<span class="token punctuation">"</span></span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>i</span><span class="token punctuation">></span></span>&#123;&#123; row.cryptocurrency &#125;&#125; (&#123;&#123; row.symbol &#125;&#125;)<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>h1</span><span class="token punctuation">></span></span><span class="token comment">&lt;!-- TradingView Widget BEGIN --></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>div</span> <span class="token attr-name">class</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>tradingview-widget-container<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>div</span> <span class="token attr-name">id</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>tradingview_05834<span class="token punctuation">"</span></span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>div</span><span class="token punctuation">></span></span>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>div</span> <span class="token attr-name">class</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>tradingview-widget-copyright<span class="token punctuation">"</span></span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>a</span> <span class="token attr-name">href</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>https://www.tradingview.com/symbols/NASDAQ-AAPL/<span class="token punctuation">"</span></span> <span class="token attr-name">rel</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>noopener<span class="token punctuation">"</span></span> <span class="token attr-name">target</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>_blank<span class="token punctuation">"</span></span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>span</span> <span class="token attr-name">class</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>blue-text<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>AAPL Chart<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>span</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>a</span><span class="token punctuation">></span></span> by TradingView<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>div</span><span class="token punctuation">></span></span>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>script</span> <span class="token attr-name">type</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>text/javascript<span class="token punctuation">"</span></span> <span class="token attr-name">src</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>https://s3.tradingview.com/tv.js<span class="token punctuation">"</span></span><span class="token punctuation">></span></span><span class="token script"></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>script</span><span class="token punctuation">></span></span>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>script</span> <span class="token attr-name">type</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>text/javascript<span class="token punctuation">"</span></span><span class="token punctuation">></span></span><span class="token script"><span class="token language-javascript">  <span class="token keyword">new</span> <span class="token class-name">TradingView<span class="token punctuation">.</span>widget</span><span class="token punctuation">(</span>  <span class="token punctuation">&#123;</span>  <span class="token string">"width"</span><span class="token operator">:</span> <span class="token string">"100%"</span><span class="token punctuation">,</span>  <span class="token string">"height"</span><span class="token operator">:</span> <span class="token number">610</span><span class="token punctuation">,</span>  <span class="token string">"symbol"</span><span class="token operator">:</span> <span class="token string">"&#123;&#123; ticker &#125;&#125;"</span><span class="token punctuation">,</span>  <span class="token string">"interval"</span><span class="token operator">:</span> <span class="token string">"D"</span><span class="token punctuation">,</span>  <span class="token string">"timezone"</span><span class="token operator">:</span> <span class="token string">"Europe/London"</span><span class="token punctuation">,</span>  <span class="token string">"theme"</span><span class="token operator">:</span> <span class="token string">"light"</span><span class="token punctuation">,</span>  <span class="token string">"style"</span><span class="token operator">:</span> <span class="token string">"1"</span><span class="token punctuation">,</span>  <span class="token string">"locale"</span><span class="token operator">:</span> <span class="token string">"en"</span><span class="token punctuation">,</span>  <span class="token string">"toolbar_bg"</span><span class="token operator">:</span> <span class="token string">"#f1f3f6"</span><span class="token punctuation">,</span>  <span class="token string">"enable_publishing"</span><span class="token operator">:</span> <span class="token boolean">false</span><span class="token punctuation">,</span>  <span class="token string">"allow_symbol_change"</span><span class="token operator">:</span> <span class="token boolean">true</span><span class="token punctuation">,</span>  <span class="token string">"container_id"</span><span class="token operator">:</span> <span class="token string">"tradingview_05834"</span><span class="token punctuation">&#125;</span>  <span class="token punctuation">)</span><span class="token punctuation">;</span>  </span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>script</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>div</span><span class="token punctuation">></span></span><span class="token comment">&lt;!-- TradingView Widget END --></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>table</span> <span class="token attr-name">class</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>ui selectable striped table<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>thead</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>tr</span><span class="token punctuation">></span></span>      <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>th</span><span class="token punctuation">></span></span>Date<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>th</span><span class="token punctuation">></span></span>      <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>th</span><span class="token punctuation">></span></span>Open<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>th</span><span class="token punctuation">></span></span>      <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>th</span><span class="token punctuation">></span></span>High<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>th</span><span class="token punctuation">></span></span>      <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>th</span><span class="token punctuation">></span></span>Low<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>th</span><span class="token punctuation">></span></span>      <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>th</span><span class="token punctuation">></span></span>Close<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>th</span><span class="token punctuation">></span></span>      <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>th</span><span class="token punctuation">></span></span>Volume<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>th</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>tr</span><span class="token punctuation">></span></span>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>thead</span><span class="token punctuation">></span></span>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>tbody</span><span class="token punctuation">></span></span>  &#123;% for bar in bars %&#125;    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>tr</span><span class="token punctuation">></span></span>      <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>td</span><span class="token punctuation">></span></span>&#123;&#123; bar.date &#125;&#125;<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>td</span><span class="token punctuation">></span></span>      <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>td</span><span class="token punctuation">></span></span>&#123;&#123; bar.open &#125;&#125;<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>td</span><span class="token punctuation">></span></span>      <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>td</span><span class="token punctuation">></span></span>&#123;&#123; bar.high &#125;&#125;<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>td</span><span class="token punctuation">></span></span>      <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>td</span><span class="token punctuation">></span></span>&#123;&#123; bar.low &#125;&#125;<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>td</span><span class="token punctuation">></span></span>      <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>td</span><span class="token punctuation">></span></span>&#123;&#123; bar.close &#125;&#125;<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>td</span><span class="token punctuation">></span></span>      <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>td</span><span class="token punctuation">></span></span>&#123;&#123; bar.volume &#125;&#125;<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>td</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>tr</span><span class="token punctuation">></span></span>    &#123;% endfor %&#125;  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>tbody</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>table</span><span class="token punctuation">></span></span>&#123;% endblock %&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><img src="/Hexo-Blog/2021/03/15/2021-03-15-visualise-crypto-with-sql-and-fastapi/tv.jpg" class=""><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>I’ve done implementing a very simple crypto web app using FastAPI, so that we can check on it efficiently and effectively. Have fun!</p>]]></content>
      
      
      <categories>
          
          <category> Data Science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> Finance </tag>
            
            <tag> SQL </tag>
            
            <tag> FastAPI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Triple Barrier Method for ML</title>
      <link href="/Hexo-Blog/2021/03/15/2021-03-15-triple-barrier-method-for-ml/"/>
      <url>/Hexo-Blog/2021/03/15/2021-03-15-triple-barrier-method-for-ml/</url>
      
        <content type="html"><![CDATA[<p>Time series prediction has been widely applied to the finance industry in applications such as stock market price and commodity price forecasting. Machine learning methods have been widely used in financial time series prediction in recent years. How to label financial time series data to determine the prediction accuracy of machine learning models and subsequently determine final investment returns is a hot topic. </p><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>A time series is a set of observations, each one being recorded at a specific time. Prediction of time series data is a relatively complex task. Since there are many factors affecting time series data, it is difficult to predict the trend of time series data accurately. Time series forecasting aims at solving various problems, specifically in the financial field.</p><h1 id="Features-and-Labels"><a href="#Features-and-Labels" class="headerlink" title="Features and Labels"></a>Features and Labels</h1><h2 id="Making-Features"><a href="#Making-Features" class="headerlink" title="Making Features"></a>Making Features</h2><p>The data for making features can be some financial reports, technical indicators, and there are many ways to make these indicators, but this will not be in the scale of this article.</p><h2 id="Making-Labels"><a href="#Making-Labels" class="headerlink" title="Making Labels"></a>Making Labels</h2><h3 id="Fixed-time-Horizon-Method"><a href="#Fixed-time-Horizon-Method" class="headerlink" title="Fixed-time Horizon Method"></a>Fixed-time Horizon Method</h3><p>If the label is too difficult to predict, the model will not be trained effectively. In the past, the most basic way to create a label was to use a <code>Fixed-time Horizon Method</code> to predict the rise and fall after w time units.</p><img src="/Hexo-Blog/2021/03/15/2021-03-15-triple-barrier-method-for-ml/fixedtimehorizon.png" class=""><p>In the above graph at <code>p(t)</code>, we want to predict whether the stock price at <code>p(t+w)</code> will be higher or lower. We can use the classification method to divide the rise and fall of the stock price into three parts, that is -1 (fall), 0 (no rise, no fall), 1 (rise), so that we can let the machine learn to predict, in the above figure, we can find that the stock price is higher than the previous one, so it is classified as 1, that is, it will rise afterwards.</p><p>However, there is a drawback to this approach, that is, when the model calls us to buy today, once we buy, we will hold for w time units, no matter the stock price rises or falls, we must continue to hold, no stop loss and stop profit, which will lead to uncontrolled risk. Of course, we can add stop-loss and stop-gain to the backtest, but that would be contrary to the original purpose of the model, as the label generated by the model is clearly holding w time units, without stop-loss and stop-gain.</p><p>To solve the above problem, Prado proposes the following new approach in his book Advances in Financial Machine Learning.</p><h3 id="Triple-Barrier-Method"><a href="#Triple-Barrier-Method" class="headerlink" title="Triple Barrier Method"></a>Triple Barrier Method</h3><img src="/Hexo-Blog/2021/03/15/2021-03-15-triple-barrier-method-for-ml/triplebarrier-1-1536x503.png" class=""><p>At first glance, it looks a bit similar to the fixed time horizon, but this method has improved the classification method a bit. In the above diagram, three different colored “bars” are used. When the price starts from p(t) and extends over time, it will definitely hit one of the three bars, and these three bars represent different meanings: </p><ul><li>1 (profit take)</li><li>0 (holding w time units)</li><li>-1 (stop loss)</li></ul><p>In this way, we can allow machine learning model to predict “stop-loss” and “profit-take”, and the trained model can match the backtest settings, increasing the predictability of machine learning model!</p><h1 id="Python-Implementation"><a href="#Python-Implementation" class="headerlink" title="Python Implementation"></a>Python Implementation</h1><p>The use of this function is to input <code>price</code>, <code>stop-loss</code>, <code>proft-take</code>, and <code>maximum holding time</code> into <code>triple_barrier()</code>, and return <code>ret</code> dataframe. The three columns in the dataframe stand for:</p><ul><li>triple_barrier_profit: future profitability until the stop loss and stop profit</li><li>triple_barrier_sell_time: the holding time</li><li>triple_barrier_signal: triggered by stop loss and stop profit</li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">triple_barrier</span><span class="token punctuation">(</span>data<span class="token punctuation">,</span> column<span class="token punctuation">,</span> ub<span class="token operator">=</span><span class="token number">1.05</span><span class="token punctuation">,</span> lb<span class="token operator">=</span><span class="token number">0.97</span><span class="token punctuation">,</span> max_period<span class="token operator">=</span><span class="token number">20</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""    Parameters    ----------    data: pd.DataFrame        A dataframe contains open, high, low, close columns.    column: str        Select the column in order to generate triple barrier signal.    ub: float, default=1.05        The upper bound represents profit-take.    lb: float, default=0.97        The lower bound represents stop-loss.    max_period: int, default=20        Time interval between current time and time of vertical barrier.    Examples    --------        >>> data = yf.download("AAPL", progress=False)        >>> ret = triple_barrier(data.Close, 1.05, 0.97, 10)    """</span>    <span class="token keyword">def</span> <span class="token function">end_price</span><span class="token punctuation">(</span>s<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> np<span class="token punctuation">.</span>append<span class="token punctuation">(</span>s<span class="token punctuation">[</span><span class="token punctuation">(</span>s <span class="token operator">/</span> s<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">></span> ub<span class="token punctuation">)</span> <span class="token operator">|</span> <span class="token punctuation">(</span>s <span class="token operator">/</span> s<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">&lt;</span> lb<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span> s<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token operator">/</span>s<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>    r <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token builtin">range</span><span class="token punctuation">(</span>max_period<span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">end_time</span><span class="token punctuation">(</span>s<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> np<span class="token punctuation">.</span>append<span class="token punctuation">(</span>r<span class="token punctuation">[</span><span class="token punctuation">(</span>s <span class="token operator">/</span> s<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">></span> ub<span class="token punctuation">)</span> <span class="token operator">|</span> <span class="token punctuation">(</span>s <span class="token operator">/</span> s<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">&lt;</span> lb<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span> max_period<span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>    price <span class="token operator">=</span> data<span class="token punctuation">[</span>column<span class="token punctuation">]</span>    p <span class="token operator">=</span> price<span class="token punctuation">.</span>rolling<span class="token punctuation">(</span>max_period<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">apply</span><span class="token punctuation">(</span>end_price<span class="token punctuation">,</span> raw<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">.</span>shift<span class="token punctuation">(</span><span class="token operator">-</span>max_period<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span>    t <span class="token operator">=</span> price<span class="token punctuation">.</span>rolling<span class="token punctuation">(</span>max_period<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">apply</span><span class="token punctuation">(</span>end_time<span class="token punctuation">,</span> raw<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">.</span>shift<span class="token punctuation">(</span><span class="token operator">-</span>max_period<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span>    t <span class="token operator">=</span> pd<span class="token punctuation">.</span>Series<span class="token punctuation">(</span><span class="token punctuation">[</span>t<span class="token punctuation">.</span>index<span class="token punctuation">[</span><span class="token builtin">int</span><span class="token punctuation">(</span>k<span class="token operator">+</span>i<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token keyword">if</span> <span class="token keyword">not</span> math<span class="token punctuation">.</span>isnan<span class="token punctuation">(</span>k<span class="token operator">+</span>i<span class="token punctuation">)</span> <span class="token keyword">else</span> np<span class="token punctuation">.</span>datetime64<span class="token punctuation">(</span><span class="token string">'NaT'</span><span class="token punctuation">)</span>                   <span class="token keyword">for</span> i<span class="token punctuation">,</span> k <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>t<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span> index<span class="token operator">=</span>t<span class="token punctuation">.</span>index<span class="token punctuation">)</span><span class="token punctuation">.</span>dropna<span class="token punctuation">(</span><span class="token punctuation">)</span>    signal <span class="token operator">=</span> pd<span class="token punctuation">.</span>Series<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> p<span class="token punctuation">.</span>index<span class="token punctuation">)</span>    signal<span class="token punctuation">.</span>loc<span class="token punctuation">[</span>p <span class="token operator">></span> ub<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">1</span>    signal<span class="token punctuation">.</span>loc<span class="token punctuation">[</span>p <span class="token operator">&lt;</span> lb<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token operator">-</span><span class="token number">1</span>    ret <span class="token operator">=</span> pd<span class="token punctuation">.</span>DataFrame<span class="token punctuation">(</span><span class="token punctuation">&#123;</span><span class="token string">'triple_barrier_profit'</span><span class="token punctuation">:</span> p<span class="token punctuation">,</span>                        <span class="token string">'triple_barrier_sell_time'</span><span class="token punctuation">:</span> t<span class="token punctuation">,</span>                        <span class="token string">'triple_barrier_signal'</span><span class="token punctuation">:</span> signal<span class="token punctuation">&#125;</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> ret<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>This labeling method seems performing super well.</p><img src="/Hexo-Blog/2021/03/15/2021-03-15-triple-barrier-method-for-ml/tbm.png" class=""><p>I also implement a backtest function to see whether Triple Barrier Method can make profit. Let’s see if the model performs well while backtesting.</p><img src="/Hexo-Blog/2021/03/15/2021-03-15-triple-barrier-method-for-ml/bt.png" class=""><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>In today’s article, we looked at implementing a basic model using the Triple Barrier Method. Next time, I’ll discuss how to train machine learning model based on this auto labeling method. Stay tuned! </p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ol><li><a href="https://www.finlab.tw/generate-labels-stop-loss-stop-profit/">https://www.finlab.tw/generate-labels-stop-loss-stop-profit/</a></li><li><a href="https://ai.plainenglish.io/start-using-better-labels-for-financial-machine-learning-6eeac691e660">https://ai.plainenglish.io/start-using-better-labels-for-financial-machine-learning-6eeac691e660</a></li><li><a href="https://github.com/boyboi86/AFML">https://github.com/boyboi86/AFML</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Finance </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> Finance </tag>
            
            <tag> ML </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Eigenvectors from Eigenvalues</title>
      <link href="/Hexo-Blog/2021/03/15/2021-03-15-eigenvectors-from-eigenvalues/"/>
      <url>/Hexo-Blog/2021/03/15/2021-03-15-eigenvectors-from-eigenvalues/</url>
      
        <content type="html"><![CDATA[<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>This article is about implementing “Eigenvectors from eigenvalues” of Terence Tao’s paper using Python and R. It’s a amazing work and mathematics contribution from Terence Tao. It is an elegant non-evident result, which makes me so excited about it!</p><h1 id="Eigenvector-eigenvalue-Identity"><a href="#Eigenvector-eigenvalue-Identity" class="headerlink" title="Eigenvector-eigenvalue Identity"></a>Eigenvector-eigenvalue Identity</h1><p>According to Eigenvector-eigenvalue Identity Theorem, We have </p><div style="display: flex;justify-content: center;">    <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -3.112ex" xmlns="http://www.w3.org/2000/svg" width="51.042ex" height="7.005ex" role="img" focusable="false" viewBox="0 -1720.9 22560.6 3096.2" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-6-TEX-N-2016" d="M133 736Q138 750 153 750Q164 750 170 739Q172 735 172 250T170 -239Q164 -250 152 -250Q144 -250 138 -244L137 -243Q133 -241 133 -179T132 250Q132 731 133 736ZM329 739Q334 750 346 750Q353 750 361 744L362 743Q366 741 366 679T367 250T367 -178T362 -243L361 -244Q355 -250 347 -250Q335 -250 329 -239Q327 -235 327 250T329 739Z"></path><path id="MJX-6-TEX-I-1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path><path id="MJX-6-TEX-I-1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path><path id="MJX-6-TEX-N-2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path><path id="MJX-6-TEX-I-1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path><path id="MJX-6-TEX-N-32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path><path id="MJX-6-TEX-LO-220F" d="M220 812Q220 813 218 819T214 829T208 840T199 853T185 866T166 878T140 887T107 893T66 896H56V950H1221V896H1211Q1080 896 1058 812V-311Q1076 -396 1211 -396H1221V-450H725V-396H735Q864 -396 888 -314Q889 -312 889 -311V896H388V292L389 -311Q405 -396 542 -396H552V-450H56V-396H66Q195 -396 219 -314Q220 -312 220 -311V812Z"></path><path id="MJX-6-TEX-I-1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path><path id="MJX-6-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-6-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-6-TEX-N-3B" d="M78 370Q78 394 95 412T138 430Q162 430 180 414T199 371Q199 346 182 328T139 310T96 327T78 370ZM78 60Q78 85 94 103T137 121Q202 121 202 8Q202 -44 183 -94T144 -169T118 -194Q115 -194 106 -186T95 -174Q94 -171 107 -155T137 -107T160 -38Q161 -32 162 -22T165 -4T165 4Q165 5 161 4T142 0Q110 0 94 18T78 60Z"></path><path id="MJX-6-TEX-N-2260" d="M166 -215T159 -215T147 -212T141 -204T139 -197Q139 -190 144 -183L306 133H70Q56 140 56 153Q56 168 72 173H327L406 327H72Q56 332 56 347Q56 360 70 367H426Q597 702 602 707Q605 716 618 716Q625 716 630 712T636 703T638 696Q638 692 471 367H707Q722 359 722 347Q722 336 708 328L451 327L371 173H708Q722 163 722 153Q722 140 707 133H351Q175 -210 170 -212Q166 -215 159 -215Z"></path><path id="MJX-6-TEX-I-1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-6-TEX-N-28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path><path id="MJX-6-TEX-I-1D706" d="M166 673Q166 685 183 694H202Q292 691 316 644Q322 629 373 486T474 207T524 67Q531 47 537 34T546 15T551 6T555 2T556 -2T550 -11H482Q457 3 450 18T399 152L354 277L340 262Q327 246 293 207T236 141Q211 112 174 69Q123 9 111 -1T83 -12Q47 -12 47 20Q47 37 61 52T199 187Q229 216 266 252T321 306L338 322Q338 323 288 462T234 612Q214 657 183 657Q166 657 166 673Z"></path><path id="MJX-6-TEX-I-1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"></path><path id="MJX-6-TEX-N-29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path><path id="MJX-6-TEX-N-2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path><path id="MJX-6-TEX-I-1D440" d="M289 629Q289 635 232 637Q208 637 201 638T194 648Q194 649 196 659Q197 662 198 666T199 671T201 676T203 679T207 681T212 683T220 683T232 684Q238 684 262 684T307 683Q386 683 398 683T414 678Q415 674 451 396L487 117L510 154Q534 190 574 254T662 394Q837 673 839 675Q840 676 842 678T846 681L852 683H948Q965 683 988 683T1017 684Q1051 684 1051 673Q1051 668 1048 656T1045 643Q1041 637 1008 637Q968 636 957 634T939 623Q936 618 867 340T797 59Q797 55 798 54T805 50T822 48T855 46H886Q892 37 892 35Q892 19 885 5Q880 0 869 0Q864 0 828 1T736 2Q675 2 644 2T609 1Q592 1 592 11Q592 13 594 25Q598 41 602 43T625 46Q652 46 685 49Q699 52 704 61Q706 65 742 207T813 490T848 631L654 322Q458 10 453 5Q451 4 449 3Q444 0 433 0Q418 0 415 7Q413 11 374 317L335 624L267 354Q200 88 200 79Q206 46 272 46H282Q288 41 289 37T286 19Q282 3 278 1Q274 0 267 0Q265 0 255 0T221 1T157 2Q127 2 95 1T58 0Q43 0 39 2T35 11Q35 13 38 25T43 40Q45 46 65 46Q135 46 154 86Q158 92 223 354T289 629Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mo"><use xlink:href="#MJX-6-TEX-N-2016"></use></g><g data-mml-node="msub" transform="translate(500, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-6-TEX-I-1D463"></use></g><g data-mml-node="TeXAtom" transform="translate(485, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-6-TEX-I-1D456"></use></g><g data-mml-node="mo" transform="translate(345, 0)"><use xlink:href="#MJX-6-TEX-N-2C"></use></g><g data-mml-node="mi" transform="translate(623, 0)"><use xlink:href="#MJX-6-TEX-I-1D457"></use></g></g></g><g data-mml-node="msup" transform="translate(1766.9, 0)"><g data-mml-node="mo"><use xlink:href="#MJX-6-TEX-N-2016"></use></g><g data-mml-node="mn" transform="translate(500, 363) scale(0.707)"><use xlink:href="#MJX-6-TEX-N-32"></use></g></g><g data-mml-node="mstyle" transform="translate(2837.1, 0)"><g data-mml-node="munderover"><g data-mml-node="mo" transform="translate(676.6, 0)"><use xlink:href="#MJX-6-TEX-LO-220F"></use></g><g data-mml-node="TeXAtom" transform="translate(0, -1123.3) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-6-TEX-I-1D458"></use></g><g data-mml-node="mo" transform="translate(521, 0)"><use xlink:href="#MJX-6-TEX-N-3D"></use></g><g data-mml-node="mn" transform="translate(1299, 0)"><use xlink:href="#MJX-6-TEX-N-31"></use></g><g data-mml-node="mo" transform="translate(1799, 0)"><use xlink:href="#MJX-6-TEX-N-3B"></use></g><g data-mml-node="mi" transform="translate(2077, 0)"><use xlink:href="#MJX-6-TEX-I-1D458"></use></g><g data-mml-node="mo" transform="translate(2598, 0)"><use xlink:href="#MJX-6-TEX-N-2260"></use></g><g data-mml-node="mi" transform="translate(3376, 0)"><use xlink:href="#MJX-6-TEX-I-1D456"></use></g></g><g data-mml-node="TeXAtom" transform="translate(1103.4, 1150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-6-TEX-I-1D45B"></use></g></g></g><g data-mml-node="mo" transform="translate(2631.1, 0)"><use xlink:href="#MJX-6-TEX-N-28"></use></g><g data-mml-node="msub" transform="translate(3020.1, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-6-TEX-I-1D706"></use></g><g data-mml-node="TeXAtom" transform="translate(583, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-6-TEX-I-1D456"></use></g></g></g><g data-mml-node="mo" transform="translate(3897.1, 0)"><use xlink:href="#MJX-6-TEX-N-28"></use></g><g data-mml-node="mi" transform="translate(4286.1, 0)"><use xlink:href="#MJX-6-TEX-I-1D434"></use></g><g data-mml-node="mo" transform="translate(5036.1, 0)"><use xlink:href="#MJX-6-TEX-N-29"></use></g><g data-mml-node="mo" transform="translate(5647.3, 0)"><use xlink:href="#MJX-6-TEX-N-2212"></use></g><g data-mml-node="msub" transform="translate(6647.5, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-6-TEX-I-1D706"></use></g><g data-mml-node="TeXAtom" transform="translate(583, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-6-TEX-I-1D458"></use></g></g></g><g data-mml-node="mo" transform="translate(7648.9, 0)"><use xlink:href="#MJX-6-TEX-N-28"></use></g><g data-mml-node="mi" transform="translate(8037.9, 0)"><use xlink:href="#MJX-6-TEX-I-1D434"></use></g><g data-mml-node="mo" transform="translate(8787.9, 0)"><use xlink:href="#MJX-6-TEX-N-29"></use></g><g data-mml-node="mo" transform="translate(9176.9, 0)"><use xlink:href="#MJX-6-TEX-N-29"></use></g><g data-mml-node="mo" transform="translate(9843.7, 0)"><use xlink:href="#MJX-6-TEX-N-3D"></use></g><g data-mml-node="munderover" transform="translate(10899.5, 0)"><g data-mml-node="mo" transform="translate(25, 0)"><use xlink:href="#MJX-6-TEX-LO-220F"></use></g><g data-mml-node="TeXAtom" transform="translate(27.9, -1107.7) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-6-TEX-I-1D458"></use></g><g data-mml-node="mo" transform="translate(521, 0)"><use xlink:href="#MJX-6-TEX-N-3D"></use></g><g data-mml-node="mn" transform="translate(1299, 0)"><use xlink:href="#MJX-6-TEX-N-31"></use></g></g><g data-mml-node="TeXAtom" transform="translate(0, 1150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-6-TEX-I-1D45B"></use></g><g data-mml-node="mo" transform="translate(600, 0)"><use xlink:href="#MJX-6-TEX-N-2212"></use></g><g data-mml-node="mn" transform="translate(1378, 0)"><use xlink:href="#MJX-6-TEX-N-31"></use></g></g></g><g data-mml-node="mo" transform="translate(12227.4, 0)"><use xlink:href="#MJX-6-TEX-N-28"></use></g><g data-mml-node="msub" transform="translate(12616.4, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-6-TEX-I-1D706"></use></g><g data-mml-node="TeXAtom" transform="translate(583, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-6-TEX-I-1D456"></use></g></g></g><g data-mml-node="mo" transform="translate(13493.4, 0)"><use xlink:href="#MJX-6-TEX-N-28"></use></g><g data-mml-node="mi" transform="translate(13882.4, 0)"><use xlink:href="#MJX-6-TEX-I-1D434"></use></g><g data-mml-node="mo" transform="translate(14632.4, 0)"><use xlink:href="#MJX-6-TEX-N-29"></use></g><g data-mml-node="mo" transform="translate(15243.6, 0)"><use xlink:href="#MJX-6-TEX-N-2212"></use></g><g data-mml-node="msub" transform="translate(16243.8, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-6-TEX-I-1D706"></use></g><g data-mml-node="TeXAtom" transform="translate(583, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-6-TEX-I-1D458"></use></g></g></g><g data-mml-node="mo" transform="translate(17245.2, 0)"><use xlink:href="#MJX-6-TEX-N-28"></use></g><g data-mml-node="msub" transform="translate(17634.2, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-6-TEX-I-1D440"></use></g><g data-mml-node="TeXAtom" transform="translate(970, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-6-TEX-I-1D457"></use></g></g></g><g data-mml-node="mo" transform="translate(18945.6, 0)"><use xlink:href="#MJX-6-TEX-N-29"></use></g><g data-mml-node="mo" transform="translate(19334.6, 0)"><use xlink:href="#MJX-6-TEX-N-29"></use></g></g></g></g></svg></mjx-container></div><h2 id="Python-Implementation"><a href="#Python-Implementation" class="headerlink" title="Python Implementation"></a>Python Implementation</h2><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">eigenvectors_from_eigenvalues</span><span class="token punctuation">(</span>A<span class="token punctuation">,</span> eig_val_A<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""    Implementation of Eigenvector-eigenvalue Identity Theorem    Reference from https://dearxxj.github.io/post/7/    Parameters:        A: (n, n) Hermitian matrix (array-like)        eig_val_A: (n, ) vector (float ndarray)    Return:         eig_vec_A: Eigenvectors of matrix A    """</span>    n <span class="token operator">=</span> A<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>    <span class="token comment"># Produce eig_val_A by scipy.linalg.eigh() function</span>    <span class="token keyword">if</span> eig_val_A <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>        eig_val_A<span class="token punctuation">,</span> _ <span class="token operator">=</span> eigh<span class="token punctuation">(</span>A<span class="token punctuation">)</span>    eig_vec_A <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">(</span>n<span class="token punctuation">,</span> n<span class="token punctuation">)</span><span class="token punctuation">)</span>    start <span class="token operator">=</span> time<span class="token punctuation">.</span>time<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">for</span> k <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>n<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment"># Remove the k-th row</span>        M <span class="token operator">=</span> np<span class="token punctuation">.</span>delete<span class="token punctuation">(</span>A<span class="token punctuation">,</span> k<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>        <span class="token comment"># Remove the k-th column</span>        M <span class="token operator">=</span> np<span class="token punctuation">.</span>delete<span class="token punctuation">(</span>M<span class="token punctuation">,</span> k<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>        <span class="token comment"># Produce eig_val_M by scipy.linalg.eigh() function</span>        eig_val_M<span class="token punctuation">,</span> _ <span class="token operator">=</span> eigh<span class="token punctuation">(</span>M<span class="token punctuation">)</span>        nominator <span class="token operator">=</span> <span class="token punctuation">[</span>np<span class="token punctuation">.</span>prod<span class="token punctuation">(</span>eig_val_A<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">-</span> eig_val_M<span class="token punctuation">)</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>n<span class="token punctuation">)</span><span class="token punctuation">]</span>        denominator <span class="token operator">=</span> <span class="token punctuation">[</span>np<span class="token punctuation">.</span>prod<span class="token punctuation">(</span>np<span class="token punctuation">.</span>delete<span class="token punctuation">(</span>eig_val_A<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">-</span> eig_val_A<span class="token punctuation">,</span> i<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>n<span class="token punctuation">)</span><span class="token punctuation">]</span>        eig_vec_A<span class="token punctuation">[</span>k<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>nominator<span class="token punctuation">)</span> <span class="token operator">/</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>denominator<span class="token punctuation">)</span>    elapse_time <span class="token operator">=</span> time<span class="token punctuation">.</span>time<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">-</span> start    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"It takes &#123;:.8f&#125;s to compute eigenvectors using Eigenvector-eigenvalue Identity."</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>elapse_time<span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> eig_vec_A<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>Test <code>eigenvectors_from_eigenvalues()</code> on matrix A.</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">A <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>eig_vec_A <span class="token operator">=</span> eigenvectors_from_eigenvalues<span class="token punctuation">(</span>A<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>eig_vec_A<span class="token punctuation">)</span>start <span class="token operator">=</span> time<span class="token punctuation">.</span>time<span class="token punctuation">(</span><span class="token punctuation">)</span>eig_val_A<span class="token punctuation">,</span> eig_vec_A <span class="token operator">=</span> eigh<span class="token punctuation">(</span>A<span class="token punctuation">)</span><span class="token punctuation">;</span> eig_vec_A<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"\nIt takes &#123;:.8f&#125;s to compute eigenvectors using scipy.linalg.eigh() function."</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>time<span class="token punctuation">.</span>time<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">-</span>start<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>eig_vec_A<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><pre class="line-numbers language-console" data-language="console"><code class="language-console">It takes 0.00070190s to compute eigenvectors using Eigenvector-eigenvalue Identity.[[0.66666667 0.33333333 0.        ] [0.16666667 0.33333333 0.5       ] [0.16666667 0.33333333 0.5       ]]It takes 0.00016832s to compute eigenvectors using scipy.linalg.eigh() function.[[ 0.81649658 -0.57735027  0.        ] [-0.40824829 -0.57735027  0.70710678] [ 0.40824829  0.57735027  0.70710678]]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="R-Implementation"><a href="#R-Implementation" class="headerlink" title="R Implementation"></a>R Implementation</h2><pre class="line-numbers language-R" data-language="R"><code class="language-R">A &#x3D; matrix(c(1, 1, -1, 1, 3, 1, -1, 1, 3), 3, 3)vec_from_val &#x3D; function(A)&#123;  # A should be Hermitian matrix  n &#x3D; sqrt(length(A))  Aeig &#x3D; eigen(A)$values  # V is eigenvecters matrix  V &#x3D; matrix(ncol&#x3D;n, nrow&#x3D;n)    for (i in 1:n)&#123;    AM &#x3D; Aeig[-i]    for (j in 1:n)&#123;      # Minors matrix B      B &#x3D; A[-j, ]      B &#x3D; B[, -j]      Beig &#x3D; eigen(B)$values      down &#x3D; 1; up &#x3D; 1      # n_0 is the dimension of B      n_0 &#x3D; n - 1            for (k in 1:n_0)&#123;        down &#x3D; down * (Aeig[i] - AM[k])        up &#x3D; up * (Aeig[i] - Beig[k])      &#125;            V[i, j] &#x3D; up &#x2F; down    &#125;  &#125;    return(t(V))&#125;vec_from_val(A)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><ol><li>The eigenvector-eigenvalue identity only yields information about the magnitude of the components of a given eigenvector, but does not directly reveal the phase of these components. Otherwise, the eigenvector-eigenvalue identity may be more computationally feasible only if one has an application that requires only the component magnitudes.</li><li>It would be a computationally intensive task in general to compute all n-1 eigenvalues of each of the n minors matrices.</li><li>An additional method would then be needed to calculate the signs of these components of eigenvectors.</li><li>It has not been seen that the eigenvector-eigenvalue identity has better speed at computing eigenvectors compared to <code>scipy.linalg.eigh()</code> function.</li></ol><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ol><li>[<a href="https://arxiv.org/pdf/1908.03795.pdf">link</a>] Terence Tao, Eigenvectors from eigenvalues: a survey of a basic identity in linear algebra.</li><li>[<a href="https://www.ias.ac.in/article/fulltext/jcsc/101/06/0499-0517">link</a>] Asok K. Mukherjee and Kali Kinkar Datta. Two new graph-theoretical methods for generation of eigenvectors of chemical graphs</li><li>[<a href="https://arxiv.org/pdf/1907.02534.pdf">link</a>] Peter B Denton, Stephen J Parke, and Xining Zhang. Eigenvalues: the Rosetta Stone for Neutrino Oscillations in Matter</li><li><a href="https://math.stackexchange.com/questions/3436475/how-to-get-eigenvectors-from-eigenvalues">https://math.stackexchange.com/questions/3436475/how-to-get-eigenvectors-from-eigenvalues</a></li><li><a href="https://terrytao.wordpress.com/2019/08/13/eigenvectors-from-eigenvalues/">https://terrytao.wordpress.com/2019/08/13/eigenvectors-from-eigenvalues/</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Math Programming </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> R </tag>
            
            <tag> Linear Algebra </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Principal Component Analysis Derivation</title>
      <link href="/Hexo-Blog/2021/03/13/2021-03-13-principal-component-analysis-derivation/"/>
      <url>/Hexo-Blog/2021/03/13/2021-03-13-principal-component-analysis-derivation/</url>
      
        <content type="html"><![CDATA[<p>Principal Component Analysis (PCA) is an important technique to understand in the fields of statistics and data science. It is a process of computing the principal components and utilising then to perform a change of basis on the data. For the purpose of visualisation, it is very hard to visulaise and understand the data in high dimensions, this is where PCA comes to the rescue.</p><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Principal Component Analysis or (PCA) is a widely used technique for dimensionality reduction of the large data set. It needs the knowledge of some linear algebra, such as vector projection, eigenvalues and eigenvectors, Lagrange multipliers, derivatives of a matrix, and covariance matrix.</p><h2 id="Derivation"><a href="#Derivation" class="headerlink" title="Derivation"></a>Derivation</h2><p>Let’s go ahead and get into it. Let’s say we have N different vectors <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.339ex" xmlns="http://www.w3.org/2000/svg" width="2.207ex" height="1.339ex" role="img" focusable="false" viewBox="0 -442 975.6 592" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-5-TEX-I-1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path><path id="MJX-5-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-5-TEX-I-1D465"></use></g><g data-mml-node="TeXAtom" transform="translate(572, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><use xlink:href="#MJX-5-TEX-N-31"></use></g></g></g></g></g></svg></mjx-container> to <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.339ex" xmlns="http://www.w3.org/2000/svg" width="2.828ex" height="1.339ex" role="img" focusable="false" viewBox="0 -442 1249.9 592" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-5-TEX-I-1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path><path id="MJX-5-TEX-I-1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-5-TEX-I-1D465"></use></g><g data-mml-node="TeXAtom" transform="translate(572, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-5-TEX-I-1D441"></use></g></g></g></g></g></svg></mjx-container> with dimension of D. Our goal of course in PCA is dimensionality reduction. So we want to map the space which has dimensionality D onto a space which has dimensionality M, where M has to be less than D bu all means. That’s the point of dimensionality reduction.</p><p>It looks like a scary and daunting task, let’s take a whole step back and look at a simpler picture. Our objective is that we want to maximise the variance of the projections onto some dimensional space. In other words, we have this D dimensional space that contains all this information and all this data, we want to reduce its dimensionality, but we want to do it in a clever way, we want to do it onto a space so that we preserve as much of the information (original variation) while reducing the dimensions.</p><h3 id="Projection"><a href="#Projection" class="headerlink" title="Projection"></a>Projection</h3><p>The projection of <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex" xmlns="http://www.w3.org/2000/svg" width="1.959ex" height="1.357ex" role="img" focusable="false" viewBox="0 -442 866 599.8" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-5-TEX-I-1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path><path id="MJX-5-TEX-I-1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-5-TEX-I-1D465"></use></g><g data-mml-node="TeXAtom" transform="translate(572, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-5-TEX-I-1D456"></use></g></g></g></g></g></svg></mjx-container> onto a potential <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="1.294ex" height="1.025ex" role="img" focusable="false" viewBox="0 -442 572 453" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-5-TEX-I-1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-5-TEX-I-1D462"></use></g></g></g></svg></mjx-container> vector can be written as</p><div style="display: flex;justify-content: center;">    <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -1.238ex" xmlns="http://www.w3.org/2000/svg" width="15.659ex" height="3.606ex" role="img" focusable="false" viewBox="0 -1046.7 6921.1 1593.9" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-5-TEX-I-1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"></path><path id="MJX-5-TEX-I-1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-5-TEX-I-1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path><path id="MJX-5-TEX-I-1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path><path id="MJX-5-TEX-I-1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-5-TEX-I-1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path><path id="MJX-5-TEX-I-1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path><path id="MJX-5-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-5-TEX-I-1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path><path id="MJX-5-TEX-N-2016" d="M133 736Q138 750 153 750Q164 750 170 739Q172 735 172 250T170 -239Q164 -250 152 -250Q144 -250 138 -244L137 -243Q133 -241 133 -179T132 250Q132 731 133 736ZM329 739Q334 750 346 750Q353 750 361 744L362 743Q366 741 366 679T367 250T367 -178T362 -243L361 -244Q355 -250 347 -250Q335 -250 329 -239Q327 -235 327 250T329 739Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-5-TEX-I-1D45D"></use></g><g data-mml-node="mi" transform="translate(503, 0)"><use xlink:href="#MJX-5-TEX-I-1D45F"></use></g><g data-mml-node="mi" transform="translate(954, 0)"><use xlink:href="#MJX-5-TEX-I-1D45C"></use></g><g data-mml-node="msub" transform="translate(1439, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-5-TEX-I-1D457"></use></g><g data-mml-node="TeXAtom" transform="translate(412, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-5-TEX-I-1D462"></use></g></g></g><g data-mml-node="msub" transform="translate(2305.5, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-5-TEX-I-1D465"></use></g><g data-mml-node="mi" transform="translate(572, -150) scale(0.707)"><use xlink:href="#MJX-5-TEX-I-1D456"></use></g></g><g data-mml-node="mo" transform="translate(3449.2, 0)"><use xlink:href="#MJX-5-TEX-N-3D"></use></g><g data-mml-node="mfrac" transform="translate(4505, 0)"><g data-mml-node="mrow" transform="translate(220, 451.6) scale(0.707)"><g data-mml-node="msup"><g data-mml-node="mi"><use xlink:href="#MJX-5-TEX-I-1D462"></use></g><g data-mml-node="mi" transform="translate(572, 363) scale(0.707)"><use xlink:href="#MJX-5-TEX-I-1D447"></use></g></g><g data-mml-node="msub" transform="translate(1119.8, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-5-TEX-I-1D465"></use></g><g data-mml-node="mi" transform="translate(572, -150) scale(0.707)"><use xlink:href="#MJX-5-TEX-I-1D456"></use></g></g></g><g data-mml-node="mrow" transform="translate(366.3, -370.3) scale(0.707)"><g data-mml-node="mo"><use xlink:href="#MJX-5-TEX-N-2016"></use></g><g data-mml-node="mi" transform="translate(500, 0)"><use xlink:href="#MJX-5-TEX-I-1D462"></use></g><g data-mml-node="mo" transform="translate(1072, 0)"><use xlink:href="#MJX-5-TEX-N-2016"></use></g></g><rect width="1604.1" height="60" x="120" y="220"></rect></g><g data-mml-node="mi" transform="translate(6349.1, 0)"><use xlink:href="#MJX-5-TEX-I-1D462"></use></g></g></g></svg></mjx-container></div><p>where u is a unit vector, so its length is 1, and we can get </p><div style="display: flex;justify-content: center;">    <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.462ex" xmlns="http://www.w3.org/2000/svg" width="15.979ex" height="2.366ex" role="img" focusable="false" viewBox="0 -841.7 7062.7 1045.7" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-5-TEX-I-1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"></path><path id="MJX-5-TEX-I-1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-5-TEX-I-1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path><path id="MJX-5-TEX-I-1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path><path id="MJX-5-TEX-I-1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-5-TEX-I-1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path><path id="MJX-5-TEX-I-1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path><path id="MJX-5-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-5-TEX-I-1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-5-TEX-I-1D45D"></use></g><g data-mml-node="mi" transform="translate(503, 0)"><use xlink:href="#MJX-5-TEX-I-1D45F"></use></g><g data-mml-node="mi" transform="translate(954, 0)"><use xlink:href="#MJX-5-TEX-I-1D45C"></use></g><g data-mml-node="msub" transform="translate(1439, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-5-TEX-I-1D457"></use></g><g data-mml-node="TeXAtom" transform="translate(412, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-5-TEX-I-1D462"></use></g></g></g><g data-mml-node="msub" transform="translate(2305.5, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-5-TEX-I-1D465"></use></g><g data-mml-node="mi" transform="translate(572, -150) scale(0.707)"><use xlink:href="#MJX-5-TEX-I-1D456"></use></g></g><g data-mml-node="mo" transform="translate(3449.2, 0)"><use xlink:href="#MJX-5-TEX-N-3D"></use></g><g data-mml-node="msup" transform="translate(4505, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-5-TEX-I-1D462"></use></g><g data-mml-node="mi" transform="translate(572, 363) scale(0.707)"><use xlink:href="#MJX-5-TEX-I-1D447"></use></g></g><g data-mml-node="msub" transform="translate(5624.8, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-5-TEX-I-1D465"></use></g><g data-mml-node="mi" transform="translate(572, -150) scale(0.707)"><use xlink:href="#MJX-5-TEX-I-1D456"></use></g></g><g data-mml-node="mi" transform="translate(6490.7, 0)"><use xlink:href="#MJX-5-TEX-I-1D462"></use></g></g></g></svg></mjx-container></div><p>Finally, we know that our mean of projections among all the data is <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="5.122ex" height="1.929ex" role="img" focusable="false" viewBox="0 -841.7 2263.8 852.7" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-5-TEX-I-1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-5-TEX-I-1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path><path id="MJX-5-TEX-I-1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path><path id="MJX-5-TEX-S4-AF" d="M69 544V590H430V544H69Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"><use xlink:href="#MJX-5-TEX-I-1D462"></use></g><g data-mml-node="mi" transform="translate(572, 363) scale(0.707)"><use xlink:href="#MJX-5-TEX-I-1D447"></use></g></g><g data-mml-node="mover" transform="translate(1119.8, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-5-TEX-I-1D465"></use></g><g data-mml-node="mo" transform="translate(0, 257.3) scale(0.707)"><svg width="808.9" height="246" x="0" y="444" viewBox="202.2 444 808.9 246"><use xlink:href="#MJX-5-TEX-S4-AF" transform="scale(2.427, 1)"></use></svg></g></g><g data-mml-node="mi" transform="translate(1691.8, 0)"><use xlink:href="#MJX-5-TEX-I-1D462"></use></g></g></g></svg></mjx-container>, since the mean is a linear operation, it behaves in the exact same way.</p><h3 id="Variance"><a href="#Variance" class="headerlink" title="Variance"></a>Variance</h3><p>Going back to our objective, our goal is to maximise the variance of the projected data. By the definition of the variance, </p><div style="display: flex;justify-content: center;">    <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.781ex" xmlns="http://www.w3.org/2000/svg" width="33.129ex" height="2.953ex" role="img" focusable="false" viewBox="0 -960 14642.9 1305" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-5-TEX-I-1D449" d="M52 648Q52 670 65 683H76Q118 680 181 680Q299 680 320 683H330Q336 677 336 674T334 656Q329 641 325 637H304Q282 635 274 635Q245 630 242 620Q242 618 271 369T301 118L374 235Q447 352 520 471T595 594Q599 601 599 609Q599 633 555 637Q537 637 537 648Q537 649 539 661Q542 675 545 679T558 683Q560 683 570 683T604 682T668 681Q737 681 755 683H762Q769 676 769 672Q769 655 760 640Q757 637 743 637Q730 636 719 635T698 630T682 623T670 615T660 608T652 599T645 592L452 282Q272 -9 266 -16Q263 -18 259 -21L241 -22H234Q216 -22 216 -15Q213 -9 177 305Q139 623 138 626Q133 637 76 637H59Q52 642 52 648Z"></path><path id="MJX-5-TEX-I-1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path><path id="MJX-5-TEX-I-1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-5-TEX-N-28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path><path id="MJX-5-TEX-I-1D44B" d="M42 0H40Q26 0 26 11Q26 15 29 27Q33 41 36 43T55 46Q141 49 190 98Q200 108 306 224T411 342Q302 620 297 625Q288 636 234 637H206Q200 643 200 645T202 664Q206 677 212 683H226Q260 681 347 681Q380 681 408 681T453 682T473 682Q490 682 490 671Q490 670 488 658Q484 643 481 640T465 637Q434 634 411 620L488 426L541 485Q646 598 646 610Q646 628 622 635Q617 635 609 637Q594 637 594 648Q594 650 596 664Q600 677 606 683H618Q619 683 643 683T697 681T738 680Q828 680 837 683H845Q852 676 852 672Q850 647 840 637H824Q790 636 763 628T722 611T698 593L687 584Q687 585 592 480L505 384Q505 383 536 304T601 142T638 56Q648 47 699 46Q734 46 734 37Q734 35 732 23Q728 7 725 4T711 1Q708 1 678 1T589 2Q528 2 496 2T461 1Q444 1 444 10Q444 11 446 25Q448 35 450 39T455 44T464 46T480 47T506 54Q523 62 523 64Q522 64 476 181L429 299Q241 95 236 84Q232 76 232 72Q232 53 261 47Q262 47 267 47T273 46Q276 46 277 46T280 45T283 42T284 35Q284 26 282 19Q279 6 276 4T261 1Q258 1 243 1T201 2T142 2Q64 2 42 0Z"></path><path id="MJX-5-TEX-N-29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path><path id="MJX-5-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-5-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-5-TEX-I-1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"></path><path id="MJX-5-TEX-SO-2211" d="M61 748Q64 750 489 750H913L954 640Q965 609 976 579T993 533T999 516H979L959 517Q936 579 886 621T777 682Q724 700 655 705T436 710H319Q183 710 183 709Q186 706 348 484T511 259Q517 250 513 244L490 216Q466 188 420 134T330 27L149 -187Q149 -188 362 -188Q388 -188 436 -188T506 -189Q679 -189 778 -162T936 -43Q946 -27 959 6H999L913 -249L489 -250Q65 -250 62 -248Q56 -246 56 -239Q56 -234 118 -161Q186 -81 245 -11L428 206Q428 207 242 462L57 717L56 728Q56 744 61 748Z"></path><path id="MJX-5-TEX-I-1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-5-TEX-I-1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-5-TEX-I-1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path><path id="MJX-5-TEX-I-1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path><path id="MJX-5-TEX-N-2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path><path id="MJX-5-TEX-S4-AF" d="M69 544V590H430V544H69Z"></path><path id="MJX-5-TEX-N-32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-5-TEX-I-1D449"></use></g><g data-mml-node="mi" transform="translate(769, 0)"><use xlink:href="#MJX-5-TEX-I-1D44E"></use></g><g data-mml-node="mi" transform="translate(1298, 0)"><use xlink:href="#MJX-5-TEX-I-1D45F"></use></g><g data-mml-node="mo" transform="translate(1749, 0)"><use xlink:href="#MJX-5-TEX-N-28"></use></g><g data-mml-node="mi" transform="translate(2138, 0)"><use xlink:href="#MJX-5-TEX-I-1D44B"></use></g><g data-mml-node="mo" transform="translate(2990, 0)"><use xlink:href="#MJX-5-TEX-N-29"></use></g><g data-mml-node="mo" transform="translate(3656.8, 0)"><use xlink:href="#MJX-5-TEX-N-3D"></use></g><g data-mml-node="mfrac" transform="translate(4712.6, 0)"><g data-mml-node="mn" transform="translate(357.2, 394) scale(0.707)"><use xlink:href="#MJX-5-TEX-N-31"></use></g><g data-mml-node="mi" transform="translate(220, -345) scale(0.707)"><use xlink:href="#MJX-5-TEX-I-1D441"></use></g><rect width="827.9" height="60" x="120" y="220"></rect></g><g data-mml-node="munderover" transform="translate(5947.1, 0)"><g data-mml-node="mo"><use xlink:href="#MJX-5-TEX-SO-2211"></use></g><g data-mml-node="TeXAtom" transform="translate(1056, 477.1) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-5-TEX-I-1D441"></use></g></g><g data-mml-node="TeXAtom" transform="translate(1056, -285.4) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-5-TEX-I-1D45B"></use></g><g data-mml-node="mo" transform="translate(600, 0)"><use xlink:href="#MJX-5-TEX-N-3D"></use></g><g data-mml-node="mn" transform="translate(1378, 0)"><use xlink:href="#MJX-5-TEX-N-31"></use></g></g></g><g data-mml-node="mo" transform="translate(8381.1, 0)"><use xlink:href="#MJX-5-TEX-N-28"></use></g><g data-mml-node="msup" transform="translate(8770.1, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-5-TEX-I-1D462"></use></g><g data-mml-node="mi" transform="translate(572, 363) scale(0.707)"><use xlink:href="#MJX-5-TEX-I-1D447"></use></g></g><g data-mml-node="msub" transform="translate(9889.9, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-5-TEX-I-1D465"></use></g><g data-mml-node="mi" transform="translate(572, -150) scale(0.707)"><use xlink:href="#MJX-5-TEX-I-1D45B"></use></g></g><g data-mml-node="mo" transform="translate(11158.4, 0)"><use xlink:href="#MJX-5-TEX-N-2212"></use></g><g data-mml-node="msup" transform="translate(12158.6, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-5-TEX-I-1D462"></use></g><g data-mml-node="mi" transform="translate(572, 363) scale(0.707)"><use xlink:href="#MJX-5-TEX-I-1D447"></use></g></g><g data-mml-node="mover" transform="translate(13278.4, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-5-TEX-I-1D465"></use></g><g data-mml-node="mo" transform="translate(0, 257.3) scale(0.707)"><svg width="808.9" height="246" x="0" y="444" viewBox="202.2 444 808.9 246"><use xlink:href="#MJX-5-TEX-S4-AF" transform="scale(2.427, 1)"></use></svg></g></g><g data-mml-node="msup" transform="translate(13850.4, 0)"><g data-mml-node="mo"><use xlink:href="#MJX-5-TEX-N-29"></use></g><g data-mml-node="mn" transform="translate(389, 363) scale(0.707)"><use xlink:href="#MJX-5-TEX-N-32"></use></g></g></g></g></svg></mjx-container></div><p>Next, </p><div style="display: flex;justify-content: center;">    <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.781ex" xmlns="http://www.w3.org/2000/svg" width="32.356ex" height="2.953ex" role="img" focusable="false" viewBox="0 -960 14301.1 1305" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-5-TEX-I-1D449" d="M52 648Q52 670 65 683H76Q118 680 181 680Q299 680 320 683H330Q336 677 336 674T334 656Q329 641 325 637H304Q282 635 274 635Q245 630 242 620Q242 618 271 369T301 118L374 235Q447 352 520 471T595 594Q599 601 599 609Q599 633 555 637Q537 637 537 648Q537 649 539 661Q542 675 545 679T558 683Q560 683 570 683T604 682T668 681Q737 681 755 683H762Q769 676 769 672Q769 655 760 640Q757 637 743 637Q730 636 719 635T698 630T682 623T670 615T660 608T652 599T645 592L452 282Q272 -9 266 -16Q263 -18 259 -21L241 -22H234Q216 -22 216 -15Q213 -9 177 305Q139 623 138 626Q133 637 76 637H59Q52 642 52 648Z"></path><path id="MJX-5-TEX-I-1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path><path id="MJX-5-TEX-I-1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-5-TEX-N-28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path><path id="MJX-5-TEX-I-1D44B" d="M42 0H40Q26 0 26 11Q26 15 29 27Q33 41 36 43T55 46Q141 49 190 98Q200 108 306 224T411 342Q302 620 297 625Q288 636 234 637H206Q200 643 200 645T202 664Q206 677 212 683H226Q260 681 347 681Q380 681 408 681T453 682T473 682Q490 682 490 671Q490 670 488 658Q484 643 481 640T465 637Q434 634 411 620L488 426L541 485Q646 598 646 610Q646 628 622 635Q617 635 609 637Q594 637 594 648Q594 650 596 664Q600 677 606 683H618Q619 683 643 683T697 681T738 680Q828 680 837 683H845Q852 676 852 672Q850 647 840 637H824Q790 636 763 628T722 611T698 593L687 584Q687 585 592 480L505 384Q505 383 536 304T601 142T638 56Q648 47 699 46Q734 46 734 37Q734 35 732 23Q728 7 725 4T711 1Q708 1 678 1T589 2Q528 2 496 2T461 1Q444 1 444 10Q444 11 446 25Q448 35 450 39T455 44T464 46T480 47T506 54Q523 62 523 64Q522 64 476 181L429 299Q241 95 236 84Q232 76 232 72Q232 53 261 47Q262 47 267 47T273 46Q276 46 277 46T280 45T283 42T284 35Q284 26 282 19Q279 6 276 4T261 1Q258 1 243 1T201 2T142 2Q64 2 42 0Z"></path><path id="MJX-5-TEX-N-29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path><path id="MJX-5-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-5-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-5-TEX-I-1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"></path><path id="MJX-5-TEX-SO-2211" d="M61 748Q64 750 489 750H913L954 640Q965 609 976 579T993 533T999 516H979L959 517Q936 579 886 621T777 682Q724 700 655 705T436 710H319Q183 710 183 709Q186 706 348 484T511 259Q517 250 513 244L490 216Q466 188 420 134T330 27L149 -187Q149 -188 362 -188Q388 -188 436 -188T506 -189Q679 -189 778 -162T936 -43Q946 -27 959 6H999L913 -249L489 -250Q65 -250 62 -248Q56 -246 56 -239Q56 -234 118 -161Q186 -81 245 -11L428 206Q428 207 242 462L57 717L56 728Q56 744 61 748Z"></path><path id="MJX-5-TEX-I-1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-5-TEX-I-1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-5-TEX-I-1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path><path id="MJX-5-TEX-I-1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path><path id="MJX-5-TEX-N-2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path><path id="MJX-5-TEX-S4-AF" d="M69 544V590H430V544H69Z"></path><path id="MJX-5-TEX-N-32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-5-TEX-I-1D449"></use></g><g data-mml-node="mi" transform="translate(769, 0)"><use xlink:href="#MJX-5-TEX-I-1D44E"></use></g><g data-mml-node="mi" transform="translate(1298, 0)"><use xlink:href="#MJX-5-TEX-I-1D45F"></use></g><g data-mml-node="mo" transform="translate(1749, 0)"><use xlink:href="#MJX-5-TEX-N-28"></use></g><g data-mml-node="mi" transform="translate(2138, 0)"><use xlink:href="#MJX-5-TEX-I-1D44B"></use></g><g data-mml-node="mo" transform="translate(2990, 0)"><use xlink:href="#MJX-5-TEX-N-29"></use></g><g data-mml-node="mo" transform="translate(3656.8, 0)"><use xlink:href="#MJX-5-TEX-N-3D"></use></g><g data-mml-node="mfrac" transform="translate(4712.6, 0)"><g data-mml-node="mn" transform="translate(357.2, 394) scale(0.707)"><use xlink:href="#MJX-5-TEX-N-31"></use></g><g data-mml-node="mi" transform="translate(220, -345) scale(0.707)"><use xlink:href="#MJX-5-TEX-I-1D441"></use></g><rect width="827.9" height="60" x="120" y="220"></rect></g><g data-mml-node="munderover" transform="translate(5947.1, 0)"><g data-mml-node="mo"><use xlink:href="#MJX-5-TEX-SO-2211"></use></g><g data-mml-node="TeXAtom" transform="translate(1056, 477.1) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-5-TEX-I-1D441"></use></g></g><g data-mml-node="TeXAtom" transform="translate(1056, -285.4) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-5-TEX-I-1D45B"></use></g><g data-mml-node="mo" transform="translate(600, 0)"><use xlink:href="#MJX-5-TEX-N-3D"></use></g><g data-mml-node="mn" transform="translate(1378, 0)"><use xlink:href="#MJX-5-TEX-N-31"></use></g></g></g><g data-mml-node="mo" transform="translate(8381.1, 0)"><use xlink:href="#MJX-5-TEX-N-28"></use></g><g data-mml-node="msup" transform="translate(8770.1, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-5-TEX-I-1D462"></use></g><g data-mml-node="mi" transform="translate(572, 363) scale(0.707)"><use xlink:href="#MJX-5-TEX-I-1D447"></use></g></g><g data-mml-node="mo" transform="translate(9889.9, 0)"><use xlink:href="#MJX-5-TEX-N-28"></use></g><g data-mml-node="msub" transform="translate(10278.9, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-5-TEX-I-1D465"></use></g><g data-mml-node="mi" transform="translate(572, -150) scale(0.707)"><use xlink:href="#MJX-5-TEX-I-1D45B"></use></g></g><g data-mml-node="mo" transform="translate(11547.4, 0)"><use xlink:href="#MJX-5-TEX-N-2212"></use></g><g data-mml-node="mover" transform="translate(12547.6, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-5-TEX-I-1D465"></use></g><g data-mml-node="mo" transform="translate(0, 257.3) scale(0.707)"><svg width="808.9" height="246" x="0" y="444" viewBox="202.2 444 808.9 246"><use xlink:href="#MJX-5-TEX-S4-AF" transform="scale(2.427, 1)"></use></svg></g></g><g data-mml-node="mo" transform="translate(13119.6, 0)"><use xlink:href="#MJX-5-TEX-N-29"></use></g><g data-mml-node="msup" transform="translate(13508.6, 0)"><g data-mml-node="mo"><use xlink:href="#MJX-5-TEX-N-29"></use></g><g data-mml-node="mn" transform="translate(389, 363) scale(0.707)"><use xlink:href="#MJX-5-TEX-N-32"></use></g></g></g></g></svg></mjx-container></div><p>And we expand it, </p><div style="display: flex;justify-content: center;">    <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.781ex" xmlns="http://www.w3.org/2000/svg" width="40.78ex" height="2.953ex" role="img" focusable="false" viewBox="0 -960 18024.8 1305" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-5-TEX-I-1D449" d="M52 648Q52 670 65 683H76Q118 680 181 680Q299 680 320 683H330Q336 677 336 674T334 656Q329 641 325 637H304Q282 635 274 635Q245 630 242 620Q242 618 271 369T301 118L374 235Q447 352 520 471T595 594Q599 601 599 609Q599 633 555 637Q537 637 537 648Q537 649 539 661Q542 675 545 679T558 683Q560 683 570 683T604 682T668 681Q737 681 755 683H762Q769 676 769 672Q769 655 760 640Q757 637 743 637Q730 636 719 635T698 630T682 623T670 615T660 608T652 599T645 592L452 282Q272 -9 266 -16Q263 -18 259 -21L241 -22H234Q216 -22 216 -15Q213 -9 177 305Q139 623 138 626Q133 637 76 637H59Q52 642 52 648Z"></path><path id="MJX-5-TEX-I-1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path><path id="MJX-5-TEX-I-1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-5-TEX-N-28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path><path id="MJX-5-TEX-I-1D44B" d="M42 0H40Q26 0 26 11Q26 15 29 27Q33 41 36 43T55 46Q141 49 190 98Q200 108 306 224T411 342Q302 620 297 625Q288 636 234 637H206Q200 643 200 645T202 664Q206 677 212 683H226Q260 681 347 681Q380 681 408 681T453 682T473 682Q490 682 490 671Q490 670 488 658Q484 643 481 640T465 637Q434 634 411 620L488 426L541 485Q646 598 646 610Q646 628 622 635Q617 635 609 637Q594 637 594 648Q594 650 596 664Q600 677 606 683H618Q619 683 643 683T697 681T738 680Q828 680 837 683H845Q852 676 852 672Q850 647 840 637H824Q790 636 763 628T722 611T698 593L687 584Q687 585 592 480L505 384Q505 383 536 304T601 142T638 56Q648 47 699 46Q734 46 734 37Q734 35 732 23Q728 7 725 4T711 1Q708 1 678 1T589 2Q528 2 496 2T461 1Q444 1 444 10Q444 11 446 25Q448 35 450 39T455 44T464 46T480 47T506 54Q523 62 523 64Q522 64 476 181L429 299Q241 95 236 84Q232 76 232 72Q232 53 261 47Q262 47 267 47T273 46Q276 46 277 46T280 45T283 42T284 35Q284 26 282 19Q279 6 276 4T261 1Q258 1 243 1T201 2T142 2Q64 2 42 0Z"></path><path id="MJX-5-TEX-N-29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path><path id="MJX-5-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-5-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-5-TEX-I-1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"></path><path id="MJX-5-TEX-SO-2211" d="M61 748Q64 750 489 750H913L954 640Q965 609 976 579T993 533T999 516H979L959 517Q936 579 886 621T777 682Q724 700 655 705T436 710H319Q183 710 183 709Q186 706 348 484T511 259Q517 250 513 244L490 216Q466 188 420 134T330 27L149 -187Q149 -188 362 -188Q388 -188 436 -188T506 -189Q679 -189 778 -162T936 -43Q946 -27 959 6H999L913 -249L489 -250Q65 -250 62 -248Q56 -246 56 -239Q56 -234 118 -161Q186 -81 245 -11L428 206Q428 207 242 462L57 717L56 728Q56 744 61 748Z"></path><path id="MJX-5-TEX-I-1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-5-TEX-I-1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-5-TEX-I-1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path><path id="MJX-5-TEX-I-1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path><path id="MJX-5-TEX-N-2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path><path id="MJX-5-TEX-S4-AF" d="M69 544V590H430V544H69Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-5-TEX-I-1D449"></use></g><g data-mml-node="mi" transform="translate(769, 0)"><use xlink:href="#MJX-5-TEX-I-1D44E"></use></g><g data-mml-node="mi" transform="translate(1298, 0)"><use xlink:href="#MJX-5-TEX-I-1D45F"></use></g><g data-mml-node="mo" transform="translate(1749, 0)"><use xlink:href="#MJX-5-TEX-N-28"></use></g><g data-mml-node="mi" transform="translate(2138, 0)"><use xlink:href="#MJX-5-TEX-I-1D44B"></use></g><g data-mml-node="mo" transform="translate(2990, 0)"><use xlink:href="#MJX-5-TEX-N-29"></use></g><g data-mml-node="mo" transform="translate(3656.8, 0)"><use xlink:href="#MJX-5-TEX-N-3D"></use></g><g data-mml-node="mfrac" transform="translate(4712.6, 0)"><g data-mml-node="mn" transform="translate(357.2, 394) scale(0.707)"><use xlink:href="#MJX-5-TEX-N-31"></use></g><g data-mml-node="mi" transform="translate(220, -345) scale(0.707)"><use xlink:href="#MJX-5-TEX-I-1D441"></use></g><rect width="827.9" height="60" x="120" y="220"></rect></g><g data-mml-node="munderover" transform="translate(5947.1, 0)"><g data-mml-node="mo"><use xlink:href="#MJX-5-TEX-SO-2211"></use></g><g data-mml-node="TeXAtom" transform="translate(1056, 477.1) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-5-TEX-I-1D441"></use></g></g><g data-mml-node="TeXAtom" transform="translate(1056, -285.4) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-5-TEX-I-1D45B"></use></g><g data-mml-node="mo" transform="translate(600, 0)"><use xlink:href="#MJX-5-TEX-N-3D"></use></g><g data-mml-node="mn" transform="translate(1378, 0)"><use xlink:href="#MJX-5-TEX-N-31"></use></g></g></g><g data-mml-node="msup" transform="translate(8547.7, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-5-TEX-I-1D462"></use></g><g data-mml-node="mi" transform="translate(572, 363) scale(0.707)"><use xlink:href="#MJX-5-TEX-I-1D447"></use></g></g><g data-mml-node="mo" transform="translate(9667.5, 0)"><use xlink:href="#MJX-5-TEX-N-28"></use></g><g data-mml-node="msub" transform="translate(10056.5, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-5-TEX-I-1D465"></use></g><g data-mml-node="mi" transform="translate(572, -150) scale(0.707)"><use xlink:href="#MJX-5-TEX-I-1D45B"></use></g></g><g data-mml-node="mo" transform="translate(11325, 0)"><use xlink:href="#MJX-5-TEX-N-2212"></use></g><g data-mml-node="mover" transform="translate(12325.3, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-5-TEX-I-1D465"></use></g><g data-mml-node="mo" transform="translate(0, 257.3) scale(0.707)"><svg width="808.9" height="246" x="0" y="444" viewBox="202.2 444 808.9 246"><use xlink:href="#MJX-5-TEX-S4-AF" transform="scale(2.427, 1)"></use></svg></g></g><g data-mml-node="mo" transform="translate(12897.3, 0)"><use xlink:href="#MJX-5-TEX-N-29"></use></g><g data-mml-node="mo" transform="translate(13286.3, 0)"><use xlink:href="#MJX-5-TEX-N-28"></use></g><g data-mml-node="msub" transform="translate(13675.3, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-5-TEX-I-1D465"></use></g><g data-mml-node="mi" transform="translate(572, -150) scale(0.707)"><use xlink:href="#MJX-5-TEX-I-1D45B"></use></g></g><g data-mml-node="mo" transform="translate(14943.7, 0)"><use xlink:href="#MJX-5-TEX-N-2212"></use></g><g data-mml-node="mover" transform="translate(15944, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-5-TEX-I-1D465"></use></g><g data-mml-node="mo" transform="translate(0, 257.3) scale(0.707)"><svg width="808.9" height="246" x="0" y="444" viewBox="202.2 444 808.9 246"><use xlink:href="#MJX-5-TEX-S4-AF" transform="scale(2.427, 1)"></use></svg></g></g><g data-mml-node="msup" transform="translate(16516, 0)"><g data-mml-node="mo"><use xlink:href="#MJX-5-TEX-N-29"></use></g><g data-mml-node="mi" transform="translate(389, 363) scale(0.707)"><use xlink:href="#MJX-5-TEX-I-1D447"></use></g></g><g data-mml-node="mi" transform="translate(17452.8, 0)"><use xlink:href="#MJX-5-TEX-I-1D462"></use></g></g></g></svg></mjx-container></div><p>Next, </p><div style="display: flex;justify-content: center;">    <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.781ex" xmlns="http://www.w3.org/2000/svg" width="40.403ex" height="2.953ex" role="img" focusable="false" viewBox="0 -960 17858.1 1305" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-5-TEX-I-1D449" d="M52 648Q52 670 65 683H76Q118 680 181 680Q299 680 320 683H330Q336 677 336 674T334 656Q329 641 325 637H304Q282 635 274 635Q245 630 242 620Q242 618 271 369T301 118L374 235Q447 352 520 471T595 594Q599 601 599 609Q599 633 555 637Q537 637 537 648Q537 649 539 661Q542 675 545 679T558 683Q560 683 570 683T604 682T668 681Q737 681 755 683H762Q769 676 769 672Q769 655 760 640Q757 637 743 637Q730 636 719 635T698 630T682 623T670 615T660 608T652 599T645 592L452 282Q272 -9 266 -16Q263 -18 259 -21L241 -22H234Q216 -22 216 -15Q213 -9 177 305Q139 623 138 626Q133 637 76 637H59Q52 642 52 648Z"></path><path id="MJX-5-TEX-I-1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path><path id="MJX-5-TEX-I-1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-5-TEX-N-28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path><path id="MJX-5-TEX-I-1D44B" d="M42 0H40Q26 0 26 11Q26 15 29 27Q33 41 36 43T55 46Q141 49 190 98Q200 108 306 224T411 342Q302 620 297 625Q288 636 234 637H206Q200 643 200 645T202 664Q206 677 212 683H226Q260 681 347 681Q380 681 408 681T453 682T473 682Q490 682 490 671Q490 670 488 658Q484 643 481 640T465 637Q434 634 411 620L488 426L541 485Q646 598 646 610Q646 628 622 635Q617 635 609 637Q594 637 594 648Q594 650 596 664Q600 677 606 683H618Q619 683 643 683T697 681T738 680Q828 680 837 683H845Q852 676 852 672Q850 647 840 637H824Q790 636 763 628T722 611T698 593L687 584Q687 585 592 480L505 384Q505 383 536 304T601 142T638 56Q648 47 699 46Q734 46 734 37Q734 35 732 23Q728 7 725 4T711 1Q708 1 678 1T589 2Q528 2 496 2T461 1Q444 1 444 10Q444 11 446 25Q448 35 450 39T455 44T464 46T480 47T506 54Q523 62 523 64Q522 64 476 181L429 299Q241 95 236 84Q232 76 232 72Q232 53 261 47Q262 47 267 47T273 46Q276 46 277 46T280 45T283 42T284 35Q284 26 282 19Q279 6 276 4T261 1Q258 1 243 1T201 2T142 2Q64 2 42 0Z"></path><path id="MJX-5-TEX-N-29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path><path id="MJX-5-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-5-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-5-TEX-I-1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"></path><path id="MJX-5-TEX-I-1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-5-TEX-I-1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path><path id="MJX-5-TEX-SO-2211" d="M61 748Q64 750 489 750H913L954 640Q965 609 976 579T993 533T999 516H979L959 517Q936 579 886 621T777 682Q724 700 655 705T436 710H319Q183 710 183 709Q186 706 348 484T511 259Q517 250 513 244L490 216Q466 188 420 134T330 27L149 -187Q149 -188 362 -188Q388 -188 436 -188T506 -189Q679 -189 778 -162T936 -43Q946 -27 959 6H999L913 -249L489 -250Q65 -250 62 -248Q56 -246 56 -239Q56 -234 118 -161Q186 -81 245 -11L428 206Q428 207 242 462L57 717L56 728Q56 744 61 748Z"></path><path id="MJX-5-TEX-I-1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-5-TEX-I-1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path><path id="MJX-5-TEX-N-2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path><path id="MJX-5-TEX-S4-AF" d="M69 544V590H430V544H69Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-5-TEX-I-1D449"></use></g><g data-mml-node="mi" transform="translate(769, 0)"><use xlink:href="#MJX-5-TEX-I-1D44E"></use></g><g data-mml-node="mi" transform="translate(1298, 0)"><use xlink:href="#MJX-5-TEX-I-1D45F"></use></g><g data-mml-node="mo" transform="translate(1749, 0)"><use xlink:href="#MJX-5-TEX-N-28"></use></g><g data-mml-node="mi" transform="translate(2138, 0)"><use xlink:href="#MJX-5-TEX-I-1D44B"></use></g><g data-mml-node="mo" transform="translate(2990, 0)"><use xlink:href="#MJX-5-TEX-N-29"></use></g><g data-mml-node="mo" transform="translate(3656.8, 0)"><use xlink:href="#MJX-5-TEX-N-3D"></use></g><g data-mml-node="mfrac" transform="translate(4712.6, 0)"><g data-mml-node="mn" transform="translate(357.2, 394) scale(0.707)"><use xlink:href="#MJX-5-TEX-N-31"></use></g><g data-mml-node="mi" transform="translate(220, -345) scale(0.707)"><use xlink:href="#MJX-5-TEX-I-1D441"></use></g><rect width="827.9" height="60" x="120" y="220"></rect></g><g data-mml-node="msup" transform="translate(5780.5, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-5-TEX-I-1D462"></use></g><g data-mml-node="mi" transform="translate(572, 363) scale(0.707)"><use xlink:href="#MJX-5-TEX-I-1D447"></use></g></g><g data-mml-node="munderover" transform="translate(7066.9, 0)"><g data-mml-node="mo"><use xlink:href="#MJX-5-TEX-SO-2211"></use></g><g data-mml-node="TeXAtom" transform="translate(1056, 477.1) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-5-TEX-I-1D441"></use></g></g><g data-mml-node="TeXAtom" transform="translate(1056, -285.4) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-5-TEX-I-1D45B"></use></g><g data-mml-node="mo" transform="translate(600, 0)"><use xlink:href="#MJX-5-TEX-N-3D"></use></g><g data-mml-node="mn" transform="translate(1378, 0)"><use xlink:href="#MJX-5-TEX-N-31"></use></g></g></g><g data-mml-node="mo" transform="translate(9500.9, 0)"><use xlink:href="#MJX-5-TEX-N-28"></use></g><g data-mml-node="msub" transform="translate(9889.9, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-5-TEX-I-1D465"></use></g><g data-mml-node="mi" transform="translate(572, -150) scale(0.707)"><use xlink:href="#MJX-5-TEX-I-1D45B"></use></g></g><g data-mml-node="mo" transform="translate(11158.4, 0)"><use xlink:href="#MJX-5-TEX-N-2212"></use></g><g data-mml-node="mover" transform="translate(12158.6, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-5-TEX-I-1D465"></use></g><g data-mml-node="mo" transform="translate(0, 257.3) scale(0.707)"><svg width="808.9" height="246" x="0" y="444" viewBox="202.2 444 808.9 246"><use xlink:href="#MJX-5-TEX-S4-AF" transform="scale(2.427, 1)"></use></svg></g></g><g data-mml-node="mo" transform="translate(12730.6, 0)"><use xlink:href="#MJX-5-TEX-N-29"></use></g><g data-mml-node="mo" transform="translate(13119.6, 0)"><use xlink:href="#MJX-5-TEX-N-28"></use></g><g data-mml-node="msub" transform="translate(13508.6, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-5-TEX-I-1D465"></use></g><g data-mml-node="mi" transform="translate(572, -150) scale(0.707)"><use xlink:href="#MJX-5-TEX-I-1D45B"></use></g></g><g data-mml-node="mo" transform="translate(14777.1, 0)"><use xlink:href="#MJX-5-TEX-N-2212"></use></g><g data-mml-node="mover" transform="translate(15777.3, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-5-TEX-I-1D465"></use></g><g data-mml-node="mo" transform="translate(0, 257.3) scale(0.707)"><svg width="808.9" height="246" x="0" y="444" viewBox="202.2 444 808.9 246"><use xlink:href="#MJX-5-TEX-S4-AF" transform="scale(2.427, 1)"></use></svg></g></g><g data-mml-node="msup" transform="translate(16349.3, 0)"><g data-mml-node="mo"><use xlink:href="#MJX-5-TEX-N-29"></use></g><g data-mml-node="mi" transform="translate(389, 363) scale(0.707)"><use xlink:href="#MJX-5-TEX-I-1D447"></use></g></g><g data-mml-node="mi" transform="translate(17286.1, 0)"><use xlink:href="#MJX-5-TEX-I-1D462"></use></g></g></g></svg></mjx-container></div><p>where <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.781ex" xmlns="http://www.w3.org/2000/svg" width="25.913ex" height="2.953ex" role="img" focusable="false" viewBox="0 -960 11453.7 1305" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-5-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-5-TEX-I-1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"></path><path id="MJX-5-TEX-SO-2211" d="M61 748Q64 750 489 750H913L954 640Q965 609 976 579T993 533T999 516H979L959 517Q936 579 886 621T777 682Q724 700 655 705T436 710H319Q183 710 183 709Q186 706 348 484T511 259Q517 250 513 244L490 216Q466 188 420 134T330 27L149 -187Q149 -188 362 -188Q388 -188 436 -188T506 -189Q679 -189 778 -162T936 -43Q946 -27 959 6H999L913 -249L489 -250Q65 -250 62 -248Q56 -246 56 -239Q56 -234 118 -161Q186 -81 245 -11L428 206Q428 207 242 462L57 717L56 728Q56 744 61 748Z"></path><path id="MJX-5-TEX-I-1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-5-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-5-TEX-N-28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path><path id="MJX-5-TEX-I-1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path><path id="MJX-5-TEX-N-2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path><path id="MJX-5-TEX-S4-AF" d="M69 544V590H430V544H69Z"></path><path id="MJX-5-TEX-N-29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path><path id="MJX-5-TEX-I-1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mfrac"><g data-mml-node="mn" transform="translate(357.2, 394) scale(0.707)"><use xlink:href="#MJX-5-TEX-N-31"></use></g><g data-mml-node="mi" transform="translate(220, -345) scale(0.707)"><use xlink:href="#MJX-5-TEX-I-1D441"></use></g><rect width="827.9" height="60" x="120" y="220"></rect></g><g data-mml-node="munderover" transform="translate(1234.6, 0)"><g data-mml-node="mo"><use xlink:href="#MJX-5-TEX-SO-2211"></use></g><g data-mml-node="TeXAtom" transform="translate(1056, 477.1) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-5-TEX-I-1D441"></use></g></g><g data-mml-node="TeXAtom" transform="translate(1056, -285.4) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-5-TEX-I-1D45B"></use></g><g data-mml-node="mo" transform="translate(600, 0)"><use xlink:href="#MJX-5-TEX-N-3D"></use></g><g data-mml-node="mn" transform="translate(1378, 0)"><use xlink:href="#MJX-5-TEX-N-31"></use></g></g></g><g data-mml-node="mo" transform="translate(3668.5, 0)"><use xlink:href="#MJX-5-TEX-N-28"></use></g><g data-mml-node="msub" transform="translate(4057.5, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-5-TEX-I-1D465"></use></g><g data-mml-node="mi" transform="translate(572, -150) scale(0.707)"><use xlink:href="#MJX-5-TEX-I-1D45B"></use></g></g><g data-mml-node="mo" transform="translate(5326, 0)"><use xlink:href="#MJX-5-TEX-N-2212"></use></g><g data-mml-node="mover" transform="translate(6326.2, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-5-TEX-I-1D465"></use></g><g data-mml-node="mo" transform="translate(0, 257.3) scale(0.707)"><svg width="808.9" height="246" x="0" y="444" viewBox="202.2 444 808.9 246"><use xlink:href="#MJX-5-TEX-S4-AF" transform="scale(2.427, 1)"></use></svg></g></g><g data-mml-node="mo" transform="translate(6898.2, 0)"><use xlink:href="#MJX-5-TEX-N-29"></use></g><g data-mml-node="mo" transform="translate(7287.2, 0)"><use xlink:href="#MJX-5-TEX-N-28"></use></g><g data-mml-node="msub" transform="translate(7676.2, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-5-TEX-I-1D465"></use></g><g data-mml-node="mi" transform="translate(572, -150) scale(0.707)"><use xlink:href="#MJX-5-TEX-I-1D45B"></use></g></g><g data-mml-node="mo" transform="translate(8944.7, 0)"><use xlink:href="#MJX-5-TEX-N-2212"></use></g><g data-mml-node="mover" transform="translate(9944.9, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-5-TEX-I-1D465"></use></g><g data-mml-node="mo" transform="translate(0, 257.3) scale(0.707)"><svg width="808.9" height="246" x="0" y="444" viewBox="202.2 444 808.9 246"><use xlink:href="#MJX-5-TEX-S4-AF" transform="scale(2.427, 1)"></use></svg></g></g><g data-mml-node="msup" transform="translate(10516.9, 0)"><g data-mml-node="mo"><use xlink:href="#MJX-5-TEX-N-29"></use></g><g data-mml-node="mi" transform="translate(389, 363) scale(0.707)"><use xlink:href="#MJX-5-TEX-I-1D447"></use></g></g></g></g></svg></mjx-container> is the closed form of the covariance matrix, so we then left</p><div style="display: flex;justify-content: center;">    <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="15.949ex" height="2.47ex" role="img" focusable="false" viewBox="0 -841.7 7049.4 1091.7" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-5-TEX-I-1D449" d="M52 648Q52 670 65 683H76Q118 680 181 680Q299 680 320 683H330Q336 677 336 674T334 656Q329 641 325 637H304Q282 635 274 635Q245 630 242 620Q242 618 271 369T301 118L374 235Q447 352 520 471T595 594Q599 601 599 609Q599 633 555 637Q537 637 537 648Q537 649 539 661Q542 675 545 679T558 683Q560 683 570 683T604 682T668 681Q737 681 755 683H762Q769 676 769 672Q769 655 760 640Q757 637 743 637Q730 636 719 635T698 630T682 623T670 615T660 608T652 599T645 592L452 282Q272 -9 266 -16Q263 -18 259 -21L241 -22H234Q216 -22 216 -15Q213 -9 177 305Q139 623 138 626Q133 637 76 637H59Q52 642 52 648Z"></path><path id="MJX-5-TEX-I-1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path><path id="MJX-5-TEX-I-1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-5-TEX-N-28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path><path id="MJX-5-TEX-I-1D44B" d="M42 0H40Q26 0 26 11Q26 15 29 27Q33 41 36 43T55 46Q141 49 190 98Q200 108 306 224T411 342Q302 620 297 625Q288 636 234 637H206Q200 643 200 645T202 664Q206 677 212 683H226Q260 681 347 681Q380 681 408 681T453 682T473 682Q490 682 490 671Q490 670 488 658Q484 643 481 640T465 637Q434 634 411 620L488 426L541 485Q646 598 646 610Q646 628 622 635Q617 635 609 637Q594 637 594 648Q594 650 596 664Q600 677 606 683H618Q619 683 643 683T697 681T738 680Q828 680 837 683H845Q852 676 852 672Q850 647 840 637H824Q790 636 763 628T722 611T698 593L687 584Q687 585 592 480L505 384Q505 383 536 304T601 142T638 56Q648 47 699 46Q734 46 734 37Q734 35 732 23Q728 7 725 4T711 1Q708 1 678 1T589 2Q528 2 496 2T461 1Q444 1 444 10Q444 11 446 25Q448 35 450 39T455 44T464 46T480 47T506 54Q523 62 523 64Q522 64 476 181L429 299Q241 95 236 84Q232 76 232 72Q232 53 261 47Q262 47 267 47T273 46Q276 46 277 46T280 45T283 42T284 35Q284 26 282 19Q279 6 276 4T261 1Q258 1 243 1T201 2T142 2Q64 2 42 0Z"></path><path id="MJX-5-TEX-N-29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path><path id="MJX-5-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-5-TEX-I-1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-5-TEX-I-1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path><path id="MJX-5-TEX-I-1D446" d="M308 24Q367 24 416 76T466 197Q466 260 414 284Q308 311 278 321T236 341Q176 383 176 462Q176 523 208 573T273 648Q302 673 343 688T407 704H418H425Q521 704 564 640Q565 640 577 653T603 682T623 704Q624 704 627 704T632 705Q645 705 645 698T617 577T585 459T569 456Q549 456 549 465Q549 471 550 475Q550 478 551 494T553 520Q553 554 544 579T526 616T501 641Q465 662 419 662Q362 662 313 616T263 510Q263 480 278 458T319 427Q323 425 389 408T456 390Q490 379 522 342T554 242Q554 216 546 186Q541 164 528 137T492 78T426 18T332 -20Q320 -22 298 -22Q199 -22 144 33L134 44L106 13Q83 -14 78 -18T65 -22Q52 -22 52 -14Q52 -11 110 221Q112 227 130 227H143Q149 221 149 216Q149 214 148 207T144 186T142 153Q144 114 160 87T203 47T255 29T308 24Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-5-TEX-I-1D449"></use></g><g data-mml-node="mi" transform="translate(769, 0)"><use xlink:href="#MJX-5-TEX-I-1D44E"></use></g><g data-mml-node="mi" transform="translate(1298, 0)"><use xlink:href="#MJX-5-TEX-I-1D45F"></use></g><g data-mml-node="mo" transform="translate(1749, 0)"><use xlink:href="#MJX-5-TEX-N-28"></use></g><g data-mml-node="mi" transform="translate(2138, 0)"><use xlink:href="#MJX-5-TEX-I-1D44B"></use></g><g data-mml-node="mo" transform="translate(2990, 0)"><use xlink:href="#MJX-5-TEX-N-29"></use></g><g data-mml-node="mo" transform="translate(3656.8, 0)"><use xlink:href="#MJX-5-TEX-N-3D"></use></g><g data-mml-node="msup" transform="translate(4712.6, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-5-TEX-I-1D462"></use></g><g data-mml-node="mi" transform="translate(572, 363) scale(0.707)"><use xlink:href="#MJX-5-TEX-I-1D447"></use></g></g><g data-mml-node="mi" transform="translate(5832.4, 0)"><use xlink:href="#MJX-5-TEX-I-1D446"></use></g><g data-mml-node="mi" transform="translate(6477.4, 0)"><use xlink:href="#MJX-5-TEX-I-1D462"></use></g></g></g></svg></mjx-container></div><h3 id="Lagrange-Multipler"><a href="#Lagrange-Multipler" class="headerlink" title="Lagrange Multipler"></a>Lagrange Multipler</h3><p>In this part, we want to maximise the variance of the projections which as we found is <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex" xmlns="http://www.w3.org/2000/svg" width="5.287ex" height="1.954ex" role="img" focusable="false" viewBox="0 -841.7 2336.8 863.7" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-5-TEX-I-1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-5-TEX-I-1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path><path id="MJX-5-TEX-I-1D446" d="M308 24Q367 24 416 76T466 197Q466 260 414 284Q308 311 278 321T236 341Q176 383 176 462Q176 523 208 573T273 648Q302 673 343 688T407 704H418H425Q521 704 564 640Q565 640 577 653T603 682T623 704Q624 704 627 704T632 705Q645 705 645 698T617 577T585 459T569 456Q549 456 549 465Q549 471 550 475Q550 478 551 494T553 520Q553 554 544 579T526 616T501 641Q465 662 419 662Q362 662 313 616T263 510Q263 480 278 458T319 427Q323 425 389 408T456 390Q490 379 522 342T554 242Q554 216 546 186Q541 164 528 137T492 78T426 18T332 -20Q320 -22 298 -22Q199 -22 144 33L134 44L106 13Q83 -14 78 -18T65 -22Q52 -22 52 -14Q52 -11 110 221Q112 227 130 227H143Q149 221 149 216Q149 214 148 207T144 186T142 153Q144 114 160 87T203 47T255 29T308 24Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"><use xlink:href="#MJX-5-TEX-I-1D462"></use></g><g data-mml-node="mi" transform="translate(572, 363) scale(0.707)"><use xlink:href="#MJX-5-TEX-I-1D447"></use></g></g><g data-mml-node="mi" transform="translate(1119.8, 0)"><use xlink:href="#MJX-5-TEX-I-1D446"></use></g><g data-mml-node="mi" transform="translate(1764.8, 0)"><use xlink:href="#MJX-5-TEX-I-1D462"></use></g></g></g></svg></mjx-container> subject to the constraint <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.186ex" xmlns="http://www.w3.org/2000/svg" width="7.976ex" height="2.09ex" role="img" focusable="false" viewBox="0 -841.7 3525.4 923.7" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-5-TEX-I-1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-5-TEX-I-1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path><path id="MJX-5-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-5-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"><use xlink:href="#MJX-5-TEX-I-1D462"></use></g><g data-mml-node="mi" transform="translate(572, 363) scale(0.707)"><use xlink:href="#MJX-5-TEX-I-1D447"></use></g></g><g data-mml-node="mi" transform="translate(1119.8, 0)"><use xlink:href="#MJX-5-TEX-I-1D462"></use></g><g data-mml-node="mo" transform="translate(1969.6, 0)"><use xlink:href="#MJX-5-TEX-N-3D"></use></g><g data-mml-node="mn" transform="translate(3025.4, 0)"><use xlink:href="#MJX-5-TEX-N-31"></use></g></g></g></svg></mjx-container>, and u means a unit vector. Using the power of Lagrange multipler, we have a new objective function, which looks like <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="18.856ex" height="2.47ex" role="img" focusable="false" viewBox="0 -841.7 8334.5 1091.7" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-5-TEX-I-1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-5-TEX-I-1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path><path id="MJX-5-TEX-I-1D446" d="M308 24Q367 24 416 76T466 197Q466 260 414 284Q308 311 278 321T236 341Q176 383 176 462Q176 523 208 573T273 648Q302 673 343 688T407 704H418H425Q521 704 564 640Q565 640 577 653T603 682T623 704Q624 704 627 704T632 705Q645 705 645 698T617 577T585 459T569 456Q549 456 549 465Q549 471 550 475Q550 478 551 494T553 520Q553 554 544 579T526 616T501 641Q465 662 419 662Q362 662 313 616T263 510Q263 480 278 458T319 427Q323 425 389 408T456 390Q490 379 522 342T554 242Q554 216 546 186Q541 164 528 137T492 78T426 18T332 -20Q320 -22 298 -22Q199 -22 144 33L134 44L106 13Q83 -14 78 -18T65 -22Q52 -22 52 -14Q52 -11 110 221Q112 227 130 227H143Q149 221 149 216Q149 214 148 207T144 186T142 153Q144 114 160 87T203 47T255 29T308 24Z"></path><path id="MJX-5-TEX-N-2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path><path id="MJX-5-TEX-I-1D706" d="M166 673Q166 685 183 694H202Q292 691 316 644Q322 629 373 486T474 207T524 67Q531 47 537 34T546 15T551 6T555 2T556 -2T550 -11H482Q457 3 450 18T399 152L354 277L340 262Q327 246 293 207T236 141Q211 112 174 69Q123 9 111 -1T83 -12Q47 -12 47 20Q47 37 61 52T199 187Q229 216 266 252T321 306L338 322Q338 323 288 462T234 612Q214 657 183 657Q166 657 166 673Z"></path><path id="MJX-5-TEX-N-28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path><path id="MJX-5-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-5-TEX-N-2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path><path id="MJX-5-TEX-N-29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"><use xlink:href="#MJX-5-TEX-I-1D462"></use></g><g data-mml-node="mi" transform="translate(572, 363) scale(0.707)"><use xlink:href="#MJX-5-TEX-I-1D447"></use></g></g><g data-mml-node="mi" transform="translate(1119.8, 0)"><use xlink:href="#MJX-5-TEX-I-1D446"></use></g><g data-mml-node="mi" transform="translate(1764.8, 0)"><use xlink:href="#MJX-5-TEX-I-1D462"></use></g><g data-mml-node="mo" transform="translate(2559, 0)"><use xlink:href="#MJX-5-TEX-N-2B"></use></g><g data-mml-node="mi" transform="translate(3559.2, 0)"><use xlink:href="#MJX-5-TEX-I-1D706"></use></g><g data-mml-node="mo" transform="translate(4142.2, 0)"><use xlink:href="#MJX-5-TEX-N-28"></use></g><g data-mml-node="mn" transform="translate(4531.2, 0)"><use xlink:href="#MJX-5-TEX-N-31"></use></g><g data-mml-node="mo" transform="translate(5253.5, 0)"><use xlink:href="#MJX-5-TEX-N-2212"></use></g><g data-mml-node="msup" transform="translate(6253.7, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-5-TEX-I-1D462"></use></g><g data-mml-node="mi" transform="translate(572, 363) scale(0.707)"><use xlink:href="#MJX-5-TEX-I-1D447"></use></g></g><g data-mml-node="mi" transform="translate(7373.5, 0)"><use xlink:href="#MJX-5-TEX-I-1D462"></use></g><g data-mml-node="mo" transform="translate(7945.5, 0)"><use xlink:href="#MJX-5-TEX-N-29"></use></g></g></g></svg></mjx-container>.</p><p>We are just going to take the derivative of this line with respect to vector <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="1.294ex" height="1.025ex" role="img" focusable="false" viewBox="0 -442 572 453" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-5-TEX-I-1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-5-TEX-I-1D462"></use></g></g></g></svg></mjx-container>, so we get </p><div style="display: flex;justify-content: center;">    <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.798ex" xmlns="http://www.w3.org/2000/svg" width="40.793ex" height="2.8ex" role="img" focusable="false" viewBox="0 -884.7 18030.7 1237.5" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-5-TEX-I-1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path><path id="MJX-5-TEX-I-1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-5-TEX-I-1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path><path id="MJX-5-TEX-I-1D446" d="M308 24Q367 24 416 76T466 197Q466 260 414 284Q308 311 278 321T236 341Q176 383 176 462Q176 523 208 573T273 648Q302 673 343 688T407 704H418H425Q521 704 564 640Q565 640 577 653T603 682T623 704Q624 704 627 704T632 705Q645 705 645 698T617 577T585 459T569 456Q549 456 549 465Q549 471 550 475Q550 478 551 494T553 520Q553 554 544 579T526 616T501 641Q465 662 419 662Q362 662 313 616T263 510Q263 480 278 458T319 427Q323 425 389 408T456 390Q490 379 522 342T554 242Q554 216 546 186Q541 164 528 137T492 78T426 18T332 -20Q320 -22 298 -22Q199 -22 144 33L134 44L106 13Q83 -14 78 -18T65 -22Q52 -22 52 -14Q52 -11 110 221Q112 227 130 227H143Q149 221 149 216Q149 214 148 207T144 186T142 153Q144 114 160 87T203 47T255 29T308 24Z"></path><path id="MJX-5-TEX-N-2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path><path id="MJX-5-TEX-I-1D706" d="M166 673Q166 685 183 694H202Q292 691 316 644Q322 629 373 486T474 207T524 67Q531 47 537 34T546 15T551 6T555 2T556 -2T550 -11H482Q457 3 450 18T399 152L354 277L340 262Q327 246 293 207T236 141Q211 112 174 69Q123 9 111 -1T83 -12Q47 -12 47 20Q47 37 61 52T199 187Q229 216 266 252T321 306L338 322Q338 323 288 462T234 612Q214 657 183 657Q166 657 166 673Z"></path><path id="MJX-5-TEX-N-28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path><path id="MJX-5-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-5-TEX-N-2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path><path id="MJX-5-TEX-N-29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path><path id="MJX-5-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-5-TEX-N-32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path><path id="MJX-5-TEX-N-22C5" d="M78 250Q78 274 95 292T138 310Q162 310 180 294T199 251Q199 226 182 208T139 190T96 207T78 250Z"></path><path id="MJX-5-TEX-N-30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mfrac"><g data-mml-node="mi" transform="translate(422.2, 394) scale(0.707)"><use xlink:href="#MJX-5-TEX-I-1D451"></use></g><g data-mml-node="mrow" transform="translate(220, -345) scale(0.707)"><g data-mml-node="mi"><use xlink:href="#MJX-5-TEX-I-1D451"></use></g><g data-mml-node="mi" transform="translate(520, 0)"><use xlink:href="#MJX-5-TEX-I-1D462"></use></g></g><rect width="972.2" height="60" x="120" y="220"></rect></g><g data-mml-node="msup" transform="translate(1212.2, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-5-TEX-I-1D462"></use></g><g data-mml-node="mi" transform="translate(572, 363) scale(0.707)"><use xlink:href="#MJX-5-TEX-I-1D447"></use></g></g><g data-mml-node="mi" transform="translate(2332, 0)"><use xlink:href="#MJX-5-TEX-I-1D446"></use></g><g data-mml-node="mi" transform="translate(2977, 0)"><use xlink:href="#MJX-5-TEX-I-1D462"></use></g><g data-mml-node="mo" transform="translate(3771.2, 0)"><use xlink:href="#MJX-5-TEX-N-2B"></use></g><g data-mml-node="mi" transform="translate(4771.4, 0)"><use xlink:href="#MJX-5-TEX-I-1D706"></use></g><g data-mml-node="mo" transform="translate(5354.4, 0)"><use xlink:href="#MJX-5-TEX-N-28"></use></g><g data-mml-node="mn" transform="translate(5743.4, 0)"><use xlink:href="#MJX-5-TEX-N-31"></use></g><g data-mml-node="mo" transform="translate(6465.6, 0)"><use xlink:href="#MJX-5-TEX-N-2212"></use></g><g data-mml-node="msup" transform="translate(7465.9, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-5-TEX-I-1D462"></use></g><g data-mml-node="mi" transform="translate(572, 363) scale(0.707)"><use xlink:href="#MJX-5-TEX-I-1D447"></use></g></g><g data-mml-node="mi" transform="translate(8585.7, 0)"><use xlink:href="#MJX-5-TEX-I-1D462"></use></g><g data-mml-node="mo" transform="translate(9157.7, 0)"><use xlink:href="#MJX-5-TEX-N-29"></use></g><g data-mml-node="mo" transform="translate(9824.4, 0)"><use xlink:href="#MJX-5-TEX-N-3D"></use></g><g data-mml-node="mn" transform="translate(10880.2, 0)"><use xlink:href="#MJX-5-TEX-N-32"></use></g><g data-mml-node="mi" transform="translate(11380.2, 0)"><use xlink:href="#MJX-5-TEX-I-1D446"></use></g><g data-mml-node="mi" transform="translate(12025.2, 0)"><use xlink:href="#MJX-5-TEX-I-1D462"></use></g><g data-mml-node="mo" transform="translate(12819.4, 0)"><use xlink:href="#MJX-5-TEX-N-2212"></use></g><g data-mml-node="mi" transform="translate(13819.7, 0)"><use xlink:href="#MJX-5-TEX-I-1D706"></use></g><g data-mml-node="mo" transform="translate(14624.9, 0)"><use xlink:href="#MJX-5-TEX-N-22C5"></use></g><g data-mml-node="mn" transform="translate(15125.1, 0)"><use xlink:href="#MJX-5-TEX-N-32"></use></g><g data-mml-node="mi" transform="translate(15625.1, 0)"><use xlink:href="#MJX-5-TEX-I-1D462"></use></g><g data-mml-node="mo" transform="translate(16474.9, 0)"><use xlink:href="#MJX-5-TEX-N-3D"></use></g><g data-mml-node="mn" transform="translate(17530.7, 0)"><use xlink:href="#MJX-5-TEX-N-30"></use></g></g></g></svg></mjx-container></div><p>We get </p><div style="display: flex;justify-content: center;">    <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.186ex" xmlns="http://www.w3.org/2000/svg" width="8.384ex" height="1.781ex" role="img" focusable="false" viewBox="0 -705 3705.6 787" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-5-TEX-I-1D446" d="M308 24Q367 24 416 76T466 197Q466 260 414 284Q308 311 278 321T236 341Q176 383 176 462Q176 523 208 573T273 648Q302 673 343 688T407 704H418H425Q521 704 564 640Q565 640 577 653T603 682T623 704Q624 704 627 704T632 705Q645 705 645 698T617 577T585 459T569 456Q549 456 549 465Q549 471 550 475Q550 478 551 494T553 520Q553 554 544 579T526 616T501 641Q465 662 419 662Q362 662 313 616T263 510Q263 480 278 458T319 427Q323 425 389 408T456 390Q490 379 522 342T554 242Q554 216 546 186Q541 164 528 137T492 78T426 18T332 -20Q320 -22 298 -22Q199 -22 144 33L134 44L106 13Q83 -14 78 -18T65 -22Q52 -22 52 -14Q52 -11 110 221Q112 227 130 227H143Q149 221 149 216Q149 214 148 207T144 186T142 153Q144 114 160 87T203 47T255 29T308 24Z"></path><path id="MJX-5-TEX-I-1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-5-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-5-TEX-I-1D706" d="M166 673Q166 685 183 694H202Q292 691 316 644Q322 629 373 486T474 207T524 67Q531 47 537 34T546 15T551 6T555 2T556 -2T550 -11H482Q457 3 450 18T399 152L354 277L340 262Q327 246 293 207T236 141Q211 112 174 69Q123 9 111 -1T83 -12Q47 -12 47 20Q47 37 61 52T199 187Q229 216 266 252T321 306L338 322Q338 323 288 462T234 612Q214 657 183 657Q166 657 166 673Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-5-TEX-I-1D446"></use></g><g data-mml-node="mi" transform="translate(645, 0)"><use xlink:href="#MJX-5-TEX-I-1D462"></use></g><g data-mml-node="mo" transform="translate(1494.8, 0)"><use xlink:href="#MJX-5-TEX-N-3D"></use></g><g data-mml-node="mi" transform="translate(2550.6, 0)"><use xlink:href="#MJX-5-TEX-I-1D706"></use></g><g data-mml-node="mi" transform="translate(3133.6, 0)"><use xlink:href="#MJX-5-TEX-I-1D462"></use></g></g></g></svg></mjx-container></div><p>This means that for u, whatever direction we choose to project on is going to have to be an eigenvector of the covariance matrix S, because this is exactly the definition of an eigenvector. But there’s lots of eigenvectors and eigenvalues, what eigenvector and what eigenvalue should we use?</p><h3 id="Eigenvectors-and-Eigenvalues"><a href="#Eigenvectors-and-Eigenvalues" class="headerlink" title="Eigenvectors and Eigenvalues"></a>Eigenvectors and Eigenvalues</h3><p>To figure this out, we know that</p><div style="display: flex;justify-content: center;">    <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="19.547ex" height="2.47ex" role="img" focusable="false" viewBox="0 -841.7 8639.7 1091.7" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-5-TEX-I-1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-5-TEX-I-1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path><path id="MJX-5-TEX-I-1D446" d="M308 24Q367 24 416 76T466 197Q466 260 414 284Q308 311 278 321T236 341Q176 383 176 462Q176 523 208 573T273 648Q302 673 343 688T407 704H418H425Q521 704 564 640Q565 640 577 653T603 682T623 704Q624 704 627 704T632 705Q645 705 645 698T617 577T585 459T569 456Q549 456 549 465Q549 471 550 475Q550 478 551 494T553 520Q553 554 544 579T526 616T501 641Q465 662 419 662Q362 662 313 616T263 510Q263 480 278 458T319 427Q323 425 389 408T456 390Q490 379 522 342T554 242Q554 216 546 186Q541 164 528 137T492 78T426 18T332 -20Q320 -22 298 -22Q199 -22 144 33L134 44L106 13Q83 -14 78 -18T65 -22Q52 -22 52 -14Q52 -11 110 221Q112 227 130 227H143Q149 221 149 216Q149 214 148 207T144 186T142 153Q144 114 160 87T203 47T255 29T308 24Z"></path><path id="MJX-5-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-5-TEX-N-28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path><path id="MJX-5-TEX-I-1D706" d="M166 673Q166 685 183 694H202Q292 691 316 644Q322 629 373 486T474 207T524 67Q531 47 537 34T546 15T551 6T555 2T556 -2T550 -11H482Q457 3 450 18T399 152L354 277L340 262Q327 246 293 207T236 141Q211 112 174 69Q123 9 111 -1T83 -12Q47 -12 47 20Q47 37 61 52T199 187Q229 216 266 252T321 306L338 322Q338 323 288 462T234 612Q214 657 183 657Q166 657 166 673Z"></path><path id="MJX-5-TEX-N-29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"><use xlink:href="#MJX-5-TEX-I-1D462"></use></g><g data-mml-node="mi" transform="translate(572, 363) scale(0.707)"><use xlink:href="#MJX-5-TEX-I-1D447"></use></g></g><g data-mml-node="mi" transform="translate(1119.8, 0)"><use xlink:href="#MJX-5-TEX-I-1D446"></use></g><g data-mml-node="mi" transform="translate(1764.8, 0)"><use xlink:href="#MJX-5-TEX-I-1D462"></use></g><g data-mml-node="mo" transform="translate(2614.6, 0)"><use xlink:href="#MJX-5-TEX-N-3D"></use></g><g data-mml-node="msup" transform="translate(3670.4, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-5-TEX-I-1D462"></use></g><g data-mml-node="mi" transform="translate(572, 363) scale(0.707)"><use xlink:href="#MJX-5-TEX-I-1D447"></use></g></g><g data-mml-node="mo" transform="translate(4790.2, 0)"><use xlink:href="#MJX-5-TEX-N-28"></use></g><g data-mml-node="mi" transform="translate(5179.2, 0)"><use xlink:href="#MJX-5-TEX-I-1D706"></use></g><g data-mml-node="mi" transform="translate(5762.2, 0)"><use xlink:href="#MJX-5-TEX-I-1D462"></use></g><g data-mml-node="mo" transform="translate(6334.2, 0)"><use xlink:href="#MJX-5-TEX-N-29"></use></g><g data-mml-node="mo" transform="translate(7000.9, 0)"><use xlink:href="#MJX-5-TEX-N-3D"></use></g><g data-mml-node="mi" transform="translate(8056.7, 0)"><use xlink:href="#MJX-5-TEX-I-1D706"></use></g></g></g></svg></mjx-container></div><p>If we want the maximum value of <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex" xmlns="http://www.w3.org/2000/svg" width="5.287ex" height="1.954ex" role="img" focusable="false" viewBox="0 -841.7 2336.8 863.7" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-5-TEX-I-1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-5-TEX-I-1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path><path id="MJX-5-TEX-I-1D446" d="M308 24Q367 24 416 76T466 197Q466 260 414 284Q308 311 278 321T236 341Q176 383 176 462Q176 523 208 573T273 648Q302 673 343 688T407 704H418H425Q521 704 564 640Q565 640 577 653T603 682T623 704Q624 704 627 704T632 705Q645 705 645 698T617 577T585 459T569 456Q549 456 549 465Q549 471 550 475Q550 478 551 494T553 520Q553 554 544 579T526 616T501 641Q465 662 419 662Q362 662 313 616T263 510Q263 480 278 458T319 427Q323 425 389 408T456 390Q490 379 522 342T554 242Q554 216 546 186Q541 164 528 137T492 78T426 18T332 -20Q320 -22 298 -22Q199 -22 144 33L134 44L106 13Q83 -14 78 -18T65 -22Q52 -22 52 -14Q52 -11 110 221Q112 227 130 227H143Q149 221 149 216Q149 214 148 207T144 186T142 153Q144 114 160 87T203 47T255 29T308 24Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"><use xlink:href="#MJX-5-TEX-I-1D462"></use></g><g data-mml-node="mi" transform="translate(572, 363) scale(0.707)"><use xlink:href="#MJX-5-TEX-I-1D447"></use></g></g><g data-mml-node="mi" transform="translate(1119.8, 0)"><use xlink:href="#MJX-5-TEX-I-1D446"></use></g><g data-mml-node="mi" transform="translate(1764.8, 0)"><use xlink:href="#MJX-5-TEX-I-1D462"></use></g></g></g></svg></mjx-container>, then we should select the dominant eigenvalue for the variance of the projected data.</p><p>To be more general, if we want to project the data onto more than just one dimension, we have to figure out what is the second biggest eigenvalue, and we use the second eigenvector corresponding to the second biggest eigenvalue, etc. You just go down in line for whatever many different components you want to end up in.</p><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>In this article, we understand the moving parts behind Principal Component Analysis (PCA), I believe this will give you some insight into what’s actually happening. I hope you are able to follow this article, stay tuned! Bye!</p>]]></content>
      
      
      <categories>
          
          <category> Data Science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Data Science </tag>
            
            <tag> Machine Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>P-Value Easy Explanation</title>
      <link href="/Hexo-Blog/2021/03/12/2021-03-12-easiest-explanation-for-p-value/"/>
      <url>/Hexo-Blog/2021/03/12/2021-03-12-easiest-explanation-for-p-value/</url>
      
        <content type="html"><![CDATA[<p>In Data Science interviews, one of the frequently asked questions is <strong>What is P-Value?</strong>. It’s hard to grasp the concept behind p-value. To understand p-value, you need to understand some background and context behind it. So, let’s start with the basics. When you conduct a piece of quantitative research (such as ML), you are inevitably attempting to answer a research question or hypothesis that you have set. One method of evaluating this research question is via a process called hypothesis testing.</p><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>In statistics, the p-value is the probability of obtaining results at least as extreme as the observed results of a statistical hypothesis test, assuming that the null hypothesis is correct. In this article, I will explain the concept and the calculation of the p-value in a simple setting.</p><h1 id="Hypothesis-Testing"><a href="#Hypothesis-Testing" class="headerlink" title="Hypothesis Testing"></a>Hypothesis Testing</h1><p>P-value is used in hypothesis testing process and its value is used in making decision. Let’s say we have two opposing statements, one is called null hypothesis (denoted as H0), and the other is called alternative hypothesis (denoted as H1). A null hypothesis is a type of conjecture proposes that there’s no difference between certain characteristics of a population (generally about population value being equal to something), and an alternative hypothesis proposed that there’s a difference.</p><p>Let’s take an simple example first. We assume that, we want to see if the machine learning average score of CSSLP students is equal to 60, that is, the population mean <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.489ex" xmlns="http://www.w3.org/2000/svg" width="1.364ex" height="1.489ex" role="img" focusable="false" viewBox="0 -442 603 658" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-4-TEX-I-1D707" d="M58 -216Q44 -216 34 -208T23 -186Q23 -176 96 116T173 414Q186 442 219 442Q231 441 239 435T249 423T251 413Q251 401 220 279T187 142Q185 131 185 107V99Q185 26 252 26Q261 26 270 27T287 31T302 38T315 45T327 55T338 65T348 77T356 88T365 100L372 110L408 253Q444 395 448 404Q461 431 491 431Q504 431 512 424T523 412T525 402L449 84Q448 79 448 68Q448 43 455 35T476 26Q485 27 496 35Q517 55 537 131Q543 151 547 152Q549 153 557 153H561Q580 153 580 144Q580 138 575 117T555 63T523 13Q510 0 491 -8Q483 -10 467 -10Q446 -10 429 -4T402 11T385 29T376 44T374 51L368 45Q362 39 350 30T324 12T288 -4T246 -11Q199 -11 153 12L129 -85Q108 -167 104 -180T92 -202Q76 -216 58 -216Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-4-TEX-I-1D707"></use></g></g></g></svg></mjx-container> is equal to 60. So perhaps the CSSLP students made a great effort in studying and it is believed that the H0 is <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.489ex" xmlns="http://www.w3.org/2000/svg" width="6.644ex" height="1.995ex" role="img" focusable="false" viewBox="0 -666 2936.6 882" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-4-TEX-I-1D707" d="M58 -216Q44 -216 34 -208T23 -186Q23 -176 96 116T173 414Q186 442 219 442Q231 441 239 435T249 423T251 413Q251 401 220 279T187 142Q185 131 185 107V99Q185 26 252 26Q261 26 270 27T287 31T302 38T315 45T327 55T338 65T348 77T356 88T365 100L372 110L408 253Q444 395 448 404Q461 431 491 431Q504 431 512 424T523 412T525 402L449 84Q448 79 448 68Q448 43 455 35T476 26Q485 27 496 35Q517 55 537 131Q543 151 547 152Q549 153 557 153H561Q580 153 580 144Q580 138 575 117T555 63T523 13Q510 0 491 -8Q483 -10 467 -10Q446 -10 429 -4T402 11T385 29T376 44T374 51L368 45Q362 39 350 30T324 12T288 -4T246 -11Q199 -11 153 12L129 -85Q108 -167 104 -180T92 -202Q76 -216 58 -216Z"></path><path id="MJX-4-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-4-TEX-N-36" d="M42 313Q42 476 123 571T303 666Q372 666 402 630T432 550Q432 525 418 510T379 495Q356 495 341 509T326 548Q326 592 373 601Q351 623 311 626Q240 626 194 566Q147 500 147 364L148 360Q153 366 156 373Q197 433 263 433H267Q313 433 348 414Q372 400 396 374T435 317Q456 268 456 210V192Q456 169 451 149Q440 90 387 34T253 -22Q225 -22 199 -14T143 16T92 75T56 172T42 313ZM257 397Q227 397 205 380T171 335T154 278T148 216Q148 133 160 97T198 39Q222 21 251 21Q302 21 329 59Q342 77 347 104T352 209Q352 289 347 316T329 361Q302 397 257 397Z"></path><path id="MJX-4-TEX-N-30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-4-TEX-I-1D707"></use></g><g data-mml-node="mo" transform="translate(880.8, 0)"><use xlink:href="#MJX-4-TEX-N-3D"></use></g><g data-mml-node="mn" transform="translate(1936.6, 0)"><use xlink:href="#MJX-4-TEX-N-36"></use><use xlink:href="#MJX-4-TEX-N-30" transform="translate(500, 0)"></use></g></g></g></svg></mjx-container>. As for the alternative hypothesis believes that the mean has changed and the score is now more than 60, that is, we have H1 of <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.489ex" xmlns="http://www.w3.org/2000/svg" width="6.644ex" height="1.995ex" role="img" focusable="false" viewBox="0 -666 2936.6 882" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-4-TEX-I-1D707" d="M58 -216Q44 -216 34 -208T23 -186Q23 -176 96 116T173 414Q186 442 219 442Q231 441 239 435T249 423T251 413Q251 401 220 279T187 142Q185 131 185 107V99Q185 26 252 26Q261 26 270 27T287 31T302 38T315 45T327 55T338 65T348 77T356 88T365 100L372 110L408 253Q444 395 448 404Q461 431 491 431Q504 431 512 424T523 412T525 402L449 84Q448 79 448 68Q448 43 455 35T476 26Q485 27 496 35Q517 55 537 131Q543 151 547 152Q549 153 557 153H561Q580 153 580 144Q580 138 575 117T555 63T523 13Q510 0 491 -8Q483 -10 467 -10Q446 -10 429 -4T402 11T385 29T376 44T374 51L368 45Q362 39 350 30T324 12T288 -4T246 -11Q199 -11 153 12L129 -85Q108 -167 104 -180T92 -202Q76 -216 58 -216Z"></path><path id="MJX-4-TEX-N-3E" d="M84 520Q84 528 88 533T96 539L99 540Q106 540 253 471T544 334L687 265Q694 260 694 250T687 235Q685 233 395 96L107 -40H101Q83 -38 83 -20Q83 -19 83 -17Q82 -10 98 -1Q117 9 248 71Q326 108 378 132L626 250L378 368Q90 504 86 509Q84 513 84 520Z"></path><path id="MJX-4-TEX-N-36" d="M42 313Q42 476 123 571T303 666Q372 666 402 630T432 550Q432 525 418 510T379 495Q356 495 341 509T326 548Q326 592 373 601Q351 623 311 626Q240 626 194 566Q147 500 147 364L148 360Q153 366 156 373Q197 433 263 433H267Q313 433 348 414Q372 400 396 374T435 317Q456 268 456 210V192Q456 169 451 149Q440 90 387 34T253 -22Q225 -22 199 -14T143 16T92 75T56 172T42 313ZM257 397Q227 397 205 380T171 335T154 278T148 216Q148 133 160 97T198 39Q222 21 251 21Q302 21 329 59Q342 77 347 104T352 209Q352 289 347 316T329 361Q302 397 257 397Z"></path><path id="MJX-4-TEX-N-30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-4-TEX-I-1D707"></use></g><g data-mml-node="mo" transform="translate(880.8, 0)"><use xlink:href="#MJX-4-TEX-N-3E"></use></g><g data-mml-node="mn" transform="translate(1936.6, 0)"><use xlink:href="#MJX-4-TEX-N-36"></use><use xlink:href="#MJX-4-TEX-N-30" transform="translate(500, 0)"></use></g></g></g></svg></mjx-container>.</p><div style="display: flex;justify-content: center;">    <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.489ex" xmlns="http://www.w3.org/2000/svg" width="11.67ex" height="2.034ex" role="img" focusable="false" viewBox="0 -683 5158.1 899" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-4-TEX-I-1D43B" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 219 683Q260 681 355 681Q389 681 418 681T463 682T483 682Q499 682 499 672Q499 670 497 658Q492 641 487 638H485Q483 638 480 638T473 638T464 637T455 637Q416 636 405 634T387 623Q384 619 355 500Q348 474 340 442T328 395L324 380Q324 378 469 378H614L615 381Q615 384 646 504Q674 619 674 627T617 637Q594 637 587 639T580 648Q580 650 582 660Q586 677 588 679T604 682Q609 682 646 681T740 680Q802 680 835 681T871 682Q888 682 888 672Q888 645 876 638H874Q872 638 869 638T862 638T853 637T844 637Q805 636 794 634T776 623Q773 618 704 340T634 58Q634 51 638 51Q646 48 692 46H723Q729 38 729 37T726 19Q722 6 716 0H701Q664 2 567 2Q533 2 504 2T458 2T437 1Q420 1 420 10Q420 15 423 24Q428 43 433 45Q437 46 448 46H454Q481 46 514 49Q520 50 522 50T528 55T534 64T540 82T547 110T558 153Q565 181 569 198Q602 330 602 331T457 332H312L279 197Q245 63 245 58Q245 51 253 49T303 46H334Q340 38 340 37T337 19Q333 6 327 0H312Q275 2 178 2Q144 2 115 2T69 2T48 1Q31 1 31 10Q31 12 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path><path id="MJX-4-TEX-N-30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path><path id="MJX-4-TEX-N-3A" d="M78 370Q78 394 95 412T138 430Q162 430 180 414T199 371Q199 346 182 328T139 310T96 327T78 370ZM78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z"></path><path id="MJX-4-TEX-I-1D707" d="M58 -216Q44 -216 34 -208T23 -186Q23 -176 96 116T173 414Q186 442 219 442Q231 441 239 435T249 423T251 413Q251 401 220 279T187 142Q185 131 185 107V99Q185 26 252 26Q261 26 270 27T287 31T302 38T315 45T327 55T338 65T348 77T356 88T365 100L372 110L408 253Q444 395 448 404Q461 431 491 431Q504 431 512 424T523 412T525 402L449 84Q448 79 448 68Q448 43 455 35T476 26Q485 27 496 35Q517 55 537 131Q543 151 547 152Q549 153 557 153H561Q580 153 580 144Q580 138 575 117T555 63T523 13Q510 0 491 -8Q483 -10 467 -10Q446 -10 429 -4T402 11T385 29T376 44T374 51L368 45Q362 39 350 30T324 12T288 -4T246 -11Q199 -11 153 12L129 -85Q108 -167 104 -180T92 -202Q76 -216 58 -216Z"></path><path id="MJX-4-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-4-TEX-N-36" d="M42 313Q42 476 123 571T303 666Q372 666 402 630T432 550Q432 525 418 510T379 495Q356 495 341 509T326 548Q326 592 373 601Q351 623 311 626Q240 626 194 566Q147 500 147 364L148 360Q153 366 156 373Q197 433 263 433H267Q313 433 348 414Q372 400 396 374T435 317Q456 268 456 210V192Q456 169 451 149Q440 90 387 34T253 -22Q225 -22 199 -14T143 16T92 75T56 172T42 313ZM257 397Q227 397 205 380T171 335T154 278T148 216Q148 133 160 97T198 39Q222 21 251 21Q302 21 329 59Q342 77 347 104T352 209Q352 289 347 316T329 361Q302 397 257 397Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-4-TEX-I-1D43B"></use></g><g data-mml-node="mn" transform="translate(888, 0)"><use xlink:href="#MJX-4-TEX-N-30"></use></g><g data-mml-node="mo" transform="translate(1665.8, 0)"><use xlink:href="#MJX-4-TEX-N-3A"></use></g><g data-mml-node="mi" transform="translate(2221.6, 0)"><use xlink:href="#MJX-4-TEX-I-1D707"></use></g><g data-mml-node="mo" transform="translate(3102.3, 0)"><use xlink:href="#MJX-4-TEX-N-3D"></use></g><g data-mml-node="mn" transform="translate(4158.1, 0)"><use xlink:href="#MJX-4-TEX-N-36"></use><use xlink:href="#MJX-4-TEX-N-30" transform="translate(500, 0)"></use></g></g></g></svg></mjx-container></div><div style="display: flex;justify-content: center;">    <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.489ex" xmlns="http://www.w3.org/2000/svg" width="11.67ex" height="2.034ex" role="img" focusable="false" viewBox="0 -683 5158.1 899" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-4-TEX-I-1D43B" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 219 683Q260 681 355 681Q389 681 418 681T463 682T483 682Q499 682 499 672Q499 670 497 658Q492 641 487 638H485Q483 638 480 638T473 638T464 637T455 637Q416 636 405 634T387 623Q384 619 355 500Q348 474 340 442T328 395L324 380Q324 378 469 378H614L615 381Q615 384 646 504Q674 619 674 627T617 637Q594 637 587 639T580 648Q580 650 582 660Q586 677 588 679T604 682Q609 682 646 681T740 680Q802 680 835 681T871 682Q888 682 888 672Q888 645 876 638H874Q872 638 869 638T862 638T853 637T844 637Q805 636 794 634T776 623Q773 618 704 340T634 58Q634 51 638 51Q646 48 692 46H723Q729 38 729 37T726 19Q722 6 716 0H701Q664 2 567 2Q533 2 504 2T458 2T437 1Q420 1 420 10Q420 15 423 24Q428 43 433 45Q437 46 448 46H454Q481 46 514 49Q520 50 522 50T528 55T534 64T540 82T547 110T558 153Q565 181 569 198Q602 330 602 331T457 332H312L279 197Q245 63 245 58Q245 51 253 49T303 46H334Q340 38 340 37T337 19Q333 6 327 0H312Q275 2 178 2Q144 2 115 2T69 2T48 1Q31 1 31 10Q31 12 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path><path id="MJX-4-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-4-TEX-N-3A" d="M78 370Q78 394 95 412T138 430Q162 430 180 414T199 371Q199 346 182 328T139 310T96 327T78 370ZM78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z"></path><path id="MJX-4-TEX-I-1D707" d="M58 -216Q44 -216 34 -208T23 -186Q23 -176 96 116T173 414Q186 442 219 442Q231 441 239 435T249 423T251 413Q251 401 220 279T187 142Q185 131 185 107V99Q185 26 252 26Q261 26 270 27T287 31T302 38T315 45T327 55T338 65T348 77T356 88T365 100L372 110L408 253Q444 395 448 404Q461 431 491 431Q504 431 512 424T523 412T525 402L449 84Q448 79 448 68Q448 43 455 35T476 26Q485 27 496 35Q517 55 537 131Q543 151 547 152Q549 153 557 153H561Q580 153 580 144Q580 138 575 117T555 63T523 13Q510 0 491 -8Q483 -10 467 -10Q446 -10 429 -4T402 11T385 29T376 44T374 51L368 45Q362 39 350 30T324 12T288 -4T246 -11Q199 -11 153 12L129 -85Q108 -167 104 -180T92 -202Q76 -216 58 -216Z"></path><path id="MJX-4-TEX-N-3E" d="M84 520Q84 528 88 533T96 539L99 540Q106 540 253 471T544 334L687 265Q694 260 694 250T687 235Q685 233 395 96L107 -40H101Q83 -38 83 -20Q83 -19 83 -17Q82 -10 98 -1Q117 9 248 71Q326 108 378 132L626 250L378 368Q90 504 86 509Q84 513 84 520Z"></path><path id="MJX-4-TEX-N-36" d="M42 313Q42 476 123 571T303 666Q372 666 402 630T432 550Q432 525 418 510T379 495Q356 495 341 509T326 548Q326 592 373 601Q351 623 311 626Q240 626 194 566Q147 500 147 364L148 360Q153 366 156 373Q197 433 263 433H267Q313 433 348 414Q372 400 396 374T435 317Q456 268 456 210V192Q456 169 451 149Q440 90 387 34T253 -22Q225 -22 199 -14T143 16T92 75T56 172T42 313ZM257 397Q227 397 205 380T171 335T154 278T148 216Q148 133 160 97T198 39Q222 21 251 21Q302 21 329 59Q342 77 347 104T352 209Q352 289 347 316T329 361Q302 397 257 397Z"></path><path id="MJX-4-TEX-N-30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-4-TEX-I-1D43B"></use></g><g data-mml-node="mn" transform="translate(888, 0)"><use xlink:href="#MJX-4-TEX-N-31"></use></g><g data-mml-node="mo" transform="translate(1665.8, 0)"><use xlink:href="#MJX-4-TEX-N-3A"></use></g><g data-mml-node="mi" transform="translate(2221.6, 0)"><use xlink:href="#MJX-4-TEX-I-1D707"></use></g><g data-mml-node="mo" transform="translate(3102.3, 0)"><use xlink:href="#MJX-4-TEX-N-3E"></use></g><g data-mml-node="mn" transform="translate(4158.1, 0)"><use xlink:href="#MJX-4-TEX-N-36"></use><use xlink:href="#MJX-4-TEX-N-30" transform="translate(500, 0)"></use></g></g></g></svg></mjx-container></div><p>The idea here is that we will take a random sample from this population (all CSSLP students), examine the sample and decide whether the samples support our null hypothesis or the alternative hypothesis. So the logic of hypothesis testing indicates that if null hypothesis is true, then the sample mean (suppose you take a random sample) and calculate the sample mean under the null hypothesis, the sample mean should be close to 60, because sample mean and population mean are expected to be close. However, if sample mean is significantly higher than 60, then we will reject null hypothesis and establish the alternative.</p><p>Let’s suppose that we took a random sample of 36 CSSLP students from population and calculate the sample mean, and let’s suppose the sample mean is 62 (<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.186ex" xmlns="http://www.w3.org/2000/svg" width="7.248ex" height="2.483ex" role="img" focusable="false" viewBox="0 -1015.5 3203.6 1097.5" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-4-TEX-I-1D44B" d="M42 0H40Q26 0 26 11Q26 15 29 27Q33 41 36 43T55 46Q141 49 190 98Q200 108 306 224T411 342Q302 620 297 625Q288 636 234 637H206Q200 643 200 645T202 664Q206 677 212 683H226Q260 681 347 681Q380 681 408 681T453 682T473 682Q490 682 490 671Q490 670 488 658Q484 643 481 640T465 637Q434 634 411 620L488 426L541 485Q646 598 646 610Q646 628 622 635Q617 635 609 637Q594 637 594 648Q594 650 596 664Q600 677 606 683H618Q619 683 643 683T697 681T738 680Q828 680 837 683H845Q852 676 852 672Q850 647 840 637H824Q790 636 763 628T722 611T698 593L687 584Q687 585 592 480L505 384Q505 383 536 304T601 142T638 56Q648 47 699 46Q734 46 734 37Q734 35 732 23Q728 7 725 4T711 1Q708 1 678 1T589 2Q528 2 496 2T461 1Q444 1 444 10Q444 11 446 25Q448 35 450 39T455 44T464 46T480 47T506 54Q523 62 523 64Q522 64 476 181L429 299Q241 95 236 84Q232 76 232 72Q232 53 261 47Q262 47 267 47T273 46Q276 46 277 46T280 45T283 42T284 35Q284 26 282 19Q279 6 276 4T261 1Q258 1 243 1T201 2T142 2Q64 2 42 0Z"></path><path id="MJX-4-TEX-S4-AF" d="M69 544V590H430V544H69Z"></path><path id="MJX-4-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-4-TEX-N-36" d="M42 313Q42 476 123 571T303 666Q372 666 402 630T432 550Q432 525 418 510T379 495Q356 495 341 509T326 548Q326 592 373 601Q351 623 311 626Q240 626 194 566Q147 500 147 364L148 360Q153 366 156 373Q197 433 263 433H267Q313 433 348 414Q372 400 396 374T435 317Q456 268 456 210V192Q456 169 451 149Q440 90 387 34T253 -22Q225 -22 199 -14T143 16T92 75T56 172T42 313ZM257 397Q227 397 205 380T171 335T154 278T148 216Q148 133 160 97T198 39Q222 21 251 21Q302 21 329 59Q342 77 347 104T352 209Q352 289 347 316T329 361Q302 397 257 397Z"></path><path id="MJX-4-TEX-N-32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mover"><g data-mml-node="mi" transform="translate(0, 0)"><use xlink:href="#MJX-4-TEX-I-1D44B"></use></g><g data-mml-node="mo" transform="translate(18, 498.3) scale(0.707)"><svg width="1204.9" height="246" x="0" y="444" viewBox="301.2 444 1204.9 246"><use xlink:href="#MJX-4-TEX-S4-AF" transform="scale(3.615, 1)"></use></svg></g></g><g data-mml-node="mo" transform="translate(1147.8, 0)"><use xlink:href="#MJX-4-TEX-N-3D"></use></g><g data-mml-node="mn" transform="translate(2203.6, 0)"><use xlink:href="#MJX-4-TEX-N-36"></use><use xlink:href="#MJX-4-TEX-N-32" transform="translate(500, 0)"></use></g></g></g></svg></mjx-container>) and suppose that we know the standard deviation is 4 (<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.186ex" xmlns="http://www.w3.org/2000/svg" width="5.44ex" height="1.717ex" role="img" focusable="false" viewBox="0 -677 2404.6 759" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-4-TEX-I-1D70E" d="M184 -11Q116 -11 74 34T31 147Q31 247 104 333T274 430Q275 431 414 431H552Q553 430 555 429T559 427T562 425T565 422T567 420T569 416T570 412T571 407T572 401Q572 357 507 357Q500 357 490 357T476 358H416L421 348Q439 310 439 263Q439 153 359 71T184 -11ZM361 278Q361 358 276 358Q152 358 115 184Q114 180 114 178Q106 141 106 117Q106 67 131 47T188 26Q242 26 287 73Q316 103 334 153T356 233T361 278Z"></path><path id="MJX-4-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-4-TEX-N-34" d="M462 0Q444 3 333 3Q217 3 199 0H190V46H221Q241 46 248 46T265 48T279 53T286 61Q287 63 287 115V165H28V211L179 442Q332 674 334 675Q336 677 355 677H373L379 671V211H471V165H379V114Q379 73 379 66T385 54Q393 47 442 46H471V0H462ZM293 211V545L74 212L183 211H293Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-4-TEX-I-1D70E"></use></g><g data-mml-node="mo" transform="translate(848.8, 0)"><use xlink:href="#MJX-4-TEX-N-3D"></use></g><g data-mml-node="mn" transform="translate(1904.6, 0)"><use xlink:href="#MJX-4-TEX-N-34"></use></g></g></g></svg></mjx-container>). If you recall, the standard deviation is a measure of variability, not all students having the same score, so this <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="1.292ex" height="1ex" role="img" focusable="false" viewBox="0 -431 571 442" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-4-TEX-I-1D70E" d="M184 -11Q116 -11 74 34T31 147Q31 247 104 333T274 430Q275 431 414 431H552Q553 430 555 429T559 427T562 425T565 422T567 420T569 416T570 412T571 407T572 401Q572 357 507 357Q500 357 490 357T476 358H416L421 348Q439 310 439 263Q439 153 359 71T184 -11ZM361 278Q361 358 276 358Q152 358 115 184Q114 180 114 178Q106 141 106 117Q106 67 131 47T188 26Q242 26 287 73Q316 103 334 153T356 233T361 278Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-4-TEX-I-1D70E"></use></g></g></g></svg></mjx-container> here is the ‘population’ standard deviation in this problem.</p><p>Now the question is our <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.186ex" xmlns="http://www.w3.org/2000/svg" width="7.248ex" height="2.483ex" role="img" focusable="false" viewBox="0 -1015.5 3203.6 1097.5" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-4-TEX-I-1D44B" d="M42 0H40Q26 0 26 11Q26 15 29 27Q33 41 36 43T55 46Q141 49 190 98Q200 108 306 224T411 342Q302 620 297 625Q288 636 234 637H206Q200 643 200 645T202 664Q206 677 212 683H226Q260 681 347 681Q380 681 408 681T453 682T473 682Q490 682 490 671Q490 670 488 658Q484 643 481 640T465 637Q434 634 411 620L488 426L541 485Q646 598 646 610Q646 628 622 635Q617 635 609 637Q594 637 594 648Q594 650 596 664Q600 677 606 683H618Q619 683 643 683T697 681T738 680Q828 680 837 683H845Q852 676 852 672Q850 647 840 637H824Q790 636 763 628T722 611T698 593L687 584Q687 585 592 480L505 384Q505 383 536 304T601 142T638 56Q648 47 699 46Q734 46 734 37Q734 35 732 23Q728 7 725 4T711 1Q708 1 678 1T589 2Q528 2 496 2T461 1Q444 1 444 10Q444 11 446 25Q448 35 450 39T455 44T464 46T480 47T506 54Q523 62 523 64Q522 64 476 181L429 299Q241 95 236 84Q232 76 232 72Q232 53 261 47Q262 47 267 47T273 46Q276 46 277 46T280 45T283 42T284 35Q284 26 282 19Q279 6 276 4T261 1Q258 1 243 1T201 2T142 2Q64 2 42 0Z"></path><path id="MJX-4-TEX-S4-AF" d="M69 544V590H430V544H69Z"></path><path id="MJX-4-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-4-TEX-N-36" d="M42 313Q42 476 123 571T303 666Q372 666 402 630T432 550Q432 525 418 510T379 495Q356 495 341 509T326 548Q326 592 373 601Q351 623 311 626Q240 626 194 566Q147 500 147 364L148 360Q153 366 156 373Q197 433 263 433H267Q313 433 348 414Q372 400 396 374T435 317Q456 268 456 210V192Q456 169 451 149Q440 90 387 34T253 -22Q225 -22 199 -14T143 16T92 75T56 172T42 313ZM257 397Q227 397 205 380T171 335T154 278T148 216Q148 133 160 97T198 39Q222 21 251 21Q302 21 329 59Q342 77 347 104T352 209Q352 289 347 316T329 361Q302 397 257 397Z"></path><path id="MJX-4-TEX-N-32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mover"><g data-mml-node="mi" transform="translate(0, 0)"><use xlink:href="#MJX-4-TEX-I-1D44B"></use></g><g data-mml-node="mo" transform="translate(18, 498.3) scale(0.707)"><svg width="1204.9" height="246" x="0" y="444" viewBox="301.2 444 1204.9 246"><use xlink:href="#MJX-4-TEX-S4-AF" transform="scale(3.615, 1)"></use></svg></g></g><g data-mml-node="mo" transform="translate(1147.8, 0)"><use xlink:href="#MJX-4-TEX-N-3D"></use></g><g data-mml-node="mn" transform="translate(2203.6, 0)"><use xlink:href="#MJX-4-TEX-N-36"></use><use xlink:href="#MJX-4-TEX-N-32" transform="translate(500, 0)"></use></g></g></g></svg></mjx-container> significantly high? How do we decide if it is significantly high? In statistics, we calculate the probability or the likelihood of the occurence of an outcome. So what we would like to calculate is that if the null hypothesis is true (if the population mean is actually 60), what is the likelihood that <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0" xmlns="http://www.w3.org/2000/svg" width="1.968ex" height="2.298ex" role="img" focusable="false" viewBox="0 -1015.5 870 1015.5" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-4-TEX-I-1D44B" d="M42 0H40Q26 0 26 11Q26 15 29 27Q33 41 36 43T55 46Q141 49 190 98Q200 108 306 224T411 342Q302 620 297 625Q288 636 234 637H206Q200 643 200 645T202 664Q206 677 212 683H226Q260 681 347 681Q380 681 408 681T453 682T473 682Q490 682 490 671Q490 670 488 658Q484 643 481 640T465 637Q434 634 411 620L488 426L541 485Q646 598 646 610Q646 628 622 635Q617 635 609 637Q594 637 594 648Q594 650 596 664Q600 677 606 683H618Q619 683 643 683T697 681T738 680Q828 680 837 683H845Q852 676 852 672Q850 647 840 637H824Q790 636 763 628T722 611T698 593L687 584Q687 585 592 480L505 384Q505 383 536 304T601 142T638 56Q648 47 699 46Q734 46 734 37Q734 35 732 23Q728 7 725 4T711 1Q708 1 678 1T589 2Q528 2 496 2T461 1Q444 1 444 10Q444 11 446 25Q448 35 450 39T455 44T464 46T480 47T506 54Q523 62 523 64Q522 64 476 181L429 299Q241 95 236 84Q232 76 232 72Q232 53 261 47Q262 47 267 47T273 46Q276 46 277 46T280 45T283 42T284 35Q284 26 282 19Q279 6 276 4T261 1Q258 1 243 1T201 2T142 2Q64 2 42 0Z"></path><path id="MJX-4-TEX-S4-AF" d="M69 544V590H430V544H69Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mover"><g data-mml-node="mi" transform="translate(0, 0)"><use xlink:href="#MJX-4-TEX-I-1D44B"></use></g><g data-mml-node="mo" transform="translate(18, 498.3) scale(0.707)"><svg width="1204.9" height="246" x="0" y="444" viewBox="301.2 444 1204.9 246"><use xlink:href="#MJX-4-TEX-S4-AF" transform="scale(3.615, 1)"></use></svg></g></g></g></g></svg></mjx-container> would be as high or higher than 62? Let’s translate it into a mathematical formula.</p><div style="display: flex;justify-content: center;">    <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -2.561ex" xmlns="http://www.w3.org/2000/svg" width="49.656ex" height="6.254ex" role="img" focusable="false" viewBox="0 -1632.1 21947.8 2764.2" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-4-TEX-I-1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path><path id="MJX-4-TEX-N-28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path><path id="MJX-4-TEX-I-1D44B" d="M42 0H40Q26 0 26 11Q26 15 29 27Q33 41 36 43T55 46Q141 49 190 98Q200 108 306 224T411 342Q302 620 297 625Q288 636 234 637H206Q200 643 200 645T202 664Q206 677 212 683H226Q260 681 347 681Q380 681 408 681T453 682T473 682Q490 682 490 671Q490 670 488 658Q484 643 481 640T465 637Q434 634 411 620L488 426L541 485Q646 598 646 610Q646 628 622 635Q617 635 609 637Q594 637 594 648Q594 650 596 664Q600 677 606 683H618Q619 683 643 683T697 681T738 680Q828 680 837 683H845Q852 676 852 672Q850 647 840 637H824Q790 636 763 628T722 611T698 593L687 584Q687 585 592 480L505 384Q505 383 536 304T601 142T638 56Q648 47 699 46Q734 46 734 37Q734 35 732 23Q728 7 725 4T711 1Q708 1 678 1T589 2Q528 2 496 2T461 1Q444 1 444 10Q444 11 446 25Q448 35 450 39T455 44T464 46T480 47T506 54Q523 62 523 64Q522 64 476 181L429 299Q241 95 236 84Q232 76 232 72Q232 53 261 47Q262 47 267 47T273 46Q276 46 277 46T280 45T283 42T284 35Q284 26 282 19Q279 6 276 4T261 1Q258 1 243 1T201 2T142 2Q64 2 42 0Z"></path><path id="MJX-4-TEX-S4-AF" d="M69 544V590H430V544H69Z"></path><path id="MJX-4-TEX-N-2265" d="M83 616Q83 624 89 630T99 636Q107 636 253 568T543 431T687 361Q694 356 694 346T687 331Q685 329 395 192L107 56H101Q83 58 83 76Q83 77 83 79Q82 86 98 95Q117 105 248 167Q326 204 378 228L626 346L360 472Q291 505 200 548Q112 589 98 597T83 616ZM84 -118Q84 -108 99 -98H678Q694 -104 694 -118Q694 -130 679 -138H98Q84 -131 84 -118Z"></path><path id="MJX-4-TEX-N-36" d="M42 313Q42 476 123 571T303 666Q372 666 402 630T432 550Q432 525 418 510T379 495Q356 495 341 509T326 548Q326 592 373 601Q351 623 311 626Q240 626 194 566Q147 500 147 364L148 360Q153 366 156 373Q197 433 263 433H267Q313 433 348 414Q372 400 396 374T435 317Q456 268 456 210V192Q456 169 451 149Q440 90 387 34T253 -22Q225 -22 199 -14T143 16T92 75T56 172T42 313ZM257 397Q227 397 205 380T171 335T154 278T148 216Q148 133 160 97T198 39Q222 21 251 21Q302 21 329 59Q342 77 347 104T352 209Q352 289 347 316T329 361Q302 397 257 397Z"></path><path id="MJX-4-TEX-N-32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path><path id="MJX-4-TEX-N-7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path><path id="MJX-4-TEX-I-1D707" d="M58 -216Q44 -216 34 -208T23 -186Q23 -176 96 116T173 414Q186 442 219 442Q231 441 239 435T249 423T251 413Q251 401 220 279T187 142Q185 131 185 107V99Q185 26 252 26Q261 26 270 27T287 31T302 38T315 45T327 55T338 65T348 77T356 88T365 100L372 110L408 253Q444 395 448 404Q461 431 491 431Q504 431 512 424T523 412T525 402L449 84Q448 79 448 68Q448 43 455 35T476 26Q485 27 496 35Q517 55 537 131Q543 151 547 152Q549 153 557 153H561Q580 153 580 144Q580 138 575 117T555 63T523 13Q510 0 491 -8Q483 -10 467 -10Q446 -10 429 -4T402 11T385 29T376 44T374 51L368 45Q362 39 350 30T324 12T288 -4T246 -11Q199 -11 153 12L129 -85Q108 -167 104 -180T92 -202Q76 -216 58 -216Z"></path><path id="MJX-4-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-4-TEX-N-30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path><path id="MJX-4-TEX-N-29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path><path id="MJX-4-TEX-I-1D44D" d="M58 8Q58 23 64 35Q64 36 329 334T596 635L586 637Q575 637 512 637H500H476Q442 637 420 635T365 624T311 598T266 548T228 469Q227 466 226 463T224 458T223 453T222 450L221 448Q218 443 202 443Q185 443 182 453L214 561Q228 606 241 651Q249 679 253 681Q256 683 487 683H718Q723 678 723 675Q723 673 717 649Q189 54 188 52L185 49H274Q369 50 377 51Q452 60 500 100T579 247Q587 272 590 277T603 282H607Q628 282 628 271Q547 5 541 2Q538 0 300 0H124Q58 0 58 8Z"></path><path id="MJX-4-TEX-N-2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path><path id="MJX-4-TEX-N-34" d="M462 0Q444 3 333 3Q217 3 199 0H190V46H221Q241 46 248 46T265 48T279 53T286 61Q287 63 287 115V165H28V211L179 442Q332 674 334 675Q336 677 355 677H373L379 671V211H471V165H379V114Q379 73 379 66T385 54Q393 47 442 46H471V0H462ZM293 211V545L74 212L183 211H293Z"></path><path id="MJX-4-TEX-N-221A" d="M95 178Q89 178 81 186T72 200T103 230T169 280T207 309Q209 311 212 311H213Q219 311 227 294T281 177Q300 134 312 108L397 -77Q398 -77 501 136T707 565T814 786Q820 800 834 800Q841 800 846 794T853 782V776L620 293L385 -193Q381 -200 366 -200Q357 -200 354 -197Q352 -195 256 15L160 225L144 214Q129 202 113 190T95 178Z"></path><path id="MJX-4-TEX-N-33" d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mtable"><g data-mml-node="mtr" transform="translate(0, 290.1)"><g data-mml-node="mtd"><g data-mml-node="mi"><use xlink:href="#MJX-4-TEX-I-1D443"></use></g><g data-mml-node="mo" transform="translate(751, 0)"><use xlink:href="#MJX-4-TEX-N-28"></use></g><g data-mml-node="mover" transform="translate(1140, 0)"><g data-mml-node="mi" transform="translate(0, 0)"><use xlink:href="#MJX-4-TEX-I-1D44B"></use></g><g data-mml-node="mo" transform="translate(18, 498.3) scale(0.707)"><svg width="1204.9" height="246" x="0" y="444" viewBox="301.2 444 1204.9 246"><use xlink:href="#MJX-4-TEX-S4-AF" transform="scale(3.615, 1)"></use></svg></g></g><g data-mml-node="mo" transform="translate(2287.8, 0)"><use xlink:href="#MJX-4-TEX-N-2265"></use></g><g data-mml-node="mn" transform="translate(3343.6, 0)"><use xlink:href="#MJX-4-TEX-N-36"></use><use xlink:href="#MJX-4-TEX-N-32" transform="translate(500, 0)"></use></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(4343.6, 0)"><g data-mml-node="mo"><use xlink:href="#MJX-4-TEX-N-7C"></use></g></g><g data-mml-node="mi" transform="translate(4621.6, 0)"><use xlink:href="#MJX-4-TEX-I-1D707"></use></g><g data-mml-node="mo" transform="translate(5502.3, 0)"><use xlink:href="#MJX-4-TEX-N-3D"></use></g><g data-mml-node="mn" transform="translate(6558.1, 0)"><use xlink:href="#MJX-4-TEX-N-36"></use><use xlink:href="#MJX-4-TEX-N-30" transform="translate(500, 0)"></use></g><g data-mml-node="mo" transform="translate(7558.1, 0)"><use xlink:href="#MJX-4-TEX-N-29"></use></g></g><g data-mml-node="mtd" transform="translate(7947.1, 0)"><g data-mml-node="mi"></g><g data-mml-node="mo" transform="translate(277.8, 0)"><use xlink:href="#MJX-4-TEX-N-3D"></use></g><g data-mml-node="mi" transform="translate(1333.6, 0)"><use xlink:href="#MJX-4-TEX-I-1D443"></use></g><g data-mml-node="mo" transform="translate(2084.6, 0)"><use xlink:href="#MJX-4-TEX-N-28"></use></g><g data-mml-node="mi" transform="translate(2473.6, 0)"><use xlink:href="#MJX-4-TEX-I-1D44D"></use></g><g data-mml-node="mo" transform="translate(3474.3, 0)"><use xlink:href="#MJX-4-TEX-N-2265"></use></g><g data-mml-node="mfrac" transform="translate(4530.1, 0)"><g data-mml-node="mrow" transform="translate(220, 676)"><g data-mml-node="mn"><use xlink:href="#MJX-4-TEX-N-36"></use><use xlink:href="#MJX-4-TEX-N-32" transform="translate(500, 0)"></use></g><g data-mml-node="mo" transform="translate(1222.2, 0)"><use xlink:href="#MJX-4-TEX-N-2212"></use></g><g data-mml-node="mn" transform="translate(2222.4, 0)"><use xlink:href="#MJX-4-TEX-N-36"></use><use xlink:href="#MJX-4-TEX-N-30" transform="translate(500, 0)"></use></g></g><g data-mml-node="mfrac" transform="translate(956.1, -832.7)"><g data-mml-node="mn" transform="translate(698.4, 394) scale(0.707)"><use xlink:href="#MJX-4-TEX-N-34"></use></g><g data-mml-node="msqrt" transform="translate(220, -511.4) scale(0.707)"><g transform="translate(853, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-4-TEX-N-33"></use><use xlink:href="#MJX-4-TEX-N-36" transform="translate(500, 0)"></use></g></g><g data-mml-node="mo" transform="translate(0, 107.1)"><use xlink:href="#MJX-4-TEX-N-221A"></use></g><rect width="1000" height="42.4" x="853" y="864.6"></rect></g><rect width="1510.3" height="60" x="120" y="220"></rect></g><rect width="3422.4" height="60" x="120" y="220"></rect></g><g data-mml-node="mo" transform="translate(8192.6, 0)"><use xlink:href="#MJX-4-TEX-N-29"></use></g><g data-mml-node="mo" transform="translate(8859.3, 0)"><use xlink:href="#MJX-4-TEX-N-3D"></use></g><g data-mml-node="mi" transform="translate(9915.1, 0)"><use xlink:href="#MJX-4-TEX-I-1D443"></use></g><g data-mml-node="mo" transform="translate(10666.1, 0)"><use xlink:href="#MJX-4-TEX-N-28"></use></g><g data-mml-node="mi" transform="translate(11055.1, 0)"><use xlink:href="#MJX-4-TEX-I-1D44D"></use></g><g data-mml-node="mo" transform="translate(12055.9, 0)"><use xlink:href="#MJX-4-TEX-N-2265"></use></g><g data-mml-node="mn" transform="translate(13111.7, 0)"><use xlink:href="#MJX-4-TEX-N-33"></use></g><g data-mml-node="mo" transform="translate(13611.7, 0)"><use xlink:href="#MJX-4-TEX-N-29"></use></g></g></g></g></g></g></svg></mjx-container></div><p>Since <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0" xmlns="http://www.w3.org/2000/svg" width="1.968ex" height="2.298ex" role="img" focusable="false" viewBox="0 -1015.5 870 1015.5" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-4-TEX-I-1D44B" d="M42 0H40Q26 0 26 11Q26 15 29 27Q33 41 36 43T55 46Q141 49 190 98Q200 108 306 224T411 342Q302 620 297 625Q288 636 234 637H206Q200 643 200 645T202 664Q206 677 212 683H226Q260 681 347 681Q380 681 408 681T453 682T473 682Q490 682 490 671Q490 670 488 658Q484 643 481 640T465 637Q434 634 411 620L488 426L541 485Q646 598 646 610Q646 628 622 635Q617 635 609 637Q594 637 594 648Q594 650 596 664Q600 677 606 683H618Q619 683 643 683T697 681T738 680Q828 680 837 683H845Q852 676 852 672Q850 647 840 637H824Q790 636 763 628T722 611T698 593L687 584Q687 585 592 480L505 384Q505 383 536 304T601 142T638 56Q648 47 699 46Q734 46 734 37Q734 35 732 23Q728 7 725 4T711 1Q708 1 678 1T589 2Q528 2 496 2T461 1Q444 1 444 10Q444 11 446 25Q448 35 450 39T455 44T464 46T480 47T506 54Q523 62 523 64Q522 64 476 181L429 299Q241 95 236 84Q232 76 232 72Q232 53 261 47Q262 47 267 47T273 46Q276 46 277 46T280 45T283 42T284 35Q284 26 282 19Q279 6 276 4T261 1Q258 1 243 1T201 2T142 2Q64 2 42 0Z"></path><path id="MJX-4-TEX-S4-AF" d="M69 544V590H430V544H69Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mover"><g data-mml-node="mi" transform="translate(0, 0)"><use xlink:href="#MJX-4-TEX-I-1D44B"></use></g><g data-mml-node="mo" transform="translate(18, 498.3) scale(0.707)"><svg width="1204.9" height="246" x="0" y="444" viewBox="301.2 444 1204.9 246"><use xlink:href="#MJX-4-TEX-S4-AF" transform="scale(3.615, 1)"></use></svg></g></g></g></g></svg></mjx-container> has a normal distribution, because <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="1.357ex" height="1.025ex" role="img" focusable="false" viewBox="0 -442 600 453" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-4-TEX-I-1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-4-TEX-I-1D45B"></use></g></g></g></svg></mjx-container> is large we can change this value 60 to a z-score, which is measured in terms of standard deviations from the mean developed by Professor Edward Altman at New York University. It turns out we are calculating the probability that <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0" xmlns="http://www.w3.org/2000/svg" width="1.636ex" height="1.545ex" role="img" focusable="false" viewBox="0 -683 723 683" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-4-TEX-I-1D44D" d="M58 8Q58 23 64 35Q64 36 329 334T596 635L586 637Q575 637 512 637H500H476Q442 637 420 635T365 624T311 598T266 548T228 469Q227 466 226 463T224 458T223 453T222 450L221 448Q218 443 202 443Q185 443 182 453L214 561Q228 606 241 651Q249 679 253 681Q256 683 487 683H718Q723 678 723 675Q723 673 717 649Q189 54 188 52L185 49H274Q369 50 377 51Q452 60 500 100T579 247Q587 272 590 277T603 282H607Q628 282 628 271Q547 5 541 2Q538 0 300 0H124Q58 0 58 8Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-4-TEX-I-1D44D"></use></g></g></g></svg></mjx-container> is greater than or equal to 3. And how do we calculate this? One way is to read the normal table or we could use the graphing calculator.</p><p>Below is the typical graph of z-scores.</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt<span class="token keyword">import</span> scipy<span class="token punctuation">.</span>stats <span class="token keyword">as</span> stplt<span class="token punctuation">.</span>figure<span class="token punctuation">(</span>figsize<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">15</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">)</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>np<span class="token punctuation">.</span>linspace<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">100</span><span class="token punctuation">)</span><span class="token punctuation">,</span> st<span class="token punctuation">.</span>norm<span class="token punctuation">.</span>pdf<span class="token punctuation">(</span>np<span class="token punctuation">.</span>linspace<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">100</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>grid<span class="token punctuation">(</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><img src="/Hexo-Blog/2021/03/12/2021-03-12-easiest-explanation-for-p-value/z-distribution.png" class=""><p>We want to calculate the area under the standard normal curve to the right of 3. This can denoted with the equation below.</p><div style="display: flex;justify-content: center;">    <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -1.334ex" xmlns="http://www.w3.org/2000/svg" width="14.904ex" height="3.949ex" role="img" focusable="false" viewBox="0 -1156.1 6587.4 1745.6" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-4-TEX-SO-222B" d="M113 -244Q113 -246 119 -251T139 -263T167 -269Q186 -269 199 -260Q220 -247 232 -218T251 -133T262 -15T276 155T297 367Q300 390 305 438T314 512T325 580T340 647T361 703T390 751T428 784T479 804Q481 804 488 804T501 805Q552 802 581 769T610 695Q610 669 594 657T561 645Q542 645 527 658T512 694Q512 705 516 714T526 729T538 737T548 742L552 743Q552 745 545 751T525 762T498 768Q475 768 460 756T434 716T418 652T407 559T398 444T387 300T369 133Q349 -38 337 -102T303 -207Q256 -306 169 -306Q119 -306 87 -272T55 -196Q55 -170 71 -158T104 -146Q123 -146 138 -159T153 -195Q153 -206 149 -215T139 -230T127 -238T117 -242L113 -244Z"></path><path id="MJX-4-TEX-N-221E" d="M55 217Q55 305 111 373T254 442Q342 442 419 381Q457 350 493 303L507 284L514 294Q618 442 747 442Q833 442 888 374T944 214Q944 128 889 59T743 -11Q657 -11 580 50Q542 81 506 128L492 147L485 137Q381 -11 252 -11Q166 -11 111 57T55 217ZM907 217Q907 285 869 341T761 397Q740 397 720 392T682 378T648 359T619 335T594 310T574 285T559 263T548 246L543 238L574 198Q605 158 622 138T664 94T714 61T765 51Q827 51 867 100T907 217ZM92 214Q92 145 131 89T239 33Q357 33 456 193L425 233Q364 312 334 337Q285 380 233 380Q171 380 132 331T92 214Z"></path><path id="MJX-4-TEX-I-1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"></path><path id="MJX-4-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-4-TEX-N-221A" d="M95 178Q89 178 81 186T72 200T103 230T169 280T207 309Q209 311 212 311H213Q219 311 227 294T281 177Q300 134 312 108L397 -77Q398 -77 501 136T707 565T814 786Q820 800 834 800Q841 800 846 794T853 782V776L620 293L385 -193Q381 -200 366 -200Q357 -200 354 -197Q352 -195 256 15L160 225L144 214Q129 202 113 190T95 178Z"></path><path id="MJX-4-TEX-N-32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path><path id="MJX-4-TEX-I-1D70B" d="M132 -11Q98 -11 98 22V33L111 61Q186 219 220 334L228 358H196Q158 358 142 355T103 336Q92 329 81 318T62 297T53 285Q51 284 38 284Q19 284 19 294Q19 300 38 329T93 391T164 429Q171 431 389 431Q549 431 553 430Q573 423 573 402Q573 371 541 360Q535 358 472 358H408L405 341Q393 269 393 222Q393 170 402 129T421 65T431 37Q431 20 417 5T381 -10Q370 -10 363 -7T347 17T331 77Q330 86 330 121Q330 170 339 226T357 318T367 358H269L268 354Q268 351 249 275T206 114T175 17Q164 -11 132 -11Z"></path><path id="MJX-4-TEX-I-1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path><path id="MJX-4-TEX-N-2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path><path id="MJX-4-TEX-I-1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path><path id="MJX-4-TEX-I-1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msubsup"><g data-mml-node="mo"><use xlink:href="#MJX-4-TEX-SO-222B"></use></g><g data-mml-node="TeXAtom" transform="translate(666.9, 532.6) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-4-TEX-N-221E"></use></g></g><g data-mml-node="TeXAtom" transform="translate(472, -340.9) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-4-TEX-I-1D467"></use></g></g></g><g data-mml-node="mfrac" transform="translate(1590.7, 0)"><g data-mml-node="mn" transform="translate(723.1, 394) scale(0.707)"><use xlink:href="#MJX-4-TEX-N-31"></use></g><g data-mml-node="msqrt" transform="translate(220, -515.3) scale(0.707)"><g transform="translate(853, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-4-TEX-N-32"></use></g><g data-mml-node="mi" transform="translate(500, 0)"><use xlink:href="#MJX-4-TEX-I-1D70B"></use></g></g><g data-mml-node="mo" transform="translate(0, 112.6)"><use xlink:href="#MJX-4-TEX-N-221A"></use></g><rect width="1070" height="42.4" x="853" y="870.1"></rect></g><rect width="1559.8" height="60" x="120" y="220"></rect></g><g data-mml-node="msup" transform="translate(3390.4, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-4-TEX-I-1D452"></use></g><g data-mml-node="TeXAtom" transform="translate(466, 363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mo"><use xlink:href="#MJX-4-TEX-N-2212"></use></g><g data-mml-node="mfrac" transform="translate(778, 0)"><g data-mml-node="msup" transform="translate(220, 394) scale(0.707)"><g data-mml-node="mi"><use xlink:href="#MJX-4-TEX-I-1D465"></use></g><g data-mml-node="mn" transform="translate(572, 363)"><use xlink:href="#MJX-4-TEX-N-32"></use></g></g><g data-mml-node="mn" transform="translate(439.9, -345) scale(0.707)"><use xlink:href="#MJX-4-TEX-N-32"></use></g><rect width="993.4" height="60" x="120" y="220"></rect></g></g></g><g data-mml-node="mstyle" transform="translate(5328.7, 0)"><g data-mml-node="mspace"></g></g><g data-mml-node="mi" transform="translate(5495.4, 0)"><use xlink:href="#MJX-4-TEX-I-1D451"></use></g><g data-mml-node="mi" transform="translate(6015.4, 0)"><use xlink:href="#MJX-4-TEX-I-1D465"></use></g></g></g></svg></mjx-container></div><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> scipy<span class="token punctuation">.</span>integrate <span class="token keyword">import</span> quad<span class="token keyword">def</span> <span class="token function">normal_distribution_pdf</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>    constant <span class="token operator">=</span> <span class="token number">1.0</span> <span class="token operator">/</span> np<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span><span class="token number">2</span><span class="token operator">*</span>np<span class="token punctuation">.</span>pi<span class="token punctuation">)</span>    <span class="token keyword">return</span><span class="token punctuation">(</span>constant <span class="token operator">*</span> np<span class="token punctuation">.</span>exp<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token operator">-</span>x<span class="token operator">**</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token operator">/</span><span class="token number">2.0</span><span class="token punctuation">)</span><span class="token punctuation">)</span>percentile<span class="token punctuation">,</span> _ <span class="token operator">=</span> quad<span class="token punctuation">(</span>normal_distribution_pdf<span class="token punctuation">,</span> <span class="token number">3.0</span><span class="token punctuation">,</span> np<span class="token punctuation">.</span>Inf<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>We can now get the percentile of 0.13%. This probability is called the <code>p-value</code>. To sum up, P-value is the probability of observing a result as high or higher than what we have observed if the null hypothesis is true. We can then conclude that, under the null hypothesis, what we have observed is highly unlikely to happen. That means null hypothesis and what we have observed, they don’t match. Because what we have observed is very unlikely to happen under null hypothesis, we reject null hypothesis and conclude that the alternative is true, which is in this case, <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.489ex" xmlns="http://www.w3.org/2000/svg" width="1.364ex" height="1.489ex" role="img" focusable="false" viewBox="0 -442 603 658" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-4-TEX-I-1D707" d="M58 -216Q44 -216 34 -208T23 -186Q23 -176 96 116T173 414Q186 442 219 442Q231 441 239 435T249 423T251 413Q251 401 220 279T187 142Q185 131 185 107V99Q185 26 252 26Q261 26 270 27T287 31T302 38T315 45T327 55T338 65T348 77T356 88T365 100L372 110L408 253Q444 395 448 404Q461 431 491 431Q504 431 512 424T523 412T525 402L449 84Q448 79 448 68Q448 43 455 35T476 26Q485 27 496 35Q517 55 537 131Q543 151 547 152Q549 153 557 153H561Q580 153 580 144Q580 138 575 117T555 63T523 13Q510 0 491 -8Q483 -10 467 -10Q446 -10 429 -4T402 11T385 29T376 44T374 51L368 45Q362 39 350 30T324 12T288 -4T246 -11Q199 -11 153 12L129 -85Q108 -167 104 -180T92 -202Q76 -216 58 -216Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-4-TEX-I-1D707"></use></g></g></g></svg></mjx-container> is bigger than 60.</p><p>However, another question pops out, how do we know the p-value is low enough? Do we have a guideline or threshold to determine this? And the answer is “YES”! Generally, if the probability of an outcome is lower than 5% (so called level of significance or alpha level) that we select in advance, we then consider that probability to be low. In our case, we calculate the p-value as 0.13%, which is obviously way lower than 5% (null hypothesis is unlikely to happen), so we reject the null hypothesis (we say the result is statistically significant).</p><p>So far, this is the concept of p-value. Let me mention one thing which is also equally important. Come back to this <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="1.292ex" height="1ex" role="img" focusable="false" viewBox="0 -431 571 442" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-4-TEX-I-1D70E" d="M184 -11Q116 -11 74 34T31 147Q31 247 104 333T274 430Q275 431 414 431H552Q553 430 555 429T559 427T562 425T565 422T567 420T569 416T570 412T571 407T572 401Q572 357 507 357Q500 357 490 357T476 358H416L421 348Q439 310 439 263Q439 153 359 71T184 -11ZM361 278Q361 358 276 358Q152 358 115 184Q114 180 114 178Q106 141 106 117Q106 67 131 47T188 26Q242 26 287 73Q316 103 334 153T356 233T361 278Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-4-TEX-I-1D70E"></use></g></g></g></svg></mjx-container> value, which is the population standard deviation. Usually, population standard deviation is not given. That means, when we take a sample of 36 observations from the population, we not only calculate the sample mean <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0" xmlns="http://www.w3.org/2000/svg" width="1.968ex" height="2.298ex" role="img" focusable="false" viewBox="0 -1015.5 870 1015.5" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-4-TEX-I-1D44B" d="M42 0H40Q26 0 26 11Q26 15 29 27Q33 41 36 43T55 46Q141 49 190 98Q200 108 306 224T411 342Q302 620 297 625Q288 636 234 637H206Q200 643 200 645T202 664Q206 677 212 683H226Q260 681 347 681Q380 681 408 681T453 682T473 682Q490 682 490 671Q490 670 488 658Q484 643 481 640T465 637Q434 634 411 620L488 426L541 485Q646 598 646 610Q646 628 622 635Q617 635 609 637Q594 637 594 648Q594 650 596 664Q600 677 606 683H618Q619 683 643 683T697 681T738 680Q828 680 837 683H845Q852 676 852 672Q850 647 840 637H824Q790 636 763 628T722 611T698 593L687 584Q687 585 592 480L505 384Q505 383 536 304T601 142T638 56Q648 47 699 46Q734 46 734 37Q734 35 732 23Q728 7 725 4T711 1Q708 1 678 1T589 2Q528 2 496 2T461 1Q444 1 444 10Q444 11 446 25Q448 35 450 39T455 44T464 46T480 47T506 54Q523 62 523 64Q522 64 476 181L429 299Q241 95 236 84Q232 76 232 72Q232 53 261 47Q262 47 267 47T273 46Q276 46 277 46T280 45T283 42T284 35Q284 26 282 19Q279 6 276 4T261 1Q258 1 243 1T201 2T142 2Q64 2 42 0Z"></path><path id="MJX-4-TEX-S4-AF" d="M69 544V590H430V544H69Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mover"><g data-mml-node="mi" transform="translate(0, 0)"><use xlink:href="#MJX-4-TEX-I-1D44B"></use></g><g data-mml-node="mo" transform="translate(18, 498.3) scale(0.707)"><svg width="1204.9" height="246" x="0" y="444" viewBox="301.2 444 1204.9 246"><use xlink:href="#MJX-4-TEX-S4-AF" transform="scale(3.615, 1)"></use></svg></g></g></g></g></svg></mjx-container>, but we also have to compute the sample standard deviation <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex" xmlns="http://www.w3.org/2000/svg" width="1.459ex" height="1.645ex" role="img" focusable="false" viewBox="0 -705 645 727" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-4-TEX-I-1D446" d="M308 24Q367 24 416 76T466 197Q466 260 414 284Q308 311 278 321T236 341Q176 383 176 462Q176 523 208 573T273 648Q302 673 343 688T407 704H418H425Q521 704 564 640Q565 640 577 653T603 682T623 704Q624 704 627 704T632 705Q645 705 645 698T617 577T585 459T569 456Q549 456 549 465Q549 471 550 475Q550 478 551 494T553 520Q553 554 544 579T526 616T501 641Q465 662 419 662Q362 662 313 616T263 510Q263 480 278 458T319 427Q323 425 389 408T456 390Q490 379 522 342T554 242Q554 216 546 186Q541 164 528 137T492 78T426 18T332 -20Q320 -22 298 -22Q199 -22 144 33L134 44L106 13Q83 -14 78 -18T65 -22Q52 -22 52 -14Q52 -11 110 221Q112 227 130 227H143Q149 221 149 216Q149 214 148 207T144 186T142 153Q144 114 160 87T203 47T255 29T308 24Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-4-TEX-I-1D446"></use></g></g></g></svg></mjx-container>. If this is the case, then some of the steps will change. Let’s compute <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex" xmlns="http://www.w3.org/2000/svg" width="1.459ex" height="1.645ex" role="img" focusable="false" viewBox="0 -705 645 727" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-4-TEX-I-1D446" d="M308 24Q367 24 416 76T466 197Q466 260 414 284Q308 311 278 321T236 341Q176 383 176 462Q176 523 208 573T273 648Q302 673 343 688T407 704H418H425Q521 704 564 640Q565 640 577 653T603 682T623 704Q624 704 627 704T632 705Q645 705 645 698T617 577T585 459T569 456Q549 456 549 465Q549 471 550 475Q550 478 551 494T553 520Q553 554 544 579T526 616T501 641Q465 662 419 662Q362 662 313 616T263 510Q263 480 278 458T319 427Q323 425 389 408T456 390Q490 379 522 342T554 242Q554 216 546 186Q541 164 528 137T492 78T426 18T332 -20Q320 -22 298 -22Q199 -22 144 33L134 44L106 13Q83 -14 78 -18T65 -22Q52 -22 52 -14Q52 -11 110 221Q112 227 130 227H143Q149 221 149 216Q149 214 148 207T144 186T142 153Q144 114 160 87T203 47T255 29T308 24Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-4-TEX-I-1D446"></use></g></g></g></svg></mjx-container> from the sample and let’s suppose <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex" xmlns="http://www.w3.org/2000/svg" width="1.459ex" height="1.645ex" role="img" focusable="false" viewBox="0 -705 645 727" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-4-TEX-I-1D446" d="M308 24Q367 24 416 76T466 197Q466 260 414 284Q308 311 278 321T236 341Q176 383 176 462Q176 523 208 573T273 648Q302 673 343 688T407 704H418H425Q521 704 564 640Q565 640 577 653T603 682T623 704Q624 704 627 704T632 705Q645 705 645 698T617 577T585 459T569 456Q549 456 549 465Q549 471 550 475Q550 478 551 494T553 520Q553 554 544 579T526 616T501 641Q465 662 419 662Q362 662 313 616T263 510Q263 480 278 458T319 427Q323 425 389 408T456 390Q490 379 522 342T554 242Q554 216 546 186Q541 164 528 137T492 78T426 18T332 -20Q320 -22 298 -22Q199 -22 144 33L134 44L106 13Q83 -14 78 -18T65 -22Q52 -22 52 -14Q52 -11 110 221Q112 227 130 227H143Q149 221 149 216Q149 214 148 207T144 186T142 153Q144 114 160 87T203 47T255 29T308 24Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-4-TEX-I-1D446"></use></g></g></g></svg></mjx-container> was also equal to 4 (<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex" xmlns="http://www.w3.org/2000/svg" width="1.459ex" height="1.645ex" role="img" focusable="false" viewBox="0 -705 645 727" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-4-TEX-I-1D446" d="M308 24Q367 24 416 76T466 197Q466 260 414 284Q308 311 278 321T236 341Q176 383 176 462Q176 523 208 573T273 648Q302 673 343 688T407 704H418H425Q521 704 564 640Q565 640 577 653T603 682T623 704Q624 704 627 704T632 705Q645 705 645 698T617 577T585 459T569 456Q549 456 549 465Q549 471 550 475Q550 478 551 494T553 520Q553 554 544 579T526 616T501 641Q465 662 419 662Q362 662 313 616T263 510Q263 480 278 458T319 427Q323 425 389 408T456 390Q490 379 522 342T554 242Q554 216 546 186Q541 164 528 137T492 78T426 18T332 -20Q320 -22 298 -22Q199 -22 144 33L134 44L106 13Q83 -14 78 -18T65 -22Q52 -22 52 -14Q52 -11 110 221Q112 227 130 227H143Q149 221 149 216Q149 214 148 207T144 186T142 153Q144 114 160 87T203 47T255 29T308 24Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-4-TEX-I-1D446"></use></g></g></g></svg></mjx-container> and <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="1.292ex" height="1ex" role="img" focusable="false" viewBox="0 -431 571 442" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-4-TEX-I-1D70E" d="M184 -11Q116 -11 74 34T31 147Q31 247 104 333T274 430Q275 431 414 431H552Q553 430 555 429T559 427T562 425T565 422T567 420T569 416T570 412T571 407T572 401Q572 357 507 357Q500 357 490 357T476 358H416L421 348Q439 310 439 263Q439 153 359 71T184 -11ZM361 278Q361 358 276 358Q152 358 115 184Q114 180 114 178Q106 141 106 117Q106 67 131 47T188 26Q242 26 287 73Q316 103 334 153T356 233T361 278Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-4-TEX-I-1D70E"></use></g></g></g></svg></mjx-container> don’t have to be equal).</p><p>The problem still remains the same, to calculate the probability of <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.312ex" xmlns="http://www.w3.org/2000/svg" width="7.248ex" height="2.61ex" role="img" focusable="false" viewBox="0 -1015.5 3203.6 1153.5" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-4-TEX-I-1D44B" d="M42 0H40Q26 0 26 11Q26 15 29 27Q33 41 36 43T55 46Q141 49 190 98Q200 108 306 224T411 342Q302 620 297 625Q288 636 234 637H206Q200 643 200 645T202 664Q206 677 212 683H226Q260 681 347 681Q380 681 408 681T453 682T473 682Q490 682 490 671Q490 670 488 658Q484 643 481 640T465 637Q434 634 411 620L488 426L541 485Q646 598 646 610Q646 628 622 635Q617 635 609 637Q594 637 594 648Q594 650 596 664Q600 677 606 683H618Q619 683 643 683T697 681T738 680Q828 680 837 683H845Q852 676 852 672Q850 647 840 637H824Q790 636 763 628T722 611T698 593L687 584Q687 585 592 480L505 384Q505 383 536 304T601 142T638 56Q648 47 699 46Q734 46 734 37Q734 35 732 23Q728 7 725 4T711 1Q708 1 678 1T589 2Q528 2 496 2T461 1Q444 1 444 10Q444 11 446 25Q448 35 450 39T455 44T464 46T480 47T506 54Q523 62 523 64Q522 64 476 181L429 299Q241 95 236 84Q232 76 232 72Q232 53 261 47Q262 47 267 47T273 46Q276 46 277 46T280 45T283 42T284 35Q284 26 282 19Q279 6 276 4T261 1Q258 1 243 1T201 2T142 2Q64 2 42 0Z"></path><path id="MJX-4-TEX-S4-AF" d="M69 544V590H430V544H69Z"></path><path id="MJX-4-TEX-N-2265" d="M83 616Q83 624 89 630T99 636Q107 636 253 568T543 431T687 361Q694 356 694 346T687 331Q685 329 395 192L107 56H101Q83 58 83 76Q83 77 83 79Q82 86 98 95Q117 105 248 167Q326 204 378 228L626 346L360 472Q291 505 200 548Q112 589 98 597T83 616ZM84 -118Q84 -108 99 -98H678Q694 -104 694 -118Q694 -130 679 -138H98Q84 -131 84 -118Z"></path><path id="MJX-4-TEX-N-36" d="M42 313Q42 476 123 571T303 666Q372 666 402 630T432 550Q432 525 418 510T379 495Q356 495 341 509T326 548Q326 592 373 601Q351 623 311 626Q240 626 194 566Q147 500 147 364L148 360Q153 366 156 373Q197 433 263 433H267Q313 433 348 414Q372 400 396 374T435 317Q456 268 456 210V192Q456 169 451 149Q440 90 387 34T253 -22Q225 -22 199 -14T143 16T92 75T56 172T42 313ZM257 397Q227 397 205 380T171 335T154 278T148 216Q148 133 160 97T198 39Q222 21 251 21Q302 21 329 59Q342 77 347 104T352 209Q352 289 347 316T329 361Q302 397 257 397Z"></path><path id="MJX-4-TEX-N-32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mover"><g data-mml-node="mi" transform="translate(0, 0)"><use xlink:href="#MJX-4-TEX-I-1D44B"></use></g><g data-mml-node="mo" transform="translate(18, 498.3) scale(0.707)"><svg width="1204.9" height="246" x="0" y="444" viewBox="301.2 444 1204.9 246"><use xlink:href="#MJX-4-TEX-S4-AF" transform="scale(3.615, 1)"></use></svg></g></g><g data-mml-node="mo" transform="translate(1147.8, 0)"><use xlink:href="#MJX-4-TEX-N-2265"></use></g><g data-mml-node="mn" transform="translate(2203.6, 0)"><use xlink:href="#MJX-4-TEX-N-36"></use><use xlink:href="#MJX-4-TEX-N-32" transform="translate(500, 0)"></use></g></g></g></svg></mjx-container> given <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.489ex" xmlns="http://www.w3.org/2000/svg" width="1.364ex" height="1.489ex" role="img" focusable="false" viewBox="0 -442 603 658" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-4-TEX-I-1D707" d="M58 -216Q44 -216 34 -208T23 -186Q23 -176 96 116T173 414Q186 442 219 442Q231 441 239 435T249 423T251 413Q251 401 220 279T187 142Q185 131 185 107V99Q185 26 252 26Q261 26 270 27T287 31T302 38T315 45T327 55T338 65T348 77T356 88T365 100L372 110L408 253Q444 395 448 404Q461 431 491 431Q504 431 512 424T523 412T525 402L449 84Q448 79 448 68Q448 43 455 35T476 26Q485 27 496 35Q517 55 537 131Q543 151 547 152Q549 153 557 153H561Q580 153 580 144Q580 138 575 117T555 63T523 13Q510 0 491 -8Q483 -10 467 -10Q446 -10 429 -4T402 11T385 29T376 44T374 51L368 45Q362 39 350 30T324 12T288 -4T246 -11Q199 -11 153 12L129 -85Q108 -167 104 -180T92 -202Q76 -216 58 -216Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-4-TEX-I-1D707"></use></g></g></g></svg></mjx-container> is equal to 60. But the result is no longer called z-score, instead, it’s called a t-score. Now the question will be what is the probability that a t-score is bigger than 3.</p><div style="display: flex;justify-content: center;">    <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -2.561ex" xmlns="http://www.w3.org/2000/svg" width="48.018ex" height="6.254ex" role="img" focusable="false" viewBox="0 -1632.1 21223.8 2764.2" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-4-TEX-I-1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path><path id="MJX-4-TEX-N-28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path><path id="MJX-4-TEX-I-1D44B" d="M42 0H40Q26 0 26 11Q26 15 29 27Q33 41 36 43T55 46Q141 49 190 98Q200 108 306 224T411 342Q302 620 297 625Q288 636 234 637H206Q200 643 200 645T202 664Q206 677 212 683H226Q260 681 347 681Q380 681 408 681T453 682T473 682Q490 682 490 671Q490 670 488 658Q484 643 481 640T465 637Q434 634 411 620L488 426L541 485Q646 598 646 610Q646 628 622 635Q617 635 609 637Q594 637 594 648Q594 650 596 664Q600 677 606 683H618Q619 683 643 683T697 681T738 680Q828 680 837 683H845Q852 676 852 672Q850 647 840 637H824Q790 636 763 628T722 611T698 593L687 584Q687 585 592 480L505 384Q505 383 536 304T601 142T638 56Q648 47 699 46Q734 46 734 37Q734 35 732 23Q728 7 725 4T711 1Q708 1 678 1T589 2Q528 2 496 2T461 1Q444 1 444 10Q444 11 446 25Q448 35 450 39T455 44T464 46T480 47T506 54Q523 62 523 64Q522 64 476 181L429 299Q241 95 236 84Q232 76 232 72Q232 53 261 47Q262 47 267 47T273 46Q276 46 277 46T280 45T283 42T284 35Q284 26 282 19Q279 6 276 4T261 1Q258 1 243 1T201 2T142 2Q64 2 42 0Z"></path><path id="MJX-4-TEX-S4-AF" d="M69 544V590H430V544H69Z"></path><path id="MJX-4-TEX-N-2265" d="M83 616Q83 624 89 630T99 636Q107 636 253 568T543 431T687 361Q694 356 694 346T687 331Q685 329 395 192L107 56H101Q83 58 83 76Q83 77 83 79Q82 86 98 95Q117 105 248 167Q326 204 378 228L626 346L360 472Q291 505 200 548Q112 589 98 597T83 616ZM84 -118Q84 -108 99 -98H678Q694 -104 694 -118Q694 -130 679 -138H98Q84 -131 84 -118Z"></path><path id="MJX-4-TEX-N-36" d="M42 313Q42 476 123 571T303 666Q372 666 402 630T432 550Q432 525 418 510T379 495Q356 495 341 509T326 548Q326 592 373 601Q351 623 311 626Q240 626 194 566Q147 500 147 364L148 360Q153 366 156 373Q197 433 263 433H267Q313 433 348 414Q372 400 396 374T435 317Q456 268 456 210V192Q456 169 451 149Q440 90 387 34T253 -22Q225 -22 199 -14T143 16T92 75T56 172T42 313ZM257 397Q227 397 205 380T171 335T154 278T148 216Q148 133 160 97T198 39Q222 21 251 21Q302 21 329 59Q342 77 347 104T352 209Q352 289 347 316T329 361Q302 397 257 397Z"></path><path id="MJX-4-TEX-N-32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path><path id="MJX-4-TEX-N-7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path><path id="MJX-4-TEX-I-1D707" d="M58 -216Q44 -216 34 -208T23 -186Q23 -176 96 116T173 414Q186 442 219 442Q231 441 239 435T249 423T251 413Q251 401 220 279T187 142Q185 131 185 107V99Q185 26 252 26Q261 26 270 27T287 31T302 38T315 45T327 55T338 65T348 77T356 88T365 100L372 110L408 253Q444 395 448 404Q461 431 491 431Q504 431 512 424T523 412T525 402L449 84Q448 79 448 68Q448 43 455 35T476 26Q485 27 496 35Q517 55 537 131Q543 151 547 152Q549 153 557 153H561Q580 153 580 144Q580 138 575 117T555 63T523 13Q510 0 491 -8Q483 -10 467 -10Q446 -10 429 -4T402 11T385 29T376 44T374 51L368 45Q362 39 350 30T324 12T288 -4T246 -11Q199 -11 153 12L129 -85Q108 -167 104 -180T92 -202Q76 -216 58 -216Z"></path><path id="MJX-4-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-4-TEX-N-30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path><path id="MJX-4-TEX-N-29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path><path id="MJX-4-TEX-I-1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path><path id="MJX-4-TEX-N-2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path><path id="MJX-4-TEX-N-34" d="M462 0Q444 3 333 3Q217 3 199 0H190V46H221Q241 46 248 46T265 48T279 53T286 61Q287 63 287 115V165H28V211L179 442Q332 674 334 675Q336 677 355 677H373L379 671V211H471V165H379V114Q379 73 379 66T385 54Q393 47 442 46H471V0H462ZM293 211V545L74 212L183 211H293Z"></path><path id="MJX-4-TEX-N-221A" d="M95 178Q89 178 81 186T72 200T103 230T169 280T207 309Q209 311 212 311H213Q219 311 227 294T281 177Q300 134 312 108L397 -77Q398 -77 501 136T707 565T814 786Q820 800 834 800Q841 800 846 794T853 782V776L620 293L385 -193Q381 -200 366 -200Q357 -200 354 -197Q352 -195 256 15L160 225L144 214Q129 202 113 190T95 178Z"></path><path id="MJX-4-TEX-N-33" d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mtable"><g data-mml-node="mtr" transform="translate(0, 290.1)"><g data-mml-node="mtd"><g data-mml-node="mi"><use xlink:href="#MJX-4-TEX-I-1D443"></use></g><g data-mml-node="mo" transform="translate(751, 0)"><use xlink:href="#MJX-4-TEX-N-28"></use></g><g data-mml-node="mover" transform="translate(1140, 0)"><g data-mml-node="mi" transform="translate(0, 0)"><use xlink:href="#MJX-4-TEX-I-1D44B"></use></g><g data-mml-node="mo" transform="translate(18, 498.3) scale(0.707)"><svg width="1204.9" height="246" x="0" y="444" viewBox="301.2 444 1204.9 246"><use xlink:href="#MJX-4-TEX-S4-AF" transform="scale(3.615, 1)"></use></svg></g></g><g data-mml-node="mo" transform="translate(2287.8, 0)"><use xlink:href="#MJX-4-TEX-N-2265"></use></g><g data-mml-node="mn" transform="translate(3343.6, 0)"><use xlink:href="#MJX-4-TEX-N-36"></use><use xlink:href="#MJX-4-TEX-N-32" transform="translate(500, 0)"></use></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(4343.6, 0)"><g data-mml-node="mo"><use xlink:href="#MJX-4-TEX-N-7C"></use></g></g><g data-mml-node="mi" transform="translate(4621.6, 0)"><use xlink:href="#MJX-4-TEX-I-1D707"></use></g><g data-mml-node="mo" transform="translate(5502.3, 0)"><use xlink:href="#MJX-4-TEX-N-3D"></use></g><g data-mml-node="mn" transform="translate(6558.1, 0)"><use xlink:href="#MJX-4-TEX-N-36"></use><use xlink:href="#MJX-4-TEX-N-30" transform="translate(500, 0)"></use></g><g data-mml-node="mo" transform="translate(7558.1, 0)"><use xlink:href="#MJX-4-TEX-N-29"></use></g></g><g data-mml-node="mtd" transform="translate(7947.1, 0)"><g data-mml-node="mi"></g><g data-mml-node="mo" transform="translate(277.8, 0)"><use xlink:href="#MJX-4-TEX-N-3D"></use></g><g data-mml-node="mi" transform="translate(1333.6, 0)"><use xlink:href="#MJX-4-TEX-I-1D443"></use></g><g data-mml-node="mo" transform="translate(2084.6, 0)"><use xlink:href="#MJX-4-TEX-N-28"></use></g><g data-mml-node="mi" transform="translate(2473.6, 0)"><use xlink:href="#MJX-4-TEX-I-1D461"></use></g><g data-mml-node="mo" transform="translate(3112.3, 0)"><use xlink:href="#MJX-4-TEX-N-2265"></use></g><g data-mml-node="mfrac" transform="translate(4168.1, 0)"><g data-mml-node="mrow" transform="translate(220, 676)"><g data-mml-node="mn"><use xlink:href="#MJX-4-TEX-N-36"></use><use xlink:href="#MJX-4-TEX-N-32" transform="translate(500, 0)"></use></g><g data-mml-node="mo" transform="translate(1222.2, 0)"><use xlink:href="#MJX-4-TEX-N-2212"></use></g><g data-mml-node="mn" transform="translate(2222.4, 0)"><use xlink:href="#MJX-4-TEX-N-36"></use><use xlink:href="#MJX-4-TEX-N-30" transform="translate(500, 0)"></use></g></g><g data-mml-node="mfrac" transform="translate(956.1, -832.7)"><g data-mml-node="mn" transform="translate(698.4, 394) scale(0.707)"><use xlink:href="#MJX-4-TEX-N-34"></use></g><g data-mml-node="msqrt" transform="translate(220, -511.4) scale(0.707)"><g transform="translate(853, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-4-TEX-N-33"></use><use xlink:href="#MJX-4-TEX-N-36" transform="translate(500, 0)"></use></g></g><g data-mml-node="mo" transform="translate(0, 107.1)"><use xlink:href="#MJX-4-TEX-N-221A"></use></g><rect width="1000" height="42.4" x="853" y="864.6"></rect></g><rect width="1510.3" height="60" x="120" y="220"></rect></g><rect width="3422.4" height="60" x="120" y="220"></rect></g><g data-mml-node="mo" transform="translate(7830.6, 0)"><use xlink:href="#MJX-4-TEX-N-29"></use></g><g data-mml-node="mo" transform="translate(8497.3, 0)"><use xlink:href="#MJX-4-TEX-N-3D"></use></g><g data-mml-node="mi" transform="translate(9553.1, 0)"><use xlink:href="#MJX-4-TEX-I-1D443"></use></g><g data-mml-node="mo" transform="translate(10304.1, 0)"><use xlink:href="#MJX-4-TEX-N-28"></use></g><g data-mml-node="mi" transform="translate(10693.1, 0)"><use xlink:href="#MJX-4-TEX-I-1D461"></use></g><g data-mml-node="mo" transform="translate(11331.9, 0)"><use xlink:href="#MJX-4-TEX-N-2265"></use></g><g data-mml-node="mn" transform="translate(12387.7, 0)"><use xlink:href="#MJX-4-TEX-N-33"></use></g><g data-mml-node="mo" transform="translate(12887.7, 0)"><use xlink:href="#MJX-4-TEX-N-29"></use></g></g></g></g></g></g></svg></mjx-container></div><p>Right now, t distribution graph is very similar to z distribution graph, but the actual shape depends on the sample size. I will show you how a t distribution graph look like in the below figure.</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> scipy<span class="token punctuation">.</span>stats <span class="token keyword">import</span> tplt<span class="token punctuation">.</span>figure<span class="token punctuation">(</span>figsize<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">15</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">for</span> df <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>np<span class="token punctuation">.</span>linspace<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">100</span><span class="token punctuation">)</span><span class="token punctuation">,</span>      t<span class="token punctuation">.</span>pdf<span class="token punctuation">(</span>np<span class="token punctuation">.</span>linspace<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">100</span><span class="token punctuation">)</span><span class="token punctuation">,</span> df<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>      label<span class="token operator">=</span><span class="token string-interpolation"><span class="token string">f"degree=</span><span class="token interpolation"><span class="token punctuation">&#123;</span>df<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">&#125;</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>legend<span class="token punctuation">(</span>loc<span class="token operator">=</span><span class="token string">"upper right"</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>grid<span class="token punctuation">(</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><img src="/Hexo-Blog/2021/03/12/2021-03-12-easiest-explanation-for-p-value/t-distribution.png" class=""><p>We got as a refresher that we will read the t-value, or the area above 3 with 35 degrees of freedom (<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.186ex" xmlns="http://www.w3.org/2000/svg" width="11.439ex" height="1.692ex" role="img" focusable="false" viewBox="0 -666 5056 748" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-4-TEX-N-33" d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z"></path><path id="MJX-4-TEX-N-36" d="M42 313Q42 476 123 571T303 666Q372 666 402 630T432 550Q432 525 418 510T379 495Q356 495 341 509T326 548Q326 592 373 601Q351 623 311 626Q240 626 194 566Q147 500 147 364L148 360Q153 366 156 373Q197 433 263 433H267Q313 433 348 414Q372 400 396 374T435 317Q456 268 456 210V192Q456 169 451 149Q440 90 387 34T253 -22Q225 -22 199 -14T143 16T92 75T56 172T42 313ZM257 397Q227 397 205 380T171 335T154 278T148 216Q148 133 160 97T198 39Q222 21 251 21Q302 21 329 59Q342 77 347 104T352 209Q352 289 347 316T329 361Q302 397 257 397Z"></path><path id="MJX-4-TEX-N-2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path><path id="MJX-4-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-4-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-4-TEX-N-35" d="M164 157Q164 133 148 117T109 101H102Q148 22 224 22Q294 22 326 82Q345 115 345 210Q345 313 318 349Q292 382 260 382H254Q176 382 136 314Q132 307 129 306T114 304Q97 304 95 310Q93 314 93 485V614Q93 664 98 664Q100 666 102 666Q103 666 123 658T178 642T253 634Q324 634 389 662Q397 666 402 666Q410 666 410 648V635Q328 538 205 538Q174 538 149 544L139 546V374Q158 388 169 396T205 412T256 420Q337 420 393 355T449 201Q449 109 385 44T229 -22Q148 -22 99 32T50 154Q50 178 61 192T84 210T107 214Q132 214 148 197T164 157Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mn"><use xlink:href="#MJX-4-TEX-N-33"></use><use xlink:href="#MJX-4-TEX-N-36" transform="translate(500, 0)"></use></g><g data-mml-node="mo" transform="translate(1222.2, 0)"><use xlink:href="#MJX-4-TEX-N-2212"></use></g><g data-mml-node="mn" transform="translate(2222.4, 0)"><use xlink:href="#MJX-4-TEX-N-31"></use></g><g data-mml-node="mo" transform="translate(3000.2, 0)"><use xlink:href="#MJX-4-TEX-N-3D"></use></g><g data-mml-node="mn" transform="translate(4056, 0)"><use xlink:href="#MJX-4-TEX-N-33"></use><use xlink:href="#MJX-4-TEX-N-35" transform="translate(500, 0)"></use></g></g></g></svg></mjx-container>).</p><p>The probability density function for t distribution is:</p><div style="display: flex;justify-content: center;">    <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -1.539ex" xmlns="http://www.w3.org/2000/svg" width="29.938ex" height="4.25ex" role="img" focusable="false" viewBox="0 -1198.4 13232.4 1878.4" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-4-TEX-I-1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"></path><path id="MJX-4-TEX-N-28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path><path id="MJX-4-TEX-I-1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path><path id="MJX-4-TEX-N-2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path><path id="MJX-4-TEX-I-1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-4-TEX-N-29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path><path id="MJX-4-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-4-TEX-N-393" d="M128 619Q121 626 117 628T101 631T58 634H25V680H554V676Q556 670 568 560T582 444V440H542V444Q542 445 538 478T523 545T492 598Q454 634 349 634H334Q264 634 249 633T233 621Q232 618 232 339L233 61Q240 54 245 52T270 48T333 46H360V0H348Q324 3 182 3Q51 3 36 0H25V46H58Q100 47 109 49T128 61V619Z"></path><path id="MJX-4-TEX-N-2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path><path id="MJX-4-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-4-TEX-N-32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path><path id="MJX-4-TEX-N-221A" d="M95 178Q89 178 81 186T72 200T103 230T169 280T207 309Q209 311 212 311H213Q219 311 227 294T281 177Q300 134 312 108L397 -77Q398 -77 501 136T707 565T814 786Q820 800 834 800Q841 800 846 794T853 782V776L620 293L385 -193Q381 -200 366 -200Q357 -200 354 -197Q352 -195 256 15L160 225L144 214Q129 202 113 190T95 178Z"></path><path id="MJX-4-TEX-I-1D70B" d="M132 -11Q98 -11 98 22V33L111 61Q186 219 220 334L228 358H196Q158 358 142 355T103 336Q92 329 81 318T62 297T53 285Q51 284 38 284Q19 284 19 294Q19 300 38 329T93 391T164 429Q171 431 389 431Q549 431 553 430Q573 423 573 402Q573 371 541 360Q535 358 472 358H408L405 341Q393 269 393 222Q393 170 402 129T421 65T431 37Q431 20 417 5T381 -10Q370 -10 363 -7T347 17T331 77Q330 86 330 121Q330 170 339 226T357 318T367 358H269L268 354Q268 351 249 275T206 114T175 17Q164 -11 132 -11Z"></path><path id="MJX-4-TEX-N-2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-4-TEX-I-1D453"></use></g><g data-mml-node="mo" transform="translate(550, 0)"><use xlink:href="#MJX-4-TEX-N-28"></use></g><g data-mml-node="mi" transform="translate(939, 0)"><use xlink:href="#MJX-4-TEX-I-1D465"></use></g><g data-mml-node="mo" transform="translate(1511, 0)"><use xlink:href="#MJX-4-TEX-N-2C"></use></g><g data-mml-node="mi" transform="translate(1955.7, 0)"><use xlink:href="#MJX-4-TEX-I-1D45B"></use></g><g data-mml-node="mo" transform="translate(2555.7, 0)"><use xlink:href="#MJX-4-TEX-N-29"></use></g><g data-mml-node="mo" transform="translate(3222.4, 0)"><use xlink:href="#MJX-4-TEX-N-3D"></use></g><g data-mml-node="mfrac" transform="translate(4278.2, 0)"><g data-mml-node="mrow" transform="translate(615.7, 584) scale(0.707)"><g data-mml-node="mi"><use xlink:href="#MJX-4-TEX-N-393"></use></g><g data-mml-node="mo" transform="translate(625, 0)"><use xlink:href="#MJX-4-TEX-N-28"></use></g><g data-mml-node="mfrac" transform="translate(1014, 0)"><g data-mml-node="mrow" transform="translate(220, 398) scale(0.707)"><g data-mml-node="mi"><use xlink:href="#MJX-4-TEX-I-1D45B"></use></g><g data-mml-node="mo" transform="translate(600, 0)"><use xlink:href="#MJX-4-TEX-N-2B"></use></g><g data-mml-node="mn" transform="translate(1378, 0)"><use xlink:href="#MJX-4-TEX-N-31"></use></g></g><g data-mml-node="mn" transform="translate(707.2, -345) scale(0.707)"><use xlink:href="#MJX-4-TEX-N-32"></use></g><rect width="1527.9" height="60" x="120" y="220"></rect></g><g data-mml-node="mo" transform="translate(2781.9, 0)"><use xlink:href="#MJX-4-TEX-N-29"></use></g></g><g data-mml-node="mrow" transform="translate(220, -436.1) scale(0.707)"><g data-mml-node="msqrt"><g transform="translate(853, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-4-TEX-I-1D70B"></use></g><g data-mml-node="mi" transform="translate(570, 0)"><use xlink:href="#MJX-4-TEX-I-1D45B"></use></g></g><g data-mml-node="mo" transform="translate(0, 0.6)"><use xlink:href="#MJX-4-TEX-N-221A"></use></g><rect width="1170" height="42.4" x="853" y="758.1"></rect></g><g data-mml-node="mi" transform="translate(2023, 0)"><use xlink:href="#MJX-4-TEX-N-393"></use></g><g data-mml-node="mo" transform="translate(2648, 0)"><use xlink:href="#MJX-4-TEX-N-28"></use></g><g data-mml-node="mfrac" transform="translate(3037, 0)"><g data-mml-node="mi" transform="translate(220, 394) scale(0.707)"><use xlink:href="#MJX-4-TEX-I-1D45B"></use></g><g data-mml-node="mn" transform="translate(255.4, -345) scale(0.707)"><use xlink:href="#MJX-4-TEX-N-32"></use></g><rect width="624.3" height="60" x="120" y="220"></rect></g><g data-mml-node="mo" transform="translate(3901.3, 0)"><use xlink:href="#MJX-4-TEX-N-29"></use></g></g><rect width="3233.7" height="60" x="120" y="220"></rect></g><g data-mml-node="mo" transform="translate(7751.9, 0)"><use xlink:href="#MJX-4-TEX-N-28"></use></g><g data-mml-node="mn" transform="translate(8140.9, 0)"><use xlink:href="#MJX-4-TEX-N-31"></use></g><g data-mml-node="mo" transform="translate(8863.1, 0)"><use xlink:href="#MJX-4-TEX-N-2B"></use></g><g data-mml-node="mfrac" transform="translate(9863.3, 0)"><g data-mml-node="msup" transform="translate(220, 394) scale(0.707)"><g data-mml-node="mi"><use xlink:href="#MJX-4-TEX-I-1D465"></use></g><g data-mml-node="mn" transform="translate(572, 363) scale(0.707)"><use xlink:href="#MJX-4-TEX-N-32"></use></g></g><g data-mml-node="mi" transform="translate(352.8, -345) scale(0.707)"><use xlink:href="#MJX-4-TEX-I-1D45B"></use></g><rect width="889.8" height="60" x="120" y="220"></rect></g><g data-mml-node="msup" transform="translate(10993.2, 0)"><g data-mml-node="mo"><use xlink:href="#MJX-4-TEX-N-29"></use></g><g data-mml-node="TeXAtom" transform="translate(389, 363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mo"><use xlink:href="#MJX-4-TEX-N-2212"></use></g><g data-mml-node="mfrac" transform="translate(778, 0)"><g data-mml-node="mrow" transform="translate(220, 398) scale(0.707)"><g data-mml-node="mi"><use xlink:href="#MJX-4-TEX-I-1D45B"></use></g><g data-mml-node="mo" transform="translate(600, 0)"><use xlink:href="#MJX-4-TEX-N-2B"></use></g><g data-mml-node="mn" transform="translate(1378, 0)"><use xlink:href="#MJX-4-TEX-N-31"></use></g></g><g data-mml-node="mn" transform="translate(707.2, -345) scale(0.707)"><use xlink:href="#MJX-4-TEX-N-32"></use></g><rect width="1527.9" height="60" x="120" y="220"></rect></g></g></g></g></g></svg></mjx-container></div><p>where <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="1.294ex" height="1.025ex" role="img" focusable="false" viewBox="0 -442 572 453" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-4-TEX-I-1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-4-TEX-I-1D465"></use></g></g></g></svg></mjx-container> is a real number and the degrees of freedom parameter <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="1.357ex" height="1.025ex" role="img" focusable="false" viewBox="0 -442 600 453" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-4-TEX-I-1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-4-TEX-I-1D45B"></use></g></g></g></svg></mjx-container> satisfies <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.09ex" xmlns="http://www.w3.org/2000/svg" width="5.506ex" height="1.597ex" role="img" focusable="false" viewBox="0 -666 2433.6 706" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-4-TEX-I-1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-4-TEX-N-3E" d="M84 520Q84 528 88 533T96 539L99 540Q106 540 253 471T544 334L687 265Q694 260 694 250T687 235Q685 233 395 96L107 -40H101Q83 -38 83 -20Q83 -19 83 -17Q82 -10 98 -1Q117 9 248 71Q326 108 378 132L626 250L378 368Q90 504 86 509Q84 513 84 520Z"></path><path id="MJX-4-TEX-N-30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-4-TEX-I-1D45B"></use></g><g data-mml-node="mo" transform="translate(877.8, 0)"><use xlink:href="#MJX-4-TEX-N-3E"></use></g><g data-mml-node="mn" transform="translate(1933.6, 0)"><use xlink:href="#MJX-4-TEX-N-30"></use></g></g></g></svg></mjx-container>. <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0" xmlns="http://www.w3.org/2000/svg" width="1.414ex" height="1.538ex" role="img" focusable="false" viewBox="0 -680 625 680" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-4-TEX-N-393" d="M128 619Q121 626 117 628T101 631T58 634H25V680H554V676Q556 670 568 560T582 444V440H542V444Q542 445 538 478T523 545T492 598Q454 634 349 634H334Q264 634 249 633T233 621Q232 618 232 339L233 61Q240 54 245 52T270 48T333 46H360V0H348Q324 3 182 3Q51 3 36 0H25V46H58Q100 47 109 49T128 61V619Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-4-TEX-N-393"></use></g></g></g></svg></mjx-container> is the gamma function.</p><p>Let’s calculate the area under t distribution to the right of 3.</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">percentile<span class="token punctuation">,</span> _ <span class="token operator">=</span> quad<span class="token punctuation">(</span>t<span class="token punctuation">.</span>pdf<span class="token punctuation">,</span> <span class="token number">3.0</span><span class="token punctuation">,</span> np<span class="token punctuation">.</span>Inf<span class="token punctuation">,</span> args<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">35</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>This area turns out to be 0.2474%, and this probability is lower than alpha 5%. So our decision will still be the same, what we have observed under null hypothesis is highly unlikely to happen, this means that our assumption that the null hypothesis is correct is most likely to be false, so the null hypothesis should be rejected.</p><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>Hypothesis testing is important not just in data science, but in every field. In this post, we know how to calculate the p-value by hand and also by using Python. Happy learning! Cheers!</p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ol><li><a href="https://www.analyticsvidhya.com/blog/2020/07/hypothesis-testing-68351/">https://www.analyticsvidhya.com/blog/2020/07/hypothesis-testing-68351/</a></li><li><a href="https://www.machinelearningplus.com/statistics/what-is-p-value/">https://www.machinelearningplus.com/statistics/what-is-p-value/</a></li><li><a href="https://www.youtube.com/watch?v=kx0xLnqJ_30&amp;list=PLCZeVeoafktVGu9rvM9PHrAdrsUURtLTo&amp;index=1&amp;t=9s&amp;ab_channel=ChandChauhan">https://www.youtube.com/watch?v=kx0xLnqJ_30&amp;list=PLCZeVeoafktVGu9rvM9PHrAdrsUURtLTo&amp;index=1&amp;t=9s&amp;ab_channel=ChandChauhan</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Statistics </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> Statistics </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Train Word Embedding Vectors on Custom Corpus</title>
      <link href="/Hexo-Blog/2021/03/02/2021-03-02-train-word-embedding-vectos-on-custom-corpus/"/>
      <url>/Hexo-Blog/2021/03/02/2021-03-02-train-word-embedding-vectos-on-custom-corpus/</url>
      
        <content type="html"><![CDATA[<p>When I was doing my dissertation project, I found out that the performance of model wasn’t quite well. I believe it’s because the domain of pre-trained GoogleNews-vectors-negative300 is different from the the dataset of mine. Hence, I decide to pre-train a word2vec model by myself. In this article, I’ll use a library called “Koan” released by Bloomberg LP. They build CBOW model using C++, which is more efficiently compared to <a href="https://github.com/tmikolov/word2vec/">word2vec</a> and <a href="https://github.com/RaRe-Technologies/gensim/">gensim</a> libraries. If you are a Windows user, and you don’t have a Linux system in your computer, please read this <a href="/Hexo-Blog/2021/01/22/2021-01-22-train-word2vec-on-wsl/" title="[article]">[article]</a> I wrote before to set up your WSL.</p><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>The reason we care about language is that, because of language, we are able to turn invisible ideas into visible actions. However, language is ambiguous at all levels: lexical, phrasal, semantic. To address this, we need to build a language model, which can convert text into vectors. The most common techniques are Bag of Words (One-Hot Encoding, TF-IDF), Distributional Word Embedding (Word2Vec, GloVe, FastText), and Contextualised Word Embedding (ELMo, BERT). In this article, I’m gonna implement Word2Vec to generate pre-trained vectors.</p><h1 id="Word2Vec"><a href="#Word2Vec" class="headerlink" title="Word2Vec"></a>Word2Vec</h1><p>Word2Vec is a statistical-based method to obtain word vectors, and it is proposed by Tomas Mikolov et al. [4] of Google in 2013. Word2Vec is available in two flavors, the CBoW model and the Skip-Gram model, which is based on neural networks which can map words to low dimensional space. CBoW model predicts the current word by context, and Skip-Gram model predicts context by current word.</p><h2 id="Text-Pre-processing"><a href="#Text-Pre-processing" class="headerlink" title="Text Pre-processing"></a>Text Pre-processing</h2><p>First, you need to read in your csv file containing texts.</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">df <span class="token operator">=</span> pd<span class="token punctuation">.</span>read_csv<span class="token punctuation">(</span><span class="token string">r"./20061020_20131126_bloomberg_news.csv"</span><span class="token punctuation">)</span>df<span class="token punctuation">[</span><span class="token string">"title"</span><span class="token punctuation">]</span> <span class="token operator">=</span> df<span class="token punctuation">[</span><span class="token string">"title"</span><span class="token punctuation">]</span><span class="token punctuation">.</span><span class="token builtin">apply</span><span class="token punctuation">(</span><span class="token builtin">str</span><span class="token punctuation">)</span>df<span class="token punctuation">[</span><span class="token string">"paragraph"</span><span class="token punctuation">]</span> <span class="token operator">=</span> df<span class="token punctuation">[</span><span class="token string">"paragraph"</span><span class="token punctuation">]</span><span class="token punctuation">.</span><span class="token builtin">apply</span><span class="token punctuation">(</span><span class="token builtin">str</span><span class="token punctuation">)</span>df<span class="token punctuation">.</span>sample<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><hr><table><thead><tr><th align="center"></th><th align="center">title</th><th align="center">timestamp</th><th align="center">paragraph</th></tr></thead><tbody><tr><td align="center">6493</td><td align="center">Coronavirus: Malaysia’s Economy Shows Doing th…</td><td align="center">2020/8/23</td><td align="center">Strict lockdowns, accommodative central banks,…</td></tr><tr><td align="center">1833</td><td align="center">Lower Rates: Trump and the Markets Picked Thei…</td><td align="center">2019/8/7</td><td align="center">Collapsing bond yields aren’t exactly a sign …</td></tr><tr><td align="center">4376</td><td align="center">Crypto Brokerage Tagomi Gets $12 Million in Se…</td><td align="center">2019/3/4</td><td align="center">Tagomi Holdings Inc., a digital asset brokerag…</td></tr></tbody></table><p>Second, put them into a list.</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">documents <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>documents<span class="token punctuation">.</span>extend<span class="token punctuation">(</span>df<span class="token punctuation">.</span>loc<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token string">"title"</span><span class="token punctuation">,</span> <span class="token string">"paragraph"</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">.</span>values<span class="token punctuation">.</span>flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>tolist<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>Third, do some text cleaning work.</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">regex</span><span class="token punctuation">(</span>text<span class="token punctuation">)</span><span class="token punctuation">:</span>    text <span class="token operator">=</span> re<span class="token punctuation">.</span>sub<span class="token punctuation">(</span><span class="token string">r"([^a-zA-Z0-9\.\?\,\!\%\']+)"</span><span class="token punctuation">,</span> <span class="token string">" "</span><span class="token punctuation">,</span> text<span class="token punctuation">)</span>    text <span class="token operator">=</span> re<span class="token punctuation">.</span>sub<span class="token punctuation">(</span><span class="token string">r"(?&lt;=\d),(?=\d)+"</span><span class="token punctuation">,</span> <span class="token string">""</span><span class="token punctuation">,</span> text<span class="token punctuation">)</span>    text <span class="token operator">=</span> re<span class="token punctuation">.</span>sub<span class="token punctuation">(</span><span class="token string">r"\,"</span><span class="token punctuation">,</span> <span class="token string">" , "</span><span class="token punctuation">,</span> text<span class="token punctuation">)</span>    text <span class="token operator">=</span> re<span class="token punctuation">.</span>sub<span class="token punctuation">(</span><span class="token string">r"\?"</span><span class="token punctuation">,</span> <span class="token string">" ? "</span><span class="token punctuation">,</span> text<span class="token punctuation">)</span>    text <span class="token operator">=</span> re<span class="token punctuation">.</span>sub<span class="token punctuation">(</span><span class="token string">r"\!"</span><span class="token punctuation">,</span> <span class="token string">" ! "</span><span class="token punctuation">,</span> text<span class="token punctuation">)</span>    text <span class="token operator">=</span> re<span class="token punctuation">.</span>sub<span class="token punctuation">(</span><span class="token string">r"\."</span><span class="token punctuation">,</span> <span class="token string">" . "</span><span class="token punctuation">,</span> text<span class="token punctuation">)</span>    text <span class="token operator">=</span> re<span class="token punctuation">.</span>sub<span class="token punctuation">(</span><span class="token string">r"  "</span><span class="token punctuation">,</span> <span class="token string">" "</span><span class="token punctuation">,</span> text<span class="token punctuation">)</span>    text <span class="token operator">=</span> text<span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> textdocs <span class="token operator">=</span> <span class="token punctuation">[</span>regex<span class="token punctuation">(</span>doc<span class="token punctuation">)</span> <span class="token keyword">for</span> doc <span class="token keyword">in</span> documents<span class="token punctuation">]</span>docs_cased <span class="token operator">=</span> <span class="token punctuation">[</span>regex<span class="token punctuation">(</span>doc<span class="token punctuation">.</span>lower<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token keyword">for</span> doc <span class="token keyword">in</span> documents<span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="Tokenisation"><a href="#Tokenisation" class="headerlink" title="Tokenisation"></a>Tokenisation</h2><p>You’ll need to prepare your corpus as a single text file with all words separated by one or more spaces or tabs.</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">progressbar</span><span class="token punctuation">(</span><span class="token builtin">iter</span><span class="token punctuation">,</span> prefix<span class="token operator">=</span><span class="token string">""</span><span class="token punctuation">,</span> size<span class="token operator">=</span><span class="token number">50</span><span class="token punctuation">,</span> <span class="token builtin">file</span><span class="token operator">=</span>sys<span class="token punctuation">.</span>stdout<span class="token punctuation">)</span><span class="token punctuation">:</span>    count <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span><span class="token builtin">iter</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">show</span><span class="token punctuation">(</span>t<span class="token punctuation">)</span><span class="token punctuation">:</span>        x <span class="token operator">=</span> <span class="token builtin">int</span><span class="token punctuation">(</span>size<span class="token operator">*</span>t<span class="token operator">/</span>count<span class="token punctuation">)</span>        <span class="token comment"># file.write("%s[%s%s] %i/%i\r" % (prefix, "#"*x, "."*(size-x), int(100*t/count), 100))</span>        <span class="token builtin">file</span><span class="token punctuation">.</span>write<span class="token punctuation">(</span><span class="token string">"&#123;&#125;[&#123;&#125;&#123;&#125;] &#123;&#125;%\r"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>prefix<span class="token punctuation">,</span> <span class="token string">"█"</span><span class="token operator">*</span>x<span class="token punctuation">,</span> <span class="token string">"."</span><span class="token operator">*</span><span class="token punctuation">(</span>size<span class="token operator">-</span>x<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token builtin">int</span><span class="token punctuation">(</span><span class="token number">100</span><span class="token operator">*</span>t<span class="token operator">/</span>count<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token builtin">file</span><span class="token punctuation">.</span>flush<span class="token punctuation">(</span><span class="token punctuation">)</span>    show<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>    <span class="token keyword">for</span> i<span class="token punctuation">,</span> item <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span><span class="token builtin">iter</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">yield</span> item        show<span class="token punctuation">(</span>i<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span>    <span class="token builtin">file</span><span class="token punctuation">.</span>write<span class="token punctuation">(</span><span class="token string">"\n"</span><span class="token punctuation">)</span>    <span class="token builtin">file</span><span class="token punctuation">.</span>flush<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">class</span> <span class="token class-name">Tokenizer</span><span class="token punctuation">(</span><span class="token builtin">object</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>                  char_level<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>                  num_tokens<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>                  pad_token<span class="token operator">=</span><span class="token string">'&lt;PAD>'</span><span class="token punctuation">,</span>                  oov_token<span class="token operator">=</span><span class="token string">'&lt;UNK>'</span><span class="token punctuation">,</span>                  token_to_index<span class="token operator">=</span><span class="token boolean">None</span>                <span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>char_level <span class="token operator">=</span> char_level        self<span class="token punctuation">.</span>separator <span class="token operator">=</span> <span class="token string">''</span> <span class="token keyword">if</span> self<span class="token punctuation">.</span>char_level <span class="token keyword">else</span> <span class="token string">' '</span>        <span class="token comment"># &lt;PAD> + &lt;UNK> tokens</span>        <span class="token keyword">if</span> num_tokens<span class="token punctuation">:</span> num_tokens <span class="token operator">-=</span> <span class="token number">2</span>        self<span class="token punctuation">.</span>num_tokens <span class="token operator">=</span> num_tokens        self<span class="token punctuation">.</span>oov_token <span class="token operator">=</span> oov_token        <span class="token keyword">if</span> <span class="token keyword">not</span> token_to_index<span class="token punctuation">:</span>            token_to_index <span class="token operator">=</span> <span class="token punctuation">&#123;</span><span class="token string">'&lt;PAD>'</span><span class="token punctuation">:</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token string">'&lt;UNK>'</span><span class="token punctuation">:</span> <span class="token number">1</span><span class="token punctuation">&#125;</span>        self<span class="token punctuation">.</span>token_to_index <span class="token operator">=</span> token_to_index        self<span class="token punctuation">.</span>index_to_token <span class="token operator">=</span> <span class="token punctuation">&#123;</span>v<span class="token punctuation">:</span> k <span class="token keyword">for</span> k<span class="token punctuation">,</span> v <span class="token keyword">in</span> self<span class="token punctuation">.</span>token_to_index<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">&#125;</span>    <span class="token keyword">def</span> <span class="token function">__len__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> <span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>token_to_index<span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">__str__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> <span class="token string-interpolation"><span class="token string">f"&lt;Tokenizer(num_tokens=</span><span class="token interpolation"><span class="token punctuation">&#123;</span><span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">&#125;</span></span><span class="token string">)>"</span></span>    <span class="token keyword">def</span> <span class="token function">fit_on_texts</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> texts<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">if</span> self<span class="token punctuation">.</span>char_level<span class="token punctuation">:</span>            all_tokens <span class="token operator">=</span> <span class="token punctuation">[</span>token <span class="token keyword">for</span> text <span class="token keyword">in</span> texts <span class="token keyword">for</span> token <span class="token keyword">in</span> text<span class="token punctuation">]</span>        <span class="token keyword">if</span> <span class="token keyword">not</span> self<span class="token punctuation">.</span>char_level<span class="token punctuation">:</span>            all_tokens <span class="token operator">=</span> <span class="token punctuation">[</span>token <span class="token keyword">for</span> text <span class="token keyword">in</span> texts <span class="token keyword">for</span> token <span class="token keyword">in</span> text<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">' '</span><span class="token punctuation">)</span><span class="token punctuation">]</span>        counts <span class="token operator">=</span> Counter<span class="token punctuation">(</span>all_tokens<span class="token punctuation">)</span><span class="token punctuation">.</span>most_common<span class="token punctuation">(</span>self<span class="token punctuation">.</span>num_tokens<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>min_token_freq <span class="token operator">=</span> counts<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>        <span class="token keyword">for</span> token<span class="token punctuation">,</span> count <span class="token keyword">in</span> progressbar<span class="token punctuation">(</span>counts<span class="token punctuation">,</span> prefix<span class="token operator">=</span><span class="token string">"VOCAB"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            index <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span>            self<span class="token punctuation">.</span>token_to_index<span class="token punctuation">[</span>token<span class="token punctuation">]</span> <span class="token operator">=</span> index            self<span class="token punctuation">.</span>index_to_token<span class="token punctuation">[</span>index<span class="token punctuation">]</span> <span class="token operator">=</span> token        <span class="token keyword">return</span> self    <span class="token keyword">def</span> <span class="token function">texts_to_sequences</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> texts<span class="token punctuation">)</span><span class="token punctuation">:</span>        sequences <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>        <span class="token keyword">for</span> text <span class="token keyword">in</span> progressbar<span class="token punctuation">(</span>texts<span class="token punctuation">,</span> prefix<span class="token operator">=</span><span class="token string">"TEXT2SEQ"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token keyword">if</span> <span class="token keyword">not</span> self<span class="token punctuation">.</span>char_level<span class="token punctuation">:</span>                text <span class="token operator">=</span> text<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">' '</span><span class="token punctuation">)</span>            sequence <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>            <span class="token keyword">for</span> token <span class="token keyword">in</span> text<span class="token punctuation">:</span>                sequence<span class="token punctuation">.</span>append<span class="token punctuation">(</span>self<span class="token punctuation">.</span>token_to_index<span class="token punctuation">.</span>get<span class="token punctuation">(</span>                    token<span class="token punctuation">,</span> self<span class="token punctuation">.</span>token_to_index<span class="token punctuation">[</span>self<span class="token punctuation">.</span>oov_token<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>            sequences<span class="token punctuation">.</span>append<span class="token punctuation">(</span>sequence<span class="token punctuation">)</span>        <span class="token keyword">return</span> sequences    <span class="token keyword">def</span> <span class="token function">sequences_to_texts</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> sequences<span class="token punctuation">)</span><span class="token punctuation">:</span>        texts <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>        <span class="token keyword">for</span> sequence <span class="token keyword">in</span> progressbar<span class="token punctuation">(</span>sequences<span class="token punctuation">,</span> prefix<span class="token operator">=</span><span class="token string">"SEQ2TEXT"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            text <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>            <span class="token keyword">for</span> index <span class="token keyword">in</span> sequence<span class="token punctuation">:</span>                text<span class="token punctuation">.</span>append<span class="token punctuation">(</span>self<span class="token punctuation">.</span>index_to_token<span class="token punctuation">.</span>get<span class="token punctuation">(</span>index<span class="token punctuation">,</span> self<span class="token punctuation">.</span>oov_token<span class="token punctuation">)</span><span class="token punctuation">)</span>            texts<span class="token punctuation">.</span>append<span class="token punctuation">(</span>self<span class="token punctuation">.</span>separator<span class="token punctuation">.</span>join<span class="token punctuation">(</span><span class="token punctuation">[</span>token <span class="token keyword">for</span> token <span class="token keyword">in</span> text<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> texts    <span class="token keyword">def</span> <span class="token function">save</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> fp<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">with</span> <span class="token builtin">open</span><span class="token punctuation">(</span>fp<span class="token punctuation">,</span> <span class="token string">'w'</span><span class="token punctuation">)</span> <span class="token keyword">as</span> fp<span class="token punctuation">:</span>            contents <span class="token operator">=</span> <span class="token punctuation">&#123;</span>                <span class="token string">'char_level'</span><span class="token punctuation">:</span> self<span class="token punctuation">.</span>char_level<span class="token punctuation">,</span>                <span class="token string">'oov_token'</span><span class="token punctuation">:</span> self<span class="token punctuation">.</span>oov_token<span class="token punctuation">,</span>                <span class="token string">'token_to_index'</span><span class="token punctuation">:</span> self<span class="token punctuation">.</span>token_to_index            <span class="token punctuation">&#125;</span>            json<span class="token punctuation">.</span>dump<span class="token punctuation">(</span>contents<span class="token punctuation">,</span> fp<span class="token punctuation">,</span> indent<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">,</span> sort_keys<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>    <span class="token decorator annotation punctuation">@classmethod</span>    <span class="token keyword">def</span> <span class="token function">load</span><span class="token punctuation">(</span>cls<span class="token punctuation">,</span> fp<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">with</span> <span class="token builtin">open</span><span class="token punctuation">(</span>fp<span class="token punctuation">,</span> <span class="token string">'r'</span><span class="token punctuation">)</span> <span class="token keyword">as</span> fp<span class="token punctuation">:</span>            kwargs <span class="token operator">=</span> json<span class="token punctuation">.</span>load<span class="token punctuation">(</span>fp<span class="token operator">=</span>fp<span class="token punctuation">)</span>        <span class="token keyword">return</span> cls<span class="token punctuation">(</span><span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><pre class="line-numbers language-python" data-language="python"><code class="language-python">tokeniser <span class="token operator">=</span> Tokenizer<span class="token punctuation">(</span>char_level<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> num_tokens<span class="token operator">=</span><span class="token number">1000000</span><span class="token punctuation">)</span>tokeniser<span class="token punctuation">.</span>fit_on_texts<span class="token punctuation">(</span>docs_cased<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span>sequences <span class="token operator">=</span> tokeniser<span class="token punctuation">.</span>texts_to_sequences<span class="token punctuation">(</span>docs_cased<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span>texts <span class="token operator">=</span> tokeniser<span class="token punctuation">.</span>sequences_to_texts<span class="token punctuation">(</span>sequences<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>sequences<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">:</span><span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"="</span><span class="token operator">*</span><span class="token number">50</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>texts<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">:</span><span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><pre class="line-numbers language-console" data-language="console"><code class="language-console">[[21789, 358380, 4, 37272, 4, 61540, 358381, 5009, 1964, 5, 2902, 37914], [21789, 4, 37272, 4, 61540, 9, 1133, 34, 1299, 3, 122, 577, 10, 123, 6313, 1253, 294, 8, 547, 11, 25, 304, 2], [7233, 80031, 1117, 546, 47, 9039, 6, 39, 2225, 7, 29623], [328, 19, 1338, 16712, 6, 126, 179, 2, 305, 241, 14, 11689, 606, 2848, 3368, 4, 3, 1166, 1794, 19, 552, 4, 32651, 34, 259, 4, 2902, 577, 10, 2514, 1352, 8, 252, 2, 9, 596, 13, 18410, 4, 850, 606, 3, 7233, 80031, 2], [304, 6076, 3389, 19, 6, 4488, 90, 1037, 488]]&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;[&#39;ethereum xet , xrp , litecoin xlc cryptocurrency alternative to bitcoin btc&#39;, &#39;ethereum , xrp , litecoin and others are giving the world ? s most famous digital currency a run for its money .&#39;, &#39;crypto opportunists create 500 more coins in new phase of mania&#39;, &#39;risk is running rampant in financial markets . stocks trade at dot come era valuations , the ipo pipeline is full , spacs are back , bitcoin ? s headed toward a record . and right on cue , here come the crypto opportunists .&#39;, &#39;money stuff exxon is in trouble over climate change&#39;]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>After tokenised our corpus, save it to a <code>news.tokens</code> file.</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">with</span> <span class="token builtin">open</span><span class="token punctuation">(</span><span class="token string">'./news.tokens'</span><span class="token punctuation">,</span> <span class="token string">'w'</span><span class="token punctuation">)</span> <span class="token keyword">as</span> f<span class="token punctuation">:</span>    <span class="token keyword">for</span> item <span class="token keyword">in</span> texts<span class="token punctuation">:</span>        f<span class="token punctuation">.</span>write<span class="token punctuation">(</span><span class="token string">"%s\n"</span> <span class="token operator">%</span> item<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h2 id="Training-Process"><a href="#Training-Process" class="headerlink" title="Training Process"></a>Training Process</h2><p>Word2vec is a two-layer neural net that processes text by “vectorizing” words. Its input is a text corpus and its output is a set of vectors: feature vectors that represent words in that corpus.</p><h3 id="CBOW"><a href="#CBOW" class="headerlink" title="CBOW"></a>CBOW</h3><p>Move your <code>news.tokens</code> file to WSL folder. In my case, it is at <code>C:\Users\yangwang\AppData\Local\Packages\CanonicalGroupLimited.Ubuntu18.04onWindows_79rhkp1fndgsc\LocalState\rootfs\home\yang\</code>.</p><p>Next, open your mobaxterm and execute the following code.</p><img src="/Hexo-Blog/2021/03/02/2021-03-02-train-word-embedding-vectos-on-custom-corpus/mobaxterm.jpg" class=""><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token function">sudo</span> ./build/koan -V <span class="token number">1000000</span> <span class="token punctuation">\</span>             --epochs <span class="token number">10</span> <span class="token punctuation">\</span>             --dim <span class="token number">300</span> <span class="token punctuation">\</span>             --negatives <span class="token number">5</span> <span class="token punctuation">\</span>             --context-size <span class="token number">5</span> <span class="token punctuation">\</span>             -l <span class="token number">0.075</span> <span class="token punctuation">\</span>             --threads <span class="token number">16</span> <span class="token punctuation">\</span>             --cbow <span class="token boolean">true</span> <span class="token punctuation">\</span>             --min-count <span class="token number">2</span> <span class="token punctuation">\</span>             --file ./news.tokens<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>Learned embeddings will be saved to <code>embeddings_$&#123;CURRENT_TIMESTAMP&#125;.txt</code> in the present working directory.</p><h3 id="Skip-Gram"><a href="#Skip-Gram" class="headerlink" title="Skip-Gram"></a>Skip-Gram</h3><p>Similarly, you can get the pre-trained vectors by Skip-Gram, just set <code>cbow</code> to <code>false</code>.</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token function">sudo</span>  ./build/koan -V <span class="token number">1000000</span> <span class="token punctuation">\</span>             --epochs <span class="token number">10</span> <span class="token punctuation">\</span>             --dim <span class="token number">300</span> <span class="token punctuation">\</span>             --negatives <span class="token number">5</span> <span class="token punctuation">\</span>             --context-size <span class="token number">5</span> <span class="token punctuation">\</span>             -l <span class="token number">0.075</span> <span class="token punctuation">\</span>             --threads <span class="token number">16</span> <span class="token punctuation">\</span>             --cbow <span class="token boolean">false</span> <span class="token punctuation">\</span>             --min-count <span class="token number">2</span> <span class="token punctuation">\</span>             --file ./news.tokens<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="Convert-GloVe-Format-to-Word2Vec-Format"><a href="#Convert-GloVe-Format-to-Word2Vec-Format" class="headerlink" title="Convert GloVe Format to Word2Vec Format"></a>Convert GloVe Format to Word2Vec Format</h2><p>Move your pre-trained vectors back to your Windows folder, and change your file name to <code>news-cbow-negative300.txt</code> (or <code>news-skipgram-negative300.txt</code>, depend on how you trained it). We then convert GloVe vectors format into the word2vec format.</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> gensim<span class="token punctuation">.</span>models <span class="token keyword">import</span> KeyedVectors<span class="token keyword">from</span> gensim<span class="token punctuation">.</span>scripts<span class="token punctuation">.</span>glove2word2vec <span class="token keyword">import</span> glove2word2vec_ <span class="token operator">=</span> glove2word2vec<span class="token punctuation">(</span><span class="token string">"./news-cbow-negative300.txt"</span><span class="token punctuation">,</span> <span class="token string">"./news-word2vec-cbow-negative300.txt"</span><span class="token punctuation">)</span>wv_from_text <span class="token operator">=</span> KeyedVectors<span class="token punctuation">.</span>load_word2vec_format<span class="token punctuation">(</span><span class="token string">"./news-word2vec-cbow-negative300.txt"</span><span class="token punctuation">,</span> binary<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>Notes</strong></p><p>GloVe format (a real example can be found on the <a href="https://nlp.stanford.edu/projects/glove/">Stanford site</a>)</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">word1 <span class="token number">0.123</span> <span class="token number">0.134</span> <span class="token number">0.532</span> <span class="token number">0.152</span>word2 <span class="token number">0.934</span> <span class="token number">0.412</span> <span class="token number">0.532</span> <span class="token number">0.159</span>word3 <span class="token number">0.334</span> <span class="token number">0.241</span> <span class="token number">0.324</span> <span class="token number">0.188</span><span class="token punctuation">..</span>.word9 <span class="token number">0.334</span> <span class="token number">0.241</span> <span class="token number">0.324</span> <span class="token number">0.188</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>Word2Vec format (a real example can be found in the <a href="https://code.google.com/archive/p/word2vec/">old w2v repository</a>).</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token number">9</span> <span class="token number">4</span>word1 <span class="token number">0.123</span> <span class="token number">0.134</span> <span class="token number">0.532</span> <span class="token number">0.152</span>word2 <span class="token number">0.934</span> <span class="token number">0.412</span> <span class="token number">0.532</span> <span class="token number">0.159</span>word3 <span class="token number">0.334</span> <span class="token number">0.241</span> <span class="token number">0.324</span> <span class="token number">0.188</span><span class="token punctuation">..</span>.word9 <span class="token number">0.334</span> <span class="token number">0.241</span> <span class="token number">0.324</span> <span class="token number">0.188</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>Voilà! You have successfully got a pre-trained word embedding!</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">wv_from_text<span class="token punctuation">.</span>similar_by_word<span class="token punctuation">(</span><span class="token string">"bitcoin"</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><hr><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token string">'cryptocurrency'</span>, <span class="token number">0.7397603392601013</span><span class="token punctuation">)</span>, <span class="token punctuation">(</span><span class="token string">'cryptocurrencies'</span>, <span class="token number">0.7099655866622925</span><span class="token punctuation">)</span>, <span class="token punctuation">(</span><span class="token string">'crypto'</span>, <span class="token number">0.6509920358657837</span><span class="token punctuation">)</span>, <span class="token punctuation">(</span><span class="token string">'xrp'</span>, <span class="token number">0.5511361360549927</span><span class="token punctuation">)</span>, <span class="token punctuation">(</span><span class="token string">'ethereum'</span>, <span class="token number">0.547865629196167</span><span class="token punctuation">)</span>, <span class="token punctuation">(</span><span class="token string">'monero'</span>, <span class="token number">0.5345401167869568</span><span class="token punctuation">)</span>, <span class="token punctuation">(</span><span class="token string">"bitcoin's"</span>, <span class="token number">0.5305401086807251</span><span class="token punctuation">)</span>, <span class="token punctuation">(</span><span class="token string">'bitcoins'</span>, <span class="token number">0.5253546237945557</span><span class="token punctuation">)</span>, <span class="token punctuation">(</span><span class="token string">'gold'</span>, <span class="token number">0.5229815244674683</span><span class="token punctuation">)</span>, <span class="token punctuation">(</span><span class="token string">'blockchain'</span>, <span class="token number">0.508536159992218</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="Train-GloVe-on-WSL"><a href="#Train-GloVe-on-WSL" class="headerlink" title="Train GloVe on WSL"></a>Train GloVe on WSL</h1><p>GloVe (Global Vectors for Word Representation) is an alternate method to create word embeddings. It is based on matrix factorization techniques on the word-context matrix.</p><h2 id="Download-GloVe"><a href="#Download-GloVe" class="headerlink" title="Download GloVe"></a>Download GloVe</h2><p>Download <a href="https://github.com/stanfordnlp/GloVe">GloVe</a> library from Standford’s GitHub</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token function">git</span> clone https://github.com/stanfordnlp/glove<span class="token builtin class-name">cd</span> glove <span class="token operator">&amp;&amp;</span> <span class="token function">make</span>./demo.sh<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h2 id="Training-Process-1"><a href="#Training-Process-1" class="headerlink" title="Training Process"></a>Training Process</h2><p>This is how you run the model:</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token function">git</span> clone http://github.com/stanfordnlp/glove<span class="token builtin class-name">cd</span> glove <span class="token operator">&amp;&amp;</span> <span class="token function">make</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>To train it on your own corpus, you just have to make changes to one file, that is <code>demo.sh</code>.</p><p>Remove the script from <code>if</code> to <code>fi</code> after <code>make</code>. Replace the <code>CORPUS</code> name with your file name, in our case, <code>news.tokens</code> There is another if loop at the end of file <code>demo.sh</code>.</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token keyword">if</span> <span class="token punctuation">[</span> <span class="token string">"<span class="token variable">$CORPUS</span>"</span> <span class="token operator">=</span> <span class="token string">'text8'</span> <span class="token punctuation">]</span><span class="token punctuation">;</span> <span class="token keyword">then</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>Replace <code>text8</code> with <code>news.tokens</code>.</p><p>Run the <code>demo.sh</code> once the changes are made.</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ ./demo.sh<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>Don’t forget to keep your corpus file directly inside the Glove folder. Make sure your corpus file is in the correct format.You’ll need to prepare your corpus as a single text file with all words separated by one or more spaces or tabs. If your corpus has multiple documents, the documents (only) should be separated by new line characters.</p><p>Sometimes, you have trouble with running <code>./demo.sh</code>. When you use <code>./demo.sh</code>, you’ll get <code>sudo: demo.sh: command not found</code>.</p><p>Here’s a summary of how to troubleshoot the <strong>Permission Denied error</strong> in our case.</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ <span class="token function">ls</span> -l demo.sh <span class="token comment"># Check file permissions of demo.sh</span>---------- <span class="token number">1</span> yang yang <span class="token number">0</span> <span class="token number">2039</span>-10-21 <span class="token number">14</span>:47 foo.sh     ^^^  ^^^ <span class="token operator">|</span> ^^^   ^^^^ ^^^^  <span class="token operator">|</span>  <span class="token operator">|</span>  <span class="token operator">|</span>      <span class="token operator">|</span>    <span class="token operator">|</span> Owner<span class="token operator">|</span> World   <span class="token operator">|</span>    <span class="token operator">|</span>     <span class="token operator">|</span>         <span class="token operator">|</span>  Name of   Group       <span class="token operator">|</span>   Group            Name of              Owner<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>Owner has no read and write access <code>rw</code> and the <code>-</code> indicates that the executable permission is missing. The <code>chmod</code> command fixes that. (Group and other only have read permission set on the file, they cannot write to it or execute it).</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token function">chmod</span> +x demo.sh<span class="token function">chmod</span> +r demo.sh<span class="token function">chmod</span> +w demo.sh<span class="token function">ls</span> -l demo.sh<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p><code>demo.sh</code> is now executable as far as Linux is concerned.</p><img src="/Hexo-Blog/2021/03/02/2021-03-02-train-word-embedding-vectos-on-custom-corpus/train-glove.jpg" class=""><h2 id="Convert-GloVe-Format-to-Word2Vec-Format-1"><a href="#Convert-GloVe-Format-to-Word2Vec-Format-1" class="headerlink" title="Convert GloVe Format to Word2Vec Format"></a>Convert GloVe Format to Word2Vec Format</h2><pre class="line-numbers language-python" data-language="python"><code class="language-python">_ <span class="token operator">=</span> glove2word2vec<span class="token punctuation">(</span><span class="token string">"./news-glove-vectors300.txt"</span><span class="token punctuation">,</span> <span class="token string">"./news-glove-w2vformat-vectors300.txt"</span><span class="token punctuation">)</span>wv_glove <span class="token operator">=</span> KeyedVectors<span class="token punctuation">.</span>load_word2vec_format<span class="token punctuation">(</span><span class="token string">"./news-glove-w2vformat-vectors300.txt"</span><span class="token punctuation">,</span> binary<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>Test it on ‘bitcoin’ token.</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">wv_glove<span class="token punctuation">.</span>similar_by_word<span class="token punctuation">(</span><span class="token string">"bitcoin"</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><hr><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token string">'cryptocurrency'</span>, <span class="token number">0.7422985434532166</span><span class="token punctuation">)</span>, <span class="token punctuation">(</span><span class="token string">'cryptocurrencies'</span>, <span class="token number">0.6949392557144165</span><span class="token punctuation">)</span>, <span class="token punctuation">(</span><span class="token string">'crypto'</span>, <span class="token number">0.6679537296295166</span><span class="token punctuation">)</span>, <span class="token punctuation">(</span><span class="token string">'blockchain'</span>, <span class="token number">0.5640972852706909</span><span class="token punctuation">)</span>, <span class="token punctuation">(</span><span class="token string">'bitcoins'</span>, <span class="token number">0.4695727825164795</span><span class="token punctuation">)</span>, <span class="token punctuation">(</span><span class="token string">'ethereum'</span>, <span class="token number">0.4689256548881531</span><span class="token punctuation">)</span>, <span class="token punctuation">(</span><span class="token string">'ether'</span>, <span class="token number">0.4526808261871338</span><span class="token punctuation">)</span>, <span class="token punctuation">(</span><span class="token string">'virtual'</span>, <span class="token number">0.43389463424682617</span><span class="token punctuation">)</span>, <span class="token punctuation">(</span><span class="token string">'tokens'</span>, <span class="token number">0.42009514570236206</span><span class="token punctuation">)</span>, <span class="token punctuation">(</span><span class="token string">'coins'</span>, <span class="token number">0.418658971786499</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="Train-FastText-on-WSL"><a href="#Train-FastText-on-WSL" class="headerlink" title="Train FastText on WSL"></a>Train FastText on WSL</h1><p>FastText is a library for efficient learning of word representations and sentence classification. FastText builds on modern Mac OS and Linux distributions. Since it uses C++11 features, it requires a compiler with good C++11 support.</p><h2 id="Download-FastText"><a href="#Download-FastText" class="headerlink" title="Download FastText"></a>Download FastText</h2><p>Install FastText.</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token function">wget</span> https://github.com/facebookresearch/fastText/archive/v0.9.2.zip<span class="token function">unzip</span> v0.9.2.zip<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>And move to the FastText directory and build it.</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token builtin class-name">cd</span> fastText-0.9.2<span class="token function">make</span>pip <span class="token function">install</span> <span class="token builtin class-name">.</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><h2 id="Training-Process-2"><a href="#Training-Process-2" class="headerlink" title="Training Process"></a>Training Process</h2><p>Training word vectors using skipgram: </p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token function">sudo</span> ./fasttext skipgram -input news.tokens -output news-fasttext-skipgram-vectors300 -minn <span class="token number">3</span> -maxn <span class="token number">6</span> -dim <span class="token number">300</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>Training word vectors using cbow:</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">sdudo ./fasttext cbow -input news.tokens -output news-fasttext-cbow-vectors300 -minn <span class="token number">3</span> -maxn <span class="token number">6</span> -dim <span class="token number">300</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>where <code>news.tokens</code> is the training file containing UTF-8 encoded text we used before. By default the word vectors will take into account character n-grams from 3 to 6 characters. At the end of optimization the program will save two files: <code>skipgram-model.bin</code> and <code>cbow-model.vec</code>. <code>model.vec</code> is a text file containing the word vectors, one per line. <code>model.bin</code> is a binary file containing the parameters of the model along with the dictionary and all hyper parameters. The binary file can be used later to compute word vectors or to restart the optimization.</p><p>Instead of training through command line, you can also train it using Python.</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> fasttextmodel <span class="token operator">=</span> fasttext<span class="token punctuation">.</span>train_unsupervised<span class="token punctuation">(</span><span class="token string">'news.tokens'</span><span class="token punctuation">,</span>                                     <span class="token string">"cbow"</span><span class="token punctuation">,</span>                                     minn<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span>                                     maxn<span class="token operator">=</span><span class="token number">6</span><span class="token punctuation">,</span>                                     dim<span class="token operator">=</span><span class="token number">300</span><span class="token punctuation">,</span>                                     epoch<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">,</span>                                     lr<span class="token operator">=</span><span class="token number">0.05</span><span class="token punctuation">,</span>                                     thread<span class="token operator">=</span><span class="token number">12</span><span class="token punctuation">)</span>model<span class="token punctuation">.</span>save_model<span class="token punctuation">(</span><span class="token string">"news-fasttext-cbow-vectors300.bin"</span><span class="token punctuation">)</span>wv_fasttext_cbow <span class="token operator">=</span> fasttext<span class="token punctuation">.</span>load_model<span class="token punctuation">(</span><span class="token string">"news-fasttext-cbow-vectors300.bin"</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>Depending on the quantity of data you have, you may want to change the parameters of the training. The <code>epoch</code> parameter controls how many times the model will loop over your data. By default, we loop over the dataset 5 times. If you dataset is extremely massive, you may want to loop over it less often. Another important parameter is the learning rate <code>-lr</code>. The higher the learning rate is, the faster the model converge to a solution but at the risk of overfitting to the dataset. The default value is <code>0.05</code> which is a good compromise. If you want to play with it we suggest to stay in the range of [0.01, 1]. Finally , fastText is multi-threaded and uses 12 threads by default. If you have less CPU cores (say 4), you can easily set the number of threads using the thread flag.</p><h2 id="Printing-Word-Vectors"><a href="#Printing-Word-Vectors" class="headerlink" title="Printing Word Vectors"></a>Printing Word Vectors</h2><pre class="line-numbers language-python" data-language="python"><code class="language-python">wv_fasttext_cbow<span class="token punctuation">.</span>get_word_vector<span class="token punctuation">(</span><span class="token string">"bitcoin"</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><hr><pre class="line-numbers language-console" data-language="console"><code class="language-console">array([-4.72412445e-02,  2.85789132e-01,  3.42660360e-02,  2.09649026e-01,       -4.54065323e-01, -1.91382036e-01, -5.00535131e-01,  1.86818153e-01,        3.03504705e-01, -1.97448403e-01,  1.50050864e-01,  6.53051957e-02,       -7.71196038e-02, -8.81627798e-02,  3.74232829e-02,  1.92417011e-01,        3.55105817e-01,  3.28541487e-01, -3.44138265e-01, -4.90421832e-01,       -2.13972241e-01,  1.74339145e-01, -3.67868505e-02,  1.09374836e-01,        3.75284493e-01,  1.03113867e-01, -1.45857438e-01, -3.04340214e-01,       -2.54121244e-01,  1.69611394e-01, -2.09063217e-01,  2.09711909e-01,       -1.41518816e-01,  1.25664864e-02,  3.95129383e-01, -1.39495045e-01,        8.94690026e-03,  4.83614445e-01,  7.68003613e-02, -1.72020838e-01,        2.65787989e-01,  6.64022043e-02,  1.34228259e-01,  4.24850464e-01,        5.29484272e-01,  7.14946613e-02, -1.55057460e-01,  6.64764345e-02,       -1.79950804e-01,  2.07342580e-02, -5.48851252e-01,  2.00532869e-01,        2.39266697e-02, -3.15076023e-01,  1.58537552e-01, -1.75947800e-01,       -4.23456818e-01,  2.27220535e-01, -1.18757211e-01, -1.85626462e-01,        2.09006771e-01, -1.08534403e-01,  2.79801786e-01, -1.84326231e-01,        3.45385611e-01,  2.19469175e-01, -1.65827513e-01, -9.27144065e-02,       -9.44910273e-02,  4.01960224e-01,  2.21235991e-01, -2.24734709e-01,        5.92879727e-02,  3.68174642e-01, -1.62111774e-01, -3.60321164e-01,       -3.73723418e-01, -2.35717162e-01, -4.61407304e-01, -1.32908091e-01,        6.76851049e-02,  2.14217320e-01, -4.72074896e-01,  1.62981063e-01,        3.71879905e-01,  1.01424217e-01, -2.97889352e-01, -3.91066521e-01,       -2.46688813e-01,  5.42590201e-01, -1.35109276e-01,  3.26993912e-01,        2.32391551e-01,  2.00287759e-01, -1.49581164e-01, -2.75721133e-01,        4.79313314e-01,  2.26864532e-01, -1.83264613e-02,  1.18657842e-01,        1.28447264e-01, -3.34220439e-01,  2.69317508e-01, -2.59843171e-01,        3.10199022e-01,  2.16098920e-01, -1.86288506e-01,  5.94185330e-02,       -4.23078507e-01,  5.34226038e-02,  2.08673358e-01, -1.05236337e-01,        3.77959639e-01, -1.97113946e-01,  3.33479345e-01,  3.94979984e-01,        1.35598034e-01,  7.51101971e-03,  2.95481265e-01, -2.15200692e-01,        2.40353987e-01,  3.65436196e-01, -1.55092150e-01,  1.55085281e-01,       -4.16599452e-01, -3.74957502e-01, -8.32035206e-03, -7.39385858e-02,        2.17583347e-02, -3.48901063e-01, -9.27907787e-03,  1.24386065e-01,        7.21558109e-02, -5.65859616e-01,  2.39448603e-02, -6.12365842e-01,       -3.45480561e-01,  6.63597524e-01, -5.31071126e-01, -3.11197668e-01,       -2.66234726e-01,  4.01567996e-01,  7.12649003e-02,  2.27668926e-01,        3.60199302e-01,  1.40796080e-01, -1.30780600e-02, -4.35646117e-01,       -3.15058351e-01,  1.79761440e-01, -7.38127008e-02, -1.57344565e-01,       -1.30275175e-01, -2.29776427e-01, -3.11963826e-01,  2.51461089e-01,       -7.77154416e-02, -1.93161428e-01, -1.22963764e-01,  1.19474560e-01,       -1.70210376e-02, -6.77634845e-04,  7.12327287e-03, -2.26126343e-01,        2.12814316e-01,  1.10432744e-01, -3.75197530e-01, -2.51778066e-01,        2.61254579e-01, -1.91191047e-01,  1.73024654e-01, -1.69590712e-01,        1.13725312e-01, -4.02675480e-01, -7.49008298e-01, -4.75077957e-01,        4.30675596e-03, -5.70537090e-01, -3.68678004e-01, -1.18338585e-01,        1.02712013e-01,  1.67967491e-02,  5.66727901e-03,  5.40452838e-01,        4.11487877e-01,  6.39163136e-01,  4.11166042e-01, -2.50596225e-01,       -1.04347736e-01, -2.55890310e-01,  1.25067562e-01,  3.32301527e-01,        1.40600502e-01, -2.42391825e-01, -1.40091211e-01, -2.05069736e-01,       -5.73189482e-02,  2.14646116e-01, -2.63260067e-01,  2.00784519e-01,        2.35700160e-01,  3.53334904e-01,  5.38006604e-01,  1.59950554e-01,        1.52627319e-01, -2.47434601e-01, -6.53754920e-02, -1.69809297e-01,       -2.81990021e-01, -4.69022483e-01, -1.67136639e-01,  2.62764134e-02,       -1.31334037e-01,  5.59901476e-01, -1.58817634e-01, -3.86552542e-01,       -3.78590643e-01,  1.53091252e-01,  1.59801438e-01,  3.00560832e-01,        9.51611772e-02, -1.25739768e-01, -2.82772869e-01, -2.11738721e-01,       -1.44721761e-01,  3.01432371e-01, -2.95276958e-02, -4.21232760e-01,        1.95821151e-01, -1.03478849e-01,  3.75818871e-02,  7.30549470e-02,       -1.24263890e-01,  4.21253517e-02,  5.34670353e-02, -6.04710579e-02,        4.18751776e-01, -1.89714432e-01,  7.75871202e-02,  2.64797509e-01,        6.84403598e-01, -2.88427889e-01,  2.65219778e-01, -9.75028351e-02,       -2.16612965e-01, -1.84845805e-01,  3.57705653e-01,  1.84521660e-01,       -2.25650191e-01, -2.41775334e-01,  6.35201484e-02,  1.05721205e-01,       -2.76269794e-01,  7.44905397e-02, -4.05652225e-01, -3.25192034e-01,        1.33607000e-01, -2.70021617e-01, -5.09377658e-01,  8.15921091e-03,        1.39862090e-01,  2.68142492e-01,  3.83002162e-01,  1.91613629e-01,        2.66971558e-01, -2.08550826e-01, -1.84474185e-01,  2.28107542e-01,       -1.41805783e-01, -3.34146500e-01,  5.33484481e-02,  1.27584279e-01,        8.07003453e-02,  1.00570947e-01, -4.74314131e-02,  2.64507622e-01,        5.04497468e-01,  8.56446847e-02,  4.17862684e-01,  1.42475590e-01,       -1.79341078e-01, -2.17798918e-01,  8.03667828e-02, -1.44884512e-01,       -2.44018864e-02, -7.17387274e-02,  8.83749798e-02,  1.36670202e-01,       -1.49312671e-02, -4.16279852e-01,  1.23666152e-01,  4.03715611e-01,        3.15533012e-01,  2.58996665e-01, -2.77972668e-01,  1.68511316e-01,        1.92251951e-01,  1.12253219e-01, -4.47139591e-01,  2.39150673e-01],      dtype&#x3D;float32)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="Nearest-neighbor-queries"><a href="#Nearest-neighbor-queries" class="headerlink" title="Nearest neighbor queries"></a>Nearest neighbor queries</h2><p>A simple way to check the quality of a word vector is to look at its nearest neighbors. This give an intuition of the type of semantic information the vectors are able to capture.</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">wv_fasttext_cbow<span class="token punctuation">.</span>get_nearest_neighbors<span class="token punctuation">(</span><span class="token string">'bitcoin'</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><hr><pre class="line-numbers language-console" data-language="console"><code class="language-console">[(0.8654916286468506, &#39;cryptocurrency&#39;), (0.8515545725822449, &#39;bitcoins&#39;), (0.8421329855918884, &#39;bitcointalk&#39;), (0.8405554890632629, &#39;cryptocurrencies&#39;), (0.8251032829284668, &#39;tcoin&#39;), (0.8214054703712463, &#39;bitcoiners&#39;), (0.8096168637275696, &quot;cryptocurrency&#39;s&quot;), (0.8051686882972717, &#39;crypto&#39;), (0.8023344278335571, &quot;bitcoin&#39;s&quot;), (0.7836618423461914, &#39;altcoin&#39;)]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>Even if the word is misspell, the fasttext model can also get the correct embedding.</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">wv_fasttext_cbow<span class="token punctuation">.</span>get_nearest_neighbors<span class="token punctuation">(</span><span class="token string">'bittcoin'</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><hr><pre class="line-numbers language-console" data-language="console"><code class="language-console">[(0.8647432923316956, &#39;tcoin&#39;), (0.8488795161247253, &#39;bitcoin&#39;), (0.8280304074287415, &#39;altcoin&#39;), (0.8253008127212524, &#39;virtcoin&#39;), (0.7866906523704529, &#39;basecoin&#39;), (0.7821307182312012, &#39;gatecoin&#39;), (0.7780086994171143, &#39;litecoin&#39;), (0.7758980989456177, &#39;estcoin&#39;), (0.7743834853172302, &#39;cryptocurrency&#39;), (0.7679258584976196, &#39;filecoin&#39;)]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>To sum up, FastText utilises subword information, while Word2Vec and GloVe don’t. The result does not make much sense when we take uncommon word like ‘weltschmerz’, most of these words are unrelated or not in the vocabulary. On the other hand, using subword information captures different variation around the word.</p><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>You can now perform various syntactic/semantic NLP word tasks with the trained vectors! Cheers!</p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> WSL </tag>
            
            <tag> Ubuntu </tag>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Twitter Hate Speech Detection</title>
      <link href="/Hexo-Blog/2021/02/07/2021-02-07-twitter-hate-speech-detection/"/>
      <url>/Hexo-Blog/2021/02/07/2021-02-07-twitter-hate-speech-detection/</url>
      
        <content type="html"><![CDATA[<p>The objective of this task is to detect hate speech in tweets. For the sake of simplicity, let’s say a tweet contains hate speech if it has a racist or sexist sentiment associated with it. So, the task is to classify racist or sexist tweets from other tweets.</p><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Hate  speech  is  an  unfortunately  common  occurrence  on  the  Internet.  Often social media sites like Facebook and Twitter face the problem of identifying and censoring  problematic  posts  while weighing the right to freedom of speech. The  importance  of  detecting  and  moderating hate  speech  is  evident  from  the  strong  connection between hate speech and actual hate crimes. Early identification of users promoting  hate  speech  could  enable  outreach  programs that attempt to prevent an escalation from speech to action. Sites such as Twitter and Facebook have been seeking  to  actively  combat  hate  speech. In spite of these reasons, NLP research on hate speech has been very limited, primarily due to the lack of a general definition of hate speech, an analysis of its demographic influences, and an investigation of the most effective features.</p><p>Formally, given a training sample of tweets and labels, where label ‘1’ denotes the tweet is racist/sexist and label ‘0’ denotes the tweet is not racist/sexist, our objective is to predict the labels on the test dataset.</p><h1 id="Import-Libraries"><a href="#Import-Libraries" class="headerlink" title="Import Libraries"></a>Import Libraries</h1><p>Let’s import the packages we need.</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> osos<span class="token punctuation">.</span>environ<span class="token punctuation">[</span><span class="token string">'CUDA_LAUNCH_BLOCKING'</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token string">"1"</span><span class="token keyword">import</span> re<span class="token keyword">import</span> string<span class="token keyword">import</span> swifter<span class="token keyword">import</span> nltk<span class="token keyword">import</span> random<span class="token keyword">import</span> tez<span class="token keyword">import</span> transformers<span class="token keyword">import</span> torch<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn<span class="token keyword">import</span> pandas <span class="token keyword">as</span> pd<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt<span class="token keyword">import</span> seaborn <span class="token keyword">as</span> sns<span class="token keyword">from</span> wordcloud <span class="token keyword">import</span> WordCloud<span class="token punctuation">,</span> STOPWORDS<span class="token keyword">from</span> transformers <span class="token keyword">import</span> AdamW<span class="token punctuation">,</span> get_linear_schedule_with_warmup<span class="token keyword">from</span> sklearn <span class="token keyword">import</span> metrics<span class="token punctuation">,</span> model_selection<span class="token punctuation">,</span> preprocessing<span class="token keyword">from</span> tez<span class="token punctuation">.</span>callbacks <span class="token keyword">import</span> EarlyStopping<span class="token keyword">from</span> tez<span class="token punctuation">.</span>callbacks <span class="token keyword">import</span> CallbackRunner<span class="token keyword">from</span> tez <span class="token keyword">import</span> enums<span class="token keyword">from</span> tez<span class="token punctuation">.</span>utils <span class="token keyword">import</span> AverageMeter<span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">import</span> Dataset<span class="token keyword">from</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">import</span> functional <span class="token keyword">as</span> F<span class="token keyword">from</span> collections <span class="token keyword">import</span> defaultdict<span class="token keyword">from</span> collections <span class="token keyword">import</span> Counter<span class="token keyword">from</span> tqdm <span class="token keyword">import</span> tqdm<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>Set the random seed, it’s useful for reproducing the issues.</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">seed_everything</span><span class="token punctuation">(</span>seed<span class="token operator">=</span><span class="token number">914</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    random<span class="token punctuation">.</span>seed<span class="token punctuation">(</span>seed<span class="token punctuation">)</span>    os<span class="token punctuation">.</span>environ<span class="token punctuation">[</span><span class="token string">'PYTHONHASHSEED'</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token builtin">str</span><span class="token punctuation">(</span>seed<span class="token punctuation">)</span>    np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>seed<span class="token punctuation">(</span>seed<span class="token punctuation">)</span>    torch<span class="token punctuation">.</span>manual_seed<span class="token punctuation">(</span>seed<span class="token punctuation">)</span>    torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>manual_seed<span class="token punctuation">(</span>seed<span class="token punctuation">)</span>    torch<span class="token punctuation">.</span>backends<span class="token punctuation">.</span>cudnn<span class="token punctuation">.</span>deterministic <span class="token operator">=</span> <span class="token boolean">True</span>seed_everything<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>Some util functions refered from <a href="https://www.kaggle.com/bminixhofer/a-validation-framework-impact-of-the-random-seed">here</a>.</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">threshold_search</span><span class="token punctuation">(</span>y_true<span class="token punctuation">,</span> y_proba<span class="token punctuation">)</span><span class="token punctuation">:</span>    best_threshold <span class="token operator">=</span> <span class="token number">0</span>    best_score <span class="token operator">=</span> <span class="token number">0</span>    <span class="token keyword">for</span> threshold <span class="token keyword">in</span> tqdm<span class="token punctuation">(</span><span class="token punctuation">[</span>i <span class="token operator">*</span> <span class="token number">0.01</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">100</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span> disable<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        score <span class="token operator">=</span> metrics<span class="token punctuation">.</span>f1_score<span class="token punctuation">(</span>y_true<span class="token operator">=</span>y_true<span class="token punctuation">,</span> y_pred<span class="token operator">=</span>y_proba <span class="token operator">></span> threshold<span class="token punctuation">)</span>        <span class="token keyword">if</span> score <span class="token operator">></span> best_score<span class="token punctuation">:</span>            best_threshold <span class="token operator">=</span> threshold            best_score <span class="token operator">=</span> score    search_result <span class="token operator">=</span> <span class="token punctuation">&#123;</span><span class="token string">'threshold'</span><span class="token punctuation">:</span> best_threshold<span class="token punctuation">,</span> <span class="token string">'f1'</span><span class="token punctuation">:</span> best_score<span class="token punctuation">&#125;</span>    <span class="token keyword">return</span> search_result<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="Configurations"><a href="#Configurations" class="headerlink" title="Configurations"></a>Configurations</h1><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">DictObj</span><span class="token punctuation">(</span><span class="token builtin">object</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> dictionary<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">import</span> pprint        self<span class="token punctuation">.</span><span class="token builtin">map</span> <span class="token operator">=</span> dictionary        pprint<span class="token punctuation">.</span>pprint<span class="token punctuation">(</span>dictionary<span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">__setattr__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> name<span class="token punctuation">,</span> value<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">if</span> name <span class="token operator">==</span> <span class="token string">'map'</span><span class="token punctuation">:</span>            <span class="token comment"># print("init set attr", name ,"value:", value)</span>            <span class="token builtin">object</span><span class="token punctuation">.</span>__setattr__<span class="token punctuation">(</span>self<span class="token punctuation">,</span> name<span class="token punctuation">,</span> value<span class="token punctuation">)</span>            <span class="token keyword">return</span>        <span class="token comment"># print('set attr called ', name, value)</span>        self<span class="token punctuation">.</span><span class="token builtin">map</span><span class="token punctuation">[</span>name<span class="token punctuation">]</span> <span class="token operator">=</span> value    <span class="token keyword">def</span> <span class="token function">__getattr__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> name<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment"># print('get attr called ', name)</span>        <span class="token keyword">return</span>  self<span class="token punctuation">.</span><span class="token builtin">map</span><span class="token punctuation">[</span>name<span class="token punctuation">]</span>PARAM <span class="token operator">=</span> DictObj<span class="token punctuation">(</span><span class="token punctuation">&#123;</span>    <span class="token string">'NUM_CLASSES'</span><span class="token punctuation">:</span> <span class="token number">2</span><span class="token punctuation">,</span>     <span class="token string">'MAX_LENGTH'</span><span class="token punctuation">:</span> <span class="token number">256</span><span class="token punctuation">,</span>     <span class="token string">'TRAIN_BATCH_SIZE'</span><span class="token punctuation">:</span> <span class="token number">8</span><span class="token punctuation">,</span>     <span class="token string">'VALID_BATCH_SIZE'</span><span class="token punctuation">:</span> <span class="token number">16</span><span class="token punctuation">,</span>     <span class="token string">'EPOCH'</span><span class="token punctuation">:</span> <span class="token number">5</span><span class="token punctuation">,</span>     <span class="token string">'DEVICE'</span><span class="token punctuation">:</span> <span class="token string">"cuda"</span><span class="token punctuation">,</span>     <span class="token string">'N_JOB'</span><span class="token punctuation">:</span> <span class="token number">0</span><span class="token punctuation">,</span>     <span class="token string">'FP16'</span><span class="token punctuation">:</span> <span class="token boolean">True</span><span class="token punctuation">,</span>     <span class="token string">'ES_PATIENCE'</span><span class="token punctuation">:</span> <span class="token number">10</span><span class="token punctuation">,</span>     <span class="token string">'TF_LOG'</span><span class="token punctuation">:</span> <span class="token string">"./logs/"</span><span class="token punctuation">,</span>     <span class="token string">'MODEL_SAVE_PATH'</span><span class="token punctuation">:</span> <span class="token string">"./models/bert.bin"</span><span class="token punctuation">&#125;</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="EDA"><a href="#EDA" class="headerlink" title="EDA"></a>EDA</h1><p>Load the data.</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">dfx <span class="token operator">=</span> pd<span class="token punctuation">.</span>read_csv<span class="token punctuation">(</span><span class="token string">"./data/train.csv"</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>Visualise the wordcloud. You can download the font from <a href="https://penguinwang96825.github.io/Hexo-Blog/download/CabinSketch-Bold.ttf">here</a>.</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">more_stopwords <span class="token operator">=</span> <span class="token punctuation">&#123;</span>    <span class="token string">'oh'</span><span class="token punctuation">,</span> <span class="token string">'will'</span><span class="token punctuation">,</span> <span class="token string">'hey'</span><span class="token punctuation">,</span> <span class="token string">'yet'</span><span class="token punctuation">,</span> <span class="token string">'ye'</span><span class="token punctuation">,</span> <span class="token string">'really'</span><span class="token punctuation">,</span>     <span class="token string">'make'</span><span class="token punctuation">,</span> <span class="token string">'amp'</span><span class="token punctuation">,</span> <span class="token string">'via'</span><span class="token punctuation">,</span> <span class="token string">'ð'</span><span class="token punctuation">,</span> <span class="token string">'¼'</span><span class="token punctuation">,</span> <span class="token string">'â'</span><span class="token punctuation">&#125;</span>STOPWORDS <span class="token operator">=</span> STOPWORDS<span class="token punctuation">.</span>union<span class="token punctuation">(</span>more_stopwords<span class="token punctuation">)</span>corpus <span class="token operator">=</span> <span class="token string">' '</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>dfx<span class="token punctuation">[</span><span class="token string">'tweet'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>no_urls_no_tags <span class="token operator">=</span> <span class="token string">" "</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span><span class="token punctuation">[</span>word <span class="token keyword">for</span> word <span class="token keyword">in</span> corpus<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span>                            <span class="token keyword">if</span> <span class="token string">'http'</span> <span class="token keyword">not</span> <span class="token keyword">in</span> word                                <span class="token keyword">and</span> <span class="token keyword">not</span> word<span class="token punctuation">.</span>startswith<span class="token punctuation">(</span><span class="token string">'@'</span><span class="token punctuation">)</span>                                <span class="token keyword">and</span> <span class="token keyword">not</span> word<span class="token punctuation">.</span>startswith<span class="token punctuation">(</span><span class="token string">'#'</span><span class="token punctuation">)</span>                                <span class="token keyword">and</span> word <span class="token operator">!=</span> <span class="token string">'RT'</span>                            <span class="token punctuation">]</span><span class="token punctuation">)</span>wordcloud <span class="token operator">=</span> WordCloud<span class="token punctuation">(</span>    font_path<span class="token operator">=</span><span class="token string">r"CabinSketch-Bold.ttf"</span><span class="token punctuation">,</span>     stopwords<span class="token operator">=</span>STOPWORDS<span class="token punctuation">,</span>    background_color<span class="token operator">=</span><span class="token string">'black'</span><span class="token punctuation">,</span>    width<span class="token operator">=</span><span class="token number">2500</span><span class="token punctuation">,</span>    height<span class="token operator">=</span><span class="token number">1400</span><span class="token punctuation">)</span><span class="token punctuation">.</span>generate<span class="token punctuation">(</span>no_urls_no_tags<span class="token punctuation">)</span>plt<span class="token punctuation">.</span>imshow<span class="token punctuation">(</span>wordcloud<span class="token punctuation">)</span>plt<span class="token punctuation">.</span>axis<span class="token punctuation">(</span><span class="token string">'off'</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><img src="/Hexo-Blog/2021/02/07/2021-02-07-twitter-hate-speech-detection/wordcloud.png" class=""><p>Clean the data.</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">nltk<span class="token punctuation">.</span>download<span class="token punctuation">(</span><span class="token string">'words'</span><span class="token punctuation">)</span>words <span class="token operator">=</span> <span class="token builtin">set</span><span class="token punctuation">(</span>nltk<span class="token punctuation">.</span>corpus<span class="token punctuation">.</span>words<span class="token punctuation">.</span>words<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">clean_tweets</span><span class="token punctuation">(</span>text<span class="token punctuation">)</span><span class="token punctuation">:</span>    text <span class="token operator">=</span> strip_links<span class="token punctuation">(</span>text<span class="token punctuation">)</span>    text <span class="token operator">=</span> strip_all_entities<span class="token punctuation">(</span>text<span class="token punctuation">)</span>    text <span class="token operator">=</span> strip_non_english_words<span class="token punctuation">(</span>text<span class="token punctuation">)</span>    <span class="token keyword">return</span> text<span class="token keyword">def</span> <span class="token function">strip_links</span><span class="token punctuation">(</span>text<span class="token punctuation">)</span><span class="token punctuation">:</span>    link_regex <span class="token operator">=</span> re<span class="token punctuation">.</span><span class="token builtin">compile</span><span class="token punctuation">(</span><span class="token string">'((https?):((//)|(\\\\))+([\w\d:#@%/;$()~_?\+-=\\\.&amp;](#!)?)*)'</span><span class="token punctuation">,</span> re<span class="token punctuation">.</span>DOTALL<span class="token punctuation">)</span>    links <span class="token operator">=</span> re<span class="token punctuation">.</span>findall<span class="token punctuation">(</span>link_regex<span class="token punctuation">,</span> text<span class="token punctuation">)</span>    <span class="token keyword">for</span> link <span class="token keyword">in</span> links<span class="token punctuation">:</span>        text <span class="token operator">=</span> text<span class="token punctuation">.</span>replace<span class="token punctuation">(</span>link<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token string">'[LINK]'</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> text<span class="token keyword">def</span> <span class="token function">strip_all_entities</span><span class="token punctuation">(</span>text<span class="token punctuation">)</span><span class="token punctuation">:</span>    entity_prefixes <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'@'</span><span class="token punctuation">,</span> <span class="token string">'#'</span><span class="token punctuation">]</span>    <span class="token keyword">for</span> separator <span class="token keyword">in</span> string<span class="token punctuation">.</span>punctuation<span class="token punctuation">.</span>replace<span class="token punctuation">(</span><span class="token string">"["</span><span class="token punctuation">,</span> <span class="token string">""</span><span class="token punctuation">)</span><span class="token punctuation">.</span>replace<span class="token punctuation">(</span><span class="token string">"]"</span><span class="token punctuation">,</span> <span class="token string">""</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">if</span> separator <span class="token keyword">not</span> <span class="token keyword">in</span> entity_prefixes<span class="token punctuation">:</span>            text <span class="token operator">=</span> text<span class="token punctuation">.</span>replace<span class="token punctuation">(</span>separator<span class="token punctuation">,</span><span class="token string">' '</span><span class="token punctuation">)</span>    words <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    <span class="token keyword">for</span> word <span class="token keyword">in</span> text<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        word <span class="token operator">=</span> word<span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">if</span> word<span class="token punctuation">:</span>            <span class="token keyword">if</span> word<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token keyword">not</span> <span class="token keyword">in</span> entity_prefixes<span class="token punctuation">:</span>                words<span class="token punctuation">.</span>append<span class="token punctuation">(</span>word<span class="token punctuation">)</span>    <span class="token keyword">return</span> <span class="token string">' '</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>words<span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">strip_non_english_words</span><span class="token punctuation">(</span>text<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">return</span> text<span class="token punctuation">.</span>encode<span class="token punctuation">(</span><span class="token string">'ascii'</span><span class="token punctuation">,</span> errors<span class="token operator">=</span><span class="token string">'ignore'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>decode<span class="token punctuation">(</span><span class="token string">"ascii"</span><span class="token punctuation">,</span> errors<span class="token operator">=</span><span class="token string">"ignore"</span><span class="token punctuation">)</span>dfx<span class="token punctuation">[</span><span class="token string">"review"</span><span class="token punctuation">]</span> <span class="token operator">=</span> dfx<span class="token punctuation">[</span><span class="token string">"tweet"</span><span class="token punctuation">]</span><span class="token punctuation">.</span>swifter<span class="token punctuation">.</span><span class="token builtin">apply</span><span class="token punctuation">(</span>clean_tweets<span class="token punctuation">)</span>dfx<span class="token punctuation">[</span><span class="token string">"length"</span><span class="token punctuation">]</span> <span class="token operator">=</span> dfx<span class="token punctuation">[</span><span class="token string">"review"</span><span class="token punctuation">]</span><span class="token punctuation">.</span>swifter<span class="token punctuation">.</span><span class="token builtin">apply</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">)</span>dfx <span class="token operator">=</span> dfx<span class="token punctuation">.</span>drop<span class="token punctuation">(</span>dfx<span class="token punctuation">[</span>dfx<span class="token punctuation">.</span>length <span class="token operator">&lt;</span> <span class="token number">10</span><span class="token punctuation">]</span><span class="token punctuation">.</span>index<span class="token punctuation">)</span>dfx <span class="token operator">=</span> dfx<span class="token punctuation">.</span>reset_index<span class="token punctuation">(</span>drop<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>Split the data.</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">df_train<span class="token punctuation">,</span> df_valid <span class="token operator">=</span> model_selection<span class="token punctuation">.</span>train_test_split<span class="token punctuation">(</span>    dfx<span class="token punctuation">,</span> test_size<span class="token operator">=</span><span class="token number">0.2</span><span class="token punctuation">,</span> random_state<span class="token operator">=</span><span class="token number">42</span><span class="token punctuation">,</span> stratify<span class="token operator">=</span>dfx<span class="token punctuation">.</span>label<span class="token punctuation">.</span>values<span class="token punctuation">)</span>df_train <span class="token operator">=</span> df_train<span class="token punctuation">.</span>reset_index<span class="token punctuation">(</span>drop<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>df_valid <span class="token operator">=</span> df_valid<span class="token punctuation">.</span>reset_index<span class="token punctuation">(</span>drop<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>Calculate weight for imbalanced dataset.</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">class_sample_count <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token builtin">len</span><span class="token punctuation">(</span>np<span class="token punctuation">.</span>where<span class="token punctuation">(</span>df_train<span class="token punctuation">.</span>label<span class="token operator">==</span>t<span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token keyword">for</span> t <span class="token keyword">in</span> np<span class="token punctuation">.</span>unique<span class="token punctuation">(</span>df_train<span class="token punctuation">.</span>label<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>weight <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">.</span> <span class="token operator">/</span> class_sample_count<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><img src="/Hexo-Blog/2021/02/07/2021-02-07-twitter-hate-speech-detection/imb.png" class=""><h1 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h1><p>Let’s create a <code>Dataset()</code> class for our twitter hate speech dataset.</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">TwitterDataset</span><span class="token punctuation">(</span>Dataset<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> review<span class="token punctuation">,</span> target<span class="token punctuation">,</span> max_len<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>review <span class="token operator">=</span> review        self<span class="token punctuation">.</span>target <span class="token operator">=</span> target        self<span class="token punctuation">.</span>tokenizer <span class="token operator">=</span> transformers<span class="token punctuation">.</span>BertTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>            <span class="token string">"bert-base-uncased"</span><span class="token punctuation">,</span> do_lower_case<span class="token operator">=</span><span class="token boolean">True</span>        <span class="token punctuation">)</span>        self<span class="token punctuation">.</span>max_len <span class="token operator">=</span> max_len    <span class="token keyword">def</span> <span class="token function">__len__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> <span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>review<span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">__getitem__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> item<span class="token punctuation">)</span><span class="token punctuation">:</span>        review <span class="token operator">=</span> <span class="token builtin">str</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>review<span class="token punctuation">[</span>item<span class="token punctuation">]</span><span class="token punctuation">)</span>        review <span class="token operator">=</span> <span class="token string">" "</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>review<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        inputs <span class="token operator">=</span> self<span class="token punctuation">.</span>tokenizer<span class="token punctuation">.</span>encode_plus<span class="token punctuation">(</span>            review<span class="token punctuation">,</span>            <span class="token boolean">None</span><span class="token punctuation">,</span>            add_special_tokens<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>            max_length<span class="token operator">=</span>self<span class="token punctuation">.</span>max_len<span class="token punctuation">,</span>            padding<span class="token operator">=</span><span class="token string">"max_length"</span><span class="token punctuation">,</span>            truncation<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>             return_tensors<span class="token operator">=</span><span class="token string">'pt'</span><span class="token punctuation">,</span>        <span class="token punctuation">)</span>        input_ids <span class="token operator">=</span> inputs<span class="token punctuation">[</span><span class="token string">"input_ids"</span><span class="token punctuation">]</span><span class="token punctuation">.</span>flatten<span class="token punctuation">(</span><span class="token punctuation">)</span>        attention_mask <span class="token operator">=</span> inputs<span class="token punctuation">[</span><span class="token string">"attention_mask"</span><span class="token punctuation">]</span><span class="token punctuation">.</span>flatten<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> <span class="token punctuation">&#123;</span>            <span class="token string">"ids"</span><span class="token punctuation">:</span> input_ids<span class="token punctuation">,</span>            <span class="token string">"mask"</span><span class="token punctuation">:</span> attention_mask<span class="token punctuation">,</span>            <span class="token string">"targets"</span><span class="token punctuation">:</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>self<span class="token punctuation">.</span>target<span class="token punctuation">[</span>item<span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">long</span><span class="token punctuation">)</span>        <span class="token punctuation">&#125;</span>train_dataset <span class="token operator">=</span> TwitterDataset<span class="token punctuation">(</span>df_train<span class="token punctuation">.</span>review<span class="token punctuation">.</span>values<span class="token punctuation">,</span> df_train<span class="token punctuation">.</span>label<span class="token punctuation">.</span>values<span class="token punctuation">,</span> max_len<span class="token operator">=</span>PARAM<span class="token punctuation">.</span>MAX_LENGTH<span class="token punctuation">)</span>valid_dataset <span class="token operator">=</span> TwitterDataset<span class="token punctuation">(</span>df_valid<span class="token punctuation">.</span>review<span class="token punctuation">.</span>values<span class="token punctuation">,</span> df_valid<span class="token punctuation">.</span>label<span class="token punctuation">.</span>values<span class="token punctuation">,</span> max_len<span class="token operator">=</span>PARAM<span class="token punctuation">.</span>MAX_LENGTH<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>We have 24575 training data and 6144 validation data.</p><h1 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h1><p>I will use Tez, a simple pytorch trainer, to build our neural network. Tez is a simple, to-the-point, library to make your pytorch training easy.</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">TwitterClassifier</span><span class="token punctuation">(</span>tez<span class="token punctuation">.</span>Model<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> num_train_steps<span class="token punctuation">,</span> num_classes<span class="token punctuation">,</span> weight<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> optimizer<span class="token operator">=</span><span class="token string">"lamb"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>tokenizer <span class="token operator">=</span> transformers<span class="token punctuation">.</span>BertTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>            <span class="token string">"bert-base-uncased"</span><span class="token punctuation">,</span> do_lower_case<span class="token operator">=</span><span class="token boolean">True</span>        <span class="token punctuation">)</span>        self<span class="token punctuation">.</span>bert <span class="token operator">=</span> transformers<span class="token punctuation">.</span>BertModel<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>            <span class="token string">"bert-base-uncased"</span><span class="token punctuation">,</span>             return_dict<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>bert_drop <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span><span class="token number">0.3</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>out <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>bert<span class="token punctuation">.</span>config<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> num_classes<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>num_train_steps <span class="token operator">=</span> num_train_steps        self<span class="token punctuation">.</span>step_scheduler_after <span class="token operator">=</span> <span class="token string">"batch"</span>        self<span class="token punctuation">.</span>optimizer <span class="token operator">=</span> optimizer        self<span class="token punctuation">.</span>history <span class="token operator">=</span> defaultdict<span class="token punctuation">(</span><span class="token builtin">list</span><span class="token punctuation">)</span>                <span class="token keyword">if</span> weight <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>            self<span class="token punctuation">.</span>weight <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>weight<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">fetch_optimizer</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        param_optimizer <span class="token operator">=</span> <span class="token builtin">list</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>named_parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        no_decay <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">"bias"</span><span class="token punctuation">,</span> <span class="token string">"LayerNorm.bias"</span><span class="token punctuation">]</span>        optimizer_parameters <span class="token operator">=</span> <span class="token punctuation">[</span>            <span class="token punctuation">&#123;</span>                <span class="token string">"params"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span>                    p <span class="token keyword">for</span> n<span class="token punctuation">,</span> p <span class="token keyword">in</span> param_optimizer <span class="token keyword">if</span> <span class="token keyword">not</span> <span class="token builtin">any</span><span class="token punctuation">(</span>nd <span class="token keyword">in</span> n <span class="token keyword">for</span> nd <span class="token keyword">in</span> no_decay<span class="token punctuation">)</span>                <span class="token punctuation">]</span><span class="token punctuation">,</span>                <span class="token string">"weight_decay"</span><span class="token punctuation">:</span> <span class="token number">0.001</span><span class="token punctuation">,</span>            <span class="token punctuation">&#125;</span><span class="token punctuation">,</span>            <span class="token punctuation">&#123;</span>                <span class="token string">"params"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span>                    p <span class="token keyword">for</span> n<span class="token punctuation">,</span> p <span class="token keyword">in</span> param_optimizer <span class="token keyword">if</span> <span class="token builtin">any</span><span class="token punctuation">(</span>nd <span class="token keyword">in</span> n <span class="token keyword">for</span> nd <span class="token keyword">in</span> no_decay<span class="token punctuation">)</span>                <span class="token punctuation">]</span><span class="token punctuation">,</span>                <span class="token string">"weight_decay"</span><span class="token punctuation">:</span> <span class="token number">0.0</span><span class="token punctuation">,</span>            <span class="token punctuation">&#125;</span><span class="token punctuation">,</span>        <span class="token punctuation">]</span>        <span class="token keyword">if</span> self<span class="token punctuation">.</span>optimizer <span class="token operator">==</span> <span class="token string">"adam"</span><span class="token punctuation">:</span>            opt <span class="token operator">=</span> AdamW<span class="token punctuation">(</span>optimizer_parameters<span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">3e</span><span class="token operator">-</span><span class="token number">5</span><span class="token punctuation">)</span>        <span class="token keyword">elif</span> self<span class="token punctuation">.</span>optimizer <span class="token operator">==</span> <span class="token string">"lamb"</span><span class="token punctuation">:</span>            opt <span class="token operator">=</span> Lamb<span class="token punctuation">(</span>optimizer_parameters<span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">3e</span><span class="token operator">-</span><span class="token number">5</span><span class="token punctuation">,</span> weight_decay<span class="token operator">=</span><span class="token number">.01</span><span class="token punctuation">,</span> betas<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">.9</span><span class="token punctuation">,</span> <span class="token number">.999</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> opt    <span class="token keyword">def</span> <span class="token function">fetch_scheduler</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        sch <span class="token operator">=</span> get_linear_schedule_with_warmup<span class="token punctuation">(</span>            self<span class="token punctuation">.</span>optimizer<span class="token punctuation">,</span> num_warmup_steps<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> num_training_steps<span class="token operator">=</span>self<span class="token punctuation">.</span>num_train_steps        <span class="token punctuation">)</span>        <span class="token keyword">return</span> sch    <span class="token keyword">def</span> <span class="token function">train_one_epoch</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> data_loader<span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>model_state <span class="token operator">=</span> enums<span class="token punctuation">.</span>ModelState<span class="token punctuation">.</span>TRAIN        losses <span class="token operator">=</span> AverageMeter<span class="token punctuation">(</span><span class="token punctuation">)</span>        tk0 <span class="token operator">=</span> tqdm<span class="token punctuation">(</span>data_loader<span class="token punctuation">,</span> total<span class="token operator">=</span><span class="token builtin">len</span><span class="token punctuation">(</span>data_loader<span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token keyword">for</span> b_idx<span class="token punctuation">,</span> data <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>tk0<span class="token punctuation">)</span><span class="token punctuation">:</span>            self<span class="token punctuation">.</span>train_state <span class="token operator">=</span> enums<span class="token punctuation">.</span>TrainingState<span class="token punctuation">.</span>TRAIN_STEP_START            loss<span class="token punctuation">,</span> metrics <span class="token operator">=</span> self<span class="token punctuation">.</span>train_one_step<span class="token punctuation">(</span>data<span class="token punctuation">)</span>            self<span class="token punctuation">.</span>train_state <span class="token operator">=</span> enums<span class="token punctuation">.</span>TrainingState<span class="token punctuation">.</span>TRAIN_STEP_END            losses<span class="token punctuation">.</span>update<span class="token punctuation">(</span>loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> data_loader<span class="token punctuation">.</span>batch_size<span class="token punctuation">)</span>            <span class="token keyword">if</span> b_idx <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>                metrics_meter <span class="token operator">=</span> <span class="token punctuation">&#123;</span>k<span class="token punctuation">:</span> AverageMeter<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">for</span> k <span class="token keyword">in</span> metrics<span class="token punctuation">&#125;</span>            monitor <span class="token operator">=</span> <span class="token punctuation">&#123;</span><span class="token punctuation">&#125;</span>            <span class="token keyword">for</span> m_m <span class="token keyword">in</span> metrics_meter<span class="token punctuation">:</span>                metrics_meter<span class="token punctuation">[</span>m_m<span class="token punctuation">]</span><span class="token punctuation">.</span>update<span class="token punctuation">(</span>metrics<span class="token punctuation">[</span>m_m<span class="token punctuation">]</span><span class="token punctuation">,</span> data_loader<span class="token punctuation">.</span>batch_size<span class="token punctuation">)</span>                monitor<span class="token punctuation">[</span>m_m<span class="token punctuation">]</span> <span class="token operator">=</span> metrics_meter<span class="token punctuation">[</span>m_m<span class="token punctuation">]</span><span class="token punctuation">.</span>avg            self<span class="token punctuation">.</span>current_train_step <span class="token operator">+=</span> <span class="token number">1</span>            tk0<span class="token punctuation">.</span>set_postfix<span class="token punctuation">(</span>loss<span class="token operator">=</span>losses<span class="token punctuation">.</span>avg<span class="token punctuation">,</span> stage<span class="token operator">=</span><span class="token string">"train"</span><span class="token punctuation">,</span> <span class="token operator">**</span>monitor<span class="token punctuation">)</span>        tk0<span class="token punctuation">.</span>close<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>update_metrics<span class="token punctuation">(</span>losses<span class="token operator">=</span>losses<span class="token punctuation">,</span> monitor<span class="token operator">=</span>monitor<span class="token punctuation">)</span>        <span class="token keyword">for</span> k<span class="token punctuation">,</span> v <span class="token keyword">in</span> monitor<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            self<span class="token punctuation">.</span>history<span class="token punctuation">[</span><span class="token string-interpolation"><span class="token string">f"train_</span><span class="token interpolation"><span class="token punctuation">&#123;</span>k<span class="token punctuation">&#125;</span></span><span class="token string">"</span></span><span class="token punctuation">]</span><span class="token punctuation">.</span>append<span class="token punctuation">(</span>v<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>history<span class="token punctuation">[</span><span class="token string">"train_loss"</span><span class="token punctuation">]</span><span class="token punctuation">.</span>append<span class="token punctuation">(</span>losses<span class="token punctuation">.</span>avg<span class="token punctuation">)</span>        <span class="token keyword">return</span> losses<span class="token punctuation">.</span>avg    <span class="token keyword">def</span> <span class="token function">validate_one_epoch</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> data_loader<span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>model_state <span class="token operator">=</span> enums<span class="token punctuation">.</span>ModelState<span class="token punctuation">.</span>VALID        losses <span class="token operator">=</span> AverageMeter<span class="token punctuation">(</span><span class="token punctuation">)</span>        tk0 <span class="token operator">=</span> tqdm<span class="token punctuation">(</span>data_loader<span class="token punctuation">,</span> total<span class="token operator">=</span><span class="token builtin">len</span><span class="token punctuation">(</span>data_loader<span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token keyword">for</span> b_idx<span class="token punctuation">,</span> data <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>tk0<span class="token punctuation">)</span><span class="token punctuation">:</span>            self<span class="token punctuation">.</span>train_state <span class="token operator">=</span> enums<span class="token punctuation">.</span>TrainingState<span class="token punctuation">.</span>VALID_STEP_START            <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>                loss<span class="token punctuation">,</span> metrics <span class="token operator">=</span> self<span class="token punctuation">.</span>validate_one_step<span class="token punctuation">(</span>data<span class="token punctuation">)</span>            self<span class="token punctuation">.</span>train_state <span class="token operator">=</span> enums<span class="token punctuation">.</span>TrainingState<span class="token punctuation">.</span>VALID_STEP_END            losses<span class="token punctuation">.</span>update<span class="token punctuation">(</span>loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> data_loader<span class="token punctuation">.</span>batch_size<span class="token punctuation">)</span>            <span class="token keyword">if</span> b_idx <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>                metrics_meter <span class="token operator">=</span> <span class="token punctuation">&#123;</span>k<span class="token punctuation">:</span> AverageMeter<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">for</span> k <span class="token keyword">in</span> metrics<span class="token punctuation">&#125;</span>            monitor <span class="token operator">=</span> <span class="token punctuation">&#123;</span><span class="token punctuation">&#125;</span>            <span class="token keyword">for</span> m_m <span class="token keyword">in</span> metrics_meter<span class="token punctuation">:</span>                metrics_meter<span class="token punctuation">[</span>m_m<span class="token punctuation">]</span><span class="token punctuation">.</span>update<span class="token punctuation">(</span>metrics<span class="token punctuation">[</span>m_m<span class="token punctuation">]</span><span class="token punctuation">,</span> data_loader<span class="token punctuation">.</span>batch_size<span class="token punctuation">)</span>                monitor<span class="token punctuation">[</span>m_m<span class="token punctuation">]</span> <span class="token operator">=</span> metrics_meter<span class="token punctuation">[</span>m_m<span class="token punctuation">]</span><span class="token punctuation">.</span>avg            tk0<span class="token punctuation">.</span>set_postfix<span class="token punctuation">(</span>loss<span class="token operator">=</span>losses<span class="token punctuation">.</span>avg<span class="token punctuation">,</span> stage<span class="token operator">=</span><span class="token string">"valid"</span><span class="token punctuation">,</span> <span class="token operator">**</span>monitor<span class="token punctuation">)</span>            self<span class="token punctuation">.</span>current_valid_step <span class="token operator">+=</span> <span class="token number">1</span>        tk0<span class="token punctuation">.</span>close<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>update_metrics<span class="token punctuation">(</span>losses<span class="token operator">=</span>losses<span class="token punctuation">,</span> monitor<span class="token operator">=</span>monitor<span class="token punctuation">)</span>        <span class="token keyword">for</span> k<span class="token punctuation">,</span> v <span class="token keyword">in</span> monitor<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            self<span class="token punctuation">.</span>history<span class="token punctuation">[</span><span class="token string-interpolation"><span class="token string">f"valid_</span><span class="token interpolation"><span class="token punctuation">&#123;</span>k<span class="token punctuation">&#125;</span></span><span class="token string">"</span></span><span class="token punctuation">]</span><span class="token punctuation">.</span>append<span class="token punctuation">(</span>v<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>history<span class="token punctuation">[</span><span class="token string">"valid_loss"</span><span class="token punctuation">]</span><span class="token punctuation">.</span>append<span class="token punctuation">(</span>losses<span class="token punctuation">.</span>avg<span class="token punctuation">)</span>        <span class="token keyword">return</span> losses<span class="token punctuation">.</span>avg        <span class="token keyword">def</span> <span class="token function">loss</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> outputs<span class="token punctuation">,</span> targets<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">if</span> targets <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>            <span class="token keyword">return</span> <span class="token boolean">None</span>        <span class="token keyword">if</span> self<span class="token punctuation">.</span>weight <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>            self<span class="token punctuation">.</span>weight <span class="token operator">=</span> self<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>to<span class="token punctuation">(</span>self<span class="token punctuation">.</span>device<span class="token punctuation">)</span>        <span class="token keyword">return</span> nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">(</span>self<span class="token punctuation">.</span>weight<span class="token punctuation">)</span><span class="token punctuation">(</span>outputs<span class="token punctuation">,</span> targets<span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">monitor_metrics</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> outputs<span class="token punctuation">,</span> targets<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">if</span> targets <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>            <span class="token keyword">return</span> <span class="token punctuation">&#123;</span><span class="token punctuation">&#125;</span>        outputs <span class="token operator">=</span> torch<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>outputs<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span>        targets <span class="token operator">=</span> targets<span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span>        accuracy <span class="token operator">=</span> metrics<span class="token punctuation">.</span>accuracy_score<span class="token punctuation">(</span>targets<span class="token punctuation">,</span> outputs<span class="token punctuation">)</span>        f1_score <span class="token operator">=</span> metrics<span class="token punctuation">.</span>f1_score<span class="token punctuation">(</span>targets<span class="token punctuation">,</span> outputs<span class="token punctuation">,</span> average<span class="token operator">=</span><span class="token string">'weighted'</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> <span class="token punctuation">&#123;</span><span class="token string">"accuracy"</span><span class="token punctuation">:</span> accuracy<span class="token punctuation">,</span> <span class="token string">"f1"</span><span class="token punctuation">:</span> f1_score<span class="token punctuation">&#125;</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> ids<span class="token punctuation">,</span> mask<span class="token punctuation">,</span> targets<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        last_hidden_states <span class="token operator">=</span> self<span class="token punctuation">.</span>bert<span class="token punctuation">(</span>ids<span class="token punctuation">,</span> attention_mask<span class="token operator">=</span>mask<span class="token punctuation">)</span>        b_o <span class="token operator">=</span> self<span class="token punctuation">.</span>bert_drop<span class="token punctuation">(</span>last_hidden_states<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span>        output <span class="token operator">=</span> self<span class="token punctuation">.</span>out<span class="token punctuation">(</span>b_o<span class="token punctuation">)</span>        loss <span class="token operator">=</span> self<span class="token punctuation">.</span>loss<span class="token punctuation">(</span>output<span class="token punctuation">,</span> targets<span class="token punctuation">)</span>        acc <span class="token operator">=</span> self<span class="token punctuation">.</span>monitor_metrics<span class="token punctuation">(</span>output<span class="token punctuation">,</span> targets<span class="token punctuation">)</span>        <span class="token keyword">return</span> output<span class="token punctuation">,</span> loss<span class="token punctuation">,</span> acc    <span class="token keyword">def</span> <span class="token function">score</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> test_dataset<span class="token punctuation">,</span> n_jobs<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">:</span>                <span class="token keyword">def</span> <span class="token function">softmax</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token keyword">return</span> np<span class="token punctuation">.</span>exp<span class="token punctuation">(</span>x<span class="token punctuation">)</span> <span class="token operator">/</span> np<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>np<span class="token punctuation">.</span>exp<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>                prediction <span class="token operator">=</span> self<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>test_dataset<span class="token punctuation">,</span> n_jobs<span class="token operator">=</span>n_jobs<span class="token punctuation">)</span>        prediction <span class="token operator">=</span> <span class="token builtin">list</span><span class="token punctuation">(</span>prediction<span class="token punctuation">)</span>        prediction <span class="token operator">=</span> softmax<span class="token punctuation">(</span>np<span class="token punctuation">.</span>vstack<span class="token punctuation">(</span>prediction<span class="token punctuation">)</span><span class="token punctuation">)</span>        prediction <span class="token operator">=</span> np<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>prediction<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>        groud_truth <span class="token operator">=</span> test_dataset<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">"target"</span><span class="token punctuation">]</span><span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>astype<span class="token punctuation">(</span>np<span class="token punctuation">.</span>int64<span class="token punctuation">)</span>        <span class="token keyword">return</span> metrics<span class="token punctuation">.</span>accuracy_score<span class="token punctuation">(</span>prediction<span class="token punctuation">,</span> groud_truth<span class="token punctuation">)</span>        <span class="token keyword">def</span> <span class="token function">plot_history</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">from</span> matplotlib<span class="token punctuation">.</span>ticker <span class="token keyword">import</span> MaxNLocator                train_loss<span class="token punctuation">,</span> valid_loss <span class="token operator">=</span> self<span class="token punctuation">.</span>history<span class="token punctuation">[</span><span class="token string">"train_loss"</span><span class="token punctuation">]</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>history<span class="token punctuation">[</span><span class="token string">"valid_loss"</span><span class="token punctuation">]</span>        train_accuracy<span class="token punctuation">,</span> valid_accuracy <span class="token operator">=</span> self<span class="token punctuation">.</span>history<span class="token punctuation">[</span><span class="token string">"train_accuracy"</span><span class="token punctuation">]</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>history<span class="token punctuation">[</span><span class="token string">"valid_accuracy"</span><span class="token punctuation">]</span>        plt<span class="token punctuation">.</span>rcParams<span class="token punctuation">[</span><span class="token string">"figure.figsize"</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">15</span><span class="token punctuation">,</span> <span class="token number">6</span>        ax1 <span class="token operator">=</span> plt<span class="token punctuation">.</span>subplot<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>        ax1<span class="token punctuation">.</span>plot<span class="token punctuation">(</span><span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>train_loss<span class="token punctuation">)</span><span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> train_loss<span class="token punctuation">,</span> color<span class="token operator">=</span><span class="token string">"tab:blue"</span><span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">"train"</span><span class="token punctuation">)</span>        ax1<span class="token punctuation">.</span>plot<span class="token punctuation">(</span><span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>valid_loss<span class="token punctuation">)</span><span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> valid_loss<span class="token punctuation">,</span> color<span class="token operator">=</span><span class="token string">"tab:orange"</span><span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">"valid"</span><span class="token punctuation">)</span>        ax1<span class="token punctuation">.</span>xaxis<span class="token punctuation">.</span>set_major_locator<span class="token punctuation">(</span>MaxNLocator<span class="token punctuation">(</span>integer<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        ax1<span class="token punctuation">.</span>title<span class="token punctuation">.</span>set_text<span class="token punctuation">(</span><span class="token string">'Loss'</span><span class="token punctuation">)</span>        ax1<span class="token punctuation">.</span>grid<span class="token punctuation">(</span><span class="token punctuation">)</span>        ax2 <span class="token operator">=</span> plt<span class="token punctuation">.</span>subplot<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>        ax2<span class="token punctuation">.</span>plot<span class="token punctuation">(</span><span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>train_accuracy<span class="token punctuation">)</span><span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> train_accuracy<span class="token punctuation">,</span> color<span class="token operator">=</span><span class="token string">"tab:blue"</span><span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">"train"</span><span class="token punctuation">)</span>        ax2<span class="token punctuation">.</span>plot<span class="token punctuation">(</span><span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>valid_accuracy<span class="token punctuation">)</span><span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> valid_accuracy<span class="token punctuation">,</span> color<span class="token operator">=</span><span class="token string">"tab:orange"</span><span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">"valid"</span><span class="token punctuation">)</span>        ax2<span class="token punctuation">.</span>xaxis<span class="token punctuation">.</span>set_major_locator<span class="token punctuation">(</span>MaxNLocator<span class="token punctuation">(</span>integer<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        ax2<span class="token punctuation">.</span>title<span class="token punctuation">.</span>set_text<span class="token punctuation">(</span><span class="token string">'Accuracy'</span><span class="token punctuation">)</span>        ax2<span class="token punctuation">.</span>grid<span class="token punctuation">(</span><span class="token punctuation">)</span>        plt<span class="token punctuation">.</span>tight_layout<span class="token punctuation">(</span><span class="token punctuation">)</span>        plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>Training large deep neural networks on massive datasets is computationally very<br>challenging.  In Yang You’s <a href="https://arxiv.org/pdf/1904.00962.pdf">paper</a>, they first study a principled layerwise adaptation strategy to accelerate training of deep neural networks using large mini-batches. Using this strategy, they develop a new layerwise adaptive large batch optimization technique called <code>LAMB</code>. The <code>LAMB</code> implementation is available <a href="https://github.com/tensorflow/addons/blob/master/tensorflow_addons/optimizers/lamb.py">online</a>.</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Lamb</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>Optimizer<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">r"""Implements Lamb algorithm.    It has been proposed in `Large Batch Optimization for Deep Learning: Training BERT in 76 minutes`_.    Arguments:        params (iterable): iterable of parameters to optimize or dicts defining            parameter groups        lr (float, optional): learning rate (default: 1e-3)        betas (Tuple[float, float], optional): coefficients used for computing            running averages of gradient and its square (default: (0.9, 0.999))        eps (float, optional): term added to the denominator to improve            numerical stability (default: 1e-8)        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)        adam (bool, optional): always use trust ratio = 1, which turns this into            Adam. Useful for comparison purposes.    .. _Large Batch Optimization for Deep Learning: Training BERT in 76 minutes:        https://arxiv.org/abs/1904.00962    """</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> params<span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">3</span><span class="token punctuation">,</span> betas<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">0.9</span><span class="token punctuation">,</span> <span class="token number">0.999</span><span class="token punctuation">)</span><span class="token punctuation">,</span> eps<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">6</span><span class="token punctuation">,</span>                 weight_decay<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> adam<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">if</span> <span class="token keyword">not</span> <span class="token number">0.0</span> <span class="token operator">&lt;=</span> lr<span class="token punctuation">:</span>            <span class="token keyword">raise</span> ValueError<span class="token punctuation">(</span><span class="token string">"Invalid learning rate: &#123;&#125;"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>lr<span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token keyword">if</span> <span class="token keyword">not</span> <span class="token number">0.0</span> <span class="token operator">&lt;=</span> eps<span class="token punctuation">:</span>            <span class="token keyword">raise</span> ValueError<span class="token punctuation">(</span><span class="token string">"Invalid epsilon value: &#123;&#125;"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>eps<span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token keyword">if</span> <span class="token keyword">not</span> <span class="token number">0.0</span> <span class="token operator">&lt;=</span> betas<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">&lt;</span> <span class="token number">1.0</span><span class="token punctuation">:</span>            <span class="token keyword">raise</span> ValueError<span class="token punctuation">(</span><span class="token string">"Invalid beta parameter at index 0: &#123;&#125;"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>betas<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token keyword">if</span> <span class="token keyword">not</span> <span class="token number">0.0</span> <span class="token operator">&lt;=</span> betas<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">&lt;</span> <span class="token number">1.0</span><span class="token punctuation">:</span>            <span class="token keyword">raise</span> ValueError<span class="token punctuation">(</span><span class="token string">"Invalid beta parameter at index 1: &#123;&#125;"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>betas<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        defaults <span class="token operator">=</span> <span class="token builtin">dict</span><span class="token punctuation">(</span>lr<span class="token operator">=</span>lr<span class="token punctuation">,</span> betas<span class="token operator">=</span>betas<span class="token punctuation">,</span> eps<span class="token operator">=</span>eps<span class="token punctuation">,</span>                        weight_decay<span class="token operator">=</span>weight_decay<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>adam <span class="token operator">=</span> adam        <span class="token builtin">super</span><span class="token punctuation">(</span>Lamb<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span>params<span class="token punctuation">,</span> defaults<span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">step</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> closure<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">"""Performs a single optimization step.        Arguments:            closure (callable, optional): A closure that reevaluates the model                and returns the loss.        """</span>        loss <span class="token operator">=</span> <span class="token boolean">None</span>        <span class="token keyword">if</span> closure <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>            loss <span class="token operator">=</span> closure<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">for</span> group <span class="token keyword">in</span> self<span class="token punctuation">.</span>param_groups<span class="token punctuation">:</span>            <span class="token keyword">for</span> p <span class="token keyword">in</span> group<span class="token punctuation">[</span><span class="token string">'params'</span><span class="token punctuation">]</span><span class="token punctuation">:</span>                <span class="token keyword">if</span> p<span class="token punctuation">.</span>grad <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>                    <span class="token keyword">continue</span>                grad <span class="token operator">=</span> p<span class="token punctuation">.</span>grad<span class="token punctuation">.</span>data                <span class="token keyword">if</span> grad<span class="token punctuation">.</span>is_sparse<span class="token punctuation">:</span>                    <span class="token keyword">raise</span> RuntimeError<span class="token punctuation">(</span><span class="token string">'Lamb does not support sparse gradients, consider SparseAdam instad.'</span><span class="token punctuation">)</span>                state <span class="token operator">=</span> self<span class="token punctuation">.</span>state<span class="token punctuation">[</span>p<span class="token punctuation">]</span>                <span class="token comment"># State initialization</span>                <span class="token keyword">if</span> <span class="token builtin">len</span><span class="token punctuation">(</span>state<span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>                    state<span class="token punctuation">[</span><span class="token string">'step'</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0</span>                    <span class="token comment"># Exponential moving average of gradient values</span>                    state<span class="token punctuation">[</span><span class="token string">'exp_avg'</span><span class="token punctuation">]</span> <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros_like<span class="token punctuation">(</span>p<span class="token punctuation">.</span>data<span class="token punctuation">)</span>                    <span class="token comment"># Exponential moving average of squared gradient values</span>                    state<span class="token punctuation">[</span><span class="token string">'exp_avg_sq'</span><span class="token punctuation">]</span> <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros_like<span class="token punctuation">(</span>p<span class="token punctuation">.</span>data<span class="token punctuation">)</span>                exp_avg<span class="token punctuation">,</span> exp_avg_sq <span class="token operator">=</span> state<span class="token punctuation">[</span><span class="token string">'exp_avg'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> state<span class="token punctuation">[</span><span class="token string">'exp_avg_sq'</span><span class="token punctuation">]</span>                beta1<span class="token punctuation">,</span> beta2 <span class="token operator">=</span> group<span class="token punctuation">[</span><span class="token string">'betas'</span><span class="token punctuation">]</span>                state<span class="token punctuation">[</span><span class="token string">'step'</span><span class="token punctuation">]</span> <span class="token operator">+=</span> <span class="token number">1</span>                <span class="token comment"># Decay the first and second moment running average coefficient</span>                <span class="token comment"># m_t</span>                exp_avg<span class="token punctuation">.</span>mul_<span class="token punctuation">(</span>beta1<span class="token punctuation">)</span><span class="token punctuation">.</span>add_<span class="token punctuation">(</span>grad<span class="token punctuation">,</span> alpha<span class="token operator">=</span><span class="token number">1</span> <span class="token operator">-</span> beta1<span class="token punctuation">)</span>                <span class="token comment"># v_t</span>                exp_avg_sq<span class="token punctuation">.</span>mul_<span class="token punctuation">(</span>beta2<span class="token punctuation">)</span><span class="token punctuation">.</span>addcmul_<span class="token punctuation">(</span>grad<span class="token punctuation">,</span> grad<span class="token punctuation">,</span> value<span class="token operator">=</span><span class="token number">1</span> <span class="token operator">-</span> beta2<span class="token punctuation">)</span>                <span class="token comment"># Paper v3 does not use debiasing.</span>                <span class="token comment"># bias_correction1 = 1 - beta1 ** state['step']</span>                <span class="token comment"># bias_correction2 = 1 - beta2 ** state['step']</span>                <span class="token comment"># Apply bias to lr to avoid broadcast.</span>                step_size <span class="token operator">=</span> group<span class="token punctuation">[</span><span class="token string">'lr'</span><span class="token punctuation">]</span> <span class="token comment"># * math.sqrt(bias_correction2) / bias_correction1</span>                weight_norm <span class="token operator">=</span> p<span class="token punctuation">.</span>data<span class="token punctuation">.</span><span class="token builtin">pow</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>clamp<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span>                adam_step <span class="token operator">=</span> exp_avg <span class="token operator">/</span> exp_avg_sq<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>add<span class="token punctuation">(</span>group<span class="token punctuation">[</span><span class="token string">'eps'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>                <span class="token keyword">if</span> group<span class="token punctuation">[</span><span class="token string">'weight_decay'</span><span class="token punctuation">]</span> <span class="token operator">!=</span> <span class="token number">0</span><span class="token punctuation">:</span>                    adam_step<span class="token punctuation">.</span>add_<span class="token punctuation">(</span>p<span class="token punctuation">.</span>data<span class="token punctuation">,</span> alpha<span class="token operator">=</span>group<span class="token punctuation">[</span><span class="token string">'weight_decay'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>                adam_norm <span class="token operator">=</span> adam_step<span class="token punctuation">.</span><span class="token builtin">pow</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span><span class="token punctuation">)</span>                <span class="token keyword">if</span> weight_norm <span class="token operator">==</span> <span class="token number">0</span> <span class="token keyword">or</span> adam_norm <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>                    trust_ratio <span class="token operator">=</span> <span class="token number">1</span>                <span class="token keyword">else</span><span class="token punctuation">:</span>                    trust_ratio <span class="token operator">=</span> weight_norm <span class="token operator">/</span> adam_norm                state<span class="token punctuation">[</span><span class="token string">'weight_norm'</span><span class="token punctuation">]</span> <span class="token operator">=</span> weight_norm                state<span class="token punctuation">[</span><span class="token string">'adam_norm'</span><span class="token punctuation">]</span> <span class="token operator">=</span> adam_norm                state<span class="token punctuation">[</span><span class="token string">'trust_ratio'</span><span class="token punctuation">]</span> <span class="token operator">=</span> trust_ratio                <span class="token keyword">if</span> self<span class="token punctuation">.</span>adam<span class="token punctuation">:</span>                    trust_ratio <span class="token operator">=</span> <span class="token number">1</span>                p<span class="token punctuation">.</span>data<span class="token punctuation">.</span>add_<span class="token punctuation">(</span>adam_step<span class="token punctuation">,</span> alpha<span class="token operator">=</span><span class="token operator">-</span>step_size <span class="token operator">*</span> trust_ratio<span class="token punctuation">)</span>        <span class="token keyword">return</span> loss<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="Start-Training"><a href="#Start-Training" class="headerlink" title="Start Training!"></a>Start Training!</h2><pre class="line-numbers language-python" data-language="python"><code class="language-python">n_train_steps <span class="token operator">=</span> <span class="token builtin">int</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>df_train<span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token number">32</span> <span class="token operator">*</span> <span class="token number">10</span><span class="token punctuation">)</span>model <span class="token operator">=</span> TwitterClassifier<span class="token punctuation">(</span>num_train_steps<span class="token operator">=</span>n_train_steps<span class="token punctuation">,</span> num_classes<span class="token operator">=</span>PARAM<span class="token punctuation">.</span>NUM_CLASSES<span class="token punctuation">,</span> weight<span class="token operator">=</span>weight<span class="token punctuation">)</span>tb_logger <span class="token operator">=</span> tez<span class="token punctuation">.</span>callbacks<span class="token punctuation">.</span>TensorBoardLogger<span class="token punctuation">(</span>log_dir<span class="token operator">=</span>PARAM<span class="token punctuation">.</span>TF_LOG<span class="token punctuation">)</span>es <span class="token operator">=</span> tez<span class="token punctuation">.</span>callbacks<span class="token punctuation">.</span>EarlyStopping<span class="token punctuation">(</span>monitor<span class="token operator">=</span><span class="token string">"valid_loss"</span><span class="token punctuation">,</span>                                  model_path<span class="token operator">=</span>PARAM<span class="token punctuation">.</span>MODEL_SAVE_PATH<span class="token punctuation">,</span>                                  patience<span class="token operator">=</span>PARAM<span class="token punctuation">.</span>ES_PATIENCE<span class="token punctuation">)</span>model<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>    train_dataset<span class="token punctuation">,</span>    valid_dataset<span class="token operator">=</span>valid_dataset<span class="token punctuation">,</span>    train_bs<span class="token operator">=</span>PARAM<span class="token punctuation">.</span>TRAIN_BATCH_SIZE<span class="token punctuation">,</span>     valid_bs<span class="token operator">=</span>PARAM<span class="token punctuation">.</span>VALID_BATCH_SIZE<span class="token punctuation">,</span>     device<span class="token operator">=</span>PARAM<span class="token punctuation">.</span>DEVICE<span class="token punctuation">,</span>    epochs<span class="token operator">=</span>PARAM<span class="token punctuation">.</span>EPOCH<span class="token punctuation">,</span>    callbacks<span class="token operator">=</span><span class="token punctuation">[</span>tb_logger<span class="token punctuation">,</span> es<span class="token punctuation">]</span><span class="token punctuation">,</span>    n_jobs<span class="token operator">=</span>PARAM<span class="token punctuation">.</span>N_JOB<span class="token punctuation">,</span>     fp16<span class="token operator">=</span>PARAM<span class="token punctuation">.</span>FP16<span class="token punctuation">,</span><span class="token punctuation">)</span>model<span class="token punctuation">.</span>save<span class="token punctuation">(</span>PARAM<span class="token punctuation">.</span>MODEL_SAVE_PATH<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><img src="/Hexo-Blog/2021/02/07/2021-02-07-twitter-hate-speech-detection/history.png" class=""><h2 id="Inference"><a href="#Inference" class="headerlink" title="Inference"></a>Inference</h2><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">TwitterInferenceClassifier</span><span class="token punctuation">(</span>tez<span class="token punctuation">.</span>Model<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> num_classes<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>tokenizer <span class="token operator">=</span> transformers<span class="token punctuation">.</span>BertTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>            <span class="token string">"bert-base-uncased"</span><span class="token punctuation">,</span> do_lower_case<span class="token operator">=</span><span class="token boolean">True</span>        <span class="token punctuation">)</span>        self<span class="token punctuation">.</span>bert <span class="token operator">=</span> transformers<span class="token punctuation">.</span>BertModel<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>            <span class="token string">"bert-base-uncased"</span><span class="token punctuation">,</span>             return_dict<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>bert_drop <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span><span class="token number">0.3</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>out <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>bert<span class="token punctuation">.</span>config<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> num_classes<span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> ids<span class="token punctuation">,</span> mask<span class="token punctuation">,</span> targets<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        last_hidden_states <span class="token operator">=</span> self<span class="token punctuation">.</span>bert<span class="token punctuation">(</span>ids<span class="token punctuation">,</span> attention_mask<span class="token operator">=</span>mask<span class="token punctuation">)</span>        b_o <span class="token operator">=</span> self<span class="token punctuation">.</span>bert_drop<span class="token punctuation">(</span>last_hidden_states<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span>        output <span class="token operator">=</span> self<span class="token punctuation">.</span>out<span class="token punctuation">(</span>b_o<span class="token punctuation">)</span>        <span class="token keyword">return</span> output<span class="token punctuation">,</span> _<span class="token punctuation">,</span> _        <span class="token keyword">def</span> <span class="token function">load</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> model_path<span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token string">"cuda"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>device <span class="token operator">=</span> device        <span class="token keyword">if</span> <span class="token builtin">next</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>device <span class="token operator">!=</span> self<span class="token punctuation">.</span>device<span class="token punctuation">:</span>            self<span class="token punctuation">.</span>to<span class="token punctuation">(</span>self<span class="token punctuation">.</span>device<span class="token punctuation">)</span>        model_dict <span class="token operator">=</span> torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span>model_path<span class="token punctuation">,</span> map_location<span class="token operator">=</span>torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span>device<span class="token punctuation">)</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>load_state_dict<span class="token punctuation">(</span>model_dict<span class="token punctuation">[</span><span class="token string">"state_dict"</span><span class="token punctuation">]</span><span class="token punctuation">)</span>model_reload <span class="token operator">=</span> TwitterInferenceClassifier<span class="token punctuation">(</span>num_classes<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>PARAM<span class="token punctuation">.</span>DEVICE<span class="token punctuation">)</span>model_reload<span class="token punctuation">.</span>load<span class="token punctuation">(</span>PARAM<span class="token punctuation">.</span>MODEL_SAVE_PATH<span class="token punctuation">,</span> device<span class="token operator">=</span>PARAM<span class="token punctuation">.</span>DEVICE<span class="token punctuation">)</span>test_df<span class="token punctuation">[</span><span class="token string">"review"</span><span class="token punctuation">]</span> <span class="token operator">=</span> test_df<span class="token punctuation">[</span><span class="token string">"tweet"</span><span class="token punctuation">]</span><span class="token punctuation">.</span>swifter<span class="token punctuation">.</span><span class="token builtin">apply</span><span class="token punctuation">(</span>clean_tweets<span class="token punctuation">)</span>test_df<span class="token punctuation">[</span><span class="token string">"length"</span><span class="token punctuation">]</span> <span class="token operator">=</span> test_df<span class="token punctuation">[</span><span class="token string">"review"</span><span class="token punctuation">]</span><span class="token punctuation">.</span>swifter<span class="token punctuation">.</span><span class="token builtin">apply</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">)</span>test_df<span class="token punctuation">[</span><span class="token string">"label"</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0</span>test_dataset <span class="token operator">=</span> TwitterDataset<span class="token punctuation">(</span>test_df<span class="token punctuation">.</span>review<span class="token punctuation">.</span>values<span class="token punctuation">,</span> test_df<span class="token punctuation">.</span>label<span class="token punctuation">.</span>values<span class="token punctuation">,</span> max_len<span class="token operator">=</span>PARAM<span class="token punctuation">.</span>MAX_LENGTH<span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">softmax</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">return</span> np<span class="token punctuation">.</span>exp<span class="token punctuation">(</span>x<span class="token punctuation">)</span> <span class="token operator">/</span> np<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>np<span class="token punctuation">.</span>exp<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>prediction <span class="token operator">=</span> model_reload<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>test_dataset<span class="token punctuation">,</span> n_jobs<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">)</span>prediction <span class="token operator">=</span> <span class="token builtin">list</span><span class="token punctuation">(</span>prediction<span class="token punctuation">)</span>prediction <span class="token operator">=</span> softmax<span class="token punctuation">(</span>np<span class="token punctuation">.</span>vstack<span class="token punctuation">(</span>prediction<span class="token punctuation">)</span><span class="token punctuation">)</span>prediction <span class="token operator">=</span> np<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>prediction<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>submit <span class="token operator">=</span> pd<span class="token punctuation">.</span>read_csv<span class="token punctuation">(</span><span class="token string">"./data/sample_submission.csv"</span><span class="token punctuation">)</span>submit<span class="token punctuation">[</span><span class="token string">"label"</span><span class="token punctuation">]</span> <span class="token operator">=</span> predictionsubmit<span class="token punctuation">.</span>to_csv<span class="token punctuation">(</span><span class="token string">"./data/submit.csv"</span><span class="token punctuation">,</span> index<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>This solution to the detection of hate speech and offensive language on Twitter through deep learning using BERT achieves reasonable accuracy (90.8%) as well as f1-score (92.1%) on validation dataset. However, it got low f1-score (29.5%) on test dataset. As we can see, the high accuracy rate was just an illusion.</p><p>Supervised learning relies on the fact that training and test data follow the same distribution. If that were not the case, then one could perfectly get a model that performs well in training data but does not on test data. And it would not be because of overfitting of the training data.</p><p>First, let’s try to reduce the complexity of the model.</p><h1 id="Naive-Bayes-Classifier"><a href="#Naive-Bayes-Classifier" class="headerlink" title="Naive Bayes Classifier"></a>Naive Bayes Classifier</h1><p>We’ll take a look at one natural language processing technique for text classification called Naive Bayes.</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> os<span class="token keyword">import</span> re<span class="token keyword">import</span> string<span class="token keyword">import</span> mathtarget_names <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'non-toxic'</span><span class="token punctuation">,</span> <span class="token string">'toxic'</span><span class="token punctuation">]</span><span class="token keyword">class</span> <span class="token class-name">HateSpeechDetector</span><span class="token punctuation">(</span><span class="token builtin">object</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">def</span> <span class="token function">clean</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> s<span class="token punctuation">)</span><span class="token punctuation">:</span>        translator <span class="token operator">=</span> <span class="token builtin">str</span><span class="token punctuation">.</span>maketrans<span class="token punctuation">(</span><span class="token string">""</span><span class="token punctuation">,</span> <span class="token string">""</span><span class="token punctuation">,</span> string<span class="token punctuation">.</span>punctuation<span class="token punctuation">)</span>        <span class="token keyword">return</span> s<span class="token punctuation">.</span>translate<span class="token punctuation">(</span>translator<span class="token punctuation">)</span>     <span class="token keyword">def</span> <span class="token function">tokenize</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> text<span class="token punctuation">)</span><span class="token punctuation">:</span>        text <span class="token operator">=</span> self<span class="token punctuation">.</span>clean<span class="token punctuation">(</span>text<span class="token punctuation">)</span><span class="token punctuation">.</span>lower<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> re<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">"\W+"</span><span class="token punctuation">,</span> text<span class="token punctuation">)</span>     <span class="token keyword">def</span> <span class="token function">get_word_counts</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> words<span class="token punctuation">)</span><span class="token punctuation">:</span>        word_counts <span class="token operator">=</span> <span class="token punctuation">&#123;</span><span class="token punctuation">&#125;</span>        <span class="token keyword">for</span> word <span class="token keyword">in</span> words<span class="token punctuation">:</span>            word_counts<span class="token punctuation">[</span>word<span class="token punctuation">]</span> <span class="token operator">=</span> word_counts<span class="token punctuation">.</span>get<span class="token punctuation">(</span>word<span class="token punctuation">,</span> <span class="token number">0.0</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token number">1.0</span>        <span class="token keyword">return</span> word_counts        <span class="token keyword">def</span> <span class="token function">fit</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> X<span class="token punctuation">,</span> Y<span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>num_reviews <span class="token operator">=</span> <span class="token punctuation">&#123;</span><span class="token punctuation">&#125;</span>        self<span class="token punctuation">.</span>log_class_priors <span class="token operator">=</span> <span class="token punctuation">&#123;</span><span class="token punctuation">&#125;</span>        self<span class="token punctuation">.</span>word_counts <span class="token operator">=</span> <span class="token punctuation">&#123;</span><span class="token punctuation">&#125;</span>        self<span class="token punctuation">.</span>vocab <span class="token operator">=</span> <span class="token builtin">set</span><span class="token punctuation">(</span><span class="token punctuation">)</span>        n <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>X<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>num_reviews<span class="token punctuation">[</span><span class="token string">'toxic'</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token number">1</span> <span class="token keyword">for</span> label <span class="token keyword">in</span> Y <span class="token keyword">if</span> label <span class="token operator">==</span> <span class="token number">1</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>num_reviews<span class="token punctuation">[</span><span class="token string">'non-toxic'</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token number">1</span> <span class="token keyword">for</span> label <span class="token keyword">in</span> Y <span class="token keyword">if</span> label <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>log_class_priors<span class="token punctuation">[</span><span class="token string">'toxic'</span><span class="token punctuation">]</span> <span class="token operator">=</span> math<span class="token punctuation">.</span>log<span class="token punctuation">(</span>self<span class="token punctuation">.</span>num_reviews<span class="token punctuation">[</span><span class="token string">'toxic'</span><span class="token punctuation">]</span> <span class="token operator">/</span> n<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>log_class_priors<span class="token punctuation">[</span><span class="token string">'non-toxic'</span><span class="token punctuation">]</span> <span class="token operator">=</span> math<span class="token punctuation">.</span>log<span class="token punctuation">(</span>self<span class="token punctuation">.</span>num_reviews<span class="token punctuation">[</span><span class="token string">'non-toxic'</span><span class="token punctuation">]</span> <span class="token operator">/</span> n<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>word_counts<span class="token punctuation">[</span><span class="token string">'toxic'</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">&#123;</span><span class="token punctuation">&#125;</span>        self<span class="token punctuation">.</span>word_counts<span class="token punctuation">[</span><span class="token string">'non-toxic'</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">&#123;</span><span class="token punctuation">&#125;</span>        <span class="token keyword">for</span> x<span class="token punctuation">,</span> y <span class="token keyword">in</span> <span class="token builtin">zip</span><span class="token punctuation">(</span>X<span class="token punctuation">,</span> Y<span class="token punctuation">)</span><span class="token punctuation">:</span>            c <span class="token operator">=</span> <span class="token string">'toxic'</span> <span class="token keyword">if</span> y <span class="token operator">==</span> <span class="token number">1</span> <span class="token keyword">else</span> <span class="token string">'non-toxic'</span>            counts <span class="token operator">=</span> self<span class="token punctuation">.</span>get_word_counts<span class="token punctuation">(</span>self<span class="token punctuation">.</span>tokenize<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>            <span class="token keyword">for</span> word<span class="token punctuation">,</span> count <span class="token keyword">in</span> counts<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>                <span class="token keyword">if</span> word <span class="token keyword">not</span> <span class="token keyword">in</span> self<span class="token punctuation">.</span>vocab<span class="token punctuation">:</span>                    self<span class="token punctuation">.</span>vocab<span class="token punctuation">.</span>add<span class="token punctuation">(</span>word<span class="token punctuation">)</span>                <span class="token keyword">if</span> word <span class="token keyword">not</span> <span class="token keyword">in</span> self<span class="token punctuation">.</span>word_counts<span class="token punctuation">[</span>c<span class="token punctuation">]</span><span class="token punctuation">:</span>                    self<span class="token punctuation">.</span>word_counts<span class="token punctuation">[</span>c<span class="token punctuation">]</span><span class="token punctuation">[</span>word<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0.0</span>                self<span class="token punctuation">.</span>word_counts<span class="token punctuation">[</span>c<span class="token punctuation">]</span><span class="token punctuation">[</span>word<span class="token punctuation">]</span> <span class="token operator">+=</span> count                    <span class="token keyword">def</span> <span class="token function">predict</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> X<span class="token punctuation">)</span><span class="token punctuation">:</span>        result <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>        <span class="token keyword">for</span> x <span class="token keyword">in</span> X<span class="token punctuation">:</span>            counts <span class="token operator">=</span> self<span class="token punctuation">.</span>get_word_counts<span class="token punctuation">(</span>self<span class="token punctuation">.</span>tokenize<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>            toxic_score <span class="token operator">=</span> <span class="token number">0</span>            non_toxic_score <span class="token operator">=</span> <span class="token number">0</span>            <span class="token keyword">for</span> word<span class="token punctuation">,</span> _ <span class="token keyword">in</span> counts<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>                <span class="token keyword">if</span> word <span class="token keyword">not</span> <span class="token keyword">in</span> self<span class="token punctuation">.</span>vocab<span class="token punctuation">:</span> <span class="token keyword">continue</span>                <span class="token comment"># Add Laplace smoothing</span>                log_w_given_toxic <span class="token operator">=</span> math<span class="token punctuation">.</span>log<span class="token punctuation">(</span>                    <span class="token punctuation">(</span>self<span class="token punctuation">.</span>word_counts<span class="token punctuation">[</span><span class="token string">'toxic'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>get<span class="token punctuation">(</span>word<span class="token punctuation">,</span> <span class="token number">0.0</span><span class="token punctuation">)</span><span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token operator">/</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>num_reviews<span class="token punctuation">[</span><span class="token string">'toxic'</span><span class="token punctuation">]</span><span class="token operator">+</span><span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>vocab<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>                log_w_given_non_toxic <span class="token operator">=</span> math<span class="token punctuation">.</span>log<span class="token punctuation">(</span>                    <span class="token punctuation">(</span>self<span class="token punctuation">.</span>word_counts<span class="token punctuation">[</span><span class="token string">'non-toxic'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>get<span class="token punctuation">(</span>word<span class="token punctuation">,</span> <span class="token number">0.0</span><span class="token punctuation">)</span><span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token operator">/</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>num_reviews<span class="token punctuation">[</span><span class="token string">'non-toxic'</span><span class="token punctuation">]</span><span class="token operator">+</span><span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>vocab<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>                toxic_score <span class="token operator">+=</span> log_w_given_toxic                non_toxic_score <span class="token operator">+=</span> log_w_given_non_toxic            toxic_score <span class="token operator">+=</span> self<span class="token punctuation">.</span>log_class_priors<span class="token punctuation">[</span><span class="token string">'toxic'</span><span class="token punctuation">]</span>            non_toxic_score <span class="token operator">+=</span> self<span class="token punctuation">.</span>log_class_priors<span class="token punctuation">[</span><span class="token string">'non-toxic'</span><span class="token punctuation">]</span>            <span class="token keyword">if</span> toxic_score <span class="token operator">></span> non_toxic_score<span class="token punctuation">:</span>                result<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>            <span class="token keyword">else</span><span class="token punctuation">:</span>                result<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>                        <span class="token keyword">return</span> result<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>Train Naive Bayes Classifier for training dataset.</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">HSD <span class="token operator">=</span> HateSpeechDetector<span class="token punctuation">(</span><span class="token punctuation">)</span>HSD<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>df_train<span class="token punctuation">[</span><span class="token string">"tweet"</span><span class="token punctuation">]</span><span class="token punctuation">,</span> df_train<span class="token punctuation">[</span><span class="token string">"label"</span><span class="token punctuation">]</span><span class="token punctuation">)</span>pred <span class="token operator">=</span> HSD<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>df_valid<span class="token punctuation">[</span><span class="token string">"tweet"</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>metrics<span class="token punctuation">.</span>classification_report<span class="token punctuation">(</span>df_valid<span class="token punctuation">[</span><span class="token string">"label"</span><span class="token punctuation">]</span><span class="token punctuation">,</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>pred<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>metrics<span class="token punctuation">.</span>accuracy_score<span class="token punctuation">(</span>df_valid<span class="token punctuation">[</span><span class="token string">"label"</span><span class="token punctuation">]</span><span class="token punctuation">,</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>pred<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>metrics<span class="token punctuation">.</span>f1_score<span class="token punctuation">(</span>df_valid<span class="token punctuation">[</span><span class="token string">"label"</span><span class="token punctuation">]</span><span class="token punctuation">,</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>pred<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>metrics<span class="token punctuation">.</span>confusion_matrix<span class="token punctuation">(</span>df_valid<span class="token punctuation">[</span><span class="token string">"label"</span><span class="token punctuation">]</span><span class="token punctuation">,</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>pred<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><pre class="line-numbers language-console" data-language="console"><code class="language-console">              precision    recall  f1-score   support           0       0.93      1.00      0.96      5701           1       1.00      0.05      0.09       443    accuracy                           0.93      6144   macro avg       0.97      0.52      0.53      6144weighted avg       0.94      0.93      0.90      61440.93147786458333340.09462365591397849[[5701    0] [ 421   22]]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>The result got worse. Naive Bayes got lower f1-score (14.7%) on test dataset. Next, I want to try under resampling strategies. We shall know that this is a highly imbalanced datasets (22803 for “0” and 1772 for “1”), so I’m going to adopted resampling technique for dealing with highly imbalanced datasets. It consists of removing samples from the majority class (under-sampling) or adding more examples from the minority class (over-sampling).</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># Class count</span>count_class_0<span class="token punctuation">,</span> count_class_1 <span class="token operator">=</span> df_train<span class="token punctuation">.</span>label<span class="token punctuation">.</span>value_counts<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment"># Divide by class</span>df_class_0 <span class="token operator">=</span> df_train<span class="token punctuation">[</span>df_train<span class="token punctuation">[</span><span class="token string">'label'</span><span class="token punctuation">]</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">]</span>df_class_1 <span class="token operator">=</span> df_train<span class="token punctuation">[</span>df_train<span class="token punctuation">[</span><span class="token string">'label'</span><span class="token punctuation">]</span> <span class="token operator">==</span> <span class="token number">1</span><span class="token punctuation">]</span>df_class_0_under <span class="token operator">=</span> df_class_0<span class="token punctuation">.</span>sample<span class="token punctuation">(</span>count_class_1<span class="token punctuation">)</span>df_train_under <span class="token operator">=</span> pd<span class="token punctuation">.</span>concat<span class="token punctuation">(</span><span class="token punctuation">[</span>df_class_0_under<span class="token punctuation">,</span> df_class_1<span class="token punctuation">]</span><span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'Random under-sampling:'</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>df_train_under<span class="token punctuation">.</span>label<span class="token punctuation">.</span>value_counts<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>rcParams<span class="token punctuation">[</span><span class="token string">"figure.figsize"</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">15</span><span class="token punctuation">,</span> <span class="token number">6</span>df_train_under<span class="token punctuation">.</span>label<span class="token punctuation">.</span>value_counts<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>plot<span class="token punctuation">(</span>kind<span class="token operator">=</span><span class="token string">'bar'</span><span class="token punctuation">,</span> title<span class="token operator">=</span><span class="token string">'Count (target)'</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><img src="/Hexo-Blog/2021/02/07/2021-02-07-twitter-hate-speech-detection/under-sampling.png" class=""><p>Re-train the Bayes model.</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">HSD <span class="token operator">=</span> HateSpeechDetector<span class="token punctuation">(</span><span class="token punctuation">)</span>HSD<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>df_train_under<span class="token punctuation">[</span><span class="token string">"tweet"</span><span class="token punctuation">]</span><span class="token punctuation">,</span> df_train_under<span class="token punctuation">[</span><span class="token string">"label"</span><span class="token punctuation">]</span><span class="token punctuation">)</span>pred <span class="token operator">=</span> HSD<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>df_valid<span class="token punctuation">[</span><span class="token string">"tweet"</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>metrics<span class="token punctuation">.</span>classification_report<span class="token punctuation">(</span>df_valid<span class="token punctuation">[</span><span class="token string">"label"</span><span class="token punctuation">]</span><span class="token punctuation">,</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>pred<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>metrics<span class="token punctuation">.</span>accuracy_score<span class="token punctuation">(</span>df_valid<span class="token punctuation">[</span><span class="token string">"label"</span><span class="token punctuation">]</span><span class="token punctuation">,</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>pred<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>metrics<span class="token punctuation">.</span>f1_score<span class="token punctuation">(</span>df_valid<span class="token punctuation">[</span><span class="token string">"label"</span><span class="token punctuation">]</span><span class="token punctuation">,</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>pred<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>metrics<span class="token punctuation">.</span>confusion_matrix<span class="token punctuation">(</span>df_valid<span class="token punctuation">[</span><span class="token string">"label"</span><span class="token punctuation">]</span><span class="token punctuation">,</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>pred<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><pre class="line-numbers language-console" data-language="console"><code class="language-console">              precision    recall  f1-score   support           0       0.99      0.85      0.92      5701           1       0.32      0.87      0.47       443    accuracy                           0.86      6144   macro avg       0.65      0.86      0.69      6144weighted avg       0.94      0.86      0.88      61440.85628255208333340.46710923355461675[[4874  827] [  56  387]]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>This time, we got a higher f1-score (46.9%) on test datasets, and it’s even better than the f1-score (29.52%) of BERT.</p><p>Therefore, I try to utilise under sampling on BERT again to see if we can increase the f1-score!</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">train_dataset <span class="token operator">=</span> TwitterDataset<span class="token punctuation">(</span>df_train_under<span class="token punctuation">.</span>review<span class="token punctuation">.</span>values<span class="token punctuation">,</span> df_train_under<span class="token punctuation">.</span>label<span class="token punctuation">.</span>values<span class="token punctuation">,</span> max_len<span class="token operator">=</span>PARAM<span class="token punctuation">.</span>MAX_LENGTH<span class="token punctuation">)</span>valid_dataset <span class="token operator">=</span> TwitterDataset<span class="token punctuation">(</span>df_valid<span class="token punctuation">.</span>review<span class="token punctuation">.</span>values<span class="token punctuation">,</span> df_valid<span class="token punctuation">.</span>label<span class="token punctuation">.</span>values<span class="token punctuation">,</span> max_len<span class="token operator">=</span>PARAM<span class="token punctuation">.</span>MAX_LENGTH<span class="token punctuation">)</span>n_train_steps <span class="token operator">=</span> <span class="token builtin">int</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>df_train<span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token number">32</span> <span class="token operator">*</span> <span class="token number">10</span><span class="token punctuation">)</span>model <span class="token operator">=</span> TwitterClassifier<span class="token punctuation">(</span>num_train_steps<span class="token operator">=</span>n_train_steps<span class="token punctuation">,</span> num_classes<span class="token operator">=</span>PARAM<span class="token punctuation">.</span>NUM_CLASSES<span class="token punctuation">,</span> weight<span class="token operator">=</span>np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1.0</span><span class="token punctuation">,</span> <span class="token number">1.0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>tb_logger <span class="token operator">=</span> tez<span class="token punctuation">.</span>callbacks<span class="token punctuation">.</span>TensorBoardLogger<span class="token punctuation">(</span>log_dir<span class="token operator">=</span>PARAM<span class="token punctuation">.</span>TF_LOG<span class="token punctuation">)</span>es <span class="token operator">=</span> tez<span class="token punctuation">.</span>callbacks<span class="token punctuation">.</span>EarlyStopping<span class="token punctuation">(</span>monitor<span class="token operator">=</span><span class="token string">"valid_loss"</span><span class="token punctuation">,</span>                                  model_path<span class="token operator">=</span><span class="token string">"./models/bert_under.bin"</span><span class="token punctuation">,</span>                                  patience<span class="token operator">=</span>PARAM<span class="token punctuation">.</span>ES_PATIENCE<span class="token punctuation">)</span>model<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>    train_dataset<span class="token punctuation">,</span>    valid_dataset<span class="token operator">=</span>valid_dataset<span class="token punctuation">,</span>    train_bs<span class="token operator">=</span>PARAM<span class="token punctuation">.</span>TRAIN_BATCH_SIZE<span class="token punctuation">,</span>     valid_bs<span class="token operator">=</span>PARAM<span class="token punctuation">.</span>VALID_BATCH_SIZE<span class="token punctuation">,</span>     device<span class="token operator">=</span>PARAM<span class="token punctuation">.</span>DEVICE<span class="token punctuation">,</span>    epochs<span class="token operator">=</span>PARAM<span class="token punctuation">.</span>EPOCH<span class="token punctuation">,</span>    callbacks<span class="token operator">=</span><span class="token punctuation">[</span>tb_logger<span class="token punctuation">,</span> es<span class="token punctuation">]</span><span class="token punctuation">,</span>    n_jobs<span class="token operator">=</span>PARAM<span class="token punctuation">.</span>N_JOB<span class="token punctuation">,</span>     fp16<span class="token operator">=</span>PARAM<span class="token punctuation">.</span>FP16<span class="token punctuation">,</span><span class="token punctuation">)</span>model<span class="token punctuation">.</span>save<span class="token punctuation">(</span><span class="token string">"./models/bert_under.bin"</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><img src="/Hexo-Blog/2021/02/07/2021-02-07-twitter-hate-speech-detection/history2.png" class=""><p>Unfortunately, BERT with under sampling technique get the lowest f1-score (13.1%)…</p><h1 id="Data-Augmentation"><a href="#Data-Augmentation" class="headerlink" title="Data Augmentation"></a>Data Augmentation</h1><p>Data augmentation in data analysis are techniques used to increase the amount of data by adding slightly modified copies of already existing data or newly created synthetic data from existing data. It acts as a regularizer and helps reduce overfitting when training a machine learning model.</p><p>In this post, I will primarily address data augmentation with regard to the Text Classification and some of these techniques listed below.</p><ul><li>Backtranslation</li><li>Synonym Word Replacement<ul><li>Pre-trained Word Embedding based: Word2Vec, , GloVe, FastText, …</li><li>Contexual Word Embedding based: ELMo, BERT, DistilBERT, …</li><li>Lexical based: Wordnet, …</li></ul></li><li>Generative Models: BERT, XLNet, RoBERTa, BART, T%</li><li>Random Operation:<ul><li>Random Insertion</li><li>Random Swapping</li><li>Random Deletion</li></ul></li></ul><h2 id="Naive-Bayes-with-Wordnet-Augmentation"><a href="#Naive-Bayes-with-Wordnet-Augmentation" class="headerlink" title="Naive Bayes with Wordnet Augmentation"></a>Naive Bayes with Wordnet Augmentation</h2><p>I use WordNet, a large linguistic database, to identify relevant synonyms.</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> nlpaug<span class="token punctuation">.</span>augmenter<span class="token punctuation">.</span>char <span class="token keyword">as</span> nac<span class="token keyword">import</span> nlpaug<span class="token punctuation">.</span>augmenter<span class="token punctuation">.</span>word <span class="token keyword">as</span> naw<span class="token keyword">import</span> nlpaug<span class="token punctuation">.</span>augmenter<span class="token punctuation">.</span>sentence <span class="token keyword">as</span> nas<span class="token keyword">import</span> nlpaug<span class="token punctuation">.</span>flow <span class="token keyword">as</span> nafc<span class="token keyword">from</span> nlpaug<span class="token punctuation">.</span>util <span class="token keyword">import</span> Action<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><code>nlpaug</code> helps you with augmenting nlp for your machine learning projects.</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">augment_text</span><span class="token punctuation">(</span>df<span class="token punctuation">,</span> samples<span class="token operator">=</span><span class="token number">300</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    aug <span class="token operator">=</span> naw<span class="token punctuation">.</span>SynonymAug<span class="token punctuation">(</span>aug_src<span class="token operator">=</span><span class="token string">'wordnet'</span><span class="token punctuation">)</span>    aug_text <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>        <span class="token comment"># Selecting the minority class samples</span>    df_n <span class="token operator">=</span> df<span class="token punctuation">[</span>df<span class="token punctuation">.</span>label<span class="token operator">==</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>reset_index<span class="token punctuation">(</span>drop<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>    <span class="token comment">## Data augmentation loop</span>    <span class="token keyword">for</span> i <span class="token keyword">in</span> tqdm<span class="token punctuation">(</span>np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>randint<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>df_n<span class="token punctuation">)</span><span class="token punctuation">,</span> samples<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        text <span class="token operator">=</span> df_n<span class="token punctuation">.</span>iloc<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'tweet'</span><span class="token punctuation">]</span>        augmented_text <span class="token operator">=</span> aug<span class="token punctuation">.</span>augment<span class="token punctuation">(</span>text<span class="token punctuation">)</span>        aug_text<span class="token punctuation">.</span>append<span class="token punctuation">(</span>augmented_text<span class="token punctuation">)</span>        <span class="token keyword">return</span> pd<span class="token punctuation">.</span>DataFrame<span class="token punctuation">(</span><span class="token punctuation">&#123;</span>        <span class="token string">'tweet'</span><span class="token punctuation">:</span> aug_text<span class="token punctuation">,</span>         <span class="token string">'label'</span><span class="token punctuation">:</span> <span class="token number">1</span><span class="token punctuation">&#125;</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>Concatenate with the original dataframe.</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">df_train_1_aug <span class="token operator">=</span> augment_text<span class="token punctuation">(</span>df_train<span class="token punctuation">,</span> samples<span class="token operator">=</span><span class="token number">20000</span><span class="token punctuation">)</span>df_train_all_aug <span class="token operator">=</span> pd<span class="token punctuation">.</span>concat<span class="token punctuation">(</span><span class="token punctuation">[</span>df_train<span class="token punctuation">,</span> df_train_1_aug<span class="token punctuation">]</span><span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>df_train_all_aug <span class="token operator">=</span> df_train_all_aug<span class="token punctuation">.</span>reset_index<span class="token punctuation">(</span>drop<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>df_train_all_aug<span class="token punctuation">[</span><span class="token string">"review"</span><span class="token punctuation">]</span> <span class="token operator">=</span> df_train_all_aug<span class="token punctuation">[</span><span class="token string">"tweet"</span><span class="token punctuation">]</span><span class="token punctuation">.</span>swifter<span class="token punctuation">.</span><span class="token builtin">apply</span><span class="token punctuation">(</span>clean_tweets<span class="token punctuation">)</span>df_train_all_aug<span class="token punctuation">[</span><span class="token string">"length"</span><span class="token punctuation">]</span> <span class="token operator">=</span> df_train_all_aug<span class="token punctuation">[</span><span class="token string">"review"</span><span class="token punctuation">]</span><span class="token punctuation">.</span>swifter<span class="token punctuation">.</span><span class="token builtin">apply</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><img src="/Hexo-Blog/2021/02/07/2021-02-07-twitter-hate-speech-detection/train-aug.png" class=""><p>Traing Naive Bayes with text augmentation.</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">HSD <span class="token operator">=</span> HateSpeechDetector<span class="token punctuation">(</span><span class="token punctuation">)</span>HSD<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>df_train_all_aug<span class="token punctuation">[</span><span class="token string">"tweet"</span><span class="token punctuation">]</span><span class="token punctuation">,</span> df_train_all_aug<span class="token punctuation">[</span><span class="token string">"label"</span><span class="token punctuation">]</span><span class="token punctuation">)</span>pred <span class="token operator">=</span> HSD<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>df_valid<span class="token punctuation">[</span><span class="token string">"tweet"</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>metrics<span class="token punctuation">.</span>classification_report<span class="token punctuation">(</span>df_valid<span class="token punctuation">[</span><span class="token string">"label"</span><span class="token punctuation">]</span><span class="token punctuation">,</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>pred<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>metrics<span class="token punctuation">.</span>accuracy_score<span class="token punctuation">(</span>df_valid<span class="token punctuation">[</span><span class="token string">"label"</span><span class="token punctuation">]</span><span class="token punctuation">,</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>pred<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>metrics<span class="token punctuation">.</span>f1_score<span class="token punctuation">(</span>df_valid<span class="token punctuation">[</span><span class="token string">"label"</span><span class="token punctuation">]</span><span class="token punctuation">,</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>pred<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>metrics<span class="token punctuation">.</span>confusion_matrix<span class="token punctuation">(</span>df_valid<span class="token punctuation">[</span><span class="token string">"label"</span><span class="token punctuation">]</span><span class="token punctuation">,</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>pred<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><pre class="line-numbers language-console" data-language="console"><code class="language-console">              precision    recall  f1-score   support           0       0.98      0.94      0.96      5701           1       0.50      0.80      0.61       443    accuracy                           0.93      6144   macro avg       0.74      0.87      0.79      6144weighted avg       0.95      0.93      0.93      61440.92692057291666660.6112554112554112[[5342  359] [  90  353]]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>It got a f1-score of 63.1% on test dataset! Seems pretty well! In this case, I was wondering, what if I do the same text augmentation operation on validation data? Will it increase the accuracy and f1-score? Let’s see!</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">df_valid_1_aug <span class="token operator">=</span> augment_text<span class="token punctuation">(</span>df_valid<span class="token punctuation">,</span> samples<span class="token operator">=</span><span class="token number">5000</span><span class="token punctuation">)</span>df_valid_all_aug <span class="token operator">=</span> pd<span class="token punctuation">.</span>concat<span class="token punctuation">(</span><span class="token punctuation">[</span>df_valid<span class="token punctuation">,</span> df_valid_1_aug<span class="token punctuation">]</span><span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>df_valid_all_aug <span class="token operator">=</span> df_valid_all_aug<span class="token punctuation">.</span>reset_index<span class="token punctuation">(</span>drop<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>df_valid_all_aug<span class="token punctuation">[</span><span class="token string">"review"</span><span class="token punctuation">]</span> <span class="token operator">=</span> df_valid_all_aug<span class="token punctuation">[</span><span class="token string">"tweet"</span><span class="token punctuation">]</span><span class="token punctuation">.</span>swifter<span class="token punctuation">.</span><span class="token builtin">apply</span><span class="token punctuation">(</span>clean_tweets<span class="token punctuation">)</span>df_valid_all_aug<span class="token punctuation">[</span><span class="token string">"length"</span><span class="token punctuation">]</span> <span class="token operator">=</span> df_valid_all_aug<span class="token punctuation">[</span><span class="token string">"review"</span><span class="token punctuation">]</span><span class="token punctuation">.</span>swifter<span class="token punctuation">.</span><span class="token builtin">apply</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><img src="/Hexo-Blog/2021/02/07/2021-02-07-twitter-hate-speech-detection/valid-aug.png" class=""><pre class="line-numbers language-python" data-language="python"><code class="language-python">HSD <span class="token operator">=</span> HateSpeechDetector<span class="token punctuation">(</span><span class="token punctuation">)</span>HSD<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>df_train_all_aug<span class="token punctuation">[</span><span class="token string">"tweet"</span><span class="token punctuation">]</span><span class="token punctuation">,</span> df_train_all_aug<span class="token punctuation">[</span><span class="token string">"label"</span><span class="token punctuation">]</span><span class="token punctuation">)</span>pred <span class="token operator">=</span> HSD<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>df_valid_all_aug<span class="token punctuation">[</span><span class="token string">"tweet"</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>metrics<span class="token punctuation">.</span>classification_report<span class="token punctuation">(</span>df_valid_all_aug<span class="token punctuation">[</span><span class="token string">"label"</span><span class="token punctuation">]</span><span class="token punctuation">,</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>pred<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>metrics<span class="token punctuation">.</span>accuracy_score<span class="token punctuation">(</span>df_valid_all_aug<span class="token punctuation">[</span><span class="token string">"label"</span><span class="token punctuation">]</span><span class="token punctuation">,</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>pred<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>metrics<span class="token punctuation">.</span>f1_score<span class="token punctuation">(</span>df_valid_all_aug<span class="token punctuation">[</span><span class="token string">"label"</span><span class="token punctuation">]</span><span class="token punctuation">,</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>pred<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>metrics<span class="token punctuation">.</span>confusion_matrix<span class="token punctuation">(</span>df_valid_all_aug<span class="token punctuation">[</span><span class="token string">"label"</span><span class="token punctuation">]</span><span class="token punctuation">,</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>pred<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><pre class="line-numbers language-console" data-language="console"><code class="language-console">              precision    recall  f1-score   support           0       0.93      0.94      0.94      5701           1       0.93      0.93      0.93      5443    accuracy                           0.93     11144   macro avg       0.93      0.93      0.93     11144weighted avg       0.93      0.93      0.93     111440.93395549174443650.9322782480677219[[5342  359] [ 377 5066]]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>It got a f1-score of 63.6% on test dataset! It’s slightly higher than only operating augmentation on training dataset.</p><h2 id="Naive-Bayes-with-BERT-Augmentation"><a href="#Naive-Bayes-with-BERT-Augmentation" class="headerlink" title="Naive Bayes with BERT Augmentation"></a>Naive Bayes with BERT Augmentation</h2><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">augment_text_using_bert</span><span class="token punctuation">(</span>df<span class="token punctuation">,</span> samples<span class="token operator">=</span><span class="token number">300</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    aug <span class="token operator">=</span> naw<span class="token punctuation">.</span>ContextualWordEmbsAug<span class="token punctuation">(</span>model_path<span class="token operator">=</span><span class="token string">'bert-base-uncased'</span><span class="token punctuation">,</span> action<span class="token operator">=</span><span class="token string">"substitute"</span><span class="token punctuation">)</span>    aug_text <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>        <span class="token comment"># Selecting the minority class samples</span>    df_n <span class="token operator">=</span> df<span class="token punctuation">[</span>df<span class="token punctuation">.</span>label<span class="token operator">==</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>reset_index<span class="token punctuation">(</span>drop<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>    <span class="token comment">## Data augmentation loop</span>    <span class="token keyword">for</span> i <span class="token keyword">in</span> tqdm<span class="token punctuation">(</span>np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>randint<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>df_n<span class="token punctuation">)</span><span class="token punctuation">,</span> samples<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        text <span class="token operator">=</span> df_n<span class="token punctuation">.</span>iloc<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'tweet'</span><span class="token punctuation">]</span>        augmented_text <span class="token operator">=</span> aug<span class="token punctuation">.</span>augment<span class="token punctuation">(</span>text<span class="token punctuation">)</span>        aug_text<span class="token punctuation">.</span>append<span class="token punctuation">(</span>augmented_text<span class="token punctuation">)</span>        <span class="token keyword">return</span> pd<span class="token punctuation">.</span>DataFrame<span class="token punctuation">(</span><span class="token punctuation">&#123;</span>        <span class="token string">'tweet'</span><span class="token punctuation">:</span> aug_text<span class="token punctuation">,</span>         <span class="token string">'label'</span><span class="token punctuation">:</span> <span class="token number">1</span><span class="token punctuation">&#125;</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>Basically, the code is the same as the previous. I listed the result (on test dataset) in one table.</p><div style="display: flex; justify-content: center;">    <table class="styled-table">        <thead>            <tr>                <th>Model</th>                <th>Augmentation</th>                <th>F1-score</th>            </tr>        </thead>        <tbody>            <tr>                <td>Naive Bayes</td>                <td>Wordnet (Train)</td>                <td>🥈 0.6307</td>            </tr>            <tr>                <td>Naive Bayes</td>                <td>Wordnet (Train+Valid)</td>                <td>🥇 0.6356</td>            </tr>            <tr>                <td>Naive Bayes</td>                <td>BERT (Train)</td>                <td>0.5136</td>            </tr>            <tr>                <td>Naive Bayes</td>                <td>BERT (Train+Valid)</td>                <td>🥉 0.5200</td>            </tr>        </tbody>    </table></div><p>Next, I investigate the wordnet augmentation on training datatset for BERT model. This results in a lower f1-score of 35.2% on test dataset. </p><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>In this report, I proposed several solutions to the detection of hate speech and offensive language on Twitter through machine learning (Naive Bayes) and deep learning (BERT). Most of the time, BERT performed worse than Naive Bayes. In the future, if I have spare time, I may try ensemble methods to see whether it can increase the f1-score significantly! See you next time!</p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ol><li><a href="https://github.com/abhishekkrthakur/tez">https://github.com/abhishekkrthakur/tez</a></li><li><a href="https://www.kaggle.com/renatobmlr/pytorch-imbalanced-classes">https://www.kaggle.com/renatobmlr/pytorch-imbalanced-classes</a></li><li><a href="https://github.com/makcedward/nlpaug/blob/master/example/textual_augmenter.ipynb">https://github.com/makcedward/nlpaug/blob/master/example/textual_augmenter.ipynb</a></li><li><a href="https://arxiv.org/pdf/1904.00962.pdf">https://arxiv.org/pdf/1904.00962.pdf</a></li><li><a href="https://pythonmachinelearning.pro/text-classification-tutorial-with-naive-bayes/">https://pythonmachinelearning.pro/text-classification-tutorial-with-naive-bayes/</a></li><li><a href="https://www.kaggle.com/getting-started/14998">https://www.kaggle.com/getting-started/14998</a></li><li><a href="https://neptune.ai/blog/data-augmentation-nlp">https://neptune.ai/blog/data-augmentation-nlp</a></li><li><a href="https://saurabhk30.medium.com/5-data-augmentation-techniques-for-text-classification-d14f6d8bd6aa">https://saurabhk30.medium.com/5-data-augmentation-techniques-for-text-classification-d14f6d8bd6aa</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> PyTorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Prevent Colab from Disconnecting</title>
      <link href="/Hexo-Blog/2021/02/06/2021-02-06-prevent-colab-from-disconnecting/"/>
      <url>/Hexo-Blog/2021/02/06/2021-02-06-prevent-colab-from-disconnecting/</url>
      
        <content type="html"><![CDATA[<p>Google Colab notebooks have an idle timeout of 90 minutes and absolute timeout of 12 hours. This means, if user does not interact with his Google Colab notebook for more than 90 minutes, its instance is automatically terminated. Also, maximum lifetime of a Colab instance is 12 hours.</p><h1 id="Solution"><a href="#Solution" class="headerlink" title="Solution"></a>Solution</h1><p>It is wonderful that Google colab can use GPU, TPU, and other computing resources for artificial intelligence calculation for free, but the calculation page will be automatically dropped after a period of no operation, and the previous training data will be lost, which is disappointing.</p><p>Finally found a way to keep it from going offline automatically, with a JavaScript program that automatically clicks the connect button.</p><p><strong>Step 1</strong>: Press the shortcut keys <code>CTRL + SHIFT + I</code> and select <code>Console</code>.</p><p><strong>Step 2</strong>: Copy and paste the below code, and hit <code>ENTER</code>, the program will be ready to run.</p><pre class="line-numbers language-javascript" data-language="javascript"><code class="language-javascript"><span class="token keyword">function</span> <span class="token function">ClickConnect</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">&#123;</span>  console<span class="token punctuation">.</span><span class="token function">log</span><span class="token punctuation">(</span><span class="token string">"Working"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>   document    <span class="token punctuation">.</span><span class="token function">querySelector</span><span class="token punctuation">(</span><span class="token string">"#top-toolbar > colab-connect-button"</span><span class="token punctuation">)</span>    <span class="token punctuation">.</span>shadowRoot    <span class="token punctuation">.</span><span class="token function">querySelector</span><span class="token punctuation">(</span><span class="token string">"#connect"</span><span class="token punctuation">)</span>    <span class="token punctuation">.</span><span class="token function">click</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">&#125;</span> <span class="token function">setInterval</span><span class="token punctuation">(</span>ClickConnect<span class="token punctuation">,</span> <span class="token number">5</span><span class="token operator">*</span><span class="token number">60000</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>If you still have problems running it or want to get more methods, you can refer <a href="https://stackoverflow.com/questions/57113226/how-to-prevent-google-colab-from-disconnecting">here</a>.</p>]]></content>
      
      
      <categories>
          
          <category> Installation </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Colab </tag>
            
            <tag> JavaScript </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SSH into Google Colab</title>
      <link href="/Hexo-Blog/2021/02/06/2021-02-06-ssh-into-google-colab/"/>
      <url>/Hexo-Blog/2021/02/06/2021-02-06-ssh-into-google-colab/</url>
      
        <content type="html"><![CDATA[<p>Sometimes the code I write may not work in my local machine, however, it works in Google Colab. So, I wanna connect to Google Colab terminal using SSH.</p><h1 id="Procedure"><a href="#Procedure" class="headerlink" title="Procedure"></a>Procedure</h1><p><strong>Step 1</strong> Create a new notebook in Google Colab.<br><strong>Step 2</strong> Copy and paste below code in Colab which installs ngrok and creates a tunnel for us (code taken from this <a href="https://gist.github.com/yashkumaratri/204755a85977586cebbb58dc971496da#file-google-colab-ssh">source</a>).</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token comment">#CODE</span><span class="token comment">#Generate root password</span><span class="token function">import</span> random, stringpassword <span class="token operator">=</span> <span class="token string">''</span>.join<span class="token punctuation">(</span>random.choice<span class="token punctuation">(</span>string.ascii_letters + string.digits<span class="token punctuation">)</span> <span class="token keyword">for</span> <span class="token for-or-select variable">i</span> <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">))</span><span class="token comment">#Download ngrok</span><span class="token operator">!</span> <span class="token function">wget</span> -q -c -nc https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip<span class="token operator">!</span> <span class="token function">unzip</span> -qq -n ngrok-stable-linux-amd64.zip<span class="token comment">#Setup sshd</span><span class="token operator">!</span> <span class="token function">apt-get</span> <span class="token function">install</span> -qq -o<span class="token operator">=</span>Dpkg::Use-Pty<span class="token operator">=</span><span class="token number">0</span> openssh-server pwgen <span class="token operator">></span> /dev/null<span class="token comment">#Set root password</span><span class="token operator">!</span> <span class="token builtin class-name">echo</span> root:<span class="token variable">$password</span> <span class="token operator">|</span> chpasswd<span class="token operator">!</span> <span class="token function">mkdir</span> -p /var/run/sshd<span class="token operator">!</span> <span class="token builtin class-name">echo</span> <span class="token string">"PermitRootLogin yes"</span> <span class="token operator">>></span> /etc/ssh/sshd_config<span class="token operator">!</span> <span class="token builtin class-name">echo</span> <span class="token string">"PasswordAuthentication yes"</span> <span class="token operator">>></span> /etc/ssh/sshd_config<span class="token operator">!</span> <span class="token builtin class-name">echo</span> <span class="token string">"LD_LIBRARY_PATH=/usr/lib64-nvidia"</span> <span class="token operator">>></span> /root/.bashrc<span class="token operator">!</span> <span class="token builtin class-name">echo</span> <span class="token string">"export LD_LIBRARY_PATH"</span> <span class="token operator">>></span> /root/.bashrc<span class="token comment">#Run sshd</span>get_ipython<span class="token punctuation">(</span><span class="token punctuation">)</span>.system_raw<span class="token punctuation">(</span><span class="token string">'/usr/sbin/sshd -D &amp;'</span><span class="token punctuation">)</span><span class="token comment">#Ask token</span>print<span class="token punctuation">(</span><span class="token string">"Copy authtoken from https://dashboard.ngrok.com/auth"</span><span class="token punctuation">)</span><span class="token function">import</span> getpassauthtoken <span class="token operator">=</span> getpass.getpass<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment">#Create tunnel</span>get_ipython<span class="token punctuation">(</span><span class="token punctuation">)</span>.system_raw<span class="token punctuation">(</span><span class="token string">'./ngrok authtoken <span class="token variable">$authtoken</span> &amp;&amp; ./ngrok tcp 22 &amp;'</span><span class="token punctuation">)</span><span class="token comment">#Print root password</span>print<span class="token punctuation">(</span><span class="token string">"Root password: &#123;&#125;"</span>.format<span class="token punctuation">(</span>password<span class="token punctuation">))</span><span class="token comment">#Get public address</span><span class="token operator">!</span> <span class="token function">curl</span> -s http://localhost:4040/api/tunnels <span class="token operator">|</span> python3 -c <span class="token punctuation">\</span>    <span class="token string">"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])"</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>Step 3</strong> Enter the authorization token which can be found in your ngrok account. So copy and paste it. Then you will be given output as following.</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">Copy authtoken from https://dashboard.ngrok.com/auth··········Root password: MryWEw9dXuWAHJgKN0O5tcp://4.tcp.ngrok.io:17523<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p><strong>Step 4</strong> Type into your local machine, and type in the password.</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token function">ssh</span> root@4.tcp.ngrok.io -p <span class="token number">17523</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><img src="/Hexo-Blog/2021/02/06/2021-02-06-ssh-into-google-colab/cmd.jpg" class=""><p><strong>Step 5</strong> Run Visual Studio code server. First, mount our Google Drive.</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token comment"># Mount Google Drive and make some folders for vscode</span>from google.colab <span class="token function">import</span> drivedrive.mount<span class="token punctuation">(</span><span class="token string">'/googledrive'</span><span class="token punctuation">)</span><span class="token operator">!</span> <span class="token function">mkdir</span> -p /googledrive/My<span class="token punctuation">\</span> Drive/colabdrive<span class="token operator">!</span> <span class="token function">mkdir</span> -p /googledrive/My<span class="token punctuation">\</span> Drive/colabdrive/root/.local/share/code-server<span class="token operator">!</span> <span class="token function">ln</span> -s /googledrive/My<span class="token punctuation">\</span> Drive/colabdrive /<span class="token operator">!</span> <span class="token function">ln</span> -s /googledrive/My<span class="token punctuation">\</span> Drive/colabdrive/root/.local/share/code-server /root/.local/share/<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>Step 6</strong> Install and run the server version of Visual Studio Code.</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token operator">!</span> <span class="token function">curl</span> -fsSL https://code-server.dev/install.sh <span class="token operator">|</span> <span class="token function">sh</span> <span class="token operator">></span> /dev/null<span class="token operator">!</span> code-server --bind-addr <span class="token number">127.0</span>.0.1:9999 --auth none <span class="token operator">&amp;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><img src="/Hexo-Blog/2021/02/06/2021-02-06-ssh-into-google-colab/vs.png" class=""><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ol><li><a href="https://towardsdatascience.com/colab-free-gpu-ssh-visual-studio-code-server-36fe1d3c5243">https://towardsdatascience.com/colab-free-gpu-ssh-visual-studio-code-server-36fe1d3c5243</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Installation </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Colab </tag>
            
            <tag> SSH </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Audio-based Song Genre Classification</title>
      <link href="/Hexo-Blog/2021/02/05/2021-02-05-audio-based-song-genre-classification/"/>
      <url>/Hexo-Blog/2021/02/05/2021-02-05-audio-based-song-genre-classification/</url>
      
        <content type="html"><![CDATA[<p>Visualizing sound is kind of a trippy concept. There are some mesmerizing ways to do that, and also more mathematical ones, which I will explore both in this article.</p><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>I wrote an <a href="/Hexo-Blog/2019/06/11/2019-06-11-categorising-song-genre-by-analysing-lyrics/" title="[article]">[article]</a> about lyrics-based song genre classifier before. I was wondering whether using the audio-based data will improve the performance. For this reason, this work takes a closer look at expoiting audio-based approach to build a multi-class classifier.</p><h1 id="Audio-Processing"><a href="#Audio-Processing" class="headerlink" title="Audio Processing"></a>Audio Processing</h1><p>Let’s take one song for instance, </p><div style="display: flex;justify-content: center;">    <div class="ready-player-1">        <audio crossorigin>            <source src="<img src="/Hexo-Blog/2021/02/05/2021-02-05-audio-based-song-genre-classification/Beyonce-Halo.mp3" class="">" type="audio/mpeg">        </audio>    </div></div><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/greghub/green-audio-player/dist/css/green-audio-player.min.css"><script src="https://cdn.jsdelivr.net/gh/greghub/green-audio-player/dist/js/green-audio-player.min.js"></script><script>    document.addEventListener('DOMContentLoaded', function() {        new GreenAudioPlayer('.ready-player-1', { showTooltips: true, showDownloadButton: false, enableKeystrokes: true });    });</script><hr><h2 id="Librosa"><a href="#Librosa" class="headerlink" title="Librosa"></a>Librosa</h2><p><code>librosa</code> is a python package for music and audio analysis. It provides the building blocks necessary to create music information retrieval systems. Let’s load the audio file we want to process first.</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">signal<span class="token punctuation">,</span> sr <span class="token operator">=</span> librosa<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">"Halo.mp3"</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>Great, a wonderful song, isn’t it? Then how does the sound look like in a graph? </p><h2 id="Waveform"><a href="#Waveform" class="headerlink" title="Waveform"></a>Waveform</h2><p>We can now take a look at its waveform. The waveform of a signal is the shape of its graph as a function of time, independent of its time and magnitude scales and of any displacement in time. <a href="https://pudding.cool/2018/02/waveforms/">Here</a> is a interesting animation of waveform written by Josh Comeau.</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">plt<span class="token punctuation">.</span>rcParams<span class="token punctuation">[</span><span class="token string">"figure.figsize"</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">15</span><span class="token punctuation">,</span> <span class="token number">6</span>librosa<span class="token punctuation">.</span>display<span class="token punctuation">.</span>waveplot<span class="token punctuation">(</span>signal<span class="token punctuation">,</span> sr<span class="token operator">=</span>sr<span class="token punctuation">,</span> alpha<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>xlabel<span class="token punctuation">(</span><span class="token string">"Time"</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>ylabel<span class="token punctuation">(</span><span class="token string">"Amplitude"</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><img src="/Hexo-Blog/2021/02/05/2021-02-05-audio-based-song-genre-classification/waveform.png" class=""><h3 id="Sampling-Rate"><a href="#Sampling-Rate" class="headerlink" title="Sampling Rate"></a>Sampling Rate</h3><p>When we use <code>librosa.load()</code> we can set a parameter called “sr”, which stands for “sampling rate”, but what exactly does this mean? In audio production, a sampling rate defines how many times per second a sound is sampled, that is, the number of amplitude that can be recorded per second. For example, the sampling rate is 1khz, which means that the amplitude of 1000 waves can be recorded in one second.</p><img src="/Hexo-Blog/2021/02/05/2021-02-05-audio-based-song-genre-classification/sr.png" class=""><p>The default sampling rate used by Librosa is 22050, but you can pass in almost any sampling rate you like. Further, the higher sample rate technically leads to more measurements per second and a closer recreation of the original audio, so 48 kHz is often used in “professional audio” contexts more than music contexts.</p><p>We can also indicate that sampling rate (Hz) is the inverse of the period <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0" xmlns="http://www.w3.org/2000/svg" width="1.593ex" height="1.532ex" role="img" focusable="false" viewBox="0 -677 704 677" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-9-TEX-I-1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-9-TEX-I-1D447"></use></g></g></g></svg></mjx-container>. Look at the figure below in detail, the higher the sample rate, the less the sampling error.</p><div style="display: flex;justify-content: center;">    <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.781ex" xmlns="http://www.w3.org/2000/svg" width="7.035ex" height="2.737ex" role="img" focusable="false" viewBox="0 -864.9 3109.3 1209.9" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-8-TEX-I-1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path><path id="MJX-8-TEX-I-1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-8-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-8-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-8-TEX-I-1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-8-TEX-I-1D460"></use></g><g data-mml-node="mi" transform="translate(469, -150) scale(0.707)"><use xlink:href="#MJX-8-TEX-I-1D45F"></use></g></g><g data-mml-node="mo" transform="translate(1115.7, 0)"><use xlink:href="#MJX-8-TEX-N-3D"></use></g><g data-mml-node="mfrac" transform="translate(2171.5, 0)"><g data-mml-node="mn" transform="translate(292.1, 394) scale(0.707)"><use xlink:href="#MJX-8-TEX-N-31"></use></g><g data-mml-node="mi" transform="translate(220, -345) scale(0.707)"><use xlink:href="#MJX-8-TEX-I-1D447"></use></g><rect width="697.8" height="60" x="120" y="220"></rect></g></g></g></svg></mjx-container></div><img src="/Hexo-Blog/2021/02/05/2021-02-05-audio-based-song-genre-classification/sampling-rate.png" class=""><p>How many samples are necessary to ensure we are preserving the information contained in the signal? If the signal contains high frequency components, we will need to sample at a higher rate to avoid losing information that is in the signal. In general, to preserve the full information in the signal, it is necessary to sample at twice the maximum frequency of the signal. This is known as the Nyquist rate. The Sampling Theorem states that a signal can be exactly reproduced if it is sampled at a frequency F, where F is greater than twice the maximum frequency in the signal.</p><p>What happens if we sample the signal at a frequency that is lower that the Nyquist rate? When the signal is converted back into a continuous time signal, it will exhibit a phenomenon called ‘aliasing’. Aliasing is the presence of unwanted components in the reconstructed signal. These components were not present when the original signal was sampled. In addition, some of the frequencies in the original signal may be lost in the reconstructed signal. Aliasing occurs because signal frequencies can overlap if the sampling frequency is too low.</p><h3 id="Frames"><a href="#Frames" class="headerlink" title="Frames"></a>Frames</h3><ul><li>Perceivable audio to chunk.</li><li>Power of 2 # of samples.</li><li>Typical values: 256 ~ 8192.</li></ul><p>Formula for duration of a frame: </p><div style="display: flex;justify-content: center;">    <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -1.033ex" xmlns="http://www.w3.org/2000/svg" width="11.168ex" height="2.99ex" role="img" focusable="false" viewBox="0 -864.9 4936.4 1321.5" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-8-TEX-I-1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path><path id="MJX-8-TEX-I-1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"></path><path id="MJX-8-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-8-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-8-TEX-I-1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path><path id="MJX-8-TEX-I-1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-8-TEX-N-22C5" d="M78 250Q78 274 95 292T138 310Q162 310 180 294T199 251Q199 226 182 208T139 190T96 207T78 250Z"></path><path id="MJX-8-TEX-I-1D43E" d="M285 628Q285 635 228 637Q205 637 198 638T191 647Q191 649 193 661Q199 681 203 682Q205 683 214 683H219Q260 681 355 681Q389 681 418 681T463 682T483 682Q500 682 500 674Q500 669 497 660Q496 658 496 654T495 648T493 644T490 641T486 639T479 638T470 637T456 637Q416 636 405 634T387 623L306 305Q307 305 490 449T678 597Q692 611 692 620Q692 635 667 637Q651 637 651 648Q651 650 654 662T659 677Q662 682 676 682Q680 682 711 681T791 680Q814 680 839 681T869 682Q889 682 889 672Q889 650 881 642Q878 637 862 637Q787 632 726 586Q710 576 656 534T556 455L509 418L518 396Q527 374 546 329T581 244Q656 67 661 61Q663 59 666 57Q680 47 717 46H738Q744 38 744 37T741 19Q737 6 731 0H720Q680 3 625 3Q503 3 488 0H478Q472 6 472 9T474 27Q478 40 480 43T491 46H494Q544 46 544 71Q544 75 517 141T485 216L427 354L359 301L291 248L268 155Q245 63 245 58Q245 51 253 49T303 46H334Q340 37 340 35Q340 19 333 5Q328 0 317 0Q314 0 280 1T180 2Q118 2 85 2T49 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-8-TEX-I-1D451"></use></g><g data-mml-node="mi" transform="translate(520, -150) scale(0.707)"><use xlink:href="#MJX-8-TEX-I-1D453"></use></g></g><g data-mml-node="mo" transform="translate(1236.7, 0)"><use xlink:href="#MJX-8-TEX-N-3D"></use></g><g data-mml-node="mfrac" transform="translate(2292.5, 0)"><g data-mml-node="mn" transform="translate(339.5, 394) scale(0.707)"><use xlink:href="#MJX-8-TEX-N-31"></use></g><g data-mml-node="msub" transform="translate(220, -345) scale(0.707)"><g data-mml-node="mi"><use xlink:href="#MJX-8-TEX-I-1D460"></use></g><g data-mml-node="mi" transform="translate(469, -150) scale(0.707)"><use xlink:href="#MJX-8-TEX-I-1D45F"></use></g></g><rect width="792.5" height="60" x="120" y="220"></rect></g><g data-mml-node="mo" transform="translate(3547.2, 0)"><use xlink:href="#MJX-8-TEX-N-22C5"></use></g><g data-mml-node="mi" transform="translate(4047.4, 0)"><use xlink:href="#MJX-8-TEX-I-1D43E"></use></g></g></g></svg></mjx-container></div><p>where <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.667ex" xmlns="http://www.w3.org/2000/svg" width="2.169ex" height="2.237ex" role="img" focusable="false" viewBox="0 -694 958.9 989" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-8-TEX-I-1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path><path id="MJX-8-TEX-I-1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-8-TEX-I-1D451"></use></g><g data-mml-node="mi" transform="translate(520, -150) scale(0.707)"><use xlink:href="#MJX-8-TEX-I-1D453"></use></g></g></g></g></svg></mjx-container> is duration of a frame, <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex" xmlns="http://www.w3.org/2000/svg" width="1.896ex" height="1.357ex" role="img" focusable="false" viewBox="0 -442 837.9 599.8" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-8-TEX-I-1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path><path id="MJX-8-TEX-I-1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-8-TEX-I-1D460"></use></g><g data-mml-node="mi" transform="translate(469, -150) scale(0.707)"><use xlink:href="#MJX-8-TEX-I-1D45F"></use></g></g></g></g></svg></mjx-container> is sample rate, and <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0" xmlns="http://www.w3.org/2000/svg" width="2.011ex" height="1.545ex" role="img" focusable="false" viewBox="0 -683 889 683" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-8-TEX-I-1D43E" d="M285 628Q285 635 228 637Q205 637 198 638T191 647Q191 649 193 661Q199 681 203 682Q205 683 214 683H219Q260 681 355 681Q389 681 418 681T463 682T483 682Q500 682 500 674Q500 669 497 660Q496 658 496 654T495 648T493 644T490 641T486 639T479 638T470 637T456 637Q416 636 405 634T387 623L306 305Q307 305 490 449T678 597Q692 611 692 620Q692 635 667 637Q651 637 651 648Q651 650 654 662T659 677Q662 682 676 682Q680 682 711 681T791 680Q814 680 839 681T869 682Q889 682 889 672Q889 650 881 642Q878 637 862 637Q787 632 726 586Q710 576 656 534T556 455L509 418L518 396Q527 374 546 329T581 244Q656 67 661 61Q663 59 666 57Q680 47 717 46H738Q744 38 744 37T741 19Q737 6 731 0H720Q680 3 625 3Q503 3 488 0H478Q472 6 472 9T474 27Q478 40 480 43T491 46H494Q544 46 544 71Q544 75 517 141T485 216L427 354L359 301L291 248L268 155Q245 63 245 58Q245 51 253 49T303 46H334Q340 37 340 35Q340 19 333 5Q328 0 317 0Q314 0 280 1T180 2Q118 2 85 2T49 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-8-TEX-I-1D43E"></use></g></g></g></svg></mjx-container> is the frame size. </p><h2 id="Spectral-Leakage"><a href="#Spectral-Leakage" class="headerlink" title="Spectral Leakage"></a>Spectral Leakage</h2><ul><li>Processed signal isn’t an integer number of periods.</li><li>Endpoints are discontinuous.</li><li>Discontinuous appear as high-frequency components not present in the original signal.</li></ul><h2 id="Windowing"><a href="#Windowing" class="headerlink" title="Windowing"></a>Windowing</h2><ul><li>Apply windowing function to each frame.</li><li>Eliminates samples t both ends of a frame.</li><li>Generates a periodic signal.</li></ul><h2 id="Hann-Window"><a href="#Hann-Window" class="headerlink" title="Hann Window"></a>Hann Window</h2><p>This function is a member of both the cosine-sum and power-of-sine families. The end points of the Hann window just touch zero.</p><div style="display: flex;justify-content: center;">    <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.912ex" xmlns="http://www.w3.org/2000/svg" width="40.504ex" height="2.913ex" role="img" focusable="false" viewBox="0 -884.7 17903 1287.7" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-8-TEX-I-1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path><path id="MJX-8-TEX-N-28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path><path id="MJX-8-TEX-I-1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path><path id="MJX-8-TEX-N-29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path><path id="MJX-8-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-8-TEX-N-30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path><path id="MJX-8-TEX-N-2E" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z"></path><path id="MJX-8-TEX-N-35" d="M164 157Q164 133 148 117T109 101H102Q148 22 224 22Q294 22 326 82Q345 115 345 210Q345 313 318 349Q292 382 260 382H254Q176 382 136 314Q132 307 129 306T114 304Q97 304 95 310Q93 314 93 485V614Q93 664 98 664Q100 666 102 666Q103 666 123 658T178 642T253 634Q324 634 389 662Q397 666 402 666Q410 666 410 648V635Q328 538 205 538Q174 538 149 544L139 546V374Q158 388 169 396T205 412T256 420Q337 420 393 355T449 201Q449 109 385 44T229 -22Q148 -22 99 32T50 154Q50 178 61 192T84 210T107 214Q132 214 148 197T164 157Z"></path><path id="MJX-8-TEX-N-22C5" d="M78 250Q78 274 95 292T138 310Q162 310 180 294T199 251Q199 226 182 208T139 190T96 207T78 250Z"></path><path id="MJX-8-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-8-TEX-N-2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path><path id="MJX-8-TEX-I-1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z"></path><path id="MJX-8-TEX-I-1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path><path id="MJX-8-TEX-I-1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path><path id="MJX-8-TEX-N-32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path><path id="MJX-8-TEX-I-1D70B" d="M132 -11Q98 -11 98 22V33L111 61Q186 219 220 334L228 358H196Q158 358 142 355T103 336Q92 329 81 318T62 297T53 285Q51 284 38 284Q19 284 19 294Q19 300 38 329T93 391T164 429Q171 431 389 431Q549 431 553 430Q573 423 573 402Q573 371 541 360Q535 358 472 358H408L405 341Q393 269 393 222Q393 170 402 129T421 65T431 37Q431 20 417 5T381 -10Q370 -10 363 -7T347 17T331 77Q330 86 330 121Q330 170 339 226T357 318T367 358H269L268 354Q268 351 249 275T206 114T175 17Q164 -11 132 -11Z"></path><path id="MJX-8-TEX-I-1D43E" d="M285 628Q285 635 228 637Q205 637 198 638T191 647Q191 649 193 661Q199 681 203 682Q205 683 214 683H219Q260 681 355 681Q389 681 418 681T463 682T483 682Q500 682 500 674Q500 669 497 660Q496 658 496 654T495 648T493 644T490 641T486 639T479 638T470 637T456 637Q416 636 405 634T387 623L306 305Q307 305 490 449T678 597Q692 611 692 620Q692 635 667 637Q651 637 651 648Q651 650 654 662T659 677Q662 682 676 682Q680 682 711 681T791 680Q814 680 839 681T869 682Q889 682 889 672Q889 650 881 642Q878 637 862 637Q787 632 726 586Q710 576 656 534T556 455L509 418L518 396Q527 374 546 329T581 244Q656 67 661 61Q663 59 666 57Q680 47 717 46H738Q744 38 744 37T741 19Q737 6 731 0H720Q680 3 625 3Q503 3 488 0H478Q472 6 472 9T474 27Q478 40 480 43T491 46H494Q544 46 544 71Q544 75 517 141T485 216L427 354L359 301L291 248L268 155Q245 63 245 58Q245 51 253 49T303 46H334Q340 37 340 35Q340 19 333 5Q328 0 317 0Q314 0 280 1T180 2Q118 2 85 2T49 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Z"></path><path id="MJX-8-TEX-N-2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path><path id="MJX-8-TEX-N-22EF" d="M78 250Q78 274 95 292T138 310Q162 310 180 294T199 251Q199 226 182 208T139 190T96 207T78 250ZM525 250Q525 274 542 292T585 310Q609 310 627 294T646 251Q646 226 629 208T586 190T543 207T525 250ZM972 250Q972 274 989 292T1032 310Q1056 310 1074 294T1093 251Q1093 226 1076 208T1033 190T990 207T972 250Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-8-TEX-I-1D464"></use></g><g data-mml-node="mo" transform="translate(716, 0)"><use xlink:href="#MJX-8-TEX-N-28"></use></g><g data-mml-node="mi" transform="translate(1105, 0)"><use xlink:href="#MJX-8-TEX-I-1D458"></use></g><g data-mml-node="mo" transform="translate(1626, 0)"><use xlink:href="#MJX-8-TEX-N-29"></use></g><g data-mml-node="mo" transform="translate(2292.8, 0)"><use xlink:href="#MJX-8-TEX-N-3D"></use></g><g data-mml-node="mn" transform="translate(3348.6, 0)"><use xlink:href="#MJX-8-TEX-N-30"></use><use xlink:href="#MJX-8-TEX-N-2E" transform="translate(500, 0)"></use><use xlink:href="#MJX-8-TEX-N-35" transform="translate(778, 0)"></use></g><g data-mml-node="mo" transform="translate(4848.8, 0)"><use xlink:href="#MJX-8-TEX-N-22C5"></use></g><g data-mml-node="mo" transform="translate(5349, 0)"><use xlink:href="#MJX-8-TEX-N-28"></use></g><g data-mml-node="mn" transform="translate(5738, 0)"><use xlink:href="#MJX-8-TEX-N-31"></use></g><g data-mml-node="mo" transform="translate(6460.2, 0)"><use xlink:href="#MJX-8-TEX-N-2212"></use></g><g data-mml-node="mi" transform="translate(7460.4, 0)"><use xlink:href="#MJX-8-TEX-I-1D450"></use></g><g data-mml-node="mi" transform="translate(7893.4, 0)"><use xlink:href="#MJX-8-TEX-I-1D45C"></use></g><g data-mml-node="mi" transform="translate(8378.4, 0)"><use xlink:href="#MJX-8-TEX-I-1D460"></use></g><g data-mml-node="mo" transform="translate(8847.4, 0)"><use xlink:href="#MJX-8-TEX-N-28"></use></g><g data-mml-node="mfrac" transform="translate(9236.4, 0)"><g data-mml-node="mrow" transform="translate(423.6, 394) scale(0.707)"><g data-mml-node="mn"><use xlink:href="#MJX-8-TEX-N-32"></use></g><g data-mml-node="mi" transform="translate(500, 0)"><use xlink:href="#MJX-8-TEX-I-1D70B"></use></g><g data-mml-node="mi" transform="translate(1070, 0)"><use xlink:href="#MJX-8-TEX-I-1D458"></use></g></g><g data-mml-node="mrow" transform="translate(220, -345) scale(0.707)"><g data-mml-node="mi"><use xlink:href="#MJX-8-TEX-I-1D43E"></use></g><g data-mml-node="mo" transform="translate(889, 0)"><use xlink:href="#MJX-8-TEX-N-2212"></use></g><g data-mml-node="mn" transform="translate(1667, 0)"><use xlink:href="#MJX-8-TEX-N-31"></use></g></g><rect width="1732.3" height="60" x="120" y="220"></rect></g><g data-mml-node="mo" transform="translate(11208.7, 0)"><use xlink:href="#MJX-8-TEX-N-29"></use></g><g data-mml-node="mo" transform="translate(11597.7, 0)"><use xlink:href="#MJX-8-TEX-N-29"></use></g><g data-mml-node="mo" transform="translate(11986.7, 0)"><use xlink:href="#MJX-8-TEX-N-2C"></use></g><g data-mml-node="mi" transform="translate(12431.4, 0)"><use xlink:href="#MJX-8-TEX-I-1D458"></use></g><g data-mml-node="mo" transform="translate(13230.2, 0)"><use xlink:href="#MJX-8-TEX-N-3D"></use></g><g data-mml-node="mn" transform="translate(14286, 0)"><use xlink:href="#MJX-8-TEX-N-31"></use></g><g data-mml-node="mo" transform="translate(14786, 0)"><use xlink:href="#MJX-8-TEX-N-2C"></use></g><g data-mml-node="mo" transform="translate(15230.6, 0)"><use xlink:href="#MJX-8-TEX-N-22EF"></use></g><g data-mml-node="mo" transform="translate(16569.3, 0)"><use xlink:href="#MJX-8-TEX-N-2C"></use></g><g data-mml-node="mi" transform="translate(17014, 0)"><use xlink:href="#MJX-8-TEX-I-1D43E"></use></g></g></g></svg></mjx-container></div><p>At both endpoints will touch zero, which is a problem losing a signal. The solution to this is to overlap the frames.</p><h3 id="Bit-Depth-amp-Bit-Rate"><a href="#Bit-Depth-amp-Bit-Rate" class="headerlink" title="Bit Depth &amp; Bit Rate"></a>Bit Depth &amp; Bit Rate</h3><p>Analog audio is a continuous wave, with an effectively infinite number of possible amplitude values. However, to measure this wave in digital audio, we need to define the wave’s amplitude as a finite value each time we sample it. </p><p>To illustrate bit depth, suppose you want to draw a picture of a sunset, but you only have 16 crayons. In real-life, sunsets come in a variety of colors, from dazzling yellows and oranges to faint reds and purples. If there were only 16 crayons, then it would be impossible to really draw all these different colors. What If you have 32 crayons, you can then use twice as many colors, although the picture still doesn’t look realistic. Thereupon, if you continue to add more crayons, you can definitely draw better.</p><p>The bit depth determines the number of possible amplitude values we can record for each sample. The most common bit depths are 16-bit, 24-bit, and 32-bit. Each is a binary term, representing a number of possible values. Systems of higher bit depths are able to express more possible values.</p><p>The bit rate of a file tells us how many bits of data are processed every second. Bit rates are usually measured in kilobits per second (kbps). To calculate the bit rate, we can use the following formula:</p><div style="display: flex;justify-content: center;">    <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.464ex" xmlns="http://www.w3.org/2000/svg" width="46.476ex" height="2.059ex" role="img" focusable="false" viewBox="0 -705 20542.4 910" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-8-TEX-I-1D435" d="M231 637Q204 637 199 638T194 649Q194 676 205 682Q206 683 335 683Q594 683 608 681Q671 671 713 636T756 544Q756 480 698 429T565 360L555 357Q619 348 660 311T702 219Q702 146 630 78T453 1Q446 0 242 0Q42 0 39 2Q35 5 35 10Q35 17 37 24Q42 43 47 45Q51 46 62 46H68Q95 46 128 49Q142 52 147 61Q150 65 219 339T288 628Q288 635 231 637ZM649 544Q649 574 634 600T585 634Q578 636 493 637Q473 637 451 637T416 636H403Q388 635 384 626Q382 622 352 506Q352 503 351 500L320 374H401Q482 374 494 376Q554 386 601 434T649 544ZM595 229Q595 273 572 302T512 336Q506 337 429 337Q311 337 310 336Q310 334 293 263T258 122L240 52Q240 48 252 48T333 46Q422 46 429 47Q491 54 543 105T595 229Z"></path><path id="MJX-8-TEX-I-1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path><path id="MJX-8-TEX-I-1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path><path id="MJX-8-TEX-I-1D445" d="M230 637Q203 637 198 638T193 649Q193 676 204 682Q206 683 378 683Q550 682 564 680Q620 672 658 652T712 606T733 563T739 529Q739 484 710 445T643 385T576 351T538 338L545 333Q612 295 612 223Q612 212 607 162T602 80V71Q602 53 603 43T614 25T640 16Q668 16 686 38T712 85Q717 99 720 102T735 105Q755 105 755 93Q755 75 731 36Q693 -21 641 -21H632Q571 -21 531 4T487 82Q487 109 502 166T517 239Q517 290 474 313Q459 320 449 321T378 323H309L277 193Q244 61 244 59Q244 55 245 54T252 50T269 48T302 46H333Q339 38 339 37T336 19Q332 6 326 0H311Q275 2 180 2Q146 2 117 2T71 2T50 1Q33 1 33 10Q33 12 36 24Q41 43 46 45Q50 46 61 46H67Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628Q287 635 230 637ZM630 554Q630 586 609 608T523 636Q521 636 500 636T462 637H440Q393 637 386 627Q385 624 352 494T319 361Q319 360 388 360Q466 361 492 367Q556 377 592 426Q608 449 619 486T630 554Z"></path><path id="MJX-8-TEX-I-1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path><path id="MJX-8-TEX-I-1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path><path id="MJX-8-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-8-TEX-I-1D439" d="M48 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q146 66 215 342T285 622Q285 629 281 629Q273 632 228 634H197Q191 640 191 642T193 659Q197 676 203 680H742Q749 676 749 669Q749 664 736 557T722 447Q720 440 702 440H690Q683 445 683 453Q683 454 686 477T689 530Q689 560 682 579T663 610T626 626T575 633T503 634H480Q398 633 393 631Q388 629 386 623Q385 622 352 492L320 363H375Q378 363 398 363T426 364T448 367T472 374T489 386Q502 398 511 419T524 457T529 475Q532 480 548 480H560Q567 475 567 470Q567 467 536 339T502 207Q500 200 482 200H470Q463 206 463 212Q463 215 468 234T473 274Q473 303 453 310T364 317H309L277 190Q245 66 245 60Q245 46 334 46H359Q365 40 365 39T363 19Q359 6 353 0H336Q295 2 185 2Q120 2 86 2T48 1Z"></path><path id="MJX-8-TEX-I-1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-8-TEX-I-1D45E" d="M33 157Q33 258 109 349T280 441Q340 441 372 389Q373 390 377 395T388 406T404 418Q438 442 450 442Q454 442 457 439T460 434Q460 425 391 149Q320 -135 320 -139Q320 -147 365 -148H390Q396 -156 396 -157T393 -175Q389 -188 383 -194H370Q339 -192 262 -192Q234 -192 211 -192T174 -192T157 -193Q143 -193 143 -185Q143 -182 145 -170Q149 -154 152 -151T172 -148Q220 -148 230 -141Q238 -136 258 -53T279 32Q279 33 272 29Q224 -10 172 -10Q117 -10 75 30T33 157ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path><path id="MJX-8-TEX-I-1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-8-TEX-I-1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-8-TEX-I-1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z"></path><path id="MJX-8-TEX-I-1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-8-TEX-N-D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path><path id="MJX-8-TEX-I-1D437" d="M287 628Q287 635 230 637Q207 637 200 638T193 647Q193 655 197 667T204 682Q206 683 403 683Q570 682 590 682T630 676Q702 659 752 597T803 431Q803 275 696 151T444 3L430 1L236 0H125H72Q48 0 41 2T33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM703 469Q703 507 692 537T666 584T629 613T590 629T555 636Q553 636 541 636T512 636T479 637H436Q392 637 386 627Q384 623 313 339T242 52Q242 48 253 48T330 47Q335 47 349 47T373 46Q499 46 581 128Q617 164 640 212T683 339T703 469Z"></path><path id="MJX-8-TEX-I-1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"></path><path id="MJX-8-TEX-I-210E" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z"></path><path id="MJX-8-TEX-I-1D436" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z"></path><path id="MJX-8-TEX-I-1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path><path id="MJX-8-TEX-I-1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-8-TEX-I-1D435"></use></g><g data-mml-node="mi" transform="translate(759, 0)"><use xlink:href="#MJX-8-TEX-I-1D456"></use></g><g data-mml-node="mi" transform="translate(1104, 0)"><use xlink:href="#MJX-8-TEX-I-1D461"></use></g><g data-mml-node="mi" transform="translate(1465, 0)"><use xlink:href="#MJX-8-TEX-I-1D445"></use></g><g data-mml-node="mi" transform="translate(2224, 0)"><use xlink:href="#MJX-8-TEX-I-1D44E"></use></g><g data-mml-node="mi" transform="translate(2753, 0)"><use xlink:href="#MJX-8-TEX-I-1D461"></use></g><g data-mml-node="mi" transform="translate(3114, 0)"><use xlink:href="#MJX-8-TEX-I-1D452"></use></g><g data-mml-node="mo" transform="translate(3857.8, 0)"><use xlink:href="#MJX-8-TEX-N-3D"></use></g><g data-mml-node="mi" transform="translate(4913.6, 0)"><use xlink:href="#MJX-8-TEX-I-1D439"></use></g><g data-mml-node="mi" transform="translate(5662.6, 0)"><use xlink:href="#MJX-8-TEX-I-1D45F"></use></g><g data-mml-node="mi" transform="translate(6113.6, 0)"><use xlink:href="#MJX-8-TEX-I-1D452"></use></g><g data-mml-node="mi" transform="translate(6579.6, 0)"><use xlink:href="#MJX-8-TEX-I-1D45E"></use></g><g data-mml-node="mi" transform="translate(7039.6, 0)"><use xlink:href="#MJX-8-TEX-I-1D462"></use></g><g data-mml-node="mi" transform="translate(7611.6, 0)"><use xlink:href="#MJX-8-TEX-I-1D452"></use></g><g data-mml-node="mi" transform="translate(8077.6, 0)"><use xlink:href="#MJX-8-TEX-I-1D45B"></use></g><g data-mml-node="mi" transform="translate(8677.6, 0)"><use xlink:href="#MJX-8-TEX-I-1D450"></use></g><g data-mml-node="mi" transform="translate(9110.6, 0)"><use xlink:href="#MJX-8-TEX-I-1D466"></use></g><g data-mml-node="mo" transform="translate(9822.8, 0)"><use xlink:href="#MJX-8-TEX-N-D7"></use></g><g data-mml-node="mi" transform="translate(10823, 0)"><use xlink:href="#MJX-8-TEX-I-1D435"></use></g><g data-mml-node="mi" transform="translate(11582, 0)"><use xlink:href="#MJX-8-TEX-I-1D456"></use></g><g data-mml-node="mi" transform="translate(11927, 0)"><use xlink:href="#MJX-8-TEX-I-1D461"></use></g><g data-mml-node="mi" transform="translate(12288, 0)"><use xlink:href="#MJX-8-TEX-I-1D437"></use></g><g data-mml-node="mi" transform="translate(13116, 0)"><use xlink:href="#MJX-8-TEX-I-1D452"></use></g><g data-mml-node="mi" transform="translate(13582, 0)"><use xlink:href="#MJX-8-TEX-I-1D45D"></use></g><g data-mml-node="mi" transform="translate(14085, 0)"><use xlink:href="#MJX-8-TEX-I-1D461"></use></g><g data-mml-node="mi" transform="translate(14446, 0)"><use xlink:href="#MJX-8-TEX-I-210E"></use></g><g data-mml-node="mo" transform="translate(15244.2, 0)"><use xlink:href="#MJX-8-TEX-N-D7"></use></g><g data-mml-node="mi" transform="translate(16244.4, 0)"><use xlink:href="#MJX-8-TEX-I-1D436"></use></g><g data-mml-node="mi" transform="translate(17004.4, 0)"><use xlink:href="#MJX-8-TEX-I-210E"></use></g><g data-mml-node="mi" transform="translate(17580.4, 0)"><use xlink:href="#MJX-8-TEX-I-1D44E"></use></g><g data-mml-node="mi" transform="translate(18109.4, 0)"><use xlink:href="#MJX-8-TEX-I-1D45B"></use></g><g data-mml-node="mi" transform="translate(18709.4, 0)"><use xlink:href="#MJX-8-TEX-I-1D45B"></use></g><g data-mml-node="mi" transform="translate(19309.4, 0)"><use xlink:href="#MJX-8-TEX-I-1D452"></use></g><g data-mml-node="mi" transform="translate(19775.4, 0)"><use xlink:href="#MJX-8-TEX-I-1D459"></use></g><g data-mml-node="mi" transform="translate(20073.4, 0)"><use xlink:href="#MJX-8-TEX-I-1D460"></use></g></g></g></svg></mjx-container></div><p>A typical, uncompressed high-quality audio file has a sample rate of 44,100 samples per second, a bit depth of 16 bits per sample and 2 channels of stereo audio. The bit rate for this file would be: <strong>44100 samples per second × 16 bits per sample × 2 channels = 1411200 bits per second (or 1411.2 kbps)</strong>. </p><p>To sum up, we can know that the higher the bit rate, the faster the data transfer speed. With a firmer understanding of sample rate and bit depth, let’s dive in next topic - spectrum.</p><h2 id="Spectrum"><a href="#Spectrum" class="headerlink" title="Spectrum"></a>Spectrum</h2><p>Before we talk about the spectrum, we need to understand the sound waves first.</p><img src="/Hexo-Blog/2021/02/05/2021-02-05-audio-based-song-genre-classification/sine.png" class=""><ul><li><strong>Amplitude</strong>: the greater the amplitude, the louder the volume of the sound, and vice versa the lower the volume.</li><li><strong>Cycle</strong>: from position 0, to the peak, then to the trough, and finally back to 0, this is called a cycle.</li><li><strong>Frequency</strong>: frequency refers to how many cycles per second, the higher the frequency, the higher the pitch.</li><li><strong>Phase</strong>: indicates the position of the waveform in the cycle, measured in degrees.</li><li><strong>Wavelength</strong>: denotes the distance between two points with the same phase degree. The longer the wavelength, the lower the frequency, the shorter the wavelength, the higher the frequency.</li></ul><p>So what is a Spectrum? A spectrum displays the different frequencies present in a sound.</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">fft <span class="token operator">=</span> np<span class="token punctuation">.</span>fft<span class="token punctuation">.</span>fft<span class="token punctuation">(</span>signal<span class="token punctuation">)</span>magnitude <span class="token operator">=</span> np<span class="token punctuation">.</span>absolute<span class="token punctuation">(</span>fft<span class="token punctuation">)</span>frequency <span class="token operator">=</span> np<span class="token punctuation">.</span>linspace<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> sr<span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>magnitude<span class="token punctuation">)</span><span class="token punctuation">)</span>left_frequency <span class="token operator">=</span> frequency<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token builtin">int</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>frequency<span class="token punctuation">)</span><span class="token operator">/</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">]</span>left_magnitude <span class="token operator">=</span> magnitude<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token builtin">int</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>magnitude<span class="token punctuation">)</span><span class="token operator">/</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">]</span>plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>left_frequency<span class="token punctuation">,</span> left_magnitude<span class="token punctuation">,</span> color<span class="token operator">=</span><span class="token string">"tab:blue"</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>xlabel<span class="token punctuation">(</span><span class="token string">"Frequency"</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>ylabel<span class="token punctuation">(</span><span class="token string">"Magnitude"</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>grid<span class="token punctuation">(</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><img src="/Hexo-Blog/2021/02/05/2021-02-05-audio-based-song-genre-classification/spectrum.png" class=""><p>I use <code>np.fft.fft</code> to perform Discrete Fourier Transform (DFT) with the efficient Fast Fourier Transform (FFT) algorithm. Fourier Transform is another mathematical representation of sound. Fourier Transform is a function that gets a signal in the time domain as input, and outputs its decomposition into frequencies.</p><p>The Fourier Transform is one of deepest insights ever made. Unfortunately, the meaning is buried within dense equations:</p><div style="display: flex;justify-content: center;">    <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -6.762ex" xmlns="http://www.w3.org/2000/svg" width="24.507ex" height="14.655ex" role="img" focusable="false" viewBox="0 -3488.8 10832.3 6477.5" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-8-TEX-I-1D44B" d="M42 0H40Q26 0 26 11Q26 15 29 27Q33 41 36 43T55 46Q141 49 190 98Q200 108 306 224T411 342Q302 620 297 625Q288 636 234 637H206Q200 643 200 645T202 664Q206 677 212 683H226Q260 681 347 681Q380 681 408 681T453 682T473 682Q490 682 490 671Q490 670 488 658Q484 643 481 640T465 637Q434 634 411 620L488 426L541 485Q646 598 646 610Q646 628 622 635Q617 635 609 637Q594 637 594 648Q594 650 596 664Q600 677 606 683H618Q619 683 643 683T697 681T738 680Q828 680 837 683H845Q852 676 852 672Q850 647 840 637H824Q790 636 763 628T722 611T698 593L687 584Q687 585 592 480L505 384Q505 383 536 304T601 142T638 56Q648 47 699 46Q734 46 734 37Q734 35 732 23Q728 7 725 4T711 1Q708 1 678 1T589 2Q528 2 496 2T461 1Q444 1 444 10Q444 11 446 25Q448 35 450 39T455 44T464 46T480 47T506 54Q523 62 523 64Q522 64 476 181L429 299Q241 95 236 84Q232 76 232 72Q232 53 261 47Q262 47 267 47T273 46Q276 46 277 46T280 45T283 42T284 35Q284 26 282 19Q279 6 276 4T261 1Q258 1 243 1T201 2T142 2Q64 2 42 0Z"></path><path id="MJX-8-TEX-I-1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path><path id="MJX-8-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-8-TEX-LO-2211" d="M60 948Q63 950 665 950H1267L1325 815Q1384 677 1388 669H1348L1341 683Q1320 724 1285 761Q1235 809 1174 838T1033 881T882 898T699 902H574H543H251L259 891Q722 258 724 252Q725 250 724 246Q721 243 460 -56L196 -356Q196 -357 407 -357Q459 -357 548 -357T676 -358Q812 -358 896 -353T1063 -332T1204 -283T1307 -196Q1328 -170 1348 -124H1388Q1388 -125 1381 -145T1356 -210T1325 -294L1267 -449L666 -450Q64 -450 61 -448Q55 -446 55 -439Q55 -437 57 -433L590 177Q590 178 557 222T452 366T322 544L56 909L55 924Q55 945 60 948Z"></path><path id="MJX-8-TEX-I-1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-8-TEX-N-30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path><path id="MJX-8-TEX-I-1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"></path><path id="MJX-8-TEX-N-2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path><path id="MJX-8-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-8-TEX-I-1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path><path id="MJX-8-TEX-N-22C5" d="M78 250Q78 274 95 292T138 310Q162 310 180 294T199 251Q199 226 182 208T139 190T96 207T78 250Z"></path><path id="MJX-8-TEX-I-1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path><path id="MJX-8-TEX-I-1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path><path id="MJX-8-TEX-N-32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path><path id="MJX-8-TEX-I-1D70B" d="M132 -11Q98 -11 98 22V33L111 61Q186 219 220 334L228 358H196Q158 358 142 355T103 336Q92 329 81 318T62 297T53 285Q51 284 38 284Q19 284 19 294Q19 300 38 329T93 391T164 429Q171 431 389 431Q549 431 553 430Q573 423 573 402Q573 371 541 360Q535 358 472 358H408L405 341Q393 269 393 222Q393 170 402 129T421 65T431 37Q431 20 417 5T381 -10Q370 -10 363 -7T347 17T331 77Q330 86 330 121Q330 170 339 226T357 318T367 358H269L268 354Q268 351 249 275T206 114T175 17Q164 -11 132 -11Z"></path><path id="MJX-8-TEX-N-2F" d="M423 750Q432 750 438 744T444 730Q444 725 271 248T92 -240Q85 -250 75 -250Q68 -250 62 -245T56 -231Q56 -221 230 257T407 740Q411 750 423 750Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mtable"><g data-mml-node="mtr" transform="translate(0, 1755.8)"><g data-mml-node="mtd" transform="translate(472.3, 0)"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-8-TEX-I-1D44B"></use></g><g data-mml-node="mi" transform="translate(828, -150) scale(0.707)"><use xlink:href="#MJX-8-TEX-I-1D458"></use></g></g><g data-mml-node="mo" transform="translate(1524.2, 0)"><use xlink:href="#MJX-8-TEX-N-3D"></use></g><g data-mml-node="munderover" transform="translate(2580, 0)"><g data-mml-node="mo" transform="translate(43.8, 0)"><use xlink:href="#MJX-8-TEX-LO-2211"></use></g><g data-mml-node="TeXAtom" transform="translate(101.8, -1087.9) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-8-TEX-I-1D45B"></use></g><g data-mml-node="mo" transform="translate(600, 0)"><use xlink:href="#MJX-8-TEX-N-3D"></use></g><g data-mml-node="mn" transform="translate(1378, 0)"><use xlink:href="#MJX-8-TEX-N-30"></use></g></g><g data-mml-node="TeXAtom" transform="translate(0, 1150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-8-TEX-I-1D441"></use></g><g data-mml-node="mo" transform="translate(888, 0)"><use xlink:href="#MJX-8-TEX-N-2212"></use></g><g data-mml-node="mn" transform="translate(1666, 0)"><use xlink:href="#MJX-8-TEX-N-31"></use></g></g></g><g data-mml-node="msub" transform="translate(4278.2, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-8-TEX-I-1D465"></use></g><g data-mml-node="mi" transform="translate(572, -150) scale(0.707)"><use xlink:href="#MJX-8-TEX-I-1D45B"></use></g></g><g data-mml-node="mo" transform="translate(5546.7, 0)"><use xlink:href="#MJX-8-TEX-N-22C5"></use></g><g data-mml-node="msup" transform="translate(6046.9, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-8-TEX-I-1D452"></use></g><g data-mml-node="TeXAtom" transform="translate(466, 413) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mo"><use xlink:href="#MJX-8-TEX-N-2212"></use></g><g data-mml-node="mi" transform="translate(778, 0)"><use xlink:href="#MJX-8-TEX-I-1D456"></use></g><g data-mml-node="mn" transform="translate(1123, 0)"><use xlink:href="#MJX-8-TEX-N-32"></use></g><g data-mml-node="mi" transform="translate(1623, 0)"><use xlink:href="#MJX-8-TEX-I-1D70B"></use></g><g data-mml-node="mi" transform="translate(2193, 0)"><use xlink:href="#MJX-8-TEX-I-1D458"></use></g><g data-mml-node="mi" transform="translate(2714, 0)"><use xlink:href="#MJX-8-TEX-I-1D45B"></use></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(3314, 0)"><g data-mml-node="mo"><use xlink:href="#MJX-8-TEX-N-2F"></use></g></g><g data-mml-node="mi" transform="translate(3814, 0)"><use xlink:href="#MJX-8-TEX-I-1D441"></use></g></g></g></g></g><g data-mml-node="mtr" transform="translate(0, -1723.1)"><g data-mml-node="mtd"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-8-TEX-I-1D465"></use></g><g data-mml-node="mi" transform="translate(572, -150) scale(0.707)"><use xlink:href="#MJX-8-TEX-I-1D45B"></use></g></g><g data-mml-node="mo" transform="translate(1324, 0)"><use xlink:href="#MJX-8-TEX-N-3D"></use></g><g data-mml-node="mfrac" transform="translate(2379.8, 0)"><g data-mml-node="mn" transform="translate(414, 676)"><use xlink:href="#MJX-8-TEX-N-31"></use></g><g data-mml-node="mi" transform="translate(220, -686)"><use xlink:href="#MJX-8-TEX-I-1D441"></use></g><rect width="1088" height="60" x="120" y="220"></rect></g><g data-mml-node="munderover" transform="translate(3874.5, 0)"><g data-mml-node="mo" transform="translate(43.8, 0)"><use xlink:href="#MJX-8-TEX-LO-2211"></use></g><g data-mml-node="TeXAtom" transform="translate(129.8, -1107.7) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-8-TEX-I-1D458"></use></g><g data-mml-node="mo" transform="translate(521, 0)"><use xlink:href="#MJX-8-TEX-N-3D"></use></g><g data-mml-node="mn" transform="translate(1299, 0)"><use xlink:href="#MJX-8-TEX-N-30"></use></g></g><g data-mml-node="TeXAtom" transform="translate(0, 1150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-8-TEX-I-1D441"></use></g><g data-mml-node="mo" transform="translate(888, 0)"><use xlink:href="#MJX-8-TEX-N-2212"></use></g><g data-mml-node="mn" transform="translate(1666, 0)"><use xlink:href="#MJX-8-TEX-N-31"></use></g></g></g><g data-mml-node="msub" transform="translate(5572.7, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-8-TEX-I-1D44B"></use></g><g data-mml-node="mi" transform="translate(828, -150) scale(0.707)"><use xlink:href="#MJX-8-TEX-I-1D458"></use></g></g><g data-mml-node="mo" transform="translate(7041.4, 0)"><use xlink:href="#MJX-8-TEX-N-22C5"></use></g><g data-mml-node="msup" transform="translate(7541.6, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-8-TEX-I-1D452"></use></g><g data-mml-node="TeXAtom" transform="translate(466, 413) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-8-TEX-I-1D456"></use></g><g data-mml-node="mn" transform="translate(345, 0)"><use xlink:href="#MJX-8-TEX-N-32"></use></g><g data-mml-node="mi" transform="translate(845, 0)"><use xlink:href="#MJX-8-TEX-I-1D70B"></use></g><g data-mml-node="mi" transform="translate(1415, 0)"><use xlink:href="#MJX-8-TEX-I-1D458"></use></g><g data-mml-node="mi" transform="translate(1936, 0)"><use xlink:href="#MJX-8-TEX-I-1D45B"></use></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(2536, 0)"><g data-mml-node="mo"><use xlink:href="#MJX-8-TEX-N-2F"></use></g></g><g data-mml-node="mi" transform="translate(3036, 0)"><use xlink:href="#MJX-8-TEX-I-1D441"></use></g></g></g></g></g></g></g></g></svg></mjx-container></div><p>Rather than jumping into the symbols, let’s experience the key idea firsthand. Here’s a plain-English metaphor from this <a href="https://betterexplained.com/articles/an-interactive-guide-to-the-fourier-transform/">article</a>:</p><ul><li><strong>What does the Fourier Transform do?</strong> Given a smoothie, it finds the recipe.</li><li><strong>How?</strong> Run the smoothie through filters to extract each ingredient.</li><li><strong>Why?</strong> Recipes are easier to analyze, compare, and modify than the smoothie itself.</li><li><strong>How do we get the smoothie back?</strong> Blend the ingredients.</li></ul><p>The Fourier Transform changes our perspective from consumer to producer, turning What do I have? into How was it made? In other words: given a smoothie, let’s find the recipe. In the process of filtration, the components and proportions do not change, so we can reverse the formula through this transformation, which is the meaning of the filter.</p><p>We can now derive the Discrete Fourier Transform from the continuous version of the Fourier series development.</p><div style="display: flex;justify-content: center;">    <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -6.74ex" xmlns="http://www.w3.org/2000/svg" width="51.84ex" height="14.61ex" role="img" focusable="false" viewBox="0 -3478.9 22913.1 6457.7" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-8-TEX-I-1D44B" d="M42 0H40Q26 0 26 11Q26 15 29 27Q33 41 36 43T55 46Q141 49 190 98Q200 108 306 224T411 342Q302 620 297 625Q288 636 234 637H206Q200 643 200 645T202 664Q206 677 212 683H226Q260 681 347 681Q380 681 408 681T453 682T473 682Q490 682 490 671Q490 670 488 658Q484 643 481 640T465 637Q434 634 411 620L488 426L541 485Q646 598 646 610Q646 628 622 635Q617 635 609 637Q594 637 594 648Q594 650 596 664Q600 677 606 683H618Q619 683 643 683T697 681T738 680Q828 680 837 683H845Q852 676 852 672Q850 647 840 637H824Q790 636 763 628T722 611T698 593L687 584Q687 585 592 480L505 384Q505 383 536 304T601 142T638 56Q648 47 699 46Q734 46 734 37Q734 35 732 23Q728 7 725 4T711 1Q708 1 678 1T589 2Q528 2 496 2T461 1Q444 1 444 10Q444 11 446 25Q448 35 450 39T455 44T464 46T480 47T506 54Q523 62 523 64Q522 64 476 181L429 299Q241 95 236 84Q232 76 232 72Q232 53 261 47Q262 47 267 47T273 46Q276 46 277 46T280 45T283 42T284 35Q284 26 282 19Q279 6 276 4T261 1Q258 1 243 1T201 2T142 2Q64 2 42 0Z"></path><path id="MJX-8-TEX-N-5B" d="M118 -250V750H255V710H158V-210H255V-250H118Z"></path><path id="MJX-8-TEX-I-1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path><path id="MJX-8-TEX-N-5D" d="M22 710V750H159V-250H22V-210H119V710H22Z"></path><path id="MJX-8-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-8-TEX-LO-2211" d="M60 948Q63 950 665 950H1267L1325 815Q1384 677 1388 669H1348L1341 683Q1320 724 1285 761Q1235 809 1174 838T1033 881T882 898T699 902H574H543H251L259 891Q722 258 724 252Q725 250 724 246Q721 243 460 -56L196 -356Q196 -357 407 -357Q459 -357 548 -357T676 -358Q812 -358 896 -353T1063 -332T1204 -283T1307 -196Q1328 -170 1348 -124H1388Q1388 -125 1381 -145T1356 -210T1325 -294L1267 -449L666 -450Q64 -450 61 -448Q55 -446 55 -439Q55 -437 57 -433L590 177Q590 178 557 222T452 366T322 544L56 909L55 924Q55 945 60 948Z"></path><path id="MJX-8-TEX-I-1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-8-TEX-N-30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path><path id="MJX-8-TEX-I-1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"></path><path id="MJX-8-TEX-N-2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path><path id="MJX-8-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-8-TEX-I-1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path><path id="MJX-8-TEX-N-22C5" d="M78 250Q78 274 95 292T138 310Q162 310 180 294T199 251Q199 226 182 208T139 190T96 207T78 250Z"></path><path id="MJX-8-TEX-I-1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path><path id="MJX-8-TEX-I-1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path><path id="MJX-8-TEX-N-32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path><path id="MJX-8-TEX-I-1D70B" d="M132 -11Q98 -11 98 22V33L111 61Q186 219 220 334L228 358H196Q158 358 142 355T103 336Q92 329 81 318T62 297T53 285Q51 284 38 284Q19 284 19 294Q19 300 38 329T93 391T164 429Q171 431 389 431Q549 431 553 430Q573 423 573 402Q573 371 541 360Q535 358 472 358H408L405 341Q393 269 393 222Q393 170 402 129T421 65T431 37Q431 20 417 5T381 -10Q370 -10 363 -7T347 17T331 77Q330 86 330 121Q330 170 339 226T357 318T367 358H269L268 354Q268 351 249 275T206 114T175 17Q164 -11 132 -11Z"></path><path id="MJX-8-TEX-N-2F" d="M423 750Q432 750 438 744T444 730Q444 725 271 248T92 -240Q85 -250 75 -250Q68 -250 62 -245T56 -231Q56 -221 230 257T407 740Q411 750 423 750Z"></path><path id="MJX-8-TEX-I-1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z"></path><path id="MJX-8-TEX-I-1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path><path id="MJX-8-TEX-I-1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path><path id="MJX-8-TEX-N-28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path><path id="MJX-8-TEX-N-29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mtable"><g data-mml-node="mtr" transform="translate(0, 1745.9)"><g data-mml-node="mtd" transform="translate(5830.5, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-8-TEX-I-1D44B"></use></g><g data-mml-node="mo" transform="translate(852, 0)"><use xlink:href="#MJX-8-TEX-N-5B"></use></g><g data-mml-node="mi" transform="translate(1130, 0)"><use xlink:href="#MJX-8-TEX-I-1D458"></use></g><g data-mml-node="mo" transform="translate(1651, 0)"><use xlink:href="#MJX-8-TEX-N-5D"></use></g><g data-mml-node="mo" transform="translate(2206.8, 0)"><use xlink:href="#MJX-8-TEX-N-3D"></use></g><g data-mml-node="munderover" transform="translate(3262.6, 0)"><g data-mml-node="mo" transform="translate(43.8, 0)"><use xlink:href="#MJX-8-TEX-LO-2211"></use></g><g data-mml-node="TeXAtom" transform="translate(101.8, -1087.9) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-8-TEX-I-1D45B"></use></g><g data-mml-node="mo" transform="translate(600, 0)"><use xlink:href="#MJX-8-TEX-N-3D"></use></g><g data-mml-node="mn" transform="translate(1378, 0)"><use xlink:href="#MJX-8-TEX-N-30"></use></g></g><g data-mml-node="TeXAtom" transform="translate(0, 1150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-8-TEX-I-1D441"></use></g><g data-mml-node="mo" transform="translate(888, 0)"><use xlink:href="#MJX-8-TEX-N-2212"></use></g><g data-mml-node="mn" transform="translate(1666, 0)"><use xlink:href="#MJX-8-TEX-N-31"></use></g></g></g><g data-mml-node="mi" transform="translate(4960.8, 0)"><use xlink:href="#MJX-8-TEX-I-1D465"></use></g><g data-mml-node="mo" transform="translate(5532.8, 0)"><use xlink:href="#MJX-8-TEX-N-5B"></use></g><g data-mml-node="mi" transform="translate(5810.8, 0)"><use xlink:href="#MJX-8-TEX-I-1D45B"></use></g><g data-mml-node="mo" transform="translate(6410.8, 0)"><use xlink:href="#MJX-8-TEX-N-5D"></use></g><g data-mml-node="mo" transform="translate(6911, 0)"><use xlink:href="#MJX-8-TEX-N-22C5"></use></g><g data-mml-node="msup" transform="translate(7411.3, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-8-TEX-I-1D452"></use></g><g data-mml-node="TeXAtom" transform="translate(466, 413) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mo"><use xlink:href="#MJX-8-TEX-N-2212"></use></g><g data-mml-node="mi" transform="translate(778, 0)"><use xlink:href="#MJX-8-TEX-I-1D456"></use></g><g data-mml-node="mn" transform="translate(1123, 0)"><use xlink:href="#MJX-8-TEX-N-32"></use></g><g data-mml-node="mi" transform="translate(1623, 0)"><use xlink:href="#MJX-8-TEX-I-1D70B"></use></g><g data-mml-node="mi" transform="translate(2193, 0)"><use xlink:href="#MJX-8-TEX-I-1D458"></use></g><g data-mml-node="mi" transform="translate(2714, 0)"><use xlink:href="#MJX-8-TEX-I-1D45B"></use></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(3314, 0)"><g data-mml-node="mo"><use xlink:href="#MJX-8-TEX-N-2F"></use></g></g><g data-mml-node="mi" transform="translate(3814, 0)"><use xlink:href="#MJX-8-TEX-I-1D441"></use></g></g></g></g></g><g data-mml-node="mtr" transform="translate(0, -1733)"><g data-mml-node="mtd"><g data-mml-node="mi"><use xlink:href="#MJX-8-TEX-I-1D44B"></use></g><g data-mml-node="mo" transform="translate(852, 0)"><use xlink:href="#MJX-8-TEX-N-5B"></use></g><g data-mml-node="mi" transform="translate(1130, 0)"><use xlink:href="#MJX-8-TEX-I-1D458"></use></g><g data-mml-node="mo" transform="translate(1651, 0)"><use xlink:href="#MJX-8-TEX-N-5D"></use></g><g data-mml-node="mo" transform="translate(2206.8, 0)"><use xlink:href="#MJX-8-TEX-N-3D"></use></g><g data-mml-node="munderover" transform="translate(3262.6, 0)"><g data-mml-node="mo" transform="translate(43.8, 0)"><use xlink:href="#MJX-8-TEX-LO-2211"></use></g><g data-mml-node="TeXAtom" transform="translate(101.8, -1087.9) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-8-TEX-I-1D45B"></use></g><g data-mml-node="mo" transform="translate(600, 0)"><use xlink:href="#MJX-8-TEX-N-3D"></use></g><g data-mml-node="mn" transform="translate(1378, 0)"><use xlink:href="#MJX-8-TEX-N-30"></use></g></g><g data-mml-node="TeXAtom" transform="translate(0, 1150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-8-TEX-I-1D441"></use></g><g data-mml-node="mo" transform="translate(888, 0)"><use xlink:href="#MJX-8-TEX-N-2212"></use></g><g data-mml-node="mn" transform="translate(1666, 0)"><use xlink:href="#MJX-8-TEX-N-31"></use></g></g></g><g data-mml-node="mi" transform="translate(4960.8, 0)"><use xlink:href="#MJX-8-TEX-I-1D465"></use></g><g data-mml-node="mo" transform="translate(5532.8, 0)"><use xlink:href="#MJX-8-TEX-N-5B"></use></g><g data-mml-node="mi" transform="translate(5810.8, 0)"><use xlink:href="#MJX-8-TEX-I-1D45B"></use></g><g data-mml-node="mo" transform="translate(6410.8, 0)"><use xlink:href="#MJX-8-TEX-N-5D"></use></g><g data-mml-node="mo" transform="translate(6911, 0)"><use xlink:href="#MJX-8-TEX-N-22C5"></use></g><g data-mml-node="mi" transform="translate(7411.3, 0)"><use xlink:href="#MJX-8-TEX-I-1D450"></use></g><g data-mml-node="mi" transform="translate(7844.3, 0)"><use xlink:href="#MJX-8-TEX-I-1D45C"></use></g><g data-mml-node="mi" transform="translate(8329.3, 0)"><use xlink:href="#MJX-8-TEX-I-1D460"></use></g><g data-mml-node="mo" transform="translate(8798.3, 0)"><use xlink:href="#MJX-8-TEX-N-28"></use></g><g data-mml-node="mfrac" transform="translate(9187.3, 0)"><g data-mml-node="mrow" transform="translate(220, 676)"><g data-mml-node="mn"><use xlink:href="#MJX-8-TEX-N-32"></use></g><g data-mml-node="mi" transform="translate(500, 0)"><use xlink:href="#MJX-8-TEX-I-1D70B"></use></g><g data-mml-node="mi" transform="translate(1070, 0)"><use xlink:href="#MJX-8-TEX-I-1D458"></use></g><g data-mml-node="mi" transform="translate(1591, 0)"><use xlink:href="#MJX-8-TEX-I-1D45B"></use></g></g><g data-mml-node="mi" transform="translate(871.5, -686)"><use xlink:href="#MJX-8-TEX-I-1D441"></use></g><rect width="2391" height="60" x="120" y="220"></rect></g><g data-mml-node="mo" transform="translate(11818.3, 0)"><use xlink:href="#MJX-8-TEX-N-29"></use></g><g data-mml-node="mo" transform="translate(12429.5, 0)"><use xlink:href="#MJX-8-TEX-N-2212"></use></g><g data-mml-node="mi" transform="translate(13429.7, 0)"><use xlink:href="#MJX-8-TEX-I-1D456"></use></g><g data-mml-node="munderover" transform="translate(13941.4, 0)"><g data-mml-node="mo" transform="translate(43.8, 0)"><use xlink:href="#MJX-8-TEX-LO-2211"></use></g><g data-mml-node="TeXAtom" transform="translate(101.8, -1087.9) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-8-TEX-I-1D45B"></use></g><g data-mml-node="mo" transform="translate(600, 0)"><use xlink:href="#MJX-8-TEX-N-3D"></use></g><g data-mml-node="mn" transform="translate(1378, 0)"><use xlink:href="#MJX-8-TEX-N-30"></use></g></g><g data-mml-node="TeXAtom" transform="translate(0, 1150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-8-TEX-I-1D441"></use></g><g data-mml-node="mo" transform="translate(888, 0)"><use xlink:href="#MJX-8-TEX-N-2212"></use></g><g data-mml-node="mn" transform="translate(1666, 0)"><use xlink:href="#MJX-8-TEX-N-31"></use></g></g></g><g data-mml-node="mi" transform="translate(15639.6, 0)"><use xlink:href="#MJX-8-TEX-I-1D465"></use></g><g data-mml-node="mo" transform="translate(16211.6, 0)"><use xlink:href="#MJX-8-TEX-N-5B"></use></g><g data-mml-node="mi" transform="translate(16489.6, 0)"><use xlink:href="#MJX-8-TEX-I-1D45B"></use></g><g data-mml-node="mo" transform="translate(17089.6, 0)"><use xlink:href="#MJX-8-TEX-N-5D"></use></g><g data-mml-node="mo" transform="translate(17589.9, 0)"><use xlink:href="#MJX-8-TEX-N-22C5"></use></g><g data-mml-node="mi" transform="translate(18090.1, 0)"><use xlink:href="#MJX-8-TEX-I-1D460"></use></g><g data-mml-node="mi" transform="translate(18559.1, 0)"><use xlink:href="#MJX-8-TEX-I-1D456"></use></g><g data-mml-node="mi" transform="translate(18904.1, 0)"><use xlink:href="#MJX-8-TEX-I-1D45B"></use></g><g data-mml-node="mo" transform="translate(19504.1, 0)"><use xlink:href="#MJX-8-TEX-N-28"></use></g><g data-mml-node="mfrac" transform="translate(19893.1, 0)"><g data-mml-node="mrow" transform="translate(220, 676)"><g data-mml-node="mn"><use xlink:href="#MJX-8-TEX-N-32"></use></g><g data-mml-node="mi" transform="translate(500, 0)"><use xlink:href="#MJX-8-TEX-I-1D70B"></use></g><g data-mml-node="mi" transform="translate(1070, 0)"><use xlink:href="#MJX-8-TEX-I-1D458"></use></g><g data-mml-node="mi" transform="translate(1591, 0)"><use xlink:href="#MJX-8-TEX-I-1D45B"></use></g></g><g data-mml-node="mi" transform="translate(871.5, -686)"><use xlink:href="#MJX-8-TEX-I-1D441"></use></g><rect width="2391" height="60" x="120" y="220"></rect></g><g data-mml-node="mo" transform="translate(22524.1, 0)"><use xlink:href="#MJX-8-TEX-N-29"></use></g></g></g></g></g></g></svg></mjx-container></div><img src="/Hexo-Blog/2021/02/05/2021-02-05-audio-based-song-genre-classification/fft.png" class=""><p>Let’s implement discrete fourier transform in python.</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">dft</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>    x <span class="token operator">=</span> np<span class="token punctuation">.</span>asarray<span class="token punctuation">(</span>x<span class="token punctuation">,</span> dtype<span class="token operator">=</span><span class="token builtin">float</span><span class="token punctuation">)</span>    N <span class="token operator">=</span> x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>    cos <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros_like<span class="token punctuation">(</span>x<span class="token punctuation">)</span>    sin <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros_like<span class="token punctuation">(</span>x<span class="token punctuation">)</span>    X <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    <span class="token keyword">for</span> k <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>N<span class="token punctuation">)</span><span class="token punctuation">:</span>        real<span class="token punctuation">,</span> image <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span>        <span class="token keyword">for</span> n <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>N<span class="token punctuation">)</span><span class="token punctuation">:</span>            real <span class="token operator">+=</span> x<span class="token punctuation">[</span>n<span class="token punctuation">]</span> <span class="token operator">*</span> math<span class="token punctuation">.</span>cos<span class="token punctuation">(</span><span class="token number">2</span> <span class="token operator">*</span> math<span class="token punctuation">.</span>pi <span class="token operator">*</span> k <span class="token operator">*</span> n <span class="token operator">/</span> N<span class="token punctuation">)</span>            image <span class="token operator">+=</span> x<span class="token punctuation">[</span>n<span class="token punctuation">]</span> <span class="token operator">*</span> math<span class="token punctuation">.</span>sin<span class="token punctuation">(</span><span class="token number">2</span> <span class="token operator">*</span> math<span class="token punctuation">.</span>pi <span class="token operator">*</span> k <span class="token operator">*</span> n <span class="token operator">/</span> N<span class="token punctuation">)</span>        X<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token builtin">complex</span><span class="token punctuation">(</span>real<span class="token punctuation">,</span> image<span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>X<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>Check whether it is correct.</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">x <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">1000</span><span class="token punctuation">,</span> <span class="token punctuation">)</span><span class="token punctuation">(</span><span class="token punctuation">(</span>fft<span class="token punctuation">(</span>x<span class="token punctuation">)</span> <span class="token operator">-</span> np<span class="token punctuation">.</span>fft<span class="token punctuation">.</span>fft<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">&lt;</span> <span class="token number">1e</span><span class="token operator">-</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">all</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><hr><pre class="line-numbers language-console" data-language="console"><code class="language-console">True<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="Intuition"><a href="#Intuition" class="headerlink" title="Intuition"></a>Intuition</h3><ul><li>Decompose a complex sound into its frequency components.</li><li>Convert time domain to frequency domain.</li><li>Compare signal with sinusoids of various frequencies.</li><li>For each frequency we get a magnitude and a phase.</li><li>High magnitude indicates high similarity bwtween the signal and a sinunoid.</li></ul><h3 id="Reconstruct-the-signal"><a href="#Reconstruct-the-signal" class="headerlink" title="Reconstruct the signal"></a>Reconstruct the signal</h3><ul><li>Superimpose sinusoids</li><li>Weight them bu the relative magnitude</li><li>Use relative phase</li><li>Original signal and FT have same information</li></ul><h3 id="Inverse-Fourier-Transform"><a href="#Inverse-Fourier-Transform" class="headerlink" title="Inverse Fourier Transform"></a>Inverse Fourier Transform</h3><ul><li>Additive synthesis (waveform -&gt; spectrum -&gt; waveform)</li></ul><h2 id="Spectrogram"><a href="#Spectrogram" class="headerlink" title="Spectrogram"></a>Spectrogram</h2><p>Short-time Fourier transform (STFT) is a sequence of Fourier transforms of a windowed signal. STFT provides the time-localized frequency information for situations in which frequency components of a signal vary over time, whereas the standard Fourier transform provides the frequency information averaged over the entire signal time interval. In this work, I use STFT to obtain sprectrogram.</p><p>A spectrogram is a visual way of representing the signal strength, or “loudness”, of a signal over time at various frequencies present in a particular waveform. Not only can one see whether there is more or less energy at, but one can also see how energy levels vary over time.</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">n_fft <span class="token operator">=</span> <span class="token number">2048</span>hop_length <span class="token operator">=</span> <span class="token number">512</span>stft <span class="token operator">=</span> librosa<span class="token punctuation">.</span>core<span class="token punctuation">.</span>stft<span class="token punctuation">(</span>signal<span class="token punctuation">,</span> n_fft<span class="token operator">=</span>n_fft<span class="token punctuation">,</span> hop_length<span class="token operator">=</span>hop_length<span class="token punctuation">)</span>spectrogram <span class="token operator">=</span> np<span class="token punctuation">.</span><span class="token builtin">abs</span><span class="token punctuation">(</span>stft<span class="token punctuation">)</span>log_spectrogram <span class="token operator">=</span> librosa<span class="token punctuation">.</span>amplitude_to_db<span class="token punctuation">(</span>spectrogram<span class="token punctuation">)</span>librosa<span class="token punctuation">.</span>display<span class="token punctuation">.</span>specshow<span class="token punctuation">(</span>log_spectrogram<span class="token punctuation">,</span> sr<span class="token operator">=</span>sr<span class="token punctuation">,</span> hop_length<span class="token operator">=</span>hop_length<span class="token punctuation">)</span>plt<span class="token punctuation">.</span>xlabel<span class="token punctuation">(</span><span class="token string">"Time"</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>ylabel<span class="token punctuation">(</span><span class="token string">"Frequency"</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>colorbar<span class="token punctuation">(</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><img src="/Hexo-Blog/2021/02/05/2021-02-05-audio-based-song-genre-classification/spectrogram.png" class=""><h2 id="Mel-frequency-Cepstrum-Coefficients-MFCC"><a href="#Mel-frequency-Cepstrum-Coefficients-MFCC" class="headerlink" title="Mel-frequency Cepstrum Coefficients (MFCC)"></a>Mel-frequency Cepstrum Coefficients (MFCC)</h2><p>Mel-frequency cepstral coefficients (MFCCs) are coefficients that collectively make up an MFC. They are derived from a type of cepstral representation of the audio clip (a nonlinear “spectrum-of-a-spectrum”). The advantage of MFCC is that it is good in error reduction and able to produce a robust feature when the signal is affected by noise.</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">mfcc <span class="token operator">=</span> librosa<span class="token punctuation">.</span>feature<span class="token punctuation">.</span>mfcc<span class="token punctuation">(</span>signal<span class="token punctuation">,</span> n_fft<span class="token operator">=</span>n_fft<span class="token punctuation">,</span> hop_length<span class="token operator">=</span>hop_length<span class="token punctuation">,</span> n_mfcc<span class="token operator">=</span><span class="token number">13</span><span class="token punctuation">)</span>librosa<span class="token punctuation">.</span>display<span class="token punctuation">.</span>specshow<span class="token punctuation">(</span>mfcc<span class="token punctuation">,</span> sr<span class="token operator">=</span>sr<span class="token punctuation">,</span> hop_length<span class="token operator">=</span>hop_length<span class="token punctuation">)</span>plt<span class="token punctuation">.</span>xlabel<span class="token punctuation">(</span><span class="token string">"Time"</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>ylabel<span class="token punctuation">(</span><span class="token string">"MFCC"</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>colorbar<span class="token punctuation">(</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><img src="/Hexo-Blog/2021/02/05/2021-02-05-audio-based-song-genre-classification/mfcc.png" class=""><h2 id="Time-domain-Features"><a href="#Time-domain-Features" class="headerlink" title="Time-domain Features"></a>Time-domain Features</h2><ul><li>Amplitude envelope (AE)</li><li>Root-mean-square energy (RMS)</li><li>Zero-crossing rate (ZCR)</li></ul><h3 id="Amplitude-envelope"><a href="#Amplitude-envelope" class="headerlink" title="Amplitude envelope"></a>Amplitude envelope</h3><ul><li>Max value of all samples in a frame: </li></ul><div style="display: flex;justify-content: center;">    <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.849ex" xmlns="http://www.w3.org/2000/svg" width="23.553ex" height="3.248ex" role="img" focusable="false" viewBox="0 -1060.7 10410.4 1435.8" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-8-TEX-I-1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"></path><path id="MJX-8-TEX-I-1D438" d="M492 213Q472 213 472 226Q472 230 477 250T482 285Q482 316 461 323T364 330H312Q311 328 277 192T243 52Q243 48 254 48T334 46Q428 46 458 48T518 61Q567 77 599 117T670 248Q680 270 683 272Q690 274 698 274Q718 274 718 261Q613 7 608 2Q605 0 322 0H133Q31 0 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q146 66 215 342T285 622Q285 629 281 629Q273 632 228 634H197Q191 640 191 642T193 659Q197 676 203 680H757Q764 676 764 669Q764 664 751 557T737 447Q735 440 717 440H705Q698 445 698 453L701 476Q704 500 704 528Q704 558 697 578T678 609T643 625T596 632T532 634H485Q397 633 392 631Q388 629 386 622Q385 619 355 499T324 377Q347 376 372 376H398Q464 376 489 391T534 472Q538 488 540 490T557 493Q562 493 565 493T570 492T572 491T574 487T577 483L544 351Q511 218 508 216Q505 213 492 213Z"></path><path id="MJX-8-TEX-I-1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path><path id="MJX-8-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-8-TEX-N-6D" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q351 442 364 440T387 434T406 426T421 417T432 406T441 395T448 384T452 374T455 366L457 361L460 365Q463 369 466 373T475 384T488 397T503 410T523 422T546 432T572 439T603 442Q729 442 740 329Q741 322 741 190V104Q741 66 743 59T754 49Q775 46 803 46H819V0H811L788 1Q764 2 737 2T699 3Q596 3 587 0H579V46H595Q656 46 656 62Q657 64 657 200Q656 335 655 343Q649 371 635 385T611 402T585 404Q540 404 506 370Q479 343 472 315T464 232V168V108Q464 78 465 68T468 55T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z"></path><path id="MJX-8-TEX-N-61" d="M137 305T115 305T78 320T63 359Q63 394 97 421T218 448Q291 448 336 416T396 340Q401 326 401 309T402 194V124Q402 76 407 58T428 40Q443 40 448 56T453 109V145H493V106Q492 66 490 59Q481 29 455 12T400 -6T353 12T329 54V58L327 55Q325 52 322 49T314 40T302 29T287 17T269 6T247 -2T221 -8T190 -11Q130 -11 82 20T34 107Q34 128 41 147T68 188T116 225T194 253T304 268H318V290Q318 324 312 340Q290 411 215 411Q197 411 181 410T156 406T148 403Q170 388 170 359Q170 334 154 320ZM126 106Q126 75 150 51T209 26Q247 26 276 49T315 109Q317 116 318 175Q318 233 317 233Q309 233 296 232T251 223T193 203T147 166T126 106Z"></path><path id="MJX-8-TEX-N-78" d="M201 0Q189 3 102 3Q26 3 17 0H11V46H25Q48 47 67 52T96 61T121 78T139 96T160 122T180 150L226 210L168 288Q159 301 149 315T133 336T122 351T113 363T107 370T100 376T94 379T88 381T80 383Q74 383 44 385H16V431H23Q59 429 126 429Q219 429 229 431H237V385Q201 381 201 369Q201 367 211 353T239 315T268 274L272 270L297 304Q329 345 329 358Q329 364 327 369T322 376T317 380T310 384L307 385H302V431H309Q324 428 408 428Q487 428 493 431H499V385H492Q443 385 411 368Q394 360 377 341T312 257L296 236L358 151Q424 61 429 57T446 50Q464 46 499 46H516V0H510H502Q494 1 482 1T457 2T432 2T414 3Q403 3 377 3T327 1L304 0H295V46H298Q309 46 320 51T331 63Q331 65 291 120L250 175Q249 174 219 133T185 88Q181 83 181 74Q181 63 188 55T206 46Q208 46 208 23V0H201Z"></path><path id="MJX-8-TEX-N-28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path><path id="MJX-8-TEX-N-2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path><path id="MJX-8-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-8-TEX-N-29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path><path id="MJX-8-TEX-N-22C5" d="M78 250Q78 274 95 292T138 310Q162 310 180 294T199 251Q199 226 182 208T139 190T96 207T78 250Z"></path><path id="MJX-8-TEX-I-1D43E" d="M285 628Q285 635 228 637Q205 637 198 638T191 647Q191 649 193 661Q199 681 203 682Q205 683 214 683H219Q260 681 355 681Q389 681 418 681T463 682T483 682Q500 682 500 674Q500 669 497 660Q496 658 496 654T495 648T493 644T490 641T486 639T479 638T470 637T456 637Q416 636 405 634T387 623L306 305Q307 305 490 449T678 597Q692 611 692 620Q692 635 667 637Q651 637 651 648Q651 650 654 662T659 677Q662 682 676 682Q680 682 711 681T791 680Q814 680 839 681T869 682Q889 682 889 672Q889 650 881 642Q878 637 862 637Q787 632 726 586Q710 576 656 534T556 455L509 418L518 396Q527 374 546 329T581 244Q656 67 661 61Q663 59 666 57Q680 47 717 46H738Q744 38 744 37T741 19Q737 6 731 0H720Q680 3 625 3Q503 3 488 0H478Q472 6 472 9T474 27Q478 40 480 43T491 46H494Q544 46 544 71Q544 75 517 141T485 216L427 354L359 301L291 248L268 155Q245 63 245 58Q245 51 253 49T303 46H334Q340 37 340 35Q340 19 333 5Q328 0 317 0Q314 0 280 1T180 2Q118 2 85 2T49 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Z"></path><path id="MJX-8-TEX-N-2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path><path id="MJX-8-TEX-I-1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path><path id="MJX-8-TEX-I-1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-8-TEX-I-1D434"></use></g><g data-mml-node="msub" transform="translate(750, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-8-TEX-I-1D438"></use></g><g data-mml-node="mi" transform="translate(738, -150) scale(0.707)"><use xlink:href="#MJX-8-TEX-I-1D461"></use></g></g><g data-mml-node="mo" transform="translate(2071, 0)"><use xlink:href="#MJX-8-TEX-N-3D"></use></g><g data-mml-node="munderover" transform="translate(3126.8, 0)"><g data-mml-node="mo"><use xlink:href="#MJX-8-TEX-N-6D"></use><use xlink:href="#MJX-8-TEX-N-61" transform="translate(833, 0)"></use><use xlink:href="#MJX-8-TEX-N-78" transform="translate(1333, 0)"></use></g><g data-mml-node="TeXAtom" transform="translate(1861, 530.4) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mo"><use xlink:href="#MJX-8-TEX-N-28"></use></g><g data-mml-node="mi" transform="translate(389, 0)"><use xlink:href="#MJX-8-TEX-I-1D461"></use></g><g data-mml-node="mo" transform="translate(750, 0)"><use xlink:href="#MJX-8-TEX-N-2B"></use></g><g data-mml-node="mn" transform="translate(1528, 0)"><use xlink:href="#MJX-8-TEX-N-31"></use></g><g data-mml-node="mo" transform="translate(2028, 0)"><use xlink:href="#MJX-8-TEX-N-29"></use></g><g data-mml-node="mo" transform="translate(2417, 0)"><use xlink:href="#MJX-8-TEX-N-22C5"></use></g><g data-mml-node="mi" transform="translate(2695, 0)"><use xlink:href="#MJX-8-TEX-I-1D43E"></use></g><g data-mml-node="mo" transform="translate(3584, 0)"><use xlink:href="#MJX-8-TEX-N-2212"></use></g><g data-mml-node="mn" transform="translate(4362, 0)"><use xlink:href="#MJX-8-TEX-N-31"></use></g></g><g data-mml-node="TeXAtom" transform="translate(1861, -317.1) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-8-TEX-I-1D458"></use></g><g data-mml-node="mo" transform="translate(521, 0)"><use xlink:href="#MJX-8-TEX-N-3D"></use></g><g data-mml-node="mi" transform="translate(1299, 0)"><use xlink:href="#MJX-8-TEX-I-1D461"></use></g><g data-mml-node="mo" transform="translate(1660, 0)"><use xlink:href="#MJX-8-TEX-N-22C5"></use></g><g data-mml-node="mi" transform="translate(1938, 0)"><use xlink:href="#MJX-8-TEX-I-1D43E"></use></g></g></g><g data-mml-node="mi" transform="translate(8642.4, 0)"><use xlink:href="#MJX-8-TEX-I-1D460"></use></g><g data-mml-node="mo" transform="translate(9111.4, 0)"><use xlink:href="#MJX-8-TEX-N-28"></use></g><g data-mml-node="mi" transform="translate(9500.4, 0)"><use xlink:href="#MJX-8-TEX-I-1D458"></use></g><g data-mml-node="mo" transform="translate(10021.4, 0)"><use xlink:href="#MJX-8-TEX-N-29"></use></g></g></g></svg></mjx-container></div><ul><li>Gives rough idea of loudness</li><li>Sensitive to outliers</li><li>Onset detection, music genre classification</li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">amplitude_envelope</span><span class="token punctuation">(</span>signal<span class="token punctuation">,</span> frame_length<span class="token punctuation">,</span> hop_length<span class="token punctuation">)</span><span class="token punctuation">:</span>    amplitude_envelope <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>signal<span class="token punctuation">)</span><span class="token punctuation">,</span> hop_length<span class="token punctuation">)</span><span class="token punctuation">:</span>        current_frame_amplitude_envelope <span class="token operator">=</span> <span class="token builtin">max</span><span class="token punctuation">(</span>signal<span class="token punctuation">[</span>i<span class="token punctuation">:</span>i<span class="token operator">+</span>frame_length<span class="token punctuation">]</span><span class="token punctuation">)</span>        amplitude_envelope<span class="token punctuation">.</span>append<span class="token punctuation">(</span>current_frame_amplitude_envelope<span class="token punctuation">)</span>    <span class="token keyword">return</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>amplitude_envelope<span class="token punctuation">)</span>FRAME_LENGTH <span class="token operator">=</span> <span class="token number">1024</span>HOP_LENGTH <span class="token operator">=</span> <span class="token number">512</span>ae_blank_space <span class="token operator">=</span> amplitude_envelope<span class="token punctuation">(</span>blank_space<span class="token punctuation">,</span> FRAME_LENGTH<span class="token punctuation">,</span> HOP_LENGTH<span class="token punctuation">)</span>frames <span class="token operator">=</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> ae_blank_space<span class="token punctuation">.</span>size<span class="token punctuation">)</span>t_interval <span class="token operator">=</span> librosa<span class="token punctuation">.</span>frames_to_time<span class="token punctuation">(</span>frames<span class="token punctuation">,</span> hop_length<span class="token operator">=</span>HOP_LENGTH<span class="token punctuation">)</span>librosa<span class="token punctuation">.</span>display<span class="token punctuation">.</span>waveplot<span class="token punctuation">(</span>blank_space<span class="token punctuation">,</span> alpha<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>t_interval<span class="token punctuation">,</span> ae_blank_space<span class="token punctuation">,</span> color<span class="token operator">=</span><span class="token string">"tab:red"</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>title<span class="token punctuation">(</span><span class="token string">"Taylor Swift Blank Space Amplitude Envelope"</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>ylim<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><img src="/Hexo-Blog/2021/02/05/2021-02-05-audio-based-song-genre-classification/ae.png" class=""><h3 id="Root-mean-square-energy"><a href="#Root-mean-square-energy" class="headerlink" title="Root-mean-square energy"></a>Root-mean-square energy</h3><ul><li>RMS of all samples in a frame: </li></ul><div style="display: flex;justify-content: center;">    <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -1.108ex" xmlns="http://www.w3.org/2000/svg" width="29.737ex" height="4.208ex" role="img" focusable="false" viewBox="0 -1370.3 13143.9 1860" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-8-TEX-I-1D445" d="M230 637Q203 637 198 638T193 649Q193 676 204 682Q206 683 378 683Q550 682 564 680Q620 672 658 652T712 606T733 563T739 529Q739 484 710 445T643 385T576 351T538 338L545 333Q612 295 612 223Q612 212 607 162T602 80V71Q602 53 603 43T614 25T640 16Q668 16 686 38T712 85Q717 99 720 102T735 105Q755 105 755 93Q755 75 731 36Q693 -21 641 -21H632Q571 -21 531 4T487 82Q487 109 502 166T517 239Q517 290 474 313Q459 320 449 321T378 323H309L277 193Q244 61 244 59Q244 55 245 54T252 50T269 48T302 46H333Q339 38 339 37T336 19Q332 6 326 0H311Q275 2 180 2Q146 2 117 2T71 2T50 1Q33 1 33 10Q33 12 36 24Q41 43 46 45Q50 46 61 46H67Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628Q287 635 230 637ZM630 554Q630 586 609 608T523 636Q521 636 500 636T462 637H440Q393 637 386 627Q385 624 352 494T319 361Q319 360 388 360Q466 361 492 367Q556 377 592 426Q608 449 619 486T630 554Z"></path><path id="MJX-8-TEX-I-1D440" d="M289 629Q289 635 232 637Q208 637 201 638T194 648Q194 649 196 659Q197 662 198 666T199 671T201 676T203 679T207 681T212 683T220 683T232 684Q238 684 262 684T307 683Q386 683 398 683T414 678Q415 674 451 396L487 117L510 154Q534 190 574 254T662 394Q837 673 839 675Q840 676 842 678T846 681L852 683H948Q965 683 988 683T1017 684Q1051 684 1051 673Q1051 668 1048 656T1045 643Q1041 637 1008 637Q968 636 957 634T939 623Q936 618 867 340T797 59Q797 55 798 54T805 50T822 48T855 46H886Q892 37 892 35Q892 19 885 5Q880 0 869 0Q864 0 828 1T736 2Q675 2 644 2T609 1Q592 1 592 11Q592 13 594 25Q598 41 602 43T625 46Q652 46 685 49Q699 52 704 61Q706 65 742 207T813 490T848 631L654 322Q458 10 453 5Q451 4 449 3Q444 0 433 0Q418 0 415 7Q413 11 374 317L335 624L267 354Q200 88 200 79Q206 46 272 46H282Q288 41 289 37T286 19Q282 3 278 1Q274 0 267 0Q265 0 255 0T221 1T157 2Q127 2 95 1T58 0Q43 0 39 2T35 11Q35 13 38 25T43 40Q45 46 65 46Q135 46 154 86Q158 92 223 354T289 629Z"></path><path id="MJX-8-TEX-I-1D446" d="M308 24Q367 24 416 76T466 197Q466 260 414 284Q308 311 278 321T236 341Q176 383 176 462Q176 523 208 573T273 648Q302 673 343 688T407 704H418H425Q521 704 564 640Q565 640 577 653T603 682T623 704Q624 704 627 704T632 705Q645 705 645 698T617 577T585 459T569 456Q549 456 549 465Q549 471 550 475Q550 478 551 494T553 520Q553 554 544 579T526 616T501 641Q465 662 419 662Q362 662 313 616T263 510Q263 480 278 458T319 427Q323 425 389 408T456 390Q490 379 522 342T554 242Q554 216 546 186Q541 164 528 137T492 78T426 18T332 -20Q320 -22 298 -22Q199 -22 144 33L134 44L106 13Q83 -14 78 -18T65 -22Q52 -22 52 -14Q52 -11 110 221Q112 227 130 227H143Q149 221 149 216Q149 214 148 207T144 186T142 153Q144 114 160 87T203 47T255 29T308 24Z"></path><path id="MJX-8-TEX-I-1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path><path id="MJX-8-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-8-TEX-N-32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path><path id="MJX-8-TEX-LO-221A" d="M1001 1150Q1017 1150 1020 1132Q1020 1127 741 244L460 -643Q453 -650 436 -650H424Q423 -647 423 -645T421 -640T419 -631T415 -617T408 -594T399 -560T385 -512T367 -448T343 -364T312 -259L203 119L138 41L111 67L212 188L264 248L472 -474L983 1140Q988 1150 1001 1150Z"></path><path id="MJX-8-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-8-TEX-I-1D43E" d="M285 628Q285 635 228 637Q205 637 198 638T191 647Q191 649 193 661Q199 681 203 682Q205 683 214 683H219Q260 681 355 681Q389 681 418 681T463 682T483 682Q500 682 500 674Q500 669 497 660Q496 658 496 654T495 648T493 644T490 641T486 639T479 638T470 637T456 637Q416 636 405 634T387 623L306 305Q307 305 490 449T678 597Q692 611 692 620Q692 635 667 637Q651 637 651 648Q651 650 654 662T659 677Q662 682 676 682Q680 682 711 681T791 680Q814 680 839 681T869 682Q889 682 889 672Q889 650 881 642Q878 637 862 637Q787 632 726 586Q710 576 656 534T556 455L509 418L518 396Q527 374 546 329T581 244Q656 67 661 61Q663 59 666 57Q680 47 717 46H738Q744 38 744 37T741 19Q737 6 731 0H720Q680 3 625 3Q503 3 488 0H478Q472 6 472 9T474 27Q478 40 480 43T491 46H494Q544 46 544 71Q544 75 517 141T485 216L427 354L359 301L291 248L268 155Q245 63 245 58Q245 51 253 49T303 46H334Q340 37 340 35Q340 19 333 5Q328 0 317 0Q314 0 280 1T180 2Q118 2 85 2T49 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Z"></path><path id="MJX-8-TEX-N-22C5" d="M78 250Q78 274 95 292T138 310Q162 310 180 294T199 251Q199 226 182 208T139 190T96 207T78 250Z"></path><path id="MJX-8-TEX-SO-2211" d="M61 748Q64 750 489 750H913L954 640Q965 609 976 579T993 533T999 516H979L959 517Q936 579 886 621T777 682Q724 700 655 705T436 710H319Q183 710 183 709Q186 706 348 484T511 259Q517 250 513 244L490 216Q466 188 420 134T330 27L149 -187Q149 -188 362 -188Q388 -188 436 -188T506 -189Q679 -189 778 -162T936 -43Q946 -27 959 6H999L913 -249L489 -250Q65 -250 62 -248Q56 -246 56 -239Q56 -234 118 -161Q186 -81 245 -11L428 206Q428 207 242 462L57 717L56 728Q56 744 61 748Z"></path><path id="MJX-8-TEX-N-28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path><path id="MJX-8-TEX-N-2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path><path id="MJX-8-TEX-N-29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path><path id="MJX-8-TEX-N-2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path><path id="MJX-8-TEX-I-1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path><path id="MJX-8-TEX-I-1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-8-TEX-I-1D445"></use></g><g data-mml-node="mi" transform="translate(759, 0)"><use xlink:href="#MJX-8-TEX-I-1D440"></use></g><g data-mml-node="msub" transform="translate(1810, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-8-TEX-I-1D446"></use></g><g data-mml-node="mi" transform="translate(613, -150) scale(0.707)"><use xlink:href="#MJX-8-TEX-I-1D461"></use></g></g><g data-mml-node="mo" transform="translate(3006, 0)"><use xlink:href="#MJX-8-TEX-N-3D"></use></g><g data-mml-node="mroot" transform="translate(4061.8, 0)"><g><g data-mml-node="mrow" transform="translate(1020, 0)"><g data-mml-node="mfrac"><g data-mml-node="mn" transform="translate(357.5, 394) scale(0.707)"><use xlink:href="#MJX-8-TEX-N-31"></use></g><g data-mml-node="mi" transform="translate(220, -345) scale(0.707)"><use xlink:href="#MJX-8-TEX-I-1D43E"></use></g><rect width="828.6" height="60" x="120" y="220"></rect></g><g data-mml-node="mo" transform="translate(1068.6, 0)"><use xlink:href="#MJX-8-TEX-N-22C5"></use></g><g data-mml-node="munderover" transform="translate(1346.6, 0)"><g data-mml-node="mo"><use xlink:href="#MJX-8-TEX-SO-2211"></use></g><g data-mml-node="TeXAtom" transform="translate(1056, 530.4) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mo"><use xlink:href="#MJX-8-TEX-N-28"></use></g><g data-mml-node="mi" transform="translate(389, 0)"><use xlink:href="#MJX-8-TEX-I-1D461"></use></g><g data-mml-node="mo" transform="translate(750, 0)"><use xlink:href="#MJX-8-TEX-N-2B"></use></g><g data-mml-node="mn" transform="translate(1528, 0)"><use xlink:href="#MJX-8-TEX-N-31"></use></g><g data-mml-node="mo" transform="translate(2028, 0)"><use xlink:href="#MJX-8-TEX-N-29"></use></g><g data-mml-node="mo" transform="translate(2417, 0)"><use xlink:href="#MJX-8-TEX-N-22C5"></use></g><g data-mml-node="mi" transform="translate(2695, 0)"><use xlink:href="#MJX-8-TEX-I-1D43E"></use></g><g data-mml-node="mo" transform="translate(3584, 0)"><use xlink:href="#MJX-8-TEX-N-2212"></use></g><g data-mml-node="mn" transform="translate(4362, 0)"><use xlink:href="#MJX-8-TEX-N-31"></use></g></g><g data-mml-node="TeXAtom" transform="translate(1056, -317.1) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-8-TEX-I-1D458"></use></g><g data-mml-node="mo" transform="translate(521, 0)"><use xlink:href="#MJX-8-TEX-N-3D"></use></g><g data-mml-node="mi" transform="translate(1299, 0)"><use xlink:href="#MJX-8-TEX-I-1D461"></use></g><g data-mml-node="mo" transform="translate(1660, 0)"><use xlink:href="#MJX-8-TEX-N-22C5"></use></g><g data-mml-node="mi" transform="translate(1938, 0)"><use xlink:href="#MJX-8-TEX-I-1D43E"></use></g></g></g><g data-mml-node="mi" transform="translate(5890.6, 0)"><use xlink:href="#MJX-8-TEX-I-1D460"></use></g><g data-mml-node="mo" transform="translate(6359.6, 0)"><use xlink:href="#MJX-8-TEX-N-28"></use></g><g data-mml-node="mi" transform="translate(6748.6, 0)"><use xlink:href="#MJX-8-TEX-I-1D458"></use></g><g data-mml-node="msup" transform="translate(7269.6, 0)"><g data-mml-node="mo"><use xlink:href="#MJX-8-TEX-N-29"></use></g><g data-mml-node="mn" transform="translate(389, 289) scale(0.707)"><use xlink:href="#MJX-8-TEX-N-32"></use></g></g></g></g><g data-mml-node="mn" transform="translate(362, 500.3) scale(0.5)"><use xlink:href="#MJX-8-TEX-N-32"></use></g><g data-mml-node="mo" transform="translate(0, 160.3)"><use xlink:href="#MJX-8-TEX-LO-221A"></use></g><rect width="8062.1" height="60" x="1020" y="1250.3"></rect></g></g></g></svg></mjx-container></div><ul><li>Indicator of loudness.</li><li>Less sensitive to outliers than AE.</li><li>Audio segmentation, music genre classification</li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python">FRAME_LENGTH <span class="token operator">=</span> <span class="token number">1024</span>HOP_LENGTH <span class="token operator">=</span> <span class="token number">512</span>rms_blank_space <span class="token operator">=</span> librosa<span class="token punctuation">.</span>feature<span class="token punctuation">.</span>rms<span class="token punctuation">(</span>blank_space<span class="token punctuation">,</span> frame_length<span class="token operator">=</span>FRAME_LENGTH<span class="token punctuation">,</span> hop_length<span class="token operator">=</span>HOP_LENGTH<span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>frames <span class="token operator">=</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> rms_blank_space<span class="token punctuation">.</span>size<span class="token punctuation">)</span>t_interval <span class="token operator">=</span> librosa<span class="token punctuation">.</span>frames_to_time<span class="token punctuation">(</span>frames<span class="token punctuation">,</span> hop_length<span class="token operator">=</span>HOP_LENGTH<span class="token punctuation">)</span>librosa<span class="token punctuation">.</span>display<span class="token punctuation">.</span>waveplot<span class="token punctuation">(</span>blank_space<span class="token punctuation">,</span> alpha<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>t_interval<span class="token punctuation">,</span> rms_blank_space<span class="token punctuation">,</span> color<span class="token operator">=</span><span class="token string">"tab:red"</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>title<span class="token punctuation">(</span><span class="token string">"Taylor Swift Blank Space Root-mean-square energy"</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>ylim<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><img src="/Hexo-Blog/2021/02/05/2021-02-05-audio-based-song-genre-classification/rmse.png" class=""><p>We can also build the function by ourselves.</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">root_mean_square</span><span class="token punctuation">(</span>signal<span class="token punctuation">,</span> frame_length<span class="token punctuation">,</span> hop_length<span class="token punctuation">)</span><span class="token punctuation">:</span>    rms <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>signal<span class="token punctuation">)</span><span class="token punctuation">,</span> hop_length<span class="token punctuation">)</span><span class="token punctuation">:</span>        current_frame_rms <span class="token operator">=</span> np<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>np<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>np<span class="token punctuation">.</span>square<span class="token punctuation">(</span>signal<span class="token punctuation">[</span>i<span class="token punctuation">:</span>i<span class="token operator">+</span>frame_length<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">/</span> frame_length<span class="token punctuation">)</span>        rms<span class="token punctuation">.</span>append<span class="token punctuation">(</span>current_frame_rms<span class="token punctuation">)</span>    <span class="token keyword">return</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>rms<span class="token punctuation">)</span>rms_blank_space_from_scratch <span class="token operator">=</span> root_mean_square<span class="token punctuation">(</span>blank_space<span class="token punctuation">,</span> FRAME_LENGTH<span class="token punctuation">,</span> HOP_LENGTH<span class="token punctuation">)</span>frames <span class="token operator">=</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> rms_blank_space_from_scratch<span class="token punctuation">.</span>size<span class="token punctuation">)</span>t_interval <span class="token operator">=</span> librosa<span class="token punctuation">.</span>frames_to_time<span class="token punctuation">(</span>frames<span class="token punctuation">,</span> hop_length<span class="token operator">=</span>HOP_LENGTH<span class="token punctuation">)</span>librosa<span class="token punctuation">.</span>display<span class="token punctuation">.</span>waveplot<span class="token punctuation">(</span>blank_space<span class="token punctuation">,</span> alpha<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>t_interval<span class="token punctuation">,</span> rms_blank_space_from_scratch<span class="token punctuation">,</span> color<span class="token operator">=</span><span class="token string">"tab:red"</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>title<span class="token punctuation">(</span><span class="token string">"Taylor Swift Blank Space Root-mean-square energy from scratch"</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>ylim<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><img src="/Hexo-Blog/2021/02/05/2021-02-05-audio-based-song-genre-classification/rmse-from-scratch.png" class=""><h3 id="Zero-crossing-rate"><a href="#Zero-crossing-rate" class="headerlink" title="Zero-crossing rate"></a>Zero-crossing rate</h3><ul><li>Number of times a signal crosses the horizontal axis: </li></ul><div style="display: flex;justify-content: center;">    <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.849ex" xmlns="http://www.w3.org/2000/svg" width="49.304ex" height="3.248ex" role="img" focusable="false" viewBox="0 -1060.7 21792.3 1435.8" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-8-TEX-I-1D44D" d="M58 8Q58 23 64 35Q64 36 329 334T596 635L586 637Q575 637 512 637H500H476Q442 637 420 635T365 624T311 598T266 548T228 469Q227 466 226 463T224 458T223 453T222 450L221 448Q218 443 202 443Q185 443 182 453L214 561Q228 606 241 651Q249 679 253 681Q256 683 487 683H718Q723 678 723 675Q723 673 717 649Q189 54 188 52L185 49H274Q369 50 377 51Q452 60 500 100T579 247Q587 272 590 277T603 282H607Q628 282 628 271Q547 5 541 2Q538 0 300 0H124Q58 0 58 8Z"></path><path id="MJX-8-TEX-I-1D436" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z"></path><path id="MJX-8-TEX-I-1D445" d="M230 637Q203 637 198 638T193 649Q193 676 204 682Q206 683 378 683Q550 682 564 680Q620 672 658 652T712 606T733 563T739 529Q739 484 710 445T643 385T576 351T538 338L545 333Q612 295 612 223Q612 212 607 162T602 80V71Q602 53 603 43T614 25T640 16Q668 16 686 38T712 85Q717 99 720 102T735 105Q755 105 755 93Q755 75 731 36Q693 -21 641 -21H632Q571 -21 531 4T487 82Q487 109 502 166T517 239Q517 290 474 313Q459 320 449 321T378 323H309L277 193Q244 61 244 59Q244 55 245 54T252 50T269 48T302 46H333Q339 38 339 37T336 19Q332 6 326 0H311Q275 2 180 2Q146 2 117 2T71 2T50 1Q33 1 33 10Q33 12 36 24Q41 43 46 45Q50 46 61 46H67Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628Q287 635 230 637ZM630 554Q630 586 609 608T523 636Q521 636 500 636T462 637H440Q393 637 386 627Q385 624 352 494T319 361Q319 360 388 360Q466 361 492 367Q556 377 592 426Q608 449 619 486T630 554Z"></path><path id="MJX-8-TEX-I-1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path><path id="MJX-8-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-8-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-8-TEX-N-32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path><path id="MJX-8-TEX-N-22C5" d="M78 250Q78 274 95 292T138 310Q162 310 180 294T199 251Q199 226 182 208T139 190T96 207T78 250Z"></path><path id="MJX-8-TEX-SO-2211" d="M61 748Q64 750 489 750H913L954 640Q965 609 976 579T993 533T999 516H979L959 517Q936 579 886 621T777 682Q724 700 655 705T436 710H319Q183 710 183 709Q186 706 348 484T511 259Q517 250 513 244L490 216Q466 188 420 134T330 27L149 -187Q149 -188 362 -188Q388 -188 436 -188T506 -189Q679 -189 778 -162T936 -43Q946 -27 959 6H999L913 -249L489 -250Q65 -250 62 -248Q56 -246 56 -239Q56 -234 118 -161Q186 -81 245 -11L428 206Q428 207 242 462L57 717L56 728Q56 744 61 748Z"></path><path id="MJX-8-TEX-N-28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path><path id="MJX-8-TEX-N-2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path><path id="MJX-8-TEX-N-29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path><path id="MJX-8-TEX-I-1D43E" d="M285 628Q285 635 228 637Q205 637 198 638T191 647Q191 649 193 661Q199 681 203 682Q205 683 214 683H219Q260 681 355 681Q389 681 418 681T463 682T483 682Q500 682 500 674Q500 669 497 660Q496 658 496 654T495 648T493 644T490 641T486 639T479 638T470 637T456 637Q416 636 405 634T387 623L306 305Q307 305 490 449T678 597Q692 611 692 620Q692 635 667 637Q651 637 651 648Q651 650 654 662T659 677Q662 682 676 682Q680 682 711 681T791 680Q814 680 839 681T869 682Q889 682 889 672Q889 650 881 642Q878 637 862 637Q787 632 726 586Q710 576 656 534T556 455L509 418L518 396Q527 374 546 329T581 244Q656 67 661 61Q663 59 666 57Q680 47 717 46H738Q744 38 744 37T741 19Q737 6 731 0H720Q680 3 625 3Q503 3 488 0H478Q472 6 472 9T474 27Q478 40 480 43T491 46H494Q544 46 544 71Q544 75 517 141T485 216L427 354L359 301L291 248L268 155Q245 63 245 58Q245 51 253 49T303 46H334Q340 37 340 35Q340 19 333 5Q328 0 317 0Q314 0 280 1T180 2Q118 2 85 2T49 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Z"></path><path id="MJX-8-TEX-N-2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path><path id="MJX-8-TEX-I-1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path><path id="MJX-8-TEX-N-7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path><path id="MJX-8-TEX-I-1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path><path id="MJX-8-TEX-I-1D454" d="M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z"></path><path id="MJX-8-TEX-I-1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-8-TEX-I-1D44D"></use></g><g data-mml-node="mi" transform="translate(723, 0)"><use xlink:href="#MJX-8-TEX-I-1D436"></use></g><g data-mml-node="msub" transform="translate(1483, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-8-TEX-I-1D445"></use></g><g data-mml-node="mi" transform="translate(759, -150) scale(0.707)"><use xlink:href="#MJX-8-TEX-I-1D461"></use></g></g><g data-mml-node="mo" transform="translate(2825, 0)"><use xlink:href="#MJX-8-TEX-N-3D"></use></g><g data-mml-node="mfrac" transform="translate(3880.8, 0)"><g data-mml-node="mn" transform="translate(220, 394) scale(0.707)"><use xlink:href="#MJX-8-TEX-N-31"></use></g><g data-mml-node="mn" transform="translate(220, -345) scale(0.707)"><use xlink:href="#MJX-8-TEX-N-32"></use></g><rect width="553.6" height="60" x="120" y="220"></rect></g><g data-mml-node="mo" transform="translate(4896.6, 0)"><use xlink:href="#MJX-8-TEX-N-22C5"></use></g><g data-mml-node="munderover" transform="translate(5396.8, 0)"><g data-mml-node="mo"><use xlink:href="#MJX-8-TEX-SO-2211"></use></g><g data-mml-node="TeXAtom" transform="translate(1056, 530.4) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mo"><use xlink:href="#MJX-8-TEX-N-28"></use></g><g data-mml-node="mi" transform="translate(389, 0)"><use xlink:href="#MJX-8-TEX-I-1D461"></use></g><g data-mml-node="mo" transform="translate(750, 0)"><use xlink:href="#MJX-8-TEX-N-2B"></use></g><g data-mml-node="mn" transform="translate(1528, 0)"><use xlink:href="#MJX-8-TEX-N-31"></use></g><g data-mml-node="mo" transform="translate(2028, 0)"><use xlink:href="#MJX-8-TEX-N-29"></use></g><g data-mml-node="mo" transform="translate(2417, 0)"><use xlink:href="#MJX-8-TEX-N-22C5"></use></g><g data-mml-node="mi" transform="translate(2695, 0)"><use xlink:href="#MJX-8-TEX-I-1D43E"></use></g><g data-mml-node="mo" transform="translate(3584, 0)"><use xlink:href="#MJX-8-TEX-N-2212"></use></g><g data-mml-node="mn" transform="translate(4362, 0)"><use xlink:href="#MJX-8-TEX-N-31"></use></g></g><g data-mml-node="TeXAtom" transform="translate(1056, -317.1) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-8-TEX-I-1D458"></use></g><g data-mml-node="mo" transform="translate(521, 0)"><use xlink:href="#MJX-8-TEX-N-3D"></use></g><g data-mml-node="mi" transform="translate(1299, 0)"><use xlink:href="#MJX-8-TEX-I-1D461"></use></g><g data-mml-node="mo" transform="translate(1660, 0)"><use xlink:href="#MJX-8-TEX-N-22C5"></use></g><g data-mml-node="mi" transform="translate(1938, 0)"><use xlink:href="#MJX-8-TEX-I-1D43E"></use></g></g></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(10107.4, 0)"><g data-mml-node="mo"><use xlink:href="#MJX-8-TEX-N-7C"></use></g></g><g data-mml-node="mi" transform="translate(10385.4, 0)"><use xlink:href="#MJX-8-TEX-I-1D460"></use></g><g data-mml-node="mi" transform="translate(10854.4, 0)"><use xlink:href="#MJX-8-TEX-I-1D454"></use></g><g data-mml-node="mi" transform="translate(11331.4, 0)"><use xlink:href="#MJX-8-TEX-I-1D45B"></use></g><g data-mml-node="mo" transform="translate(11931.4, 0)"><use xlink:href="#MJX-8-TEX-N-28"></use></g><g data-mml-node="mi" transform="translate(12320.4, 0)"><use xlink:href="#MJX-8-TEX-I-1D460"></use></g><g data-mml-node="mo" transform="translate(12789.4, 0)"><use xlink:href="#MJX-8-TEX-N-28"></use></g><g data-mml-node="mi" transform="translate(13178.4, 0)"><use xlink:href="#MJX-8-TEX-I-1D458"></use></g><g data-mml-node="mo" transform="translate(13699.4, 0)"><use xlink:href="#MJX-8-TEX-N-29"></use></g><g data-mml-node="mo" transform="translate(14088.4, 0)"><use xlink:href="#MJX-8-TEX-N-29"></use></g><g data-mml-node="mo" transform="translate(14699.7, 0)"><use xlink:href="#MJX-8-TEX-N-2212"></use></g><g data-mml-node="mi" transform="translate(15699.9, 0)"><use xlink:href="#MJX-8-TEX-I-1D460"></use></g><g data-mml-node="mi" transform="translate(16168.9, 0)"><use xlink:href="#MJX-8-TEX-I-1D454"></use></g><g data-mml-node="mi" transform="translate(16645.9, 0)"><use xlink:href="#MJX-8-TEX-I-1D45B"></use></g><g data-mml-node="mo" transform="translate(17245.9, 0)"><use xlink:href="#MJX-8-TEX-N-28"></use></g><g data-mml-node="mi" transform="translate(17634.9, 0)"><use xlink:href="#MJX-8-TEX-I-1D460"></use></g><g data-mml-node="mo" transform="translate(18103.9, 0)"><use xlink:href="#MJX-8-TEX-N-28"></use></g><g data-mml-node="mi" transform="translate(18492.9, 0)"><use xlink:href="#MJX-8-TEX-I-1D458"></use></g><g data-mml-node="mo" transform="translate(19236.1, 0)"><use xlink:href="#MJX-8-TEX-N-2B"></use></g><g data-mml-node="mn" transform="translate(20236.3, 0)"><use xlink:href="#MJX-8-TEX-N-31"></use></g><g data-mml-node="mo" transform="translate(20736.3, 0)"><use xlink:href="#MJX-8-TEX-N-29"></use></g><g data-mml-node="mo" transform="translate(21125.3, 0)"><use xlink:href="#MJX-8-TEX-N-29"></use></g><g data-mml-node="mo" transform="translate(21514.3, 0)"><use xlink:href="#MJX-8-TEX-N-7C"></use></g></g></g></svg></mjx-container></div><ul><li>Recognition of percussive vs pitched sounds</li><li>Monophonic pitch estimation</li><li>Voice / Unvoiced decision for speech signals</li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python">zrc_blank_space <span class="token operator">=</span> librosa<span class="token punctuation">.</span>feature<span class="token punctuation">.</span>zero_crossing_rate<span class="token punctuation">(</span>blank_space<span class="token punctuation">,</span> frame_length<span class="token operator">=</span>FRAME_LENGTH<span class="token punctuation">,</span> hop_length<span class="token operator">=</span>HOP_LENGTH<span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>frames <span class="token operator">=</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> zrc_blank_space<span class="token punctuation">.</span>size<span class="token punctuation">)</span>t_interval <span class="token operator">=</span> librosa<span class="token punctuation">.</span>frames_to_time<span class="token punctuation">(</span>frames<span class="token punctuation">,</span> hop_length<span class="token operator">=</span>HOP_LENGTH<span class="token punctuation">)</span>plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>t_interval<span class="token punctuation">,</span> zrc_blank_space<span class="token operator">*</span>FRAME_LENGTH<span class="token punctuation">,</span> color<span class="token operator">=</span><span class="token string">"tab:red"</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>title<span class="token punctuation">(</span><span class="token string">"Taylor Swift Blank Zero-crossing rate"</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>ylim<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">50</span><span class="token punctuation">,</span> <span class="token number">600</span><span class="token punctuation">)</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><img src="/Hexo-Blog/2021/02/05/2021-02-05-audio-based-song-genre-classification/zcr.png" class=""><h1 id="Prepare-Data"><a href="#Prepare-Data" class="headerlink" title="Prepare Data"></a>Prepare Data</h1><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> os<span class="token keyword">import</span> json<span class="token keyword">import</span> mathDATASETPATH <span class="token operator">=</span> <span class="token string">"mp3"</span>JSONPATH <span class="token operator">=</span> <span class="token string">"data.json"</span><span class="token keyword">def</span> <span class="token function">save_mfcc</span><span class="token punctuation">(</span>dataset_path<span class="token punctuation">,</span> json_path<span class="token punctuation">,</span> sr<span class="token operator">=</span><span class="token number">22050</span><span class="token punctuation">,</span> duration<span class="token operator">=</span><span class="token number">30</span><span class="token punctuation">,</span> n_mfcc<span class="token operator">=</span><span class="token number">13</span><span class="token punctuation">,</span> n_fft<span class="token operator">=</span><span class="token number">2048</span><span class="token punctuation">,</span> hop_length<span class="token operator">=</span><span class="token number">512</span><span class="token punctuation">,</span> num_segment<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    data <span class="token operator">=</span> <span class="token punctuation">&#123;</span>        <span class="token string">"mapping"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         <span class="token string">"mfcc"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         <span class="token string">"labels"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    <span class="token punctuation">&#125;</span>    samples_per_track <span class="token operator">=</span> sr <span class="token operator">*</span> duration    num_samples_per_segment <span class="token operator">=</span> <span class="token builtin">int</span><span class="token punctuation">(</span>samples_per_track <span class="token operator">/</span> num_segment<span class="token punctuation">)</span>    expected_num_mfcc_vectors_per_segment <span class="token operator">=</span> math<span class="token punctuation">.</span>ceil<span class="token punctuation">(</span>num_samples_per_segment <span class="token operator">/</span> hop_length<span class="token punctuation">)</span>        <span class="token comment"># Loop through all the genre folders</span>    <span class="token keyword">for</span> i<span class="token punctuation">,</span> <span class="token punctuation">(</span>dirpath<span class="token punctuation">,</span> dirnames<span class="token punctuation">,</span> filenames<span class="token punctuation">)</span> <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>os<span class="token punctuation">.</span>walk<span class="token punctuation">(</span>dataset_path<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">if</span> dirpath <span class="token keyword">is</span> <span class="token keyword">not</span> dataset_path<span class="token punctuation">:</span>            <span class="token comment"># Save semantic labels</span>            dirpath_components <span class="token operator">=</span> dirpath<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">"/"</span><span class="token punctuation">)</span>            semantic_label <span class="token operator">=</span> dirpath_components<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span>            data<span class="token punctuation">[</span><span class="token string">"mapping"</span><span class="token punctuation">]</span><span class="token punctuation">.</span>append<span class="token punctuation">(</span>semantic_label<span class="token punctuation">)</span>            <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"\nProcessing </span><span class="token interpolation"><span class="token punctuation">&#123;</span>semantic_label<span class="token punctuation">&#125;</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>                        <span class="token comment"># Process files for a specific genres</span>            <span class="token keyword">for</span> f <span class="token keyword">in</span> filenames<span class="token punctuation">:</span>                file_path <span class="token operator">=</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>dirpath<span class="token punctuation">,</span> f<span class="token punctuation">)</span>                signal<span class="token punctuation">,</span> sr <span class="token operator">=</span> librosa<span class="token punctuation">.</span>load<span class="token punctuation">(</span>file_path<span class="token punctuation">,</span> sr<span class="token operator">=</span>sr<span class="token punctuation">)</span>                                <span class="token comment"># Process segments extracting mfcc and store data</span>                <span class="token keyword">for</span> s <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_segment<span class="token punctuation">)</span><span class="token punctuation">:</span>                    start_sample <span class="token operator">=</span> num_samples_per_segment <span class="token operator">*</span> s                    finish_sample <span class="token operator">=</span> start_sample <span class="token operator">+</span> num_samples_per_segment                    mfcc <span class="token operator">=</span> librosa<span class="token punctuation">.</span>feature<span class="token punctuation">.</span>mfcc<span class="token punctuation">(</span>signal<span class="token punctuation">[</span>start_sample<span class="token punctuation">:</span>finish_sample<span class="token punctuation">]</span><span class="token punctuation">,</span>                                                 sr<span class="token operator">=</span>sr<span class="token punctuation">,</span>                                                 n_mfcc<span class="token operator">=</span><span class="token number">13</span><span class="token punctuation">,</span>                                                 n_fft<span class="token operator">=</span><span class="token number">2048</span><span class="token punctuation">,</span>                                                 hop_length<span class="token operator">=</span><span class="token number">512</span><span class="token punctuation">)</span>                    mfcc <span class="token operator">=</span> mfcc<span class="token punctuation">.</span>T                                        <span class="token comment"># Store mfcc for segment if it has the expected length</span>                    <span class="token keyword">if</span> <span class="token builtin">len</span><span class="token punctuation">(</span>mfcc<span class="token punctuation">)</span> <span class="token operator">==</span> expected_num_mfcc_vectors_per_segment<span class="token punctuation">:</span>                        data<span class="token punctuation">[</span><span class="token string">"mfcc"</span><span class="token punctuation">]</span><span class="token punctuation">.</span>append<span class="token punctuation">(</span>mfcc<span class="token punctuation">.</span>tolist<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>                        data<span class="token punctuation">[</span><span class="token string">"labels"</span><span class="token punctuation">]</span><span class="token punctuation">.</span>append<span class="token punctuation">(</span>i<span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>                        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"</span><span class="token interpolation"><span class="token punctuation">&#123;</span>file_path<span class="token punctuation">&#125;</span></span><span class="token string"> | segments: </span><span class="token interpolation"><span class="token punctuation">&#123;</span>s<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">&#125;</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>        <span class="token comment"># Save data to a json file</span>    <span class="token keyword">with</span> <span class="token builtin">open</span><span class="token punctuation">(</span>json_path<span class="token punctuation">,</span> <span class="token string">"w"</span><span class="token punctuation">)</span> <span class="token keyword">as</span> fp<span class="token punctuation">:</span>        json<span class="token punctuation">.</span>dump<span class="token punctuation">(</span>data<span class="token punctuation">,</span> fp<span class="token punctuation">,</span> indent<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">load_data</span><span class="token punctuation">(</span>json_path<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">with</span> <span class="token builtin">open</span><span class="token punctuation">(</span>json_path<span class="token punctuation">,</span> <span class="token string">"r"</span><span class="token punctuation">)</span> <span class="token keyword">as</span> fp<span class="token punctuation">:</span>        data <span class="token operator">=</span> json<span class="token punctuation">.</span>load<span class="token punctuation">(</span>fp<span class="token punctuation">)</span>            features <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>data<span class="token punctuation">[</span><span class="token string">"mfcc"</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    targets <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>data<span class="token punctuation">[</span><span class="token string">"labels"</span><span class="token punctuation">]</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> features<span class="token punctuation">,</span> targetssave_mfcc<span class="token punctuation">(</span>DATASETPATH<span class="token punctuation">,</span> JSONPATH<span class="token punctuation">)</span>features<span class="token punctuation">,</span> targets <span class="token operator">=</span> load_data<span class="token punctuation">(</span>JSONPATH<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>Split the data into training dataset and validation dataset.</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>model_selection <span class="token keyword">import</span> train_test_splitX_trainvalid<span class="token punctuation">,</span> X_test<span class="token punctuation">,</span> y_trainvalid<span class="token punctuation">,</span> y_test <span class="token operator">=</span> train_test_split<span class="token punctuation">(</span>features<span class="token punctuation">,</span> targets<span class="token punctuation">,</span> test_size<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">,</span> stratify<span class="token operator">=</span>targets<span class="token punctuation">)</span>X_train<span class="token punctuation">,</span> X_valid<span class="token punctuation">,</span> y_train<span class="token punctuation">,</span> y_valid <span class="token operator">=</span> train_test_split<span class="token punctuation">(</span>X_trainvalid<span class="token punctuation">,</span> y_trainvalid<span class="token punctuation">,</span> test_size<span class="token operator">=</span><span class="token number">0.125</span><span class="token punctuation">,</span> stratify<span class="token operator">=</span>y_trainvalid<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><h1 id="Modeling"><a href="#Modeling" class="headerlink" title="Modeling"></a>Modeling</h1><p>Load the libraries.</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> tez<span class="token keyword">import</span> torch<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn<span class="token keyword">from</span> sklearn <span class="token keyword">import</span> metrics<span class="token punctuation">,</span> model_selection<span class="token punctuation">,</span> preprocessing<span class="token keyword">from</span> tez<span class="token punctuation">.</span>callbacks <span class="token keyword">import</span> EarlyStopping<span class="token keyword">from</span> tez <span class="token keyword">import</span> enums<span class="token keyword">from</span> tez<span class="token punctuation">.</span>utils <span class="token keyword">import</span> AverageMeter<span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">import</span> Dataset<span class="token keyword">from</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">import</span> functional <span class="token keyword">as</span> F<span class="token keyword">from</span> collections <span class="token keyword">import</span> defaultdict<span class="token keyword">from</span> tqdm <span class="token keyword">import</span> tqdm<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>Create <code>Dataset()</code> class from PyTorch.</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">SongGenreDataset</span><span class="token punctuation">(</span>Dataset<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> features<span class="token punctuation">,</span> targets<span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>features <span class="token operator">=</span> features        self<span class="token punctuation">.</span>targets <span class="token operator">=</span> targets    <span class="token keyword">def</span> <span class="token function">__len__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> <span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>features<span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">__getitem__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> idx<span class="token punctuation">)</span><span class="token punctuation">:</span>        target <span class="token operator">=</span> self<span class="token punctuation">.</span>targets<span class="token punctuation">[</span>idx<span class="token punctuation">]</span>        target <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>target<span class="token punctuation">)</span>        target <span class="token operator">=</span> target<span class="token punctuation">.</span>astype<span class="token punctuation">(</span>np<span class="token punctuation">.</span>float64<span class="token punctuation">)</span>        feature <span class="token operator">=</span> self<span class="token punctuation">.</span>features<span class="token punctuation">[</span>idx<span class="token punctuation">]</span>        feature <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span>feature<span class="token punctuation">]</span><span class="token punctuation">)</span>        feature <span class="token operator">=</span> feature<span class="token punctuation">.</span>astype<span class="token punctuation">(</span>np<span class="token punctuation">.</span>float64<span class="token punctuation">)</span>        feature <span class="token operator">=</span> np<span class="token punctuation">.</span>expand_dims<span class="token punctuation">(</span>feature<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>        sample <span class="token operator">=</span> <span class="token punctuation">&#123;</span>            <span class="token string">'feature'</span><span class="token punctuation">:</span> torch<span class="token punctuation">.</span>from_numpy<span class="token punctuation">(</span>feature<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>             <span class="token string">'target'</span><span class="token punctuation">:</span> torch<span class="token punctuation">.</span>from_numpy<span class="token punctuation">(</span>target<span class="token punctuation">)</span><span class="token punctuation">&#125;</span>        <span class="token keyword">return</span> sample<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><pre class="line-numbers language-python" data-language="python"><code class="language-python">train_dataset <span class="token operator">=</span> SongGenreDataset<span class="token punctuation">(</span>X_train<span class="token punctuation">,</span> y_train<span class="token punctuation">)</span>valid_dataset <span class="token operator">=</span> SongGenreDataset<span class="token punctuation">(</span>X_valid<span class="token punctuation">,</span> y_valid<span class="token punctuation">)</span>test_dataset <span class="token operator">=</span> SongGenreDataset<span class="token punctuation">(</span>X_test<span class="token punctuation">,</span> y_test<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>Create a simple MLP model.</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">SongGenreMLPClassifier</span><span class="token punctuation">(</span>tez<span class="token punctuation">.</span>Model<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> num_classes<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>dropout <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span><span class="token number">0.1</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>fc1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">259</span><span class="token operator">*</span><span class="token number">13</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>out <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">,</span> num_classes<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>step_scheduler_after <span class="token operator">=</span> <span class="token string">"epoch"</span>        self<span class="token punctuation">.</span>num_classes <span class="token operator">=</span> num_classes        self<span class="token punctuation">.</span>history <span class="token operator">=</span> defaultdict<span class="token punctuation">(</span><span class="token builtin">list</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">monitor_metrics</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> outputs<span class="token punctuation">,</span> targets<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">if</span> targets <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>            <span class="token keyword">return</span> <span class="token punctuation">&#123;</span><span class="token punctuation">&#125;</span>        outputs <span class="token operator">=</span> torch<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>outputs<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span>        targets <span class="token operator">=</span> targets<span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span>        accuracy <span class="token operator">=</span> metrics<span class="token punctuation">.</span>accuracy_score<span class="token punctuation">(</span>targets<span class="token punctuation">,</span> outputs<span class="token punctuation">)</span>        <span class="token keyword">return</span> <span class="token punctuation">&#123;</span><span class="token string">"accuracy"</span><span class="token punctuation">:</span> accuracy<span class="token punctuation">&#125;</span>    <span class="token keyword">def</span> <span class="token function">fetch_optimizer</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        opt <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>self<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">3e</span><span class="token operator">-</span><span class="token number">4</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> opt    <span class="token keyword">def</span> <span class="token function">fetch_scheduler</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        sch <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>lr_scheduler<span class="token punctuation">.</span>CosineAnnealingWarmRestarts<span class="token punctuation">(</span>            self<span class="token punctuation">.</span>optimizer<span class="token punctuation">,</span> T_0<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">,</span> T_mult<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> eta_min<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">6</span><span class="token punctuation">,</span> last_epoch<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span>        <span class="token punctuation">)</span>        <span class="token keyword">return</span> sch    <span class="token keyword">def</span> <span class="token function">train_one_epoch</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> data_loader<span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>model_state <span class="token operator">=</span> enums<span class="token punctuation">.</span>ModelState<span class="token punctuation">.</span>TRAIN        losses <span class="token operator">=</span> AverageMeter<span class="token punctuation">(</span><span class="token punctuation">)</span>        tk0 <span class="token operator">=</span> tqdm<span class="token punctuation">(</span>data_loader<span class="token punctuation">,</span> total<span class="token operator">=</span><span class="token builtin">len</span><span class="token punctuation">(</span>data_loader<span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token keyword">for</span> b_idx<span class="token punctuation">,</span> data <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>tk0<span class="token punctuation">)</span><span class="token punctuation">:</span>            self<span class="token punctuation">.</span>train_state <span class="token operator">=</span> enums<span class="token punctuation">.</span>TrainingState<span class="token punctuation">.</span>TRAIN_STEP_START            loss<span class="token punctuation">,</span> metrics <span class="token operator">=</span> self<span class="token punctuation">.</span>train_one_step<span class="token punctuation">(</span>data<span class="token punctuation">)</span>            self<span class="token punctuation">.</span>train_state <span class="token operator">=</span> enums<span class="token punctuation">.</span>TrainingState<span class="token punctuation">.</span>TRAIN_STEP_END            losses<span class="token punctuation">.</span>update<span class="token punctuation">(</span>loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> data_loader<span class="token punctuation">.</span>batch_size<span class="token punctuation">)</span>            <span class="token keyword">if</span> b_idx <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>                metrics_meter <span class="token operator">=</span> <span class="token punctuation">&#123;</span>k<span class="token punctuation">:</span> AverageMeter<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">for</span> k <span class="token keyword">in</span> metrics<span class="token punctuation">&#125;</span>            monitor <span class="token operator">=</span> <span class="token punctuation">&#123;</span><span class="token punctuation">&#125;</span>            <span class="token keyword">for</span> m_m <span class="token keyword">in</span> metrics_meter<span class="token punctuation">:</span>                metrics_meter<span class="token punctuation">[</span>m_m<span class="token punctuation">]</span><span class="token punctuation">.</span>update<span class="token punctuation">(</span>metrics<span class="token punctuation">[</span>m_m<span class="token punctuation">]</span><span class="token punctuation">,</span> data_loader<span class="token punctuation">.</span>batch_size<span class="token punctuation">)</span>                monitor<span class="token punctuation">[</span>m_m<span class="token punctuation">]</span> <span class="token operator">=</span> metrics_meter<span class="token punctuation">[</span>m_m<span class="token punctuation">]</span><span class="token punctuation">.</span>avg            self<span class="token punctuation">.</span>current_train_step <span class="token operator">+=</span> <span class="token number">1</span>            tk0<span class="token punctuation">.</span>set_postfix<span class="token punctuation">(</span>loss<span class="token operator">=</span>losses<span class="token punctuation">.</span>avg<span class="token punctuation">,</span> stage<span class="token operator">=</span><span class="token string">"train"</span><span class="token punctuation">,</span> <span class="token operator">**</span>monitor<span class="token punctuation">)</span>        tk0<span class="token punctuation">.</span>close<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>update_metrics<span class="token punctuation">(</span>losses<span class="token operator">=</span>losses<span class="token punctuation">,</span> monitor<span class="token operator">=</span>monitor<span class="token punctuation">)</span>        <span class="token keyword">for</span> k<span class="token punctuation">,</span> v <span class="token keyword">in</span> monitor<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            self<span class="token punctuation">.</span>history<span class="token punctuation">[</span><span class="token string-interpolation"><span class="token string">f"train_</span><span class="token interpolation"><span class="token punctuation">&#123;</span>k<span class="token punctuation">&#125;</span></span><span class="token string">"</span></span><span class="token punctuation">]</span><span class="token punctuation">.</span>append<span class="token punctuation">(</span>v<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>history<span class="token punctuation">[</span><span class="token string">"train_loss"</span><span class="token punctuation">]</span><span class="token punctuation">.</span>append<span class="token punctuation">(</span>losses<span class="token punctuation">.</span>avg<span class="token punctuation">)</span>        <span class="token keyword">return</span> losses<span class="token punctuation">.</span>avg    <span class="token keyword">def</span> <span class="token function">validate_one_epoch</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> data_loader<span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>model_state <span class="token operator">=</span> enums<span class="token punctuation">.</span>ModelState<span class="token punctuation">.</span>VALID        losses <span class="token operator">=</span> AverageMeter<span class="token punctuation">(</span><span class="token punctuation">)</span>        tk0 <span class="token operator">=</span> tqdm<span class="token punctuation">(</span>data_loader<span class="token punctuation">,</span> total<span class="token operator">=</span><span class="token builtin">len</span><span class="token punctuation">(</span>data_loader<span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token keyword">for</span> b_idx<span class="token punctuation">,</span> data <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>tk0<span class="token punctuation">)</span><span class="token punctuation">:</span>            self<span class="token punctuation">.</span>train_state <span class="token operator">=</span> enums<span class="token punctuation">.</span>TrainingState<span class="token punctuation">.</span>VALID_STEP_START            <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>                loss<span class="token punctuation">,</span> metrics <span class="token operator">=</span> self<span class="token punctuation">.</span>validate_one_step<span class="token punctuation">(</span>data<span class="token punctuation">)</span>            self<span class="token punctuation">.</span>train_state <span class="token operator">=</span> enums<span class="token punctuation">.</span>TrainingState<span class="token punctuation">.</span>VALID_STEP_END            losses<span class="token punctuation">.</span>update<span class="token punctuation">(</span>loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> data_loader<span class="token punctuation">.</span>batch_size<span class="token punctuation">)</span>            <span class="token keyword">if</span> b_idx <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>                metrics_meter <span class="token operator">=</span> <span class="token punctuation">&#123;</span>k<span class="token punctuation">:</span> AverageMeter<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">for</span> k <span class="token keyword">in</span> metrics<span class="token punctuation">&#125;</span>            monitor <span class="token operator">=</span> <span class="token punctuation">&#123;</span><span class="token punctuation">&#125;</span>            <span class="token keyword">for</span> m_m <span class="token keyword">in</span> metrics_meter<span class="token punctuation">:</span>                metrics_meter<span class="token punctuation">[</span>m_m<span class="token punctuation">]</span><span class="token punctuation">.</span>update<span class="token punctuation">(</span>metrics<span class="token punctuation">[</span>m_m<span class="token punctuation">]</span><span class="token punctuation">,</span> data_loader<span class="token punctuation">.</span>batch_size<span class="token punctuation">)</span>                monitor<span class="token punctuation">[</span>m_m<span class="token punctuation">]</span> <span class="token operator">=</span> metrics_meter<span class="token punctuation">[</span>m_m<span class="token punctuation">]</span><span class="token punctuation">.</span>avg            tk0<span class="token punctuation">.</span>set_postfix<span class="token punctuation">(</span>loss<span class="token operator">=</span>losses<span class="token punctuation">.</span>avg<span class="token punctuation">,</span> stage<span class="token operator">=</span><span class="token string">"valid"</span><span class="token punctuation">,</span> <span class="token operator">**</span>monitor<span class="token punctuation">)</span>            self<span class="token punctuation">.</span>current_valid_step <span class="token operator">+=</span> <span class="token number">1</span>        tk0<span class="token punctuation">.</span>close<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>update_metrics<span class="token punctuation">(</span>losses<span class="token operator">=</span>losses<span class="token punctuation">,</span> monitor<span class="token operator">=</span>monitor<span class="token punctuation">)</span>        <span class="token keyword">for</span> k<span class="token punctuation">,</span> v <span class="token keyword">in</span> monitor<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            self<span class="token punctuation">.</span>history<span class="token punctuation">[</span><span class="token string-interpolation"><span class="token string">f"valid_</span><span class="token interpolation"><span class="token punctuation">&#123;</span>k<span class="token punctuation">&#125;</span></span><span class="token string">"</span></span><span class="token punctuation">]</span><span class="token punctuation">.</span>append<span class="token punctuation">(</span>v<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>history<span class="token punctuation">[</span><span class="token string">"valid_loss"</span><span class="token punctuation">]</span><span class="token punctuation">.</span>append<span class="token punctuation">(</span>losses<span class="token punctuation">.</span>avg<span class="token punctuation">)</span>        <span class="token keyword">return</span> losses<span class="token punctuation">.</span>avg        <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> feature<span class="token punctuation">,</span> target<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        target <span class="token operator">=</span> target<span class="token punctuation">.</span><span class="token builtin">long</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>self<span class="token punctuation">.</span>device<span class="token punctuation">)</span>        x <span class="token operator">=</span> feature<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>feature<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>        x <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>fc1<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>        outputs <span class="token operator">=</span> self<span class="token punctuation">.</span>out<span class="token punctuation">(</span>self<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>        outputs <span class="token operator">=</span> outputs<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>self<span class="token punctuation">.</span>device<span class="token punctuation">)</span>        <span class="token keyword">if</span> target <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>            loss <span class="token operator">=</span> nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">(</span>outputs<span class="token punctuation">,</span> target<span class="token punctuation">)</span>            metrics <span class="token operator">=</span> self<span class="token punctuation">.</span>monitor_metrics<span class="token punctuation">(</span>outputs<span class="token punctuation">,</span> target<span class="token punctuation">)</span>            <span class="token keyword">return</span> outputs<span class="token punctuation">,</span> loss<span class="token punctuation">,</span> metrics                <span class="token keyword">return</span> outputs<span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token boolean">None</span>        <span class="token keyword">def</span> <span class="token function">score</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> test_dataset<span class="token punctuation">,</span> n_jobs<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">:</span>                <span class="token keyword">def</span> <span class="token function">softmax</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token keyword">return</span> np<span class="token punctuation">.</span>exp<span class="token punctuation">(</span>x<span class="token punctuation">)</span> <span class="token operator">/</span> np<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>np<span class="token punctuation">.</span>exp<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>                prediction <span class="token operator">=</span> self<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>test_dataset<span class="token punctuation">,</span> n_jobs<span class="token operator">=</span>n_jobs<span class="token punctuation">)</span>        prediction <span class="token operator">=</span> <span class="token builtin">list</span><span class="token punctuation">(</span>prediction<span class="token punctuation">)</span>        prediction <span class="token operator">=</span> softmax<span class="token punctuation">(</span>np<span class="token punctuation">.</span>vstack<span class="token punctuation">(</span>prediction<span class="token punctuation">)</span><span class="token punctuation">)</span>        prediction <span class="token operator">=</span> np<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>prediction<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>        groud_truth <span class="token operator">=</span> test_dataset<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">"target"</span><span class="token punctuation">]</span><span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>astype<span class="token punctuation">(</span>np<span class="token punctuation">.</span>int64<span class="token punctuation">)</span>        <span class="token keyword">return</span> metrics<span class="token punctuation">.</span>accuracy_score<span class="token punctuation">(</span>prediction<span class="token punctuation">,</span> groud_truth<span class="token punctuation">)</span>        <span class="token keyword">def</span> <span class="token function">plot_history</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">from</span> matplotlib<span class="token punctuation">.</span>ticker <span class="token keyword">import</span> MaxNLocator                train_loss<span class="token punctuation">,</span> valid_loss <span class="token operator">=</span> self<span class="token punctuation">.</span>history<span class="token punctuation">[</span><span class="token string">"train_loss"</span><span class="token punctuation">]</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>history<span class="token punctuation">[</span><span class="token string">"valid_loss"</span><span class="token punctuation">]</span>        train_accuracy<span class="token punctuation">,</span> valid_accuracy <span class="token operator">=</span> self<span class="token punctuation">.</span>history<span class="token punctuation">[</span><span class="token string">"train_accuracy"</span><span class="token punctuation">]</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>history<span class="token punctuation">[</span><span class="token string">"valid_accuracy"</span><span class="token punctuation">]</span>        plt<span class="token punctuation">.</span>rcParams<span class="token punctuation">[</span><span class="token string">"figure.figsize"</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">15</span><span class="token punctuation">,</span> <span class="token number">6</span>        ax1 <span class="token operator">=</span> plt<span class="token punctuation">.</span>subplot<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>        ax1<span class="token punctuation">.</span>plot<span class="token punctuation">(</span><span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>train_loss<span class="token punctuation">)</span><span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> train_loss<span class="token punctuation">,</span> color<span class="token operator">=</span><span class="token string">"tab:blue"</span><span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">"train"</span><span class="token punctuation">)</span>        ax1<span class="token punctuation">.</span>plot<span class="token punctuation">(</span><span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>valid_loss<span class="token punctuation">)</span><span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> valid_loss<span class="token punctuation">,</span> color<span class="token operator">=</span><span class="token string">"tab:orange"</span><span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">"valid"</span><span class="token punctuation">)</span>        ax1<span class="token punctuation">.</span>xaxis<span class="token punctuation">.</span>set_major_locator<span class="token punctuation">(</span>MaxNLocator<span class="token punctuation">(</span>integer<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        ax1<span class="token punctuation">.</span>title<span class="token punctuation">.</span>set_text<span class="token punctuation">(</span><span class="token string">'Loss'</span><span class="token punctuation">)</span>        ax1<span class="token punctuation">.</span>grid<span class="token punctuation">(</span><span class="token punctuation">)</span>        ax2 <span class="token operator">=</span> plt<span class="token punctuation">.</span>subplot<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>        ax2<span class="token punctuation">.</span>plot<span class="token punctuation">(</span><span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>train_accuracy<span class="token punctuation">)</span><span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> train_accuracy<span class="token punctuation">,</span> color<span class="token operator">=</span><span class="token string">"tab:blue"</span><span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">"train"</span><span class="token punctuation">)</span>        ax2<span class="token punctuation">.</span>plot<span class="token punctuation">(</span><span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>valid_accuracy<span class="token punctuation">)</span><span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> valid_accuracy<span class="token punctuation">,</span> color<span class="token operator">=</span><span class="token string">"tab:orange"</span><span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">"valid"</span><span class="token punctuation">)</span>        ax2<span class="token punctuation">.</span>xaxis<span class="token punctuation">.</span>set_major_locator<span class="token punctuation">(</span>MaxNLocator<span class="token punctuation">(</span>integer<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        ax2<span class="token punctuation">.</span>title<span class="token punctuation">.</span>set_text<span class="token punctuation">(</span><span class="token string">'Accuracy'</span><span class="token punctuation">)</span>        ax2<span class="token punctuation">.</span>grid<span class="token punctuation">(</span><span class="token punctuation">)</span>        plt<span class="token punctuation">.</span>tight_layout<span class="token punctuation">(</span><span class="token punctuation">)</span>        plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>Start training!</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">model <span class="token operator">=</span> SongGenreMLPClassifier<span class="token punctuation">(</span>num_classes<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">)</span>MODEL_PATH <span class="token operator">=</span> <span class="token string">"./models/"</span>MODEL_NAME <span class="token operator">=</span> <span class="token string">"SongGenreMLPClassifier"</span>es <span class="token operator">=</span> EarlyStopping<span class="token punctuation">(</span>    monitor<span class="token operator">=</span><span class="token string">"valid_loss"</span><span class="token punctuation">,</span>    model_path<span class="token operator">=</span>os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>MODEL_PATH<span class="token punctuation">,</span> MODEL_NAME <span class="token operator">+</span> <span class="token string">".bin"</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    patience<span class="token operator">=</span><span class="token number">15</span><span class="token punctuation">,</span>    mode<span class="token operator">=</span><span class="token string">"min"</span><span class="token punctuation">,</span><span class="token punctuation">)</span>model<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>    train_dataset<span class="token punctuation">,</span>    valid_dataset<span class="token operator">=</span>valid_dataset<span class="token punctuation">,</span>    train_bs<span class="token operator">=</span><span class="token number">16</span><span class="token punctuation">,</span>    valid_bs<span class="token operator">=</span><span class="token number">16</span><span class="token punctuation">,</span>    device<span class="token operator">=</span><span class="token string">"cuda"</span><span class="token punctuation">,</span>    epochs<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">,</span>     fp16<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>     callbacks<span class="token operator">=</span><span class="token punctuation">[</span>es<span class="token punctuation">]</span><span class="token punctuation">,</span>     n_jobs<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>model<span class="token punctuation">.</span>plot_history<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><img src="/Hexo-Blog/2021/02/05/2021-02-05-audio-based-song-genre-classification/history.png" class=""><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>In the end, we got a accuracy score of 50.7% on the test dataset. For your information, I only utilise 150 songs, which is not a large amount of data, but it achieves a higher score than most of the lyrics-based models (this <a href="/Hexo-Blog/2019/06/11/2019-06-11-categorising-song-genre-by-analysing-lyrics/" title="article">article</a> I wrote before). In the future, I will investigate more different models to improve the performance for the audio-based classifier! Stay tuned!</p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ol><li><a href="https://github.community/t/is-it-possible-to-open-a-sound-file/10377/2">https://github.community/t/is-it-possible-to-open-a-sound-file/10377/2</a></li><li><a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Element/audio">https://developer.mozilla.org/en-US/docs/Web/HTML/Element/audio</a></li><li><a href="https://www.izotope.com/en/learn/digital-audio-basics-sample-rate-and-bit-depth.html">https://www.izotope.com/en/learn/digital-audio-basics-sample-rate-and-bit-depth.html</a></li><li><a href="https://www.youtube.com/watch?v=daB9naGBVv4&amp;list=RDCMUCZPFjMe1uRSirmSpznqvJfQ&amp;index=6&amp;ab_channel=ValerioVelardo-TheSoundofAI">https://www.youtube.com/watch?v=daB9naGBVv4&amp;list=RDCMUCZPFjMe1uRSirmSpznqvJfQ&amp;index=6&amp;ab_channel=ValerioVelardo-TheSoundofAI</a></li><li><a href="http://www2.egr.uh.edu/~glover/applets/Sampling/Sampling.html">http://www2.egr.uh.edu/~glover/applets/Sampling/Sampling.html</a></li><li><a href="https://www.dewresearch.com/products/163-derivation-of-the-dft">https://www.dewresearch.com/products/163-derivation-of-the-dft</a></li><li><a href="https://teropa.info/harmonics-explorer/">https://teropa.info/harmonics-explorer/</a></li><li><a href="https://towardsdatascience.com/extract-features-of-music-75a3f9bc265d">https://towardsdatascience.com/extract-features-of-music-75a3f9bc265d</a></li><li><a href="https://haythamfayek.com/2016/04/21/speech-processing-for-machine-learning.html">https://haythamfayek.com/2016/04/21/speech-processing-for-machine-learning.html</a></li><li><a href="https://www.kaggle.com/ilyamich/mfcc-implementation-and-tutorial">https://www.kaggle.com/ilyamich/mfcc-implementation-and-tutorial</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Speech </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> Librosa </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Matrix Decomposition</title>
      <link href="/Hexo-Blog/2021/01/29/2021-01-29-matrix-decomposition/"/>
      <url>/Hexo-Blog/2021/01/29/2021-01-29-matrix-decomposition/</url>
      
        <content type="html"><![CDATA[<p>The most important application for decomposition is in data fitting. The following discussion is mostly presented in terms of different methods of decomposition for linear function.</p><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>The method of least squares is a standard approach in regression analysis to approximate the solution of overdetermined systems (sets of equations in which there are more equations than unknowns) by minimizing the sum of the squares of the residuals made in the results of every single equation. An matrix decomposition is a way of reducing a matrix into its constituent parts. It’s an approach that can specify more complex matrix operation that can be performed on the decomposed matrx rather than on the origin matrix itself. There are various matrix decomposition methods, such as LU decomposition, QR decomposition, SVD decomposition, and Cholesky decomposition, etc.</p><h1 id="LU-Decomposition"><a href="#LU-Decomposition" class="headerlink" title="LU Decomposition"></a>LU Decomposition</h1><blockquote><p>Least Square: Let <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.09ex" xmlns="http://www.w3.org/2000/svg" width="10.049ex" height="1.804ex" role="img" focusable="false" viewBox="0 -757.2 4441.8 797.2" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-8-TEX-I-1D44B" d="M42 0H40Q26 0 26 11Q26 15 29 27Q33 41 36 43T55 46Q141 49 190 98Q200 108 306 224T411 342Q302 620 297 625Q288 636 234 637H206Q200 643 200 645T202 664Q206 677 212 683H226Q260 681 347 681Q380 681 408 681T453 682T473 682Q490 682 490 671Q490 670 488 658Q484 643 481 640T465 637Q434 634 411 620L488 426L541 485Q646 598 646 610Q646 628 622 635Q617 635 609 637Q594 637 594 648Q594 650 596 664Q600 677 606 683H618Q619 683 643 683T697 681T738 680Q828 680 837 683H845Q852 676 852 672Q850 647 840 637H824Q790 636 763 628T722 611T698 593L687 584Q687 585 592 480L505 384Q505 383 536 304T601 142T638 56Q648 47 699 46Q734 46 734 37Q734 35 732 23Q728 7 725 4T711 1Q708 1 678 1T589 2Q528 2 496 2T461 1Q444 1 444 10Q444 11 446 25Q448 35 450 39T455 44T464 46T480 47T506 54Q523 62 523 64Q522 64 476 181L429 299Q241 95 236 84Q232 76 232 72Q232 53 261 47Q262 47 267 47T273 46Q276 46 277 46T280 45T283 42T284 35Q284 26 282 19Q279 6 276 4T261 1Q258 1 243 1T201 2T142 2Q64 2 42 0Z"></path><path id="MJX-8-TEX-N-2208" d="M84 250Q84 372 166 450T360 539Q361 539 377 539T419 540T469 540H568Q583 532 583 520Q583 511 570 501L466 500Q355 499 329 494Q280 482 242 458T183 409T147 354T129 306T124 272V270H568Q583 262 583 250T568 230H124V228Q124 207 134 177T167 112T231 48T328 7Q355 1 466 0H570Q583 -10 583 -20Q583 -32 568 -40H471Q464 -40 446 -40T417 -41Q262 -41 172 45Q84 127 84 250Z"></path><path id="MJX-8-TEX-D-211D" d="M17 665Q17 672 28 683H221Q415 681 439 677Q461 673 481 667T516 654T544 639T566 623T584 607T597 592T607 578T614 565T618 554L621 548Q626 530 626 497Q626 447 613 419Q578 348 473 326L455 321Q462 310 473 292T517 226T578 141T637 72T686 35Q705 30 705 16Q705 7 693 -1H510Q503 6 404 159L306 310H268V183Q270 67 271 59Q274 42 291 38Q295 37 319 35Q344 35 353 28Q362 17 353 3L346 -1H28Q16 5 16 16Q16 35 55 35Q96 38 101 52Q106 60 106 341T101 632Q95 645 55 648Q17 648 17 665ZM241 35Q238 42 237 45T235 78T233 163T233 337V621L237 635L244 648H133Q136 641 137 638T139 603T141 517T141 341Q141 131 140 89T134 37Q133 36 133 35H241ZM457 496Q457 540 449 570T425 615T400 634T377 643Q374 643 339 648Q300 648 281 635Q271 628 270 610T268 481V346H284Q327 346 375 352Q421 364 439 392T457 496ZM492 537T492 496T488 427T478 389T469 371T464 361Q464 360 465 360Q469 360 497 370Q593 400 593 495Q593 592 477 630L457 637L461 626Q474 611 488 561Q492 537 492 496ZM464 243Q411 317 410 317Q404 317 401 315Q384 315 370 312H346L526 35H619L606 50Q553 109 464 243Z"></path><path id="MJX-8-TEX-I-1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-8-TEX-N-D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path><path id="MJX-8-TEX-I-1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-8-TEX-I-1D44B"></use></g><g data-mml-node="mo" transform="translate(1129.8, 0)"><use xlink:href="#MJX-8-TEX-N-2208"></use></g><g data-mml-node="msup" transform="translate(2074.6, 0)"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-8-TEX-D-211D"></use></g></g><g data-mml-node="TeXAtom" transform="translate(722, 410.1) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-8-TEX-I-1D45A"></use></g><g data-mml-node="mo" transform="translate(878, 0)"><use xlink:href="#MJX-8-TEX-N-D7"></use></g><g data-mml-node="mi" transform="translate(1656, 0)"><use xlink:href="#MJX-8-TEX-I-1D45B"></use></g></g></g></g></g></svg></mjx-container> with <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.09ex" xmlns="http://www.w3.org/2000/svg" width="6.361ex" height="1.312ex" role="img" focusable="false" viewBox="0 -540 2811.6 580" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-7-TEX-I-1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-7-TEX-N-3E" d="M84 520Q84 528 88 533T96 539L99 540Q106 540 253 471T544 334L687 265Q694 260 694 250T687 235Q685 233 395 96L107 -40H101Q83 -38 83 -20Q83 -19 83 -17Q82 -10 98 -1Q117 9 248 71Q326 108 378 132L626 250L378 368Q90 504 86 509Q84 513 84 520Z"></path><path id="MJX-7-TEX-I-1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-7-TEX-I-1D45A"></use></g><g data-mml-node="mo" transform="translate(1155.8, 0)"><use xlink:href="#MJX-7-TEX-N-3E"></use></g><g data-mml-node="mi" transform="translate(2211.6, 0)"><use xlink:href="#MJX-7-TEX-I-1D45B"></use></g></g></g></svg></mjx-container>, <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.464ex" xmlns="http://www.w3.org/2000/svg" width="9.07ex" height="2.457ex" role="img" focusable="false" viewBox="0 -881 4009.1 1086" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-7-TEX-I-1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-7-TEX-N-2208" d="M84 250Q84 372 166 450T360 539Q361 539 377 539T419 540T469 540H568Q583 532 583 520Q583 511 570 501L466 500Q355 499 329 494Q280 482 242 458T183 409T147 354T129 306T124 272V270H568Q583 262 583 250T568 230H124V228Q124 207 134 177T167 112T231 48T328 7Q355 1 466 0H570Q583 -10 583 -20Q583 -32 568 -40H471Q464 -40 446 -40T417 -41Q262 -41 172 45Q84 127 84 250Z"></path><path id="MJX-7-TEX-D-211D" d="M17 665Q17 672 28 683H221Q415 681 439 677Q461 673 481 667T516 654T544 639T566 623T584 607T597 592T607 578T614 565T618 554L621 548Q626 530 626 497Q626 447 613 419Q578 348 473 326L455 321Q462 310 473 292T517 226T578 141T637 72T686 35Q705 30 705 16Q705 7 693 -1H510Q503 6 404 159L306 310H268V183Q270 67 271 59Q274 42 291 38Q295 37 319 35Q344 35 353 28Q362 17 353 3L346 -1H28Q16 5 16 16Q16 35 55 35Q96 38 101 52Q106 60 106 341T101 632Q95 645 55 648Q17 648 17 665ZM241 35Q238 42 237 45T235 78T233 163T233 337V621L237 635L244 648H133Q136 641 137 638T139 603T141 517T141 341Q141 131 140 89T134 37Q133 36 133 35H241ZM457 496Q457 540 449 570T425 615T400 634T377 643Q374 643 339 648Q300 648 281 635Q271 628 270 610T268 481V346H284Q327 346 375 352Q421 364 439 392T457 496ZM492 537T492 496T488 427T478 389T469 371T464 361Q464 360 465 360Q469 360 497 370Q593 400 593 495Q593 592 477 630L457 637L461 626Q474 611 488 561Q492 537 492 496ZM464 243Q411 317 410 317Q404 317 401 315Q384 315 370 312H346L526 35H619L606 50Q553 109 464 243Z"></path><path id="MJX-7-TEX-I-1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-7-TEX-N-D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path><path id="MJX-7-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-7-TEX-I-1D466"></use></g><g data-mml-node="mo" transform="translate(767.8, 0)"><use xlink:href="#MJX-7-TEX-N-2208"></use></g><g data-mml-node="msup" transform="translate(1712.6, 0)"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-7-TEX-D-211D"></use></g></g><g data-mml-node="TeXAtom" transform="translate(722, 410.1) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-7-TEX-I-1D45A"></use></g><g data-mml-node="mo" transform="translate(878, 0)"><use xlink:href="#MJX-7-TEX-N-D7"></use></g><g data-mml-node="mn" transform="translate(1656, 0)"><use xlink:href="#MJX-7-TEX-N-31"></use></g></g></g></g></g></svg></mjx-container>, and <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.439ex" xmlns="http://www.w3.org/2000/svg" width="8.798ex" height="2.432ex" role="img" focusable="false" viewBox="0 -881 3888.5 1075" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-7-TEX-I-1D6FD" d="M29 -194Q23 -188 23 -186Q23 -183 102 134T186 465Q208 533 243 584T309 658Q365 705 429 705H431Q493 705 533 667T573 570Q573 465 469 396L482 383Q533 332 533 252Q533 139 448 65T257 -10Q227 -10 203 -2T165 17T143 40T131 59T126 65L62 -188Q60 -194 42 -194H29ZM353 431Q392 431 427 419L432 422Q436 426 439 429T449 439T461 453T472 471T484 495T493 524T501 560Q503 569 503 593Q503 611 502 616Q487 667 426 667Q384 667 347 643T286 582T247 514T224 455Q219 439 186 308T152 168Q151 163 151 147Q151 99 173 68Q204 26 260 26Q302 26 349 51T425 137Q441 171 449 214T457 279Q457 337 422 372Q380 358 347 358H337Q258 358 258 389Q258 396 261 403Q275 431 353 431Z"></path><path id="MJX-7-TEX-N-2208" d="M84 250Q84 372 166 450T360 539Q361 539 377 539T419 540T469 540H568Q583 532 583 520Q583 511 570 501L466 500Q355 499 329 494Q280 482 242 458T183 409T147 354T129 306T124 272V270H568Q583 262 583 250T568 230H124V228Q124 207 134 177T167 112T231 48T328 7Q355 1 466 0H570Q583 -10 583 -20Q583 -32 568 -40H471Q464 -40 446 -40T417 -41Q262 -41 172 45Q84 127 84 250Z"></path><path id="MJX-7-TEX-D-211D" d="M17 665Q17 672 28 683H221Q415 681 439 677Q461 673 481 667T516 654T544 639T566 623T584 607T597 592T607 578T614 565T618 554L621 548Q626 530 626 497Q626 447 613 419Q578 348 473 326L455 321Q462 310 473 292T517 226T578 141T637 72T686 35Q705 30 705 16Q705 7 693 -1H510Q503 6 404 159L306 310H268V183Q270 67 271 59Q274 42 291 38Q295 37 319 35Q344 35 353 28Q362 17 353 3L346 -1H28Q16 5 16 16Q16 35 55 35Q96 38 101 52Q106 60 106 341T101 632Q95 645 55 648Q17 648 17 665ZM241 35Q238 42 237 45T235 78T233 163T233 337V621L237 635L244 648H133Q136 641 137 638T139 603T141 517T141 341Q141 131 140 89T134 37Q133 36 133 35H241ZM457 496Q457 540 449 570T425 615T400 634T377 643Q374 643 339 648Q300 648 281 635Q271 628 270 610T268 481V346H284Q327 346 375 352Q421 364 439 392T457 496ZM492 537T492 496T488 427T478 389T469 371T464 361Q464 360 465 360Q469 360 497 370Q593 400 593 495Q593 592 477 630L457 637L461 626Q474 611 488 561Q492 537 492 496ZM464 243Q411 317 410 317Q404 317 401 315Q384 315 370 312H346L526 35H619L606 50Q553 109 464 243Z"></path><path id="MJX-7-TEX-I-1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-7-TEX-N-D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path><path id="MJX-7-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-7-TEX-I-1D6FD"></use></g><g data-mml-node="mo" transform="translate(843.8, 0)"><use xlink:href="#MJX-7-TEX-N-2208"></use></g><g data-mml-node="msup" transform="translate(1788.6, 0)"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-7-TEX-D-211D"></use></g></g><g data-mml-node="TeXAtom" transform="translate(722, 410.1) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-7-TEX-I-1D45B"></use></g><g data-mml-node="mo" transform="translate(600, 0)"><use xlink:href="#MJX-7-TEX-N-D7"></use></g><g data-mml-node="mn" transform="translate(1378, 0)"><use xlink:href="#MJX-7-TEX-N-31"></use></g></g></g></g></g></svg></mjx-container>. We aim to solve <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.464ex" xmlns="http://www.w3.org/2000/svg" width="7.334ex" height="2.059ex" role="img" focusable="false" viewBox="0 -705 3241.6 910" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-7-TEX-I-1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-7-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-7-TEX-I-1D44B" d="M42 0H40Q26 0 26 11Q26 15 29 27Q33 41 36 43T55 46Q141 49 190 98Q200 108 306 224T411 342Q302 620 297 625Q288 636 234 637H206Q200 643 200 645T202 664Q206 677 212 683H226Q260 681 347 681Q380 681 408 681T453 682T473 682Q490 682 490 671Q490 670 488 658Q484 643 481 640T465 637Q434 634 411 620L488 426L541 485Q646 598 646 610Q646 628 622 635Q617 635 609 637Q594 637 594 648Q594 650 596 664Q600 677 606 683H618Q619 683 643 683T697 681T738 680Q828 680 837 683H845Q852 676 852 672Q850 647 840 637H824Q790 636 763 628T722 611T698 593L687 584Q687 585 592 480L505 384Q505 383 536 304T601 142T638 56Q648 47 699 46Q734 46 734 37Q734 35 732 23Q728 7 725 4T711 1Q708 1 678 1T589 2Q528 2 496 2T461 1Q444 1 444 10Q444 11 446 25Q448 35 450 39T455 44T464 46T480 47T506 54Q523 62 523 64Q522 64 476 181L429 299Q241 95 236 84Q232 76 232 72Q232 53 261 47Q262 47 267 47T273 46Q276 46 277 46T280 45T283 42T284 35Q284 26 282 19Q279 6 276 4T261 1Q258 1 243 1T201 2T142 2Q64 2 42 0Z"></path><path id="MJX-7-TEX-I-1D6FD" d="M29 -194Q23 -188 23 -186Q23 -183 102 134T186 465Q208 533 243 584T309 658Q365 705 429 705H431Q493 705 533 667T573 570Q573 465 469 396L482 383Q533 332 533 252Q533 139 448 65T257 -10Q227 -10 203 -2T165 17T143 40T131 59T126 65L62 -188Q60 -194 42 -194H29ZM353 431Q392 431 427 419L432 422Q436 426 439 429T449 439T461 453T472 471T484 495T493 524T501 560Q503 569 503 593Q503 611 502 616Q487 667 426 667Q384 667 347 643T286 582T247 514T224 455Q219 439 186 308T152 168Q151 163 151 147Q151 99 173 68Q204 26 260 26Q302 26 349 51T425 137Q441 171 449 214T457 279Q457 337 422 372Q380 358 347 358H337Q258 358 258 389Q258 396 261 403Q275 431 353 431Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-7-TEX-I-1D466"></use></g><g data-mml-node="mo" transform="translate(767.8, 0)"><use xlink:href="#MJX-7-TEX-N-3D"></use></g><g data-mml-node="mi" transform="translate(1823.6, 0)"><use xlink:href="#MJX-7-TEX-I-1D44B"></use></g><g data-mml-node="mi" transform="translate(2675.6, 0)"><use xlink:href="#MJX-7-TEX-I-1D6FD"></use></g></g></g></svg></mjx-container> where <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.439ex" xmlns="http://www.w3.org/2000/svg" width="1.394ex" height="2.765ex" role="img" focusable="false" viewBox="0 -1028 616.3 1222" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-7-TEX-I-1D6FD" d="M29 -194Q23 -188 23 -186Q23 -183 102 134T186 465Q208 533 243 584T309 658Q365 705 429 705H431Q493 705 533 667T573 570Q573 465 469 396L482 383Q533 332 533 252Q533 139 448 65T257 -10Q227 -10 203 -2T165 17T143 40T131 59T126 65L62 -188Q60 -194 42 -194H29ZM353 431Q392 431 427 419L432 422Q436 426 439 429T449 439T461 453T472 471T484 495T493 524T501 560Q503 569 503 593Q503 611 502 616Q487 667 426 667Q384 667 347 643T286 582T247 514T224 455Q219 439 186 308T152 168Q151 163 151 147Q151 99 173 68Q204 26 260 26Q302 26 349 51T425 137Q441 171 449 214T457 279Q457 337 422 372Q380 358 347 358H337Q258 358 258 389Q258 396 261 403Q275 431 353 431Z"></path><path id="MJX-7-TEX-N-5E" d="M112 560L249 694L257 686Q387 562 387 560L361 531Q359 532 303 581L250 627L195 580Q182 569 169 557T148 538L140 532Q138 530 125 546L112 560Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mover"><g data-mml-node="mi"><use xlink:href="#MJX-7-TEX-I-1D6FD"></use></g><g data-mml-node="mo" transform="translate(116.3, 234)"><use xlink:href="#MJX-7-TEX-N-5E"></use></g></g></g></g></g></svg></mjx-container> is the least square estimator. The least squares solution for <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="17.931ex" height="2.891ex" role="img" focusable="false" viewBox="0 -1028 7925.5 1278" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-7-TEX-I-1D6FD" d="M29 -194Q23 -188 23 -186Q23 -183 102 134T186 465Q208 533 243 584T309 658Q365 705 429 705H431Q493 705 533 667T573 570Q573 465 469 396L482 383Q533 332 533 252Q533 139 448 65T257 -10Q227 -10 203 -2T165 17T143 40T131 59T126 65L62 -188Q60 -194 42 -194H29ZM353 431Q392 431 427 419L432 422Q436 426 439 429T449 439T461 453T472 471T484 495T493 524T501 560Q503 569 503 593Q503 611 502 616Q487 667 426 667Q384 667 347 643T286 582T247 514T224 455Q219 439 186 308T152 168Q151 163 151 147Q151 99 173 68Q204 26 260 26Q302 26 349 51T425 137Q441 171 449 214T457 279Q457 337 422 372Q380 358 347 358H337Q258 358 258 389Q258 396 261 403Q275 431 353 431Z"></path><path id="MJX-7-TEX-N-5E" d="M112 560L249 694L257 686Q387 562 387 560L361 531Q359 532 303 581L250 627L195 580Q182 569 169 557T148 538L140 532Q138 530 125 546L112 560Z"></path><path id="MJX-7-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-7-TEX-N-28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path><path id="MJX-7-TEX-I-1D44B" d="M42 0H40Q26 0 26 11Q26 15 29 27Q33 41 36 43T55 46Q141 49 190 98Q200 108 306 224T411 342Q302 620 297 625Q288 636 234 637H206Q200 643 200 645T202 664Q206 677 212 683H226Q260 681 347 681Q380 681 408 681T453 682T473 682Q490 682 490 671Q490 670 488 658Q484 643 481 640T465 637Q434 634 411 620L488 426L541 485Q646 598 646 610Q646 628 622 635Q617 635 609 637Q594 637 594 648Q594 650 596 664Q600 677 606 683H618Q619 683 643 683T697 681T738 680Q828 680 837 683H845Q852 676 852 672Q850 647 840 637H824Q790 636 763 628T722 611T698 593L687 584Q687 585 592 480L505 384Q505 383 536 304T601 142T638 56Q648 47 699 46Q734 46 734 37Q734 35 732 23Q728 7 725 4T711 1Q708 1 678 1T589 2Q528 2 496 2T461 1Q444 1 444 10Q444 11 446 25Q448 35 450 39T455 44T464 46T480 47T506 54Q523 62 523 64Q522 64 476 181L429 299Q241 95 236 84Q232 76 232 72Q232 53 261 47Q262 47 267 47T273 46Q276 46 277 46T280 45T283 42T284 35Q284 26 282 19Q279 6 276 4T261 1Q258 1 243 1T201 2T142 2Q64 2 42 0Z"></path><path id="MJX-7-TEX-I-1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path><path id="MJX-7-TEX-N-29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path><path id="MJX-7-TEX-N-2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path><path id="MJX-7-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-7-TEX-I-1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mover"><g data-mml-node="mi"><use xlink:href="#MJX-7-TEX-I-1D6FD"></use></g><g data-mml-node="mo" transform="translate(116.3, 234)"><use xlink:href="#MJX-7-TEX-N-5E"></use></g></g></g><g data-mml-node="mo" transform="translate(894.1, 0)"><use xlink:href="#MJX-7-TEX-N-3D"></use></g><g data-mml-node="mo" transform="translate(1949.9, 0)"><use xlink:href="#MJX-7-TEX-N-28"></use></g><g data-mml-node="msup" transform="translate(2338.9, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-7-TEX-I-1D44B"></use></g><g data-mml-node="TeXAtom" transform="translate(903.2, 363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-7-TEX-I-1D447"></use></g></g></g><g data-mml-node="mi" transform="translate(3789.9, 0)"><use xlink:href="#MJX-7-TEX-I-1D44B"></use></g><g data-mml-node="msup" transform="translate(4641.9, 0)"><g data-mml-node="mo"><use xlink:href="#MJX-7-TEX-N-29"></use></g><g data-mml-node="TeXAtom" transform="translate(389, 363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mo"><use xlink:href="#MJX-7-TEX-N-2212"></use></g><g data-mml-node="mn" transform="translate(778, 0)"><use xlink:href="#MJX-7-TEX-N-31"></use></g></g></g><g data-mml-node="msup" transform="translate(5984.5, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-7-TEX-I-1D44B"></use></g><g data-mml-node="TeXAtom" transform="translate(903.2, 363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-7-TEX-I-1D447"></use></g></g></g><g data-mml-node="mi" transform="translate(7435.5, 0)"><use xlink:href="#MJX-7-TEX-I-1D466"></use></g></g></g></svg></mjx-container> can obtained using different decomposition methods on <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0" xmlns="http://www.w3.org/2000/svg" width="5.21ex" height="1.904ex" role="img" focusable="false" viewBox="0 -841.7 2303 841.7" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-7-TEX-I-1D44B" d="M42 0H40Q26 0 26 11Q26 15 29 27Q33 41 36 43T55 46Q141 49 190 98Q200 108 306 224T411 342Q302 620 297 625Q288 636 234 637H206Q200 643 200 645T202 664Q206 677 212 683H226Q260 681 347 681Q380 681 408 681T453 682T473 682Q490 682 490 671Q490 670 488 658Q484 643 481 640T465 637Q434 634 411 620L488 426L541 485Q646 598 646 610Q646 628 622 635Q617 635 609 637Q594 637 594 648Q594 650 596 664Q600 677 606 683H618Q619 683 643 683T697 681T738 680Q828 680 837 683H845Q852 676 852 672Q850 647 840 637H824Q790 636 763 628T722 611T698 593L687 584Q687 585 592 480L505 384Q505 383 536 304T601 142T638 56Q648 47 699 46Q734 46 734 37Q734 35 732 23Q728 7 725 4T711 1Q708 1 678 1T589 2Q528 2 496 2T461 1Q444 1 444 10Q444 11 446 25Q448 35 450 39T455 44T464 46T480 47T506 54Q523 62 523 64Q522 64 476 181L429 299Q241 95 236 84Q232 76 232 72Q232 53 261 47Q262 47 267 47T273 46Q276 46 277 46T280 45T283 42T284 35Q284 26 282 19Q279 6 276 4T261 1Q258 1 243 1T201 2T142 2Q64 2 42 0Z"></path><path id="MJX-7-TEX-I-1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"><use xlink:href="#MJX-7-TEX-I-1D44B"></use></g><g data-mml-node="TeXAtom" transform="translate(903.2, 363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-7-TEX-I-1D447"></use></g></g></g><g data-mml-node="mi" transform="translate(1451, 0)"><use xlink:href="#MJX-7-TEX-I-1D44B"></use></g></g></g></svg></mjx-container>.</p></blockquote><p>When using LU, we have <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="32.533ex" height="2.891ex" role="img" focusable="false" viewBox="0 -1028 14379.8 1278" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-7-TEX-I-1D6FD" d="M29 -194Q23 -188 23 -186Q23 -183 102 134T186 465Q208 533 243 584T309 658Q365 705 429 705H431Q493 705 533 667T573 570Q573 465 469 396L482 383Q533 332 533 252Q533 139 448 65T257 -10Q227 -10 203 -2T165 17T143 40T131 59T126 65L62 -188Q60 -194 42 -194H29ZM353 431Q392 431 427 419L432 422Q436 426 439 429T449 439T461 453T472 471T484 495T493 524T501 560Q503 569 503 593Q503 611 502 616Q487 667 426 667Q384 667 347 643T286 582T247 514T224 455Q219 439 186 308T152 168Q151 163 151 147Q151 99 173 68Q204 26 260 26Q302 26 349 51T425 137Q441 171 449 214T457 279Q457 337 422 372Q380 358 347 358H337Q258 358 258 389Q258 396 261 403Q275 431 353 431Z"></path><path id="MJX-7-TEX-N-5E" d="M112 560L249 694L257 686Q387 562 387 560L361 531Q359 532 303 581L250 627L195 580Q182 569 169 557T148 538L140 532Q138 530 125 546L112 560Z"></path><path id="MJX-7-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-7-TEX-N-28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path><path id="MJX-7-TEX-I-1D44B" d="M42 0H40Q26 0 26 11Q26 15 29 27Q33 41 36 43T55 46Q141 49 190 98Q200 108 306 224T411 342Q302 620 297 625Q288 636 234 637H206Q200 643 200 645T202 664Q206 677 212 683H226Q260 681 347 681Q380 681 408 681T453 682T473 682Q490 682 490 671Q490 670 488 658Q484 643 481 640T465 637Q434 634 411 620L488 426L541 485Q646 598 646 610Q646 628 622 635Q617 635 609 637Q594 637 594 648Q594 650 596 664Q600 677 606 683H618Q619 683 643 683T697 681T738 680Q828 680 837 683H845Q852 676 852 672Q850 647 840 637H824Q790 636 763 628T722 611T698 593L687 584Q687 585 592 480L505 384Q505 383 536 304T601 142T638 56Q648 47 699 46Q734 46 734 37Q734 35 732 23Q728 7 725 4T711 1Q708 1 678 1T589 2Q528 2 496 2T461 1Q444 1 444 10Q444 11 446 25Q448 35 450 39T455 44T464 46T480 47T506 54Q523 62 523 64Q522 64 476 181L429 299Q241 95 236 84Q232 76 232 72Q232 53 261 47Q262 47 267 47T273 46Q276 46 277 46T280 45T283 42T284 35Q284 26 282 19Q279 6 276 4T261 1Q258 1 243 1T201 2T142 2Q64 2 42 0Z"></path><path id="MJX-7-TEX-I-1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path><path id="MJX-7-TEX-N-29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path><path id="MJX-7-TEX-N-2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path><path id="MJX-7-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-7-TEX-I-1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-7-TEX-I-1D43F" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path><path id="MJX-7-TEX-I-1D448" d="M107 637Q73 637 71 641Q70 643 70 649Q70 673 81 682Q83 683 98 683Q139 681 234 681Q268 681 297 681T342 682T362 682Q378 682 378 672Q378 670 376 658Q371 641 366 638H364Q362 638 359 638T352 638T343 637T334 637Q295 636 284 634T266 623Q265 621 238 518T184 302T154 169Q152 155 152 140Q152 86 183 55T269 24Q336 24 403 69T501 205L552 406Q599 598 599 606Q599 633 535 637Q511 637 511 648Q511 650 513 660Q517 676 519 679T529 683Q532 683 561 682T645 680Q696 680 723 681T752 682Q767 682 767 672Q767 650 759 642Q756 637 737 637Q666 633 648 597Q646 592 598 404Q557 235 548 205Q515 105 433 42T263 -22Q171 -22 116 34T60 167V183Q60 201 115 421Q164 622 164 628Q164 635 107 637Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mover"><g data-mml-node="mi"><use xlink:href="#MJX-7-TEX-I-1D6FD"></use></g><g data-mml-node="mo" transform="translate(116.3, 234)"><use xlink:href="#MJX-7-TEX-N-5E"></use></g></g></g><g data-mml-node="mo" transform="translate(894.1, 0)"><use xlink:href="#MJX-7-TEX-N-3D"></use></g><g data-mml-node="mo" transform="translate(1949.9, 0)"><use xlink:href="#MJX-7-TEX-N-28"></use></g><g data-mml-node="msup" transform="translate(2338.9, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-7-TEX-I-1D44B"></use></g><g data-mml-node="TeXAtom" transform="translate(903.2, 363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-7-TEX-I-1D447"></use></g></g></g><g data-mml-node="mi" transform="translate(3789.9, 0)"><use xlink:href="#MJX-7-TEX-I-1D44B"></use></g><g data-mml-node="msup" transform="translate(4641.9, 0)"><g data-mml-node="mo"><use xlink:href="#MJX-7-TEX-N-29"></use></g><g data-mml-node="TeXAtom" transform="translate(389, 363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mo"><use xlink:href="#MJX-7-TEX-N-2212"></use></g><g data-mml-node="mn" transform="translate(778, 0)"><use xlink:href="#MJX-7-TEX-N-31"></use></g></g></g><g data-mml-node="msup" transform="translate(5984.5, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-7-TEX-I-1D44B"></use></g><g data-mml-node="TeXAtom" transform="translate(903.2, 363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-7-TEX-I-1D447"></use></g></g></g><g data-mml-node="mi" transform="translate(7435.5, 0)"><use xlink:href="#MJX-7-TEX-I-1D466"></use></g><g data-mml-node="mo" transform="translate(8203.3, 0)"><use xlink:href="#MJX-7-TEX-N-3D"></use></g><g data-mml-node="mo" transform="translate(9259.1, 0)"><use xlink:href="#MJX-7-TEX-N-28"></use></g><g data-mml-node="mi" transform="translate(9648.1, 0)"><use xlink:href="#MJX-7-TEX-I-1D43F"></use></g><g data-mml-node="mi" transform="translate(10329.1, 0)"><use xlink:href="#MJX-7-TEX-I-1D448"></use></g><g data-mml-node="msup" transform="translate(11096.1, 0)"><g data-mml-node="mo"><use xlink:href="#MJX-7-TEX-N-29"></use></g><g data-mml-node="TeXAtom" transform="translate(389, 363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mo"><use xlink:href="#MJX-7-TEX-N-2212"></use></g><g data-mml-node="mn" transform="translate(778, 0)"><use xlink:href="#MJX-7-TEX-N-31"></use></g></g></g><g data-mml-node="msup" transform="translate(12438.8, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-7-TEX-I-1D44B"></use></g><g data-mml-node="TeXAtom" transform="translate(903.2, 363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-7-TEX-I-1D447"></use></g></g></g><g data-mml-node="mi" transform="translate(13889.8, 0)"><use xlink:href="#MJX-7-TEX-I-1D466"></use></g></g></g></svg></mjx-container>, decomposing the square matrix <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0" xmlns="http://www.w3.org/2000/svg" width="5.21ex" height="1.904ex" role="img" focusable="false" viewBox="0 -841.7 2303 841.7" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-7-TEX-I-1D44B" d="M42 0H40Q26 0 26 11Q26 15 29 27Q33 41 36 43T55 46Q141 49 190 98Q200 108 306 224T411 342Q302 620 297 625Q288 636 234 637H206Q200 643 200 645T202 664Q206 677 212 683H226Q260 681 347 681Q380 681 408 681T453 682T473 682Q490 682 490 671Q490 670 488 658Q484 643 481 640T465 637Q434 634 411 620L488 426L541 485Q646 598 646 610Q646 628 622 635Q617 635 609 637Q594 637 594 648Q594 650 596 664Q600 677 606 683H618Q619 683 643 683T697 681T738 680Q828 680 837 683H845Q852 676 852 672Q850 647 840 637H824Q790 636 763 628T722 611T698 593L687 584Q687 585 592 480L505 384Q505 383 536 304T601 142T638 56Q648 47 699 46Q734 46 734 37Q734 35 732 23Q728 7 725 4T711 1Q708 1 678 1T589 2Q528 2 496 2T461 1Q444 1 444 10Q444 11 446 25Q448 35 450 39T455 44T464 46T480 47T506 54Q523 62 523 64Q522 64 476 181L429 299Q241 95 236 84Q232 76 232 72Q232 53 261 47Q262 47 267 47T273 46Q276 46 277 46T280 45T283 42T284 35Q284 26 282 19Q279 6 276 4T261 1Q258 1 243 1T201 2T142 2Q64 2 42 0Z"></path><path id="MJX-7-TEX-I-1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"><use xlink:href="#MJX-7-TEX-I-1D44B"></use></g><g data-mml-node="TeXAtom" transform="translate(903.2, 363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-7-TEX-I-1D447"></use></g></g></g><g data-mml-node="mi" transform="translate(1451, 0)"><use xlink:href="#MJX-7-TEX-I-1D44B"></use></g></g></g></svg></mjx-container> into <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0" xmlns="http://www.w3.org/2000/svg" width="1.541ex" height="1.545ex" role="img" focusable="false" viewBox="0 -683 681 683" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-7-TEX-I-1D43F" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-7-TEX-I-1D43F"></use></g></g></g></svg></mjx-container> and <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex" xmlns="http://www.w3.org/2000/svg" width="1.735ex" height="1.595ex" role="img" focusable="false" viewBox="0 -683 767 705" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-7-TEX-I-1D448" d="M107 637Q73 637 71 641Q70 643 70 649Q70 673 81 682Q83 683 98 683Q139 681 234 681Q268 681 297 681T342 682T362 682Q378 682 378 672Q378 670 376 658Q371 641 366 638H364Q362 638 359 638T352 638T343 637T334 637Q295 636 284 634T266 623Q265 621 238 518T184 302T154 169Q152 155 152 140Q152 86 183 55T269 24Q336 24 403 69T501 205L552 406Q599 598 599 606Q599 633 535 637Q511 637 511 648Q511 650 513 660Q517 676 519 679T529 683Q532 683 561 682T645 680Q696 680 723 681T752 682Q767 682 767 672Q767 650 759 642Q756 637 737 637Q666 633 648 597Q646 592 598 404Q557 235 548 205Q515 105 433 42T263 -22Q171 -22 116 34T60 167V183Q60 201 115 421Q164 622 164 628Q164 635 107 637Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-7-TEX-I-1D448"></use></g></g></g></svg></mjx-container> components. The factors <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0" xmlns="http://www.w3.org/2000/svg" width="1.541ex" height="1.545ex" role="img" focusable="false" viewBox="0 -683 681 683" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-7-TEX-I-1D43F" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-7-TEX-I-1D43F"></use></g></g></g></svg></mjx-container> and <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex" xmlns="http://www.w3.org/2000/svg" width="1.735ex" height="1.595ex" role="img" focusable="false" viewBox="0 -683 767 705" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-7-TEX-I-1D448" d="M107 637Q73 637 71 641Q70 643 70 649Q70 673 81 682Q83 683 98 683Q139 681 234 681Q268 681 297 681T342 682T362 682Q378 682 378 672Q378 670 376 658Q371 641 366 638H364Q362 638 359 638T352 638T343 637T334 637Q295 636 284 634T266 623Q265 621 238 518T184 302T154 169Q152 155 152 140Q152 86 183 55T269 24Q336 24 403 69T501 205L552 406Q599 598 599 606Q599 633 535 637Q511 637 511 648Q511 650 513 660Q517 676 519 679T529 683Q532 683 561 682T645 680Q696 680 723 681T752 682Q767 682 767 672Q767 650 759 642Q756 637 737 637Q666 633 648 597Q646 592 598 404Q557 235 548 205Q515 105 433 42T263 -22Q171 -22 116 34T60 167V183Q60 201 115 421Q164 622 164 628Q164 635 107 637Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-7-TEX-I-1D448"></use></g></g></g></svg></mjx-container> are triangular matrices. A variation of this decomposition that is numerically more stable to solve in practice is called the PLU decomposition, or the LU decomposition with partial pivoting, where <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0" xmlns="http://www.w3.org/2000/svg" width="1.699ex" height="1.545ex" role="img" focusable="false" viewBox="0 -683 751 683" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-7-TEX-I-1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-7-TEX-I-1D443"></use></g></g></g></svg></mjx-container> is a so-called permutation matrix, <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0" xmlns="http://www.w3.org/2000/svg" width="1.541ex" height="1.545ex" role="img" focusable="false" viewBox="0 -683 681 683" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-7-TEX-I-1D43F" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-7-TEX-I-1D43F"></use></g></g></g></svg></mjx-container> is lower triangular, and <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex" xmlns="http://www.w3.org/2000/svg" width="1.735ex" height="1.595ex" role="img" focusable="false" viewBox="0 -683 767 705" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-7-TEX-I-1D448" d="M107 637Q73 637 71 641Q70 643 70 649Q70 673 81 682Q83 683 98 683Q139 681 234 681Q268 681 297 681T342 682T362 682Q378 682 378 672Q378 670 376 658Q371 641 366 638H364Q362 638 359 638T352 638T343 637T334 637Q295 636 284 634T266 623Q265 621 238 518T184 302T154 169Q152 155 152 140Q152 86 183 55T269 24Q336 24 403 69T501 205L552 406Q599 598 599 606Q599 633 535 637Q511 637 511 648Q511 650 513 660Q517 676 519 679T529 683Q532 683 561 682T645 680Q696 680 723 681T752 682Q767 682 767 672Q767 650 759 642Q756 637 737 637Q666 633 648 597Q646 592 598 404Q557 235 548 205Q515 105 433 42T263 -22Q171 -22 116 34T60 167V183Q60 201 115 421Q164 622 164 628Q164 635 107 637Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-7-TEX-I-1D448"></use></g></g></g></svg></mjx-container> is upper triangular. Lower and upper triangular matrices are computationally easier than your typical invertible matrix. The matrix P is easy to deal with as well since it is mostly full of zeros.  This <a href="https://www.youtube.com/watch?v=UlWcofkUDDU&ab_channel=Mathispower4u">video</a> explains how to find the LU decomposition of a square matrix using a shortcut involving the opposite of multipliers used when performing row operations. There is also another <a href="https://math.unm.edu/~loring/links/linear_s08/LU.pdf">posting</a> describe LU and PLU factorisation with some examples.</p><h2 id="Pseudocode"><a href="#Pseudocode" class="headerlink" title="Pseudocode"></a>Pseudocode</h2><p><strong>Step 1.</strong> Start with three candidate matrices:</p><ul><li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.186ex" xmlns="http://www.w3.org/2000/svg" width="7.13ex" height="1.731ex" role="img" focusable="false" viewBox="0 -683 3151.6 765" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-7-TEX-I-1D448" d="M107 637Q73 637 71 641Q70 643 70 649Q70 673 81 682Q83 683 98 683Q139 681 234 681Q268 681 297 681T342 682T362 682Q378 682 378 672Q378 670 376 658Q371 641 366 638H364Q362 638 359 638T352 638T343 637T334 637Q295 636 284 634T266 623Q265 621 238 518T184 302T154 169Q152 155 152 140Q152 86 183 55T269 24Q336 24 403 69T501 205L552 406Q599 598 599 606Q599 633 535 637Q511 637 511 648Q511 650 513 660Q517 676 519 679T529 683Q532 683 561 682T645 680Q696 680 723 681T752 682Q767 682 767 672Q767 650 759 642Q756 637 737 637Q666 633 648 597Q646 592 598 404Q557 235 548 205Q515 105 433 42T263 -22Q171 -22 116 34T60 167V183Q60 201 115 421Q164 622 164 628Q164 635 107 637Z"></path><path id="MJX-7-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-7-TEX-I-1D440" d="M289 629Q289 635 232 637Q208 637 201 638T194 648Q194 649 196 659Q197 662 198 666T199 671T201 676T203 679T207 681T212 683T220 683T232 684Q238 684 262 684T307 683Q386 683 398 683T414 678Q415 674 451 396L487 117L510 154Q534 190 574 254T662 394Q837 673 839 675Q840 676 842 678T846 681L852 683H948Q965 683 988 683T1017 684Q1051 684 1051 673Q1051 668 1048 656T1045 643Q1041 637 1008 637Q968 636 957 634T939 623Q936 618 867 340T797 59Q797 55 798 54T805 50T822 48T855 46H886Q892 37 892 35Q892 19 885 5Q880 0 869 0Q864 0 828 1T736 2Q675 2 644 2T609 1Q592 1 592 11Q592 13 594 25Q598 41 602 43T625 46Q652 46 685 49Q699 52 704 61Q706 65 742 207T813 490T848 631L654 322Q458 10 453 5Q451 4 449 3Q444 0 433 0Q418 0 415 7Q413 11 374 317L335 624L267 354Q200 88 200 79Q206 46 272 46H282Q288 41 289 37T286 19Q282 3 278 1Q274 0 267 0Q265 0 255 0T221 1T157 2Q127 2 95 1T58 0Q43 0 39 2T35 11Q35 13 38 25T43 40Q45 46 65 46Q135 46 154 86Q158 92 223 354T289 629Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-7-TEX-I-1D448"></use></g><g data-mml-node="mo" transform="translate(1044.8, 0)"><use xlink:href="#MJX-7-TEX-N-3D"></use></g><g data-mml-node="mi" transform="translate(2100.6, 0)"><use xlink:href="#MJX-7-TEX-I-1D440"></use></g></g></g></svg></mjx-container></li><li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.65ex" xmlns="http://www.w3.org/2000/svg" width="8.167ex" height="2.195ex" role="img" focusable="false" viewBox="0 -683 3609.7 970.2" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-7-TEX-I-1D43F" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path><path id="MJX-7-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-7-TEX-N-30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path><path id="MJX-7-TEX-I-1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-7-TEX-N-2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-7-TEX-I-1D43F"></use></g><g data-mml-node="mo" transform="translate(958.8, 0)"><use xlink:href="#MJX-7-TEX-N-3D"></use></g><g data-mml-node="msub" transform="translate(2014.6, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-7-TEX-N-30"></use></g><g data-mml-node="TeXAtom" transform="translate(500, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-7-TEX-I-1D45B"></use></g><g data-mml-node="mo" transform="translate(600, 0)"><use xlink:href="#MJX-7-TEX-N-2C"></use></g><g data-mml-node="mi" transform="translate(878, 0)"><use xlink:href="#MJX-7-TEX-I-1D45B"></use></g></g></g></g></g></svg></mjx-container></li><li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex" xmlns="http://www.w3.org/2000/svg" width="8.149ex" height="2.261ex" role="img" focusable="false" viewBox="0 -841.7 3602.1 999.5" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-7-TEX-I-1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path><path id="MJX-7-TEX-I-1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path><path id="MJX-7-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-7-TEX-I-1D43C" d="M43 1Q26 1 26 10Q26 12 29 24Q34 43 39 45Q42 46 54 46H60Q120 46 136 53Q137 53 138 54Q143 56 149 77T198 273Q210 318 216 344Q286 624 286 626Q284 630 284 631Q274 637 213 637H193Q184 643 189 662Q193 677 195 680T209 683H213Q285 681 359 681Q481 681 487 683H497Q504 676 504 672T501 655T494 639Q491 637 471 637Q440 637 407 634Q393 631 388 623Q381 609 337 432Q326 385 315 341Q245 65 245 59Q245 52 255 50T307 46H339Q345 38 345 37T342 19Q338 6 332 0H316Q279 2 179 2Q143 2 113 2T65 2T43 1Z"></path><path id="MJX-7-TEX-I-1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"><use xlink:href="#MJX-7-TEX-I-1D443"></use></g><g data-mml-node="TeXAtom" transform="translate(806.5, 363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-7-TEX-I-1D447"></use></g></g></g><g data-mml-node="mo" transform="translate(1632, 0)"><use xlink:href="#MJX-7-TEX-N-3D"></use></g><g data-mml-node="msub" transform="translate(2687.8, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-7-TEX-I-1D43C"></use></g><g data-mml-node="TeXAtom" transform="translate(440, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-7-TEX-I-1D45B"></use></g></g></g></g></g></svg></mjx-container></li></ul><p>where <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0" xmlns="http://www.w3.org/2000/svg" width="1.541ex" height="1.545ex" role="img" focusable="false" viewBox="0 -683 681 683" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-7-TEX-I-1D43F" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-7-TEX-I-1D43F"></use></g></g></g></svg></mjx-container> is a <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="5.481ex" height="1.136ex" role="img" focusable="false" viewBox="0 -491 2422.4 502" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-5-TEX-I-1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-5-TEX-N-D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-5-TEX-I-1D45B"></use></g><g data-mml-node="mo" transform="translate(822.2, 0)"><use xlink:href="#MJX-5-TEX-N-D7"></use></g><g data-mml-node="mi" transform="translate(1822.4, 0)"><use xlink:href="#MJX-5-TEX-I-1D45B"></use></g></g></g></svg></mjx-container> zeros matrix and <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.339ex" xmlns="http://www.w3.org/2000/svg" width="2.692ex" height="1.885ex" role="img" focusable="false" viewBox="0 -683 1189.8 833" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-5-TEX-I-1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path><path id="MJX-5-TEX-I-1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-5-TEX-I-1D443"></use></g><g data-mml-node="TeXAtom" transform="translate(642, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-5-TEX-I-1D447"></use></g></g></g></g></g></svg></mjx-container> is a <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="5.481ex" height="1.136ex" role="img" focusable="false" viewBox="0 -491 2422.4 502" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-5-TEX-I-1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-5-TEX-N-D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-5-TEX-I-1D45B"></use></g><g data-mml-node="mo" transform="translate(822.2, 0)"><use xlink:href="#MJX-5-TEX-N-D7"></use></g><g data-mml-node="mi" transform="translate(1822.4, 0)"><use xlink:href="#MJX-5-TEX-I-1D45B"></use></g></g></g></svg></mjx-container> identity matrix.</p><p><strong>Step 2.</strong> For <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.439ex" xmlns="http://www.w3.org/2000/svg" width="17.361ex" height="1.946ex" role="img" focusable="false" viewBox="0 -666 7673.7 860" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-5-TEX-I-1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path><path id="MJX-5-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-5-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-5-TEX-N-2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path><path id="MJX-5-TEX-N-32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path><path id="MJX-5-TEX-N-2026" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60ZM525 60Q525 84 542 102T585 120Q609 120 627 104T646 61Q646 36 629 18T586 0T543 17T525 60ZM972 60Q972 84 989 102T1032 120Q1056 120 1074 104T1093 61Q1093 36 1076 18T1033 0T990 17T972 60Z"></path><path id="MJX-5-TEX-I-1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-5-TEX-N-2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-5-TEX-I-1D456"></use></g><g data-mml-node="mo" transform="translate(622.8, 0)"><use xlink:href="#MJX-5-TEX-N-3D"></use></g><g data-mml-node="mn" transform="translate(1678.6, 0)"><use xlink:href="#MJX-5-TEX-N-31"></use></g><g data-mml-node="mo" transform="translate(2178.6, 0)"><use xlink:href="#MJX-5-TEX-N-2C"></use></g><g data-mml-node="mn" transform="translate(2623.2, 0)"><use xlink:href="#MJX-5-TEX-N-32"></use></g><g data-mml-node="mo" transform="translate(3123.2, 0)"><use xlink:href="#MJX-5-TEX-N-2C"></use></g><g data-mml-node="mo" transform="translate(3567.9, 0)"><use xlink:href="#MJX-5-TEX-N-2026"></use></g><g data-mml-node="mo" transform="translate(4906.6, 0)"><use xlink:href="#MJX-5-TEX-N-2C"></use></g><g data-mml-node="mi" transform="translate(5351.2, 0)"><use xlink:href="#MJX-5-TEX-I-1D45B"></use></g><g data-mml-node="mo" transform="translate(6173.4, 0)"><use xlink:href="#MJX-5-TEX-N-2212"></use></g><g data-mml-node="mn" transform="translate(7173.7, 0)"><use xlink:href="#MJX-5-TEX-N-31"></use></g></g></g></svg></mjx-container>, find the row <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.462ex" xmlns="http://www.w3.org/2000/svg" width="0.932ex" height="1.957ex" role="img" focusable="false" viewBox="0 -661 412 865" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-5-TEX-I-1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-5-TEX-I-1D457"></use></g></g></g></svg></mjx-container> with the largest entry in absolute value on or below the diagonal of the <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="0.781ex" height="1.52ex" role="img" focusable="false" viewBox="0 -661 345 672" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-5-TEX-I-1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-5-TEX-I-1D456"></use></g></g></g></svg></mjx-container>-row and swap rows <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="0.781ex" height="1.52ex" role="img" focusable="false" viewBox="0 -661 345 672" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-5-TEX-I-1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-5-TEX-I-1D456"></use></g></g></g></svg></mjx-container> and <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.462ex" xmlns="http://www.w3.org/2000/svg" width="0.932ex" height="1.957ex" role="img" focusable="false" viewBox="0 -661 412 865" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-5-TEX-I-1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-5-TEX-I-1D457"></use></g></g></g></svg></mjx-container> in all three matrices, <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.339ex" xmlns="http://www.w3.org/2000/svg" width="2.692ex" height="1.885ex" role="img" focusable="false" viewBox="0 -683 1189.8 833" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-5-TEX-I-1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path><path id="MJX-5-TEX-I-1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-5-TEX-I-1D443"></use></g><g data-mml-node="TeXAtom" transform="translate(642, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-5-TEX-I-1D447"></use></g></g></g></g></g></svg></mjx-container>, <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0" xmlns="http://www.w3.org/2000/svg" width="1.541ex" height="1.545ex" role="img" focusable="false" viewBox="0 -683 681 683" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-5-TEX-I-1D43F" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-5-TEX-I-1D43F"></use></g></g></g></svg></mjx-container>, and <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex" xmlns="http://www.w3.org/2000/svg" width="1.735ex" height="1.595ex" role="img" focusable="false" viewBox="0 -683 767 705" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-5-TEX-I-1D448" d="M107 637Q73 637 71 641Q70 643 70 649Q70 673 81 682Q83 683 98 683Q139 681 234 681Q268 681 297 681T342 682T362 682Q378 682 378 672Q378 670 376 658Q371 641 366 638H364Q362 638 359 638T352 638T343 637T334 637Q295 636 284 634T266 623Q265 621 238 518T184 302T154 169Q152 155 152 140Q152 86 183 55T269 24Q336 24 403 69T501 205L552 406Q599 598 599 606Q599 633 535 637Q511 637 511 648Q511 650 513 660Q517 676 519 679T529 683Q532 683 561 682T645 680Q696 680 723 681T752 682Q767 682 767 672Q767 650 759 642Q756 637 737 637Q666 633 648 597Q646 592 598 404Q557 235 548 205Q515 105 433 42T263 -22Q171 -22 116 34T60 167V183Q60 201 115 421Q164 622 164 628Q164 635 107 637Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-5-TEX-I-1D448"></use></g></g></g></svg></mjx-container>. If this maximum entry is zero, then terminate this loop and indicate that the matrix is singular (invertible).</p><p><strong>Step 3.</strong> Inside the first loop, create a second for loop, for <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.462ex" xmlns="http://www.w3.org/2000/svg" width="15.025ex" height="1.968ex" role="img" focusable="false" viewBox="0 -666 6641 870" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-4-TEX-I-1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path><path id="MJX-4-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-4-TEX-I-1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path><path id="MJX-4-TEX-N-2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path><path id="MJX-4-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-4-TEX-N-2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path><path id="MJX-4-TEX-N-2026" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60ZM525 60Q525 84 542 102T585 120Q609 120 627 104T646 61Q646 36 629 18T586 0T543 17T525 60ZM972 60Q972 84 989 102T1032 120Q1056 120 1074 104T1093 61Q1093 36 1076 18T1033 0T990 17T972 60Z"></path><path id="MJX-4-TEX-I-1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-4-TEX-I-1D457"></use></g><g data-mml-node="mo" transform="translate(689.8, 0)"><use xlink:href="#MJX-4-TEX-N-3D"></use></g><g data-mml-node="mi" transform="translate(1745.6, 0)"><use xlink:href="#MJX-4-TEX-I-1D456"></use></g><g data-mml-node="mo" transform="translate(2312.8, 0)"><use xlink:href="#MJX-4-TEX-N-2B"></use></g><g data-mml-node="mn" transform="translate(3313, 0)"><use xlink:href="#MJX-4-TEX-N-31"></use></g><g data-mml-node="mo" transform="translate(3813, 0)"><use xlink:href="#MJX-4-TEX-N-2C"></use></g><g data-mml-node="mo" transform="translate(4257.7, 0)"><use xlink:href="#MJX-4-TEX-N-2026"></use></g><g data-mml-node="mo" transform="translate(5596.3, 0)"><use xlink:href="#MJX-4-TEX-N-2C"></use></g><g data-mml-node="mi" transform="translate(6041, 0)"><use xlink:href="#MJX-4-TEX-I-1D45B"></use></g></g></g></svg></mjx-container>, calculate the scalar value <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -1.24ex" xmlns="http://www.w3.org/2000/svg" width="8.484ex" height="3.413ex" role="img" focusable="false" viewBox="0 -960.3 3750 1508.4" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-3-TEX-I-1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path><path id="MJX-3-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-3-TEX-N-2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path><path id="MJX-3-TEX-I-1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-3-TEX-I-1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path><path id="MJX-3-TEX-N-2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path><path id="MJX-3-TEX-I-1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-3-TEX-I-1D460"></use></g><g data-mml-node="mo" transform="translate(746.8, 0)"><use xlink:href="#MJX-3-TEX-N-3D"></use></g><g data-mml-node="mfrac" transform="translate(1802.6, 0)"><g data-mml-node="mrow" transform="translate(220, 548.1) scale(0.707)"><g data-mml-node="mo"><use xlink:href="#MJX-3-TEX-N-2212"></use></g><g data-mml-node="msub" transform="translate(778, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-3-TEX-I-1D462"></use></g><g data-mml-node="TeXAtom" transform="translate(572, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-3-TEX-I-1D457"></use></g><g data-mml-node="mo" transform="translate(412, 0)"><use xlink:href="#MJX-3-TEX-N-2C"></use></g><g data-mml-node="mi" transform="translate(690, 0)"><use xlink:href="#MJX-3-TEX-I-1D456"></use></g></g></g></g><g data-mml-node="msub" transform="translate(511.8, -345) scale(0.707)"><g data-mml-node="mi"><use xlink:href="#MJX-3-TEX-I-1D462"></use></g><g data-mml-node="TeXAtom" transform="translate(572, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-3-TEX-I-1D456"></use></g><g data-mml-node="mo" transform="translate(345, 0)"><use xlink:href="#MJX-3-TEX-N-2C"></use></g><g data-mml-node="mi" transform="translate(623, 0)"><use xlink:href="#MJX-3-TEX-I-1D456"></use></g></g></g><rect width="1707.4" height="60" x="120" y="220"></rect></g></g></g></svg></mjx-container>. Next, add <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.023ex" xmlns="http://www.w3.org/2000/svg" width="1.061ex" height="1.023ex" role="img" focusable="false" viewBox="0 -442 469 452" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-3-TEX-I-1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-3-TEX-I-1D460"></use></g></g></g></svg></mjx-container> times row <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="0.781ex" height="1.52ex" role="img" focusable="false" viewBox="0 -661 345 672" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-3-TEX-I-1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-3-TEX-I-1D456"></use></g></g></g></svg></mjx-container> onto row <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.462ex" xmlns="http://www.w3.org/2000/svg" width="0.932ex" height="1.957ex" role="img" focusable="false" viewBox="0 -661 412 865" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-3-TEX-I-1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-3-TEX-I-1D457"></use></g></g></g></svg></mjx-container> in the matrix $U$ and set the entry <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.667ex" xmlns="http://www.w3.org/2000/svg" width="9.38ex" height="2.213ex" role="img" focusable="false" viewBox="0 -683 4145.9 978" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-3-TEX-I-1D43F" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path><path id="MJX-3-TEX-I-1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path><path id="MJX-3-TEX-N-2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path><path id="MJX-3-TEX-I-1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-3-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-3-TEX-N-2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path><path id="MJX-3-TEX-I-1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-3-TEX-I-1D43F"></use></g><g data-mml-node="TeXAtom" transform="translate(681, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-3-TEX-I-1D457"></use></g><g data-mml-node="mo" transform="translate(412, 0)"><use xlink:href="#MJX-3-TEX-N-2C"></use></g><g data-mml-node="mi" transform="translate(690, 0)"><use xlink:href="#MJX-3-TEX-I-1D466"></use></g></g></g><g data-mml-node="mo" transform="translate(1843.2, 0)"><use xlink:href="#MJX-3-TEX-N-3D"></use></g><g data-mml-node="mo" transform="translate(2898.9, 0)"><use xlink:href="#MJX-3-TEX-N-2212"></use></g><g data-mml-node="mi" transform="translate(3676.9, 0)"><use xlink:href="#MJX-3-TEX-I-1D460"></use></g></g></g></svg></mjx-container>.</p><p><strong>Step 4.</strong> Having iterated from <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.439ex" xmlns="http://www.w3.org/2000/svg" width="17.361ex" height="1.946ex" role="img" focusable="false" viewBox="0 -666 7673.7 860" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-3-TEX-I-1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path><path id="MJX-3-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-3-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-3-TEX-N-2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path><path id="MJX-3-TEX-N-32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path><path id="MJX-3-TEX-N-2026" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60ZM525 60Q525 84 542 102T585 120Q609 120 627 104T646 61Q646 36 629 18T586 0T543 17T525 60ZM972 60Q972 84 989 102T1032 120Q1056 120 1074 104T1093 61Q1093 36 1076 18T1033 0T990 17T972 60Z"></path><path id="MJX-3-TEX-I-1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-3-TEX-N-2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-3-TEX-I-1D456"></use></g><g data-mml-node="mo" transform="translate(622.8, 0)"><use xlink:href="#MJX-3-TEX-N-3D"></use></g><g data-mml-node="mn" transform="translate(1678.6, 0)"><use xlink:href="#MJX-3-TEX-N-31"></use></g><g data-mml-node="mo" transform="translate(2178.6, 0)"><use xlink:href="#MJX-3-TEX-N-2C"></use></g><g data-mml-node="mn" transform="translate(2623.2, 0)"><use xlink:href="#MJX-3-TEX-N-32"></use></g><g data-mml-node="mo" transform="translate(3123.2, 0)"><use xlink:href="#MJX-3-TEX-N-2C"></use></g><g data-mml-node="mo" transform="translate(3567.9, 0)"><use xlink:href="#MJX-3-TEX-N-2026"></use></g><g data-mml-node="mo" transform="translate(4906.6, 0)"><use xlink:href="#MJX-3-TEX-N-2C"></use></g><g data-mml-node="mi" transform="translate(5351.2, 0)"><use xlink:href="#MJX-3-TEX-I-1D45B"></use></g><g data-mml-node="mo" transform="translate(6173.4, 0)"><use xlink:href="#MJX-3-TEX-N-2212"></use></g><g data-mml-node="mn" transform="translate(7173.7, 0)"><use xlink:href="#MJX-3-TEX-N-31"></use></g></g></g></svg></mjx-container>, finish by adding the identity matrix onto <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex" xmlns="http://www.w3.org/2000/svg" width="10.933ex" height="1.902ex" role="img" focusable="false" viewBox="0 -683 4832.3 840.8" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-3-TEX-I-1D43F" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path><path id="MJX-3-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-3-TEX-N-2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path><path id="MJX-3-TEX-I-1D43C" d="M43 1Q26 1 26 10Q26 12 29 24Q34 43 39 45Q42 46 54 46H60Q120 46 136 53Q137 53 138 54Q143 56 149 77T198 273Q210 318 216 344Q286 624 286 626Q284 630 284 631Q274 637 213 637H193Q184 643 189 662Q193 677 195 680T209 683H213Q285 681 359 681Q481 681 487 683H497Q504 676 504 672T501 655T494 639Q491 637 471 637Q440 637 407 634Q393 631 388 623Q381 609 337 432Q326 385 315 341Q245 65 245 59Q245 52 255 50T307 46H339Q345 38 345 37T342 19Q338 6 332 0H316Q279 2 179 2Q143 2 113 2T65 2T43 1Z"></path><path id="MJX-3-TEX-I-1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-3-TEX-I-1D43F"></use></g><g data-mml-node="mo" transform="translate(958.8, 0)"><use xlink:href="#MJX-3-TEX-N-3D"></use></g><g data-mml-node="mi" transform="translate(2014.6, 0)"><use xlink:href="#MJX-3-TEX-I-1D43F"></use></g><g data-mml-node="mo" transform="translate(2917.8, 0)"><use xlink:href="#MJX-3-TEX-N-2B"></use></g><g data-mml-node="msub" transform="translate(3918, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-3-TEX-I-1D43C"></use></g><g data-mml-node="TeXAtom" transform="translate(440, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-3-TEX-I-1D45B"></use></g></g></g></g></g></svg></mjx-container>. These are the <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.339ex" xmlns="http://www.w3.org/2000/svg" width="2.692ex" height="1.885ex" role="img" focusable="false" viewBox="0 -683 1189.8 833" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-3-TEX-I-1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path><path id="MJX-3-TEX-I-1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-3-TEX-I-1D443"></use></g><g data-mml-node="TeXAtom" transform="translate(642, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-3-TEX-I-1D447"></use></g></g></g></g></g></svg></mjx-container>, <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0" xmlns="http://www.w3.org/2000/svg" width="1.541ex" height="1.545ex" role="img" focusable="false" viewBox="0 -683 681 683" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-3-TEX-I-1D43F" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-3-TEX-I-1D43F"></use></g></g></g></svg></mjx-container>, and <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex" xmlns="http://www.w3.org/2000/svg" width="1.735ex" height="1.595ex" role="img" focusable="false" viewBox="0 -683 767 705" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-3-TEX-I-1D448" d="M107 637Q73 637 71 641Q70 643 70 649Q70 673 81 682Q83 683 98 683Q139 681 234 681Q268 681 297 681T342 682T362 682Q378 682 378 672Q378 670 376 658Q371 641 366 638H364Q362 638 359 638T352 638T343 637T334 637Q295 636 284 634T266 623Q265 621 238 518T184 302T154 169Q152 155 152 140Q152 86 183 55T269 24Q336 24 403 69T501 205L552 406Q599 598 599 606Q599 633 535 637Q511 637 511 648Q511 650 513 660Q517 676 519 679T529 683Q532 683 561 682T645 680Q696 680 723 681T752 682Q767 682 767 672Q767 650 759 642Q756 637 737 637Q666 633 648 597Q646 592 598 404Q557 235 548 205Q515 105 433 42T263 -22Q171 -22 116 34T60 167V183Q60 201 115 421Q164 622 164 628Q164 635 107 637Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-3-TEX-I-1D448"></use></g></g></g></svg></mjx-container> matrices of the PLU decomposition of matrix <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0" xmlns="http://www.w3.org/2000/svg" width="2.378ex" height="1.545ex" role="img" focusable="false" viewBox="0 -683 1051 683" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-3-TEX-I-1D440" d="M289 629Q289 635 232 637Q208 637 201 638T194 648Q194 649 196 659Q197 662 198 666T199 671T201 676T203 679T207 681T212 683T220 683T232 684Q238 684 262 684T307 683Q386 683 398 683T414 678Q415 674 451 396L487 117L510 154Q534 190 574 254T662 394Q837 673 839 675Q840 676 842 678T846 681L852 683H948Q965 683 988 683T1017 684Q1051 684 1051 673Q1051 668 1048 656T1045 643Q1041 637 1008 637Q968 636 957 634T939 623Q936 618 867 340T797 59Q797 55 798 54T805 50T822 48T855 46H886Q892 37 892 35Q892 19 885 5Q880 0 869 0Q864 0 828 1T736 2Q675 2 644 2T609 1Q592 1 592 11Q592 13 594 25Q598 41 602 43T625 46Q652 46 685 49Q699 52 704 61Q706 65 742 207T813 490T848 631L654 322Q458 10 453 5Q451 4 449 3Q444 0 433 0Q418 0 415 7Q413 11 374 317L335 624L267 354Q200 88 200 79Q206 46 272 46H282Q288 41 289 37T286 19Q282 3 278 1Q274 0 267 0Q265 0 255 0T221 1T157 2Q127 2 95 1T58 0Q43 0 39 2T35 11Q35 13 38 25T43 40Q45 46 65 46Q135 46 154 86Q158 92 223 354T289 629Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-3-TEX-I-1D440"></use></g></g></g></svg></mjx-container>.</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Decomposition</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""    References    ----------    [1] https://ece.uwaterloo.ca/~dwharder/NumericalAnalysis/04LinearAlgebra/lup/    [2] https://math.unm.edu/~loring/links/linear_s08/LU.pdf    [3] https://johnfoster.pge.utexas.edu/numerical-methods-book/LinearAlgebra_LU.html    """</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">pass</span>        <span class="token keyword">def</span> <span class="token function">plu</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> A<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment"># Step 1. Inittiate three cadidate matrices</span>        n <span class="token operator">=</span> A<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>        P <span class="token operator">=</span> np<span class="token punctuation">.</span>identity<span class="token punctuation">(</span>n<span class="token punctuation">)</span>        L <span class="token operator">=</span> np<span class="token punctuation">.</span>identity<span class="token punctuation">(</span>n<span class="token punctuation">)</span>        U <span class="token operator">=</span> A<span class="token punctuation">.</span>copy<span class="token punctuation">(</span><span class="token punctuation">)</span>                PF <span class="token operator">=</span> np<span class="token punctuation">.</span>identity<span class="token punctuation">(</span>n<span class="token punctuation">)</span>        LF <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>shape<span class="token operator">=</span><span class="token punctuation">(</span>n<span class="token punctuation">,</span> n<span class="token punctuation">)</span><span class="token punctuation">)</span>                <span class="token comment"># Step 2. Loop over rows find the row with the largest entry in absolute</span>        <span class="token comment"># value on or below the diagonal of the i-th row</span>        <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>n<span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            index <span class="token operator">=</span> np<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span><span class="token builtin">abs</span><span class="token punctuation">(</span>U<span class="token punctuation">[</span>i<span class="token punctuation">:</span><span class="token punctuation">,</span> i<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>            index <span class="token operator">=</span> index <span class="token operator">+</span> i            <span class="token keyword">if</span> index <span class="token operator">!=</span> i<span class="token punctuation">:</span>                P <span class="token operator">=</span> np<span class="token punctuation">.</span>identity<span class="token punctuation">(</span>n<span class="token punctuation">)</span>                P<span class="token punctuation">[</span><span class="token punctuation">[</span>index<span class="token punctuation">,</span> i<span class="token punctuation">]</span><span class="token punctuation">,</span> i<span class="token punctuation">:</span>n<span class="token punctuation">]</span> <span class="token operator">=</span> P<span class="token punctuation">[</span><span class="token punctuation">[</span>i<span class="token punctuation">,</span> index<span class="token punctuation">]</span><span class="token punctuation">,</span> i<span class="token punctuation">:</span>n<span class="token punctuation">]</span>                U<span class="token punctuation">[</span><span class="token punctuation">[</span>index<span class="token punctuation">,</span> i<span class="token punctuation">]</span><span class="token punctuation">,</span> i<span class="token punctuation">:</span>n<span class="token punctuation">]</span> <span class="token operator">=</span> U<span class="token punctuation">[</span><span class="token punctuation">[</span>i<span class="token punctuation">,</span> index<span class="token punctuation">]</span><span class="token punctuation">,</span> i<span class="token punctuation">:</span>n<span class="token punctuation">]</span>                 PF <span class="token operator">=</span> np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>P<span class="token punctuation">,</span> PF<span class="token punctuation">)</span>                LF <span class="token operator">=</span> np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>P<span class="token punctuation">,</span> LF<span class="token punctuation">)</span>            L <span class="token operator">=</span> np<span class="token punctuation">.</span>identity<span class="token punctuation">(</span>n<span class="token punctuation">)</span>            <span class="token comment"># Step 3. Calculate the scalar value</span>            <span class="token keyword">for</span> j <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>i<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">,</span> n<span class="token punctuation">)</span><span class="token punctuation">:</span>                L<span class="token punctuation">[</span>j<span class="token punctuation">,</span> i<span class="token punctuation">]</span>  <span class="token operator">=</span> <span class="token operator">-</span><span class="token punctuation">(</span>U<span class="token punctuation">[</span>j<span class="token punctuation">,</span> i<span class="token punctuation">]</span> <span class="token operator">/</span> U<span class="token punctuation">[</span>i<span class="token punctuation">,</span> i<span class="token punctuation">]</span><span class="token punctuation">)</span>                LF<span class="token punctuation">[</span>j<span class="token punctuation">,</span> i<span class="token punctuation">]</span> <span class="token operator">=</span>  <span class="token punctuation">(</span>U<span class="token punctuation">[</span>j<span class="token punctuation">,</span> i<span class="token punctuation">]</span> <span class="token operator">/</span> U<span class="token punctuation">[</span>i<span class="token punctuation">,</span> i<span class="token punctuation">]</span><span class="token punctuation">)</span>            U <span class="token operator">=</span> np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>L<span class="token punctuation">,</span> U<span class="token punctuation">)</span>        <span class="token comment"># Step 4. Add identity matrix onto L</span>        np<span class="token punctuation">.</span>fill_diagonal<span class="token punctuation">(</span>LF<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> PF<span class="token punctuation">,</span> LF<span class="token punctuation">,</span> U<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="QR-decomposition"><a href="#QR-decomposition" class="headerlink" title="QR decomposition"></a>QR decomposition</h1><p>In linear algebra, a QR decomposition, also known as a QR factorization or QU factorization is a decomposition of a matrix <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0" xmlns="http://www.w3.org/2000/svg" width="1.697ex" height="1.62ex" role="img" focusable="false" viewBox="0 -716 750 716" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-3-TEX-I-1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-3-TEX-I-1D434"></use></g></g></g></svg></mjx-container>, either square or rectangular, into a product <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.439ex" xmlns="http://www.w3.org/2000/svg" width="8.221ex" height="2.059ex" role="img" focusable="false" viewBox="0 -716 3633.6 910" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-3-TEX-I-1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"></path><path id="MJX-3-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-3-TEX-I-1D444" d="M399 -80Q399 -47 400 -30T402 -11V-7L387 -11Q341 -22 303 -22Q208 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435Q740 255 592 107Q529 47 461 16L444 8V3Q444 2 449 -24T470 -66T516 -82Q551 -82 583 -60T625 -3Q631 11 638 11Q647 11 649 2Q649 -6 639 -34T611 -100T557 -165T481 -194Q399 -194 399 -87V-80ZM636 468Q636 523 621 564T580 625T530 655T477 665Q429 665 379 640Q277 591 215 464T153 216Q153 110 207 59Q231 38 236 38V46Q236 86 269 120T347 155Q372 155 390 144T417 114T429 82T435 55L448 64Q512 108 557 185T619 334T636 468ZM314 18Q362 18 404 39L403 49Q399 104 366 115Q354 117 347 117Q344 117 341 117T337 118Q317 118 296 98T274 52Q274 18 314 18Z"></path><path id="MJX-3-TEX-I-1D445" d="M230 637Q203 637 198 638T193 649Q193 676 204 682Q206 683 378 683Q550 682 564 680Q620 672 658 652T712 606T733 563T739 529Q739 484 710 445T643 385T576 351T538 338L545 333Q612 295 612 223Q612 212 607 162T602 80V71Q602 53 603 43T614 25T640 16Q668 16 686 38T712 85Q717 99 720 102T735 105Q755 105 755 93Q755 75 731 36Q693 -21 641 -21H632Q571 -21 531 4T487 82Q487 109 502 166T517 239Q517 290 474 313Q459 320 449 321T378 323H309L277 193Q244 61 244 59Q244 55 245 54T252 50T269 48T302 46H333Q339 38 339 37T336 19Q332 6 326 0H311Q275 2 180 2Q146 2 117 2T71 2T50 1Q33 1 33 10Q33 12 36 24Q41 43 46 45Q50 46 61 46H67Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628Q287 635 230 637ZM630 554Q630 586 609 608T523 636Q521 636 500 636T462 637H440Q393 637 386 627Q385 624 352 494T319 361Q319 360 388 360Q466 361 492 367Q556 377 592 426Q608 449 619 486T630 554Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-3-TEX-I-1D434"></use></g><g data-mml-node="mo" transform="translate(1027.8, 0)"><use xlink:href="#MJX-3-TEX-N-3D"></use></g><g data-mml-node="mi" transform="translate(2083.6, 0)"><use xlink:href="#MJX-3-TEX-I-1D444"></use></g><g data-mml-node="mi" transform="translate(2874.6, 0)"><use xlink:href="#MJX-3-TEX-I-1D445"></use></g></g></g></svg></mjx-container> of an orthogonal matrix <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.439ex" xmlns="http://www.w3.org/2000/svg" width="1.79ex" height="2.032ex" role="img" focusable="false" viewBox="0 -704 791 898" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-3-TEX-I-1D444" d="M399 -80Q399 -47 400 -30T402 -11V-7L387 -11Q341 -22 303 -22Q208 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435Q740 255 592 107Q529 47 461 16L444 8V3Q444 2 449 -24T470 -66T516 -82Q551 -82 583 -60T625 -3Q631 11 638 11Q647 11 649 2Q649 -6 639 -34T611 -100T557 -165T481 -194Q399 -194 399 -87V-80ZM636 468Q636 523 621 564T580 625T530 655T477 665Q429 665 379 640Q277 591 215 464T153 216Q153 110 207 59Q231 38 236 38V46Q236 86 269 120T347 155Q372 155 390 144T417 114T429 82T435 55L448 64Q512 108 557 185T619 334T636 468ZM314 18Q362 18 404 39L403 49Q399 104 366 115Q354 117 347 117Q344 117 341 117T337 118Q317 118 296 98T274 52Q274 18 314 18Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-3-TEX-I-1D444"></use></g></g></g></svg></mjx-container> and an upper triangular matrix <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.048ex" xmlns="http://www.w3.org/2000/svg" width="1.717ex" height="1.593ex" role="img" focusable="false" viewBox="0 -683 759 704" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-2-TEX-I-1D445" d="M230 637Q203 637 198 638T193 649Q193 676 204 682Q206 683 378 683Q550 682 564 680Q620 672 658 652T712 606T733 563T739 529Q739 484 710 445T643 385T576 351T538 338L545 333Q612 295 612 223Q612 212 607 162T602 80V71Q602 53 603 43T614 25T640 16Q668 16 686 38T712 85Q717 99 720 102T735 105Q755 105 755 93Q755 75 731 36Q693 -21 641 -21H632Q571 -21 531 4T487 82Q487 109 502 166T517 239Q517 290 474 313Q459 320 449 321T378 323H309L277 193Q244 61 244 59Q244 55 245 54T252 50T269 48T302 46H333Q339 38 339 37T336 19Q332 6 326 0H311Q275 2 180 2Q146 2 117 2T71 2T50 1Q33 1 33 10Q33 12 36 24Q41 43 46 45Q50 46 61 46H67Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628Q287 635 230 637ZM630 554Q630 586 609 608T523 636Q521 636 500 636T462 637H440Q393 637 386 627Q385 624 352 494T319 361Q319 360 388 360Q466 361 492 367Q556 377 592 426Q608 449 619 486T630 554Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D445"></use></g></g></g></svg></mjx-container>. There are in fact a couple of methods to compute a QR decomposition. These include the the <code>Gram–Schmidt process</code>, <code>Householder transformations</code>, and <code>Givens rotations</code>.</p><p>If <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.09ex" xmlns="http://www.w3.org/2000/svg" width="11.294ex" height="2.112ex" role="img" focusable="false" viewBox="0 -893.3 4991.7 933.3" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-2-TEX-I-1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"></path><path id="MJX-2-TEX-N-2208" d="M84 250Q84 372 166 450T360 539Q361 539 377 539T419 540T469 540H568Q583 532 583 520Q583 511 570 501L466 500Q355 499 329 494Q280 482 242 458T183 409T147 354T129 306T124 272V270H568Q583 262 583 250T568 230H124V228Q124 207 134 177T167 112T231 48T328 7Q355 1 466 0H570Q583 -10 583 -20Q583 -32 568 -40H471Q464 -40 446 -40T417 -41Q262 -41 172 45Q84 127 84 250Z"></path><path id="MJX-2-TEX-D-211D" d="M17 665Q17 672 28 683H221Q415 681 439 677Q461 673 481 667T516 654T544 639T566 623T584 607T597 592T607 578T614 565T618 554L621 548Q626 530 626 497Q626 447 613 419Q578 348 473 326L455 321Q462 310 473 292T517 226T578 141T637 72T686 35Q705 30 705 16Q705 7 693 -1H510Q503 6 404 159L306 310H268V183Q270 67 271 59Q274 42 291 38Q295 37 319 35Q344 35 353 28Q362 17 353 3L346 -1H28Q16 5 16 16Q16 35 55 35Q96 38 101 52Q106 60 106 341T101 632Q95 645 55 648Q17 648 17 665ZM241 35Q238 42 237 45T235 78T233 163T233 337V621L237 635L244 648H133Q136 641 137 638T139 603T141 517T141 341Q141 131 140 89T134 37Q133 36 133 35H241ZM457 496Q457 540 449 570T425 615T400 634T377 643Q374 643 339 648Q300 648 281 635Q271 628 270 610T268 481V346H284Q327 346 375 352Q421 364 439 392T457 496ZM492 537T492 496T488 427T478 389T469 371T464 361Q464 360 465 360Q469 360 497 370Q593 400 593 495Q593 592 477 630L457 637L461 626Q474 611 488 561Q492 537 492 496ZM464 243Q411 317 410 317Q404 317 401 315Q384 315 370 312H346L526 35H619L606 50Q553 109 464 243Z"></path><path id="MJX-2-TEX-N-D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D434"></use></g><g data-mml-node="mo" transform="translate(1027.8, 0)"><use xlink:href="#MJX-2-TEX-N-2208"></use></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(1972.6, 0)"><g data-mml-node="msup"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-D-211D"></use></g><g data-mml-node="TeXAtom" transform="translate(722, 363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><text data-variant="double-struck" transform="matrix(1 0 0 -1 0 0)" font-size="884px">𝕞</text></g><g data-mml-node="mo" transform="translate(1200, 0)"><use xlink:href="#MJX-2-TEX-N-D7"></use></g><g data-mml-node="mi" transform="translate(1978, 0)"><text data-variant="double-struck" transform="matrix(1 0 0 -1 0 0)" font-size="884px">𝕟</text></g></g></g></g></g></g></svg></mjx-container> has linearly independent columns, then it can factored as: </p><div style="display: flex;justify-content: center;">    <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -6.81ex" xmlns="http://www.w3.org/2000/svg" width="44.653ex" height="14.751ex" role="img" focusable="false" viewBox="0 -3510 19736.7 6520" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-2-TEX-I-1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"></path><path id="MJX-2-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-2-TEX-N-5B" d="M118 -250V750H255V710H158V-210H255V-250H118Z"></path><path id="MJX-2-TEX-I-1D45E" d="M33 157Q33 258 109 349T280 441Q340 441 372 389Q373 390 377 395T388 406T404 418Q438 442 450 442Q454 442 457 439T460 434Q460 425 391 149Q320 -135 320 -139Q320 -147 365 -148H390Q396 -156 396 -157T393 -175Q389 -188 383 -194H370Q339 -192 262 -192Q234 -192 211 -192T174 -192T157 -193Q143 -193 143 -185Q143 -182 145 -170Q149 -154 152 -151T172 -148Q220 -148 230 -141Q238 -136 258 -53T279 32Q279 33 272 29Q224 -10 172 -10Q117 -10 75 30T33 157ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path><path id="MJX-2-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-2-TEX-N-32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path><path id="MJX-2-TEX-N-2026" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60ZM525 60Q525 84 542 102T585 120Q609 120 627 104T646 61Q646 36 629 18T586 0T543 17T525 60ZM972 60Q972 84 989 102T1032 120Q1056 120 1074 104T1093 61Q1093 36 1076 18T1033 0T990 17T972 60Z"></path><path id="MJX-2-TEX-I-1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-2-TEX-N-5D" d="M22 710V750H159V-250H22V-210H119V710H22Z"></path><path id="MJX-2-TEX-S4-23A1" d="M319 -645V1154H666V1070H403V-645H319Z"></path><path id="MJX-2-TEX-S4-23A3" d="M319 -644V1155H403V-560H666V-644H319Z"></path><path id="MJX-2-TEX-S4-23A2" d="M319 0V602H403V0H319Z"></path><path id="MJX-2-TEX-I-1D445" d="M230 637Q203 637 198 638T193 649Q193 676 204 682Q206 683 378 683Q550 682 564 680Q620 672 658 652T712 606T733 563T739 529Q739 484 710 445T643 385T576 351T538 338L545 333Q612 295 612 223Q612 212 607 162T602 80V71Q602 53 603 43T614 25T640 16Q668 16 686 38T712 85Q717 99 720 102T735 105Q755 105 755 93Q755 75 731 36Q693 -21 641 -21H632Q571 -21 531 4T487 82Q487 109 502 166T517 239Q517 290 474 313Q459 320 449 321T378 323H309L277 193Q244 61 244 59Q244 55 245 54T252 50T269 48T302 46H333Q339 38 339 37T336 19Q332 6 326 0H311Q275 2 180 2Q146 2 117 2T71 2T50 1Q33 1 33 10Q33 12 36 24Q41 43 46 45Q50 46 61 46H67Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628Q287 635 230 637ZM630 554Q630 586 609 608T523 636Q521 636 500 636T462 637H440Q393 637 386 627Q385 624 352 494T319 361Q319 360 388 360Q466 361 492 367Q556 377 592 426Q608 449 619 486T630 554Z"></path><path id="MJX-2-TEX-N-30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path><path id="MJX-2-TEX-N-22EE" d="M78 30Q78 54 95 72T138 90Q162 90 180 74T199 31Q199 6 182 -12T139 -30T96 -13T78 30ZM78 440Q78 464 95 482T138 500Q162 500 180 484T199 441Q199 416 182 398T139 380T96 397T78 440ZM78 840Q78 864 95 882T138 900Q162 900 180 884T199 841Q199 816 182 798T139 780T96 797T78 840Z"></path><path id="MJX-2-TEX-N-22F1" d="M133 760Q133 784 150 802T193 820Q217 820 235 804T254 761Q254 736 237 718T194 700T151 717T133 760ZM580 460Q580 484 597 502T640 520Q664 520 682 504T701 461Q701 436 684 418T641 400T598 417T580 460ZM1027 160Q1027 184 1044 202T1087 220Q1111 220 1129 204T1148 161Q1148 136 1131 118T1088 100T1045 117T1027 160Z"></path><path id="MJX-2-TEX-S4-23A4" d="M0 1070V1154H347V-645H263V1070H0Z"></path><path id="MJX-2-TEX-S4-23A6" d="M263 -560V1155H347V-644H0V-560H263Z"></path><path id="MJX-2-TEX-S4-23A5" d="M263 0V602H347V0H263Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D434"></use></g><g data-mml-node="mo" transform="translate(1027.8, 0)"><use xlink:href="#MJX-2-TEX-N-3D"></use></g><g data-mml-node="mrow" transform="translate(2083.6, 0)"><g data-mml-node="mo"><use xlink:href="#MJX-2-TEX-N-5B"></use></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(278, 0)"><g data-mml-node="mtable"><g data-mml-node="mtr"><g data-mml-node="mtd"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D45E"></use></g><g data-mml-node="TeXAtom" transform="translate(446, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-31"></use></g></g></g></g><g data-mml-node="mtd" transform="translate(1849.6, 0)"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D45E"></use></g><g data-mml-node="TeXAtom" transform="translate(446, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-32"></use></g></g></g></g><g data-mml-node="mtd" transform="translate(3699.1, 0)"><g data-mml-node="mo"><use xlink:href="#MJX-2-TEX-N-2026"></use></g></g><g data-mml-node="mtd" transform="translate(5871.1, 0)"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D45E"></use></g><g data-mml-node="TeXAtom" transform="translate(446, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D45B"></use></g></g></g></g></g></g></g><g data-mml-node="mo" transform="translate(7069.4, 0)"><use xlink:href="#MJX-2-TEX-N-5D"></use></g></g><g data-mml-node="mrow" transform="translate(9430.9, 0)"><g data-mml-node="mo"><use xlink:href="#MJX-2-TEX-S4-23A1" transform="translate(0, 2356)"></use><use xlink:href="#MJX-2-TEX-S4-23A3" transform="translate(0, -2366)"></use><svg width="667" height="3122" y="-1311" x="0" viewBox="0 780.5 667 3122"><use xlink:href="#MJX-2-TEX-S4-23A2" transform="scale(1, 7.779)"></use></svg></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(667, 0)"><g data-mml-node="mtable"><g data-mml-node="mtr" transform="translate(0, 2760)"><g data-mml-node="mtd"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D445"></use></g><g data-mml-node="TeXAtom" transform="translate(759, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-31"></use><use xlink:href="#MJX-2-TEX-N-31" transform="translate(500, 0)"></use></g></g></g></g><g data-mml-node="mtd" transform="translate(2516.1, 0)"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D445"></use></g><g data-mml-node="TeXAtom" transform="translate(759, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-31"></use><use xlink:href="#MJX-2-TEX-N-32" transform="translate(500, 0)"></use></g></g></g></g><g data-mml-node="mtd" transform="translate(5087.2, 0)"><g data-mml-node="mo"><use xlink:href="#MJX-2-TEX-N-2026"></use></g></g><g data-mml-node="mtd" transform="translate(7349.6, 0)"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D445"></use></g><g data-mml-node="TeXAtom" transform="translate(759, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-31"></use></g><g data-mml-node="mi" transform="translate(500, 0)"><use xlink:href="#MJX-2-TEX-I-1D45B"></use></g></g></g></g></g><g data-mml-node="mtr" transform="translate(0, 1360)"><g data-mml-node="mtd" transform="translate(508.1, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(2516.1, 0)"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D445"></use></g><g data-mml-node="TeXAtom" transform="translate(759, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-32"></use><use xlink:href="#MJX-2-TEX-N-32" transform="translate(500, 0)"></use></g></g></g></g><g data-mml-node="mtd" transform="translate(5087.2, 0)"><g data-mml-node="mo"><use xlink:href="#MJX-2-TEX-N-2026"></use></g></g><g data-mml-node="mtd" transform="translate(7349.6, 0)"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D445"></use></g><g data-mml-node="TeXAtom" transform="translate(759, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-32"></use></g><g data-mml-node="mi" transform="translate(500, 0)"><use xlink:href="#MJX-2-TEX-I-1D45B"></use></g></g></g></g></g><g data-mml-node="mtr" transform="translate(0, -810)"><g data-mml-node="mtd" transform="translate(619.1, 0)"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mo"><use xlink:href="#MJX-2-TEX-N-22EE"></use></g></g></g><g data-mml-node="mtd" transform="translate(3135.2, 0)"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mo"><use xlink:href="#MJX-2-TEX-N-22EE"></use></g></g></g><g data-mml-node="mtd" transform="translate(5032.2, 0)"><g data-mml-node="mo"><use xlink:href="#MJX-2-TEX-N-22F1"></use></g></g><g data-mml-node="mtd" transform="translate(8004, 0)"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mo"><use xlink:href="#MJX-2-TEX-N-22EE"></use></g></g></g></g><g data-mml-node="mtr" transform="translate(0, -2760)"><g data-mml-node="mtd" transform="translate(508.1, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(3024.2, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(5534.2, 0)"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mo"><use xlink:href="#MJX-2-TEX-N-22EE"></use></g></g></g><g data-mml-node="mtd" transform="translate(7314.2, 0)"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D445"></use></g><g data-mml-node="TeXAtom" transform="translate(759, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D45B"></use></g><g data-mml-node="mi" transform="translate(600, 0)"><use xlink:href="#MJX-2-TEX-I-1D45B"></use></g></g></g></g></g></g></g><g data-mml-node="mo" transform="translate(9638.7, 0)"><use xlink:href="#MJX-2-TEX-S4-23A4" transform="translate(0, 2356)"></use><use xlink:href="#MJX-2-TEX-S4-23A6" transform="translate(0, -2366)"></use><svg width="667" height="3122" y="-1311" x="0" viewBox="0 780.5 667 3122"><use xlink:href="#MJX-2-TEX-S4-23A5" transform="scale(1, 7.779)"></use></svg></g></g></g></g></svg></mjx-container></div><p>where <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.439ex" xmlns="http://www.w3.org/2000/svg" width="11.973ex" height="1.439ex" role="img" focusable="false" viewBox="0 -442 5292 636" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-2-TEX-I-1D45E" d="M33 157Q33 258 109 349T280 441Q340 441 372 389Q373 390 377 395T388 406T404 418Q438 442 450 442Q454 442 457 439T460 434Q460 425 391 149Q320 -135 320 -139Q320 -147 365 -148H390Q396 -156 396 -157T393 -175Q389 -188 383 -194H370Q339 -192 262 -192Q234 -192 211 -192T174 -192T157 -193Q143 -193 143 -185Q143 -182 145 -170Q149 -154 152 -151T172 -148Q220 -148 230 -141Q238 -136 258 -53T279 32Q279 33 272 29Q224 -10 172 -10Q117 -10 75 30T33 157ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path><path id="MJX-2-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-2-TEX-N-2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path><path id="MJX-2-TEX-N-32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path><path id="MJX-2-TEX-N-2026" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60ZM525 60Q525 84 542 102T585 120Q609 120 627 104T646 61Q646 36 629 18T586 0T543 17T525 60ZM972 60Q972 84 989 102T1032 120Q1056 120 1074 104T1093 61Q1093 36 1076 18T1033 0T990 17T972 60Z"></path><path id="MJX-2-TEX-I-1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D45E"></use></g><g data-mml-node="TeXAtom" transform="translate(446, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-31"></use></g></g></g><g data-mml-node="mo" transform="translate(849.6, 0)"><use xlink:href="#MJX-2-TEX-N-2C"></use></g><g data-mml-node="msub" transform="translate(1294.2, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D45E"></use></g><g data-mml-node="TeXAtom" transform="translate(446, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-32"></use></g></g></g><g data-mml-node="mo" transform="translate(2143.8, 0)"><use xlink:href="#MJX-2-TEX-N-2C"></use></g><g data-mml-node="mo" transform="translate(2588.4, 0)"><use xlink:href="#MJX-2-TEX-N-2026"></use></g><g data-mml-node="mo" transform="translate(3927.1, 0)"><use xlink:href="#MJX-2-TEX-N-2C"></use></g><g data-mml-node="msub" transform="translate(4371.8, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D45E"></use></g><g data-mml-node="TeXAtom" transform="translate(446, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D45B"></use></g></g></g></g></g></svg></mjx-container> are orthogonal <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="1.986ex" height="1.025ex" role="img" focusable="false" viewBox="0 -442 878 453" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-2-TEX-I-1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D45A"></use></g></g></g></svg></mjx-container>-vectors (<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.439ex" xmlns="http://www.w3.org/2000/svg" width="8.976ex" height="2.343ex" role="img" focusable="false" viewBox="0 -841.7 3967.4 1035.7" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-2-TEX-I-1D444" d="M399 -80Q399 -47 400 -30T402 -11V-7L387 -11Q341 -22 303 -22Q208 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435Q740 255 592 107Q529 47 461 16L444 8V3Q444 2 449 -24T470 -66T516 -82Q551 -82 583 -60T625 -3Q631 11 638 11Q647 11 649 2Q649 -6 639 -34T611 -100T557 -165T481 -194Q399 -194 399 -87V-80ZM636 468Q636 523 621 564T580 625T530 655T477 665Q429 665 379 640Q277 591 215 464T153 216Q153 110 207 59Q231 38 236 38V46Q236 86 269 120T347 155Q372 155 390 144T417 114T429 82T435 55L448 64Q512 108 557 185T619 334T636 468ZM314 18Q362 18 404 39L403 49Q399 104 366 115Q354 117 347 117Q344 117 341 117T337 118Q317 118 296 98T274 52Q274 18 314 18Z"></path><path id="MJX-2-TEX-I-1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path><path id="MJX-2-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-2-TEX-I-1D43C" d="M43 1Q26 1 26 10Q26 12 29 24Q34 43 39 45Q42 46 54 46H60Q120 46 136 53Q137 53 138 54Q143 56 149 77T198 273Q210 318 216 344Q286 624 286 626Q284 630 284 631Q274 637 213 637H193Q184 643 189 662Q193 677 195 680T209 683H213Q285 681 359 681Q481 681 487 683H497Q504 676 504 672T501 655T494 639Q491 637 471 637Q440 637 407 634Q393 631 388 623Q381 609 337 432Q326 385 315 341Q245 65 245 59Q245 52 255 50T307 46H339Q345 38 345 37T342 19Q338 6 332 0H316Q279 2 179 2Q143 2 113 2T65 2T43 1Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D444"></use></g><g data-mml-node="TeXAtom" transform="translate(791, 363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D447"></use></g></g></g><g data-mml-node="mi" transform="translate(1338.8, 0)"><use xlink:href="#MJX-2-TEX-I-1D444"></use></g><g data-mml-node="mo" transform="translate(2407.6, 0)"><use xlink:href="#MJX-2-TEX-N-3D"></use></g><g data-mml-node="mi" transform="translate(3463.4, 0)"><use xlink:href="#MJX-2-TEX-I-1D43C"></use></g></g></g></svg></mjx-container>), that is, <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="8.085ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 3573.5 1000" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-2-TEX-N-2016" d="M133 736Q138 750 153 750Q164 750 170 739Q172 735 172 250T170 -239Q164 -250 152 -250Q144 -250 138 -244L137 -243Q133 -241 133 -179T132 250Q132 731 133 736ZM329 739Q334 750 346 750Q353 750 361 744L362 743Q366 741 366 679T367 250T367 -178T362 -243L361 -244Q355 -250 347 -250Q335 -250 329 -239Q327 -235 327 250T329 739Z"></path><path id="MJX-2-TEX-I-1D45E" d="M33 157Q33 258 109 349T280 441Q340 441 372 389Q373 390 377 395T388 406T404 418Q438 442 450 442Q454 442 457 439T460 434Q460 425 391 149Q320 -135 320 -139Q320 -147 365 -148H390Q396 -156 396 -157T393 -175Q389 -188 383 -194H370Q339 -192 262 -192Q234 -192 211 -192T174 -192T157 -193Q143 -193 143 -185Q143 -182 145 -170Q149 -154 152 -151T172 -148Q220 -148 230 -141Q238 -136 258 -53T279 32Q279 33 272 29Q224 -10 172 -10Q117 -10 75 30T33 157ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path><path id="MJX-2-TEX-I-1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path><path id="MJX-2-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-2-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mo"><use xlink:href="#MJX-2-TEX-N-2016"></use></g><g data-mml-node="msub" transform="translate(500, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D45E"></use></g><g data-mml-node="TeXAtom" transform="translate(446, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D456"></use></g></g></g><g data-mml-node="mo" transform="translate(1240, 0)"><use xlink:href="#MJX-2-TEX-N-2016"></use></g><g data-mml-node="mo" transform="translate(2017.7, 0)"><use xlink:href="#MJX-2-TEX-N-3D"></use></g><g data-mml-node="mn" transform="translate(3073.5, 0)"><use xlink:href="#MJX-2-TEX-N-31"></use></g></g></g></svg></mjx-container>, <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.666ex" xmlns="http://www.w3.org/2000/svg" width="8.324ex" height="2.57ex" role="img" focusable="false" viewBox="0 -841.7 3679.4 1136" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-2-TEX-I-1D45E" d="M33 157Q33 258 109 349T280 441Q340 441 372 389Q373 390 377 395T388 406T404 418Q438 442 450 442Q454 442 457 439T460 434Q460 425 391 149Q320 -135 320 -139Q320 -147 365 -148H390Q396 -156 396 -157T393 -175Q389 -188 383 -194H370Q339 -192 262 -192Q234 -192 211 -192T174 -192T157 -193Q143 -193 143 -185Q143 -182 145 -170Q149 -154 152 -151T172 -148Q220 -148 230 -141Q238 -136 258 -53T279 32Q279 33 272 29Q224 -10 172 -10Q117 -10 75 30T33 157ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path><path id="MJX-2-TEX-I-1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path><path id="MJX-2-TEX-I-1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path><path id="MJX-2-TEX-I-1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path><path id="MJX-2-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-2-TEX-N-30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msubsup"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D45E"></use></g><g data-mml-node="TeXAtom" transform="translate(510.7, 363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D447"></use></g></g><g data-mml-node="TeXAtom" transform="translate(446, -284.4) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D456"></use></g></g></g><g data-mml-node="msub" transform="translate(1058.5, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D45E"></use></g><g data-mml-node="TeXAtom" transform="translate(446, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D457"></use></g></g></g><g data-mml-node="mo" transform="translate(2123.6, 0)"><use xlink:href="#MJX-2-TEX-N-3D"></use></g><g data-mml-node="mn" transform="translate(3179.4, 0)"><use xlink:href="#MJX-2-TEX-N-30"></use></g></g></g></svg></mjx-container> if <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.486ex" xmlns="http://www.w3.org/2000/svg" width="4.73ex" height="2.106ex" role="img" focusable="false" viewBox="0 -716 2090.6 931" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-2-TEX-I-1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path><path id="MJX-2-TEX-N-2260" d="M166 -215T159 -215T147 -212T141 -204T139 -197Q139 -190 144 -183L306 133H70Q56 140 56 153Q56 168 72 173H327L406 327H72Q56 332 56 347Q56 360 70 367H426Q597 702 602 707Q605 716 618 716Q625 716 630 712T636 703T638 696Q638 692 471 367H707Q722 359 722 347Q722 336 708 328L451 327L371 173H708Q722 163 722 153Q722 140 707 133H351Q175 -210 170 -212Q166 -215 159 -215Z"></path><path id="MJX-2-TEX-I-1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D456"></use></g><g data-mml-node="mo" transform="translate(622.8, 0)"><use xlink:href="#MJX-2-TEX-N-2260"></use></g><g data-mml-node="mi" transform="translate(1678.6, 0)"><use xlink:href="#MJX-2-TEX-I-1D457"></use></g></g></g></svg></mjx-container>. Moreover, diagonal elements <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex" xmlns="http://www.w3.org/2000/svg" width="2.934ex" height="1.902ex" role="img" focusable="false" viewBox="0 -683 1296.9 840.8" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-2-TEX-I-1D445" d="M230 637Q203 637 198 638T193 649Q193 676 204 682Q206 683 378 683Q550 682 564 680Q620 672 658 652T712 606T733 563T739 529Q739 484 710 445T643 385T576 351T538 338L545 333Q612 295 612 223Q612 212 607 162T602 80V71Q602 53 603 43T614 25T640 16Q668 16 686 38T712 85Q717 99 720 102T735 105Q755 105 755 93Q755 75 731 36Q693 -21 641 -21H632Q571 -21 531 4T487 82Q487 109 502 166T517 239Q517 290 474 313Q459 320 449 321T378 323H309L277 193Q244 61 244 59Q244 55 245 54T252 50T269 48T302 46H333Q339 38 339 37T336 19Q332 6 326 0H311Q275 2 180 2Q146 2 117 2T71 2T50 1Q33 1 33 10Q33 12 36 24Q41 43 46 45Q50 46 61 46H67Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628Q287 635 230 637ZM630 554Q630 586 609 608T523 636Q521 636 500 636T462 637H440Q393 637 386 627Q385 624 352 494T319 361Q319 360 388 360Q466 361 492 367Q556 377 592 426Q608 449 619 486T630 554Z"></path><path id="MJX-2-TEX-I-1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D445"></use></g><g data-mml-node="TeXAtom" transform="translate(759, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D456"></use></g><g data-mml-node="mi" transform="translate(345, 0)"><use xlink:href="#MJX-2-TEX-I-1D456"></use></g></g></g></g></g></svg></mjx-container> are nonzero (<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.048ex" xmlns="http://www.w3.org/2000/svg" width="1.717ex" height="1.593ex" role="img" focusable="false" viewBox="0 -683 759 704" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-2-TEX-I-1D445" d="M230 637Q203 637 198 638T193 649Q193 676 204 682Q206 683 378 683Q550 682 564 680Q620 672 658 652T712 606T733 563T739 529Q739 484 710 445T643 385T576 351T538 338L545 333Q612 295 612 223Q612 212 607 162T602 80V71Q602 53 603 43T614 25T640 16Q668 16 686 38T712 85Q717 99 720 102T735 105Q755 105 755 93Q755 75 731 36Q693 -21 641 -21H632Q571 -21 531 4T487 82Q487 109 502 166T517 239Q517 290 474 313Q459 320 449 321T378 323H309L277 193Q244 61 244 59Q244 55 245 54T252 50T269 48T302 46H333Q339 38 339 37T336 19Q332 6 326 0H311Q275 2 180 2Q146 2 117 2T71 2T50 1Q33 1 33 10Q33 12 36 24Q41 43 46 45Q50 46 61 46H67Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628Q287 635 230 637ZM630 554Q630 586 609 608T523 636Q521 636 500 636T462 637H440Q393 637 386 627Q385 624 352 494T319 361Q319 360 388 360Q466 361 492 367Q556 377 592 426Q608 449 619 486T630 554Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D445"></use></g></g></g></svg></mjx-container> is nonsingular), and most definitions require <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex" xmlns="http://www.w3.org/2000/svg" width="7.082ex" height="1.902ex" role="img" focusable="false" viewBox="0 -683 3130.5 840.8" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-2-TEX-I-1D445" d="M230 637Q203 637 198 638T193 649Q193 676 204 682Q206 683 378 683Q550 682 564 680Q620 672 658 652T712 606T733 563T739 529Q739 484 710 445T643 385T576 351T538 338L545 333Q612 295 612 223Q612 212 607 162T602 80V71Q602 53 603 43T614 25T640 16Q668 16 686 38T712 85Q717 99 720 102T735 105Q755 105 755 93Q755 75 731 36Q693 -21 641 -21H632Q571 -21 531 4T487 82Q487 109 502 166T517 239Q517 290 474 313Q459 320 449 321T378 323H309L277 193Q244 61 244 59Q244 55 245 54T252 50T269 48T302 46H333Q339 38 339 37T336 19Q332 6 326 0H311Q275 2 180 2Q146 2 117 2T71 2T50 1Q33 1 33 10Q33 12 36 24Q41 43 46 45Q50 46 61 46H67Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628Q287 635 230 637ZM630 554Q630 586 609 608T523 636Q521 636 500 636T462 637H440Q393 637 386 627Q385 624 352 494T319 361Q319 360 388 360Q466 361 492 367Q556 377 592 426Q608 449 619 486T630 554Z"></path><path id="MJX-2-TEX-I-1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path><path id="MJX-2-TEX-N-3E" d="M84 520Q84 528 88 533T96 539L99 540Q106 540 253 471T544 334L687 265Q694 260 694 250T687 235Q685 233 395 96L107 -40H101Q83 -38 83 -20Q83 -19 83 -17Q82 -10 98 -1Q117 9 248 71Q326 108 378 132L626 250L378 368Q90 504 86 509Q84 513 84 520Z"></path><path id="MJX-2-TEX-N-30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D445"></use></g><g data-mml-node="TeXAtom" transform="translate(759, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D456"></use></g><g data-mml-node="mi" transform="translate(345, 0)"><use xlink:href="#MJX-2-TEX-I-1D456"></use></g></g></g><g data-mml-node="mo" transform="translate(1574.7, 0)"><use xlink:href="#MJX-2-TEX-N-3E"></use></g><g data-mml-node="mn" transform="translate(2630.5, 0)"><use xlink:href="#MJX-2-TEX-N-30"></use></g></g></g></svg></mjx-container>, this makes <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.439ex" xmlns="http://www.w3.org/2000/svg" width="1.79ex" height="2.032ex" role="img" focusable="false" viewBox="0 -704 791 898" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-2-TEX-I-1D444" d="M399 -80Q399 -47 400 -30T402 -11V-7L387 -11Q341 -22 303 -22Q208 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435Q740 255 592 107Q529 47 461 16L444 8V3Q444 2 449 -24T470 -66T516 -82Q551 -82 583 -60T625 -3Q631 11 638 11Q647 11 649 2Q649 -6 639 -34T611 -100T557 -165T481 -194Q399 -194 399 -87V-80ZM636 468Q636 523 621 564T580 625T530 655T477 665Q429 665 379 640Q277 591 215 464T153 216Q153 110 207 59Q231 38 236 38V46Q236 86 269 120T347 155Q372 155 390 144T417 114T429 82T435 55L448 64Q512 108 557 185T619 334T636 468ZM314 18Q362 18 404 39L403 49Q399 104 366 115Q354 117 347 117Q344 117 341 117T337 118Q317 118 296 98T274 52Q274 18 314 18Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D444"></use></g></g></g></svg></mjx-container> and <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.048ex" xmlns="http://www.w3.org/2000/svg" width="1.717ex" height="1.593ex" role="img" focusable="false" viewBox="0 -683 759 704" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-2-TEX-I-1D445" d="M230 637Q203 637 198 638T193 649Q193 676 204 682Q206 683 378 683Q550 682 564 680Q620 672 658 652T712 606T733 563T739 529Q739 484 710 445T643 385T576 351T538 338L545 333Q612 295 612 223Q612 212 607 162T602 80V71Q602 53 603 43T614 25T640 16Q668 16 686 38T712 85Q717 99 720 102T735 105Q755 105 755 93Q755 75 731 36Q693 -21 641 -21H632Q571 -21 531 4T487 82Q487 109 502 166T517 239Q517 290 474 313Q459 320 449 321T378 323H309L277 193Q244 61 244 59Q244 55 245 54T252 50T269 48T302 46H333Q339 38 339 37T336 19Q332 6 326 0H311Q275 2 180 2Q146 2 117 2T71 2T50 1Q33 1 33 10Q33 12 36 24Q41 43 46 45Q50 46 61 46H67Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628Q287 635 230 637ZM630 554Q630 586 609 608T523 636Q521 636 500 636T462 637H440Q393 637 386 627Q385 624 352 494T319 361Q319 360 388 360Q466 361 492 367Q556 377 592 426Q608 449 619 486T630 554Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D445"></use></g></g></g></svg></mjx-container> unique.</p><p>Before dive into how to calculate QR factorisation, we should know what problem or application we can tackle with or apply for.</p><ul><li>Linear equations</li><li>Generalised linear regression model </li><li>Singular-value decomposition in the Jacobi-Kogbetliantz approach</li><li>Automatic removal of an object from an image</li></ul><h2 id="Algorithms-for-QR"><a href="#Algorithms-for-QR" class="headerlink" title="Algorithms for QR"></a>Algorithms for QR</h2><ol><li>Gram-Schmidt process</li></ol><ul><li>Complexity is <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="5.388ex" height="1.912ex" role="img" focusable="false" viewBox="0 -833.9 2381.6 844.9" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-2-TEX-N-32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path><path id="MJX-2-TEX-I-1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-2-TEX-I-1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-32"></use></g><g data-mml-node="mi" transform="translate(500, 0)"><use xlink:href="#MJX-2-TEX-I-1D45A"></use></g><g data-mml-node="msup" transform="translate(1378, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D45B"></use></g><g data-mml-node="TeXAtom" transform="translate(600, 363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-32"></use></g></g></g></g></g></svg></mjx-container> flops</li><li>Not recommended in practice (sensitive to rounding errors)</li></ul><ol start="2"><li>Modified Gram-Schmidt process</li></ol><ul><li>Complexity is <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="5.388ex" height="1.912ex" role="img" focusable="false" viewBox="0 -833.9 2381.6 844.9" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-2-TEX-N-32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path><path id="MJX-2-TEX-I-1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-2-TEX-I-1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-32"></use></g><g data-mml-node="mi" transform="translate(500, 0)"><use xlink:href="#MJX-2-TEX-I-1D45A"></use></g><g data-mml-node="msup" transform="translate(1378, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D45B"></use></g><g data-mml-node="TeXAtom" transform="translate(600, 363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-32"></use></g></g></g></g></g></svg></mjx-container> flops</li><li>Better numerical properties</li></ul><ol start="3"><li>Householder transformations</li></ol><ul><li>Complexity is <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.816ex" xmlns="http://www.w3.org/2000/svg" width="12.22ex" height="2.773ex" role="img" focusable="false" viewBox="0 -864.9 5401.1 1225.5" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-2-TEX-N-32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path><path id="MJX-2-TEX-I-1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-2-TEX-I-1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-2-TEX-N-2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path><path id="MJX-2-TEX-N-33" d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-32"></use></g><g data-mml-node="mi" transform="translate(500, 0)"><use xlink:href="#MJX-2-TEX-I-1D45A"></use></g><g data-mml-node="msup" transform="translate(1378, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D45B"></use></g><g data-mml-node="TeXAtom" transform="translate(600, 363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-32"></use></g></g></g><g data-mml-node="mo" transform="translate(2603.8, 0)"><use xlink:href="#MJX-2-TEX-N-2212"></use></g><g data-mml-node="mfrac" transform="translate(3604, 0)"><g data-mml-node="mn" transform="translate(220, 394) scale(0.707)"><use xlink:href="#MJX-2-TEX-N-32"></use></g><g data-mml-node="mn" transform="translate(220, -345) scale(0.707)"><use xlink:href="#MJX-2-TEX-N-33"></use></g><rect width="553.6" height="60" x="120" y="220"></rect></g><g data-mml-node="msup" transform="translate(4397.6, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D45B"></use></g><g data-mml-node="TeXAtom" transform="translate(600, 363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-33"></use></g></g></g></g></g></svg></mjx-container> flops</li><li>Represents <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.439ex" xmlns="http://www.w3.org/2000/svg" width="1.79ex" height="2.032ex" role="img" focusable="false" viewBox="0 -704 791 898" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-2-TEX-I-1D444" d="M399 -80Q399 -47 400 -30T402 -11V-7L387 -11Q341 -22 303 -22Q208 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435Q740 255 592 107Q529 47 461 16L444 8V3Q444 2 449 -24T470 -66T516 -82Q551 -82 583 -60T625 -3Q631 11 638 11Q647 11 649 2Q649 -6 639 -34T611 -100T557 -165T481 -194Q399 -194 399 -87V-80ZM636 468Q636 523 621 564T580 625T530 655T477 665Q429 665 379 640Q277 591 215 464T153 216Q153 110 207 59Q231 38 236 38V46Q236 86 269 120T347 155Q372 155 390 144T417 114T429 82T435 55L448 64Q512 108 557 185T619 334T636 468ZM314 18Q362 18 404 39L403 49Q399 104 366 115Q354 117 347 117Q344 117 341 117T337 118Q317 118 296 98T274 52Q274 18 314 18Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D444"></use></g></g></g></svg></mjx-container> as a product of elementary orthogonal matrices</li><li>The most widely used algorithm</li></ul><h3 id="Gram-Schmidt-Process"><a href="#Gram-Schmidt-Process" class="headerlink" title="Gram-Schmidt Process"></a>Gram-Schmidt Process</h3><p>The goal of Gram-Schmidt process is to calculate orthogonal basis <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.439ex" xmlns="http://www.w3.org/2000/svg" width="12.828ex" height="2.249ex" role="img" focusable="false" viewBox="0 -800 5670 994" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-2-TEX-I-1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-2-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-2-TEX-N-20D7" d="M377 694Q377 702 382 708T397 714Q404 714 409 709Q414 705 419 690Q429 653 460 633Q471 626 471 615Q471 606 468 603T454 594Q411 572 379 531Q377 529 374 525T369 519T364 517T357 516Q350 516 344 521T337 536Q337 555 384 595H213L42 596Q29 605 29 615Q29 622 42 635H401Q377 673 377 694Z"></path><path id="MJX-2-TEX-N-2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path><path id="MJX-2-TEX-N-32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path><path id="MJX-2-TEX-N-22EF" d="M78 250Q78 274 95 292T138 310Q162 310 180 294T199 251Q199 226 182 208T139 190T96 207T78 250ZM525 250Q525 274 542 292T585 310Q609 310 627 294T646 251Q646 226 629 208T586 190T543 207T525 250ZM972 250Q972 274 989 292T1032 310Q1056 310 1074 294T1093 251Q1093 226 1076 208T1033 190T990 207T972 250Z"></path><path id="MJX-2-TEX-I-1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mover"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D462"></use></g><g data-mml-node="TeXAtom" transform="translate(572, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-31"></use></g></g></g><g data-mml-node="mo" transform="translate(265.6, -14)"><use xlink:href="#MJX-2-TEX-N-20D7"></use></g></g></g><g data-mml-node="mo" transform="translate(975.6, 0)"><use xlink:href="#MJX-2-TEX-N-2C"></use></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(1420.2, 0)"><g data-mml-node="mover"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D462"></use></g><g data-mml-node="TeXAtom" transform="translate(572, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-32"></use></g></g></g><g data-mml-node="mo" transform="translate(265.6, -14)"><use xlink:href="#MJX-2-TEX-N-20D7"></use></g></g></g><g data-mml-node="mo" transform="translate(2395.8, 0)"><use xlink:href="#MJX-2-TEX-N-2C"></use></g><g data-mml-node="mo" transform="translate(2840.4, 0)"><use xlink:href="#MJX-2-TEX-N-22EF"></use></g><g data-mml-node="mo" transform="translate(4179.1, 0)"><use xlink:href="#MJX-2-TEX-N-2C"></use></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(4623.8, 0)"><g data-mml-node="mover"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D462"></use></g><g data-mml-node="TeXAtom" transform="translate(572, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D45B"></use></g></g></g><g data-mml-node="mo" transform="translate(300.9, -14)"><use xlink:href="#MJX-2-TEX-N-20D7"></use></g></g></g></g></g></svg></mjx-container> from original basis <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.439ex" xmlns="http://www.w3.org/2000/svg" width="12.238ex" height="2.251ex" role="img" focusable="false" viewBox="0 -801 5409 995" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-2-TEX-I-1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path><path id="MJX-2-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-2-TEX-N-20D7" d="M377 694Q377 702 382 708T397 714Q404 714 409 709Q414 705 419 690Q429 653 460 633Q471 626 471 615Q471 606 468 603T454 594Q411 572 379 531Q377 529 374 525T369 519T364 517T357 516Q350 516 344 521T337 536Q337 555 384 595H213L42 596Q29 605 29 615Q29 622 42 635H401Q377 673 377 694Z"></path><path id="MJX-2-TEX-N-2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path><path id="MJX-2-TEX-N-32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path><path id="MJX-2-TEX-N-22EF" d="M78 250Q78 274 95 292T138 310Q162 310 180 294T199 251Q199 226 182 208T139 190T96 207T78 250ZM525 250Q525 274 542 292T585 310Q609 310 627 294T646 251Q646 226 629 208T586 190T543 207T525 250ZM972 250Q972 274 989 292T1032 310Q1056 310 1074 294T1093 251Q1093 226 1076 208T1033 190T990 207T972 250Z"></path><path id="MJX-2-TEX-I-1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mover"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D463"></use></g><g data-mml-node="TeXAtom" transform="translate(485, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-31"></use></g></g></g><g data-mml-node="mo" transform="translate(222.1, -13)"><use xlink:href="#MJX-2-TEX-N-20D7"></use></g></g></g><g data-mml-node="mo" transform="translate(888.6, 0)"><use xlink:href="#MJX-2-TEX-N-2C"></use></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(1333.2, 0)"><g data-mml-node="mover"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D463"></use></g><g data-mml-node="TeXAtom" transform="translate(485, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-32"></use></g></g></g><g data-mml-node="mo" transform="translate(222.1, -13)"><use xlink:href="#MJX-2-TEX-N-20D7"></use></g></g></g><g data-mml-node="mo" transform="translate(2221.8, 0)"><use xlink:href="#MJX-2-TEX-N-2C"></use></g><g data-mml-node="mo" transform="translate(2666.4, 0)"><use xlink:href="#MJX-2-TEX-N-22EF"></use></g><g data-mml-node="mo" transform="translate(4005.1, 0)"><use xlink:href="#MJX-2-TEX-N-2C"></use></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(4449.8, 0)"><g data-mml-node="mover"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D463"></use></g><g data-mml-node="TeXAtom" transform="translate(485, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D45B"></use></g></g></g><g data-mml-node="mo" transform="translate(257.4, -13)"><use xlink:href="#MJX-2-TEX-N-20D7"></use></g></g></g></g></g></svg></mjx-container>, and this can also be represented in summation notation:</p><div style="display: flex;justify-content: center;">    <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -1.318ex" xmlns="http://www.w3.org/2000/svg" width="22.41ex" height="3.621ex" role="img" focusable="false" viewBox="0 -1018 9905.3 1600.4" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-2-TEX-I-1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-2-TEX-I-1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path><path id="MJX-2-TEX-N-20D7" d="M377 694Q377 702 382 708T397 714Q404 714 409 709Q414 705 419 690Q429 653 460 633Q471 626 471 615Q471 606 468 603T454 594Q411 572 379 531Q377 529 374 525T369 519T364 517T357 516Q350 516 344 521T337 536Q337 555 384 595H213L42 596Q29 605 29 615Q29 622 42 635H401Q377 673 377 694Z"></path><path id="MJX-2-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-2-TEX-I-1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path><path id="MJX-2-TEX-N-2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path><path id="MJX-2-TEX-SO-2211" d="M61 748Q64 750 489 750H913L954 640Q965 609 976 579T993 533T999 516H979L959 517Q936 579 886 621T777 682Q724 700 655 705T436 710H319Q183 710 183 709Q186 706 348 484T511 259Q517 250 513 244L490 216Q466 188 420 134T330 27L149 -187Q149 -188 362 -188Q388 -188 436 -188T506 -189Q679 -189 778 -162T936 -43Q946 -27 959 6H999L913 -249L489 -250Q65 -250 62 -248Q56 -246 56 -239Q56 -234 118 -161Q186 -81 245 -11L428 206Q428 207 242 462L57 717L56 728Q56 744 61 748Z"></path><path id="MJX-2-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-2-TEX-I-1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path><path id="MJX-2-TEX-N-22C5" d="M78 250Q78 274 95 292T138 310Q162 310 180 294T199 251Q199 226 182 208T139 190T96 207T78 250Z"></path><path id="MJX-2-TEX-N-2016" d="M133 736Q138 750 153 750Q164 750 170 739Q172 735 172 250T170 -239Q164 -250 152 -250Q144 -250 138 -244L137 -243Q133 -241 133 -179T132 250Q132 731 133 736ZM329 739Q334 750 346 750Q353 750 361 744L362 743Q366 741 366 679T367 250T367 -178T362 -243L361 -244Q355 -250 347 -250Q335 -250 329 -239Q327 -235 327 250T329 739Z"></path><path id="MJX-2-TEX-N-32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mover"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D462"></use></g><g data-mml-node="TeXAtom" transform="translate(572, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D458"></use></g></g></g><g data-mml-node="mo" transform="translate(273, -14)"><use xlink:href="#MJX-2-TEX-N-20D7"></use></g></g></g><g data-mml-node="mo" transform="translate(1268.2, 0)"><use xlink:href="#MJX-2-TEX-N-3D"></use></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(2324, 0)"><g data-mml-node="mover"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D463"></use></g><g data-mml-node="TeXAtom" transform="translate(485, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D458"></use></g></g></g><g data-mml-node="mo" transform="translate(229.5, -13)"><use xlink:href="#MJX-2-TEX-N-20D7"></use></g></g></g><g data-mml-node="mo" transform="translate(3449.6, 0)"><use xlink:href="#MJX-2-TEX-N-2212"></use></g><g data-mml-node="munderover" transform="translate(4449.8, 0)"><g data-mml-node="mo"><use xlink:href="#MJX-2-TEX-SO-2211"></use></g><g data-mml-node="TeXAtom" transform="translate(1056, 477.1) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D458"></use></g><g data-mml-node="mo" transform="translate(521, 0)"><use xlink:href="#MJX-2-TEX-N-2212"></use></g><g data-mml-node="mn" transform="translate(1299, 0)"><use xlink:href="#MJX-2-TEX-N-31"></use></g></g><g data-mml-node="TeXAtom" transform="translate(1056, -285.4) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D456"></use></g><g data-mml-node="mo" transform="translate(345, 0)"><use xlink:href="#MJX-2-TEX-N-3D"></use></g><g data-mml-node="mn" transform="translate(1123, 0)"><use xlink:href="#MJX-2-TEX-N-31"></use></g></g></g><g data-mml-node="mfrac" transform="translate(6994.6, 0)"><g data-mml-node="mrow" transform="translate(298.5, 451.6) scale(0.707)"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mover"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D463"></use></g><g data-mml-node="TeXAtom" transform="translate(485, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D458"></use></g></g></g><g data-mml-node="mo" transform="translate(229.5, -13)"><use xlink:href="#MJX-2-TEX-N-20D7"></use></g></g></g><g data-mml-node="mo" transform="translate(903.4, 0)"><use xlink:href="#MJX-2-TEX-N-22C5"></use></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(1181.4, 0)"><g data-mml-node="mover"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D462"></use></g><g data-mml-node="TeXAtom" transform="translate(572, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D456"></use></g></g></g><g data-mml-node="mo" transform="translate(210.8, -14)"><use xlink:href="#MJX-2-TEX-N-20D7"></use></g></g></g></g><g data-mml-node="mrow" transform="translate(220, -405.7) scale(0.707)"><g data-mml-node="mo"><use xlink:href="#MJX-2-TEX-N-2016"></use></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(500, 0)"><g data-mml-node="mover"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D462"></use></g><g data-mml-node="TeXAtom" transform="translate(572, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D456"></use></g></g></g><g data-mml-node="mo" transform="translate(210.8, -14)"><use xlink:href="#MJX-2-TEX-N-20D7"></use></g></g></g><g data-mml-node="msup" transform="translate(1366, 0)"><g data-mml-node="mo"><use xlink:href="#MJX-2-TEX-N-2016"></use></g><g data-mml-node="mn" transform="translate(500, 289) scale(0.707)"><use xlink:href="#MJX-2-TEX-N-32"></use></g></g></g><rect width="1804.8" height="60" x="120" y="220"></rect></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(9039.3, 0)"><g data-mml-node="mover"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D462"></use></g><g data-mml-node="TeXAtom" transform="translate(572, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D456"></use></g></g></g><g data-mml-node="mo" transform="translate(210.8, -14)"><use xlink:href="#MJX-2-TEX-N-20D7"></use></g></g></g></g></g></svg></mjx-container></div><p>A full calculation process can be found in this youtube <a href="https://www.youtube.com/watch?v=zHbfZWZJTGc&ab_channel=ProfessorDaveExplains">video</a> presented by Dave.</p><h4 id="Matlab-code"><a href="#Matlab-code" class="headerlink" title="Matlab code"></a>Matlab code</h4><pre class="line-numbers language-matlab" data-language="matlab"><code class="language-matlab"><span class="token keyword">function</span> <span class="token punctuation">[</span>Q<span class="token punctuation">,</span> R<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token function">gram_schmidt_qr</span><span class="token punctuation">(</span>A<span class="token punctuation">)</span>    <span class="token punctuation">[</span>m<span class="token punctuation">,</span> n<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token function">size</span><span class="token punctuation">(</span>A<span class="token punctuation">)</span><span class="token punctuation">;</span>    Q <span class="token operator">=</span> <span class="token function">zeros</span><span class="token punctuation">(</span>m<span class="token punctuation">,</span> n<span class="token punctuation">)</span><span class="token punctuation">;</span>    R <span class="token operator">=</span> <span class="token function">zeros</span><span class="token punctuation">(</span>n<span class="token punctuation">,</span> n<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">for</span> <span class="token number">j</span> <span class="token operator">=</span> <span class="token number">1</span><span class="token operator">:</span>n        <span class="token function">R</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token operator">:</span><span class="token number">j</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">j</span><span class="token punctuation">)</span> <span class="token operator">=</span> <span class="token function">Q</span><span class="token punctuation">(</span><span class="token operator">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token operator">:</span><span class="token number">j</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token operator">'</span> <span class="token operator">*</span> <span class="token function">A</span><span class="token punctuation">(</span><span class="token operator">:</span><span class="token punctuation">,</span> <span class="token number">j</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        v <span class="token operator">=</span> <span class="token function">A</span><span class="token punctuation">(</span><span class="token operator">:</span><span class="token punctuation">,</span> <span class="token number">j</span><span class="token punctuation">)</span> <span class="token operator">-</span> <span class="token function">Q</span><span class="token punctuation">(</span><span class="token operator">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token operator">:</span><span class="token number">j</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token function">R</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token operator">:</span><span class="token number">j</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">j</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">R</span><span class="token punctuation">(</span><span class="token number">j</span><span class="token punctuation">,</span> <span class="token number">j</span><span class="token punctuation">)</span> <span class="token operator">=</span> <span class="token function">norm</span><span class="token punctuation">(</span>v<span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">Q</span><span class="token punctuation">(</span><span class="token operator">:</span><span class="token punctuation">,</span> <span class="token number">j</span><span class="token punctuation">)</span> <span class="token operator">=</span> v <span class="token operator">/</span> <span class="token function">R</span><span class="token punctuation">(</span><span class="token number">j</span><span class="token punctuation">,</span> <span class="token number">j</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">end</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="Python-code"><a href="#Python-code" class="headerlink" title="Python code"></a>Python code</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Decomposition</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">pass</span>    <span class="token keyword">def</span> <span class="token function">gram_schmidt_qr</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> A<span class="token punctuation">)</span><span class="token punctuation">:</span>        m<span class="token punctuation">,</span> n <span class="token operator">=</span> A<span class="token punctuation">.</span>shape        Q <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>shape<span class="token operator">=</span><span class="token punctuation">(</span>m<span class="token punctuation">,</span> n<span class="token punctuation">)</span><span class="token punctuation">)</span>        R <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>shape<span class="token operator">=</span><span class="token punctuation">(</span>n<span class="token punctuation">,</span> n<span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token keyword">for</span> j <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>n<span class="token punctuation">)</span><span class="token punctuation">:</span>            v <span class="token operator">=</span> A<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> j<span class="token punctuation">]</span>            <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>j<span class="token punctuation">)</span><span class="token punctuation">:</span>                q <span class="token operator">=</span> Q<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> i<span class="token punctuation">]</span>                R<span class="token punctuation">[</span>i<span class="token punctuation">,</span> j<span class="token punctuation">]</span> <span class="token operator">=</span> q<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>v<span class="token punctuation">)</span>                v <span class="token operator">=</span> v <span class="token operator">-</span> R<span class="token punctuation">[</span>i<span class="token punctuation">,</span> j<span class="token punctuation">]</span> <span class="token operator">*</span> q            Q<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> j<span class="token punctuation">]</span> <span class="token operator">=</span> v <span class="token operator">/</span> np<span class="token punctuation">.</span>linalg<span class="token punctuation">.</span>norm<span class="token punctuation">(</span>v<span class="token punctuation">)</span>            R<span class="token punctuation">[</span>j<span class="token punctuation">,</span> j<span class="token punctuation">]</span> <span class="token operator">=</span> np<span class="token punctuation">.</span>linalg<span class="token punctuation">.</span>norm<span class="token punctuation">(</span>v<span class="token punctuation">)</span>        <span class="token keyword">return</span> Q<span class="token punctuation">,</span> R<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="Modified-Gram-Schmidt-Process"><a href="#Modified-Gram-Schmidt-Process" class="headerlink" title="Modified Gram-Schmidt Process"></a>Modified Gram-Schmidt Process</h3><p>In 1966 John Rice showed by experiments that the two different versions of the Gram–Schmidt orthogonalization, classical (CGS) and modified (MGS) have very different properties when executed in finite precision arithmetic. Only for n = 2 are CGS and MGS numerically equivalent.</p><p>Instead of computing the vector <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex" xmlns="http://www.w3.org/2000/svg" width="2.241ex" height="2.167ex" role="img" focusable="false" viewBox="0 -800 990.4 957.8" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-1-TEX-I-1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path><path id="MJX-1-TEX-N-20D7" d="M377 694Q377 702 382 708T397 714Q404 714 409 709Q414 705 419 690Q429 653 460 633Q471 626 471 615Q471 606 468 603T454 594Q411 572 379 531Q377 529 374 525T369 519T364 517T357 516Q350 516 344 521T337 536Q337 555 384 595H213L42 596Q29 605 29 615Q29 622 42 635H401Q377 673 377 694Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mover"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D462"></use></g><g data-mml-node="TeXAtom" transform="translate(572, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D458"></use></g></g></g><g data-mml-node="mo" transform="translate(273, -14)"><use xlink:href="#MJX-1-TEX-N-20D7"></use></g></g></g></g></g></svg></mjx-container> as <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -1.318ex" xmlns="http://www.w3.org/2000/svg" width="22.41ex" height="3.621ex" role="img" focusable="false" viewBox="0 -1018 9905.3 1600.4" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-1-TEX-I-1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path><path id="MJX-1-TEX-N-20D7" d="M377 694Q377 702 382 708T397 714Q404 714 409 709Q414 705 419 690Q429 653 460 633Q471 626 471 615Q471 606 468 603T454 594Q411 572 379 531Q377 529 374 525T369 519T364 517T357 516Q350 516 344 521T337 536Q337 555 384 595H213L42 596Q29 605 29 615Q29 622 42 635H401Q377 673 377 694Z"></path><path id="MJX-1-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-1-TEX-I-1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path><path id="MJX-1-TEX-N-2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path><path id="MJX-1-TEX-SO-2211" d="M61 748Q64 750 489 750H913L954 640Q965 609 976 579T993 533T999 516H979L959 517Q936 579 886 621T777 682Q724 700 655 705T436 710H319Q183 710 183 709Q186 706 348 484T511 259Q517 250 513 244L490 216Q466 188 420 134T330 27L149 -187Q149 -188 362 -188Q388 -188 436 -188T506 -189Q679 -189 778 -162T936 -43Q946 -27 959 6H999L913 -249L489 -250Q65 -250 62 -248Q56 -246 56 -239Q56 -234 118 -161Q186 -81 245 -11L428 206Q428 207 242 462L57 717L56 728Q56 744 61 748Z"></path><path id="MJX-1-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-1-TEX-I-1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path><path id="MJX-1-TEX-N-22C5" d="M78 250Q78 274 95 292T138 310Q162 310 180 294T199 251Q199 226 182 208T139 190T96 207T78 250Z"></path><path id="MJX-1-TEX-N-2016" d="M133 736Q138 750 153 750Q164 750 170 739Q172 735 172 250T170 -239Q164 -250 152 -250Q144 -250 138 -244L137 -243Q133 -241 133 -179T132 250Q132 731 133 736ZM329 739Q334 750 346 750Q353 750 361 744L362 743Q366 741 366 679T367 250T367 -178T362 -243L361 -244Q355 -250 347 -250Q335 -250 329 -239Q327 -235 327 250T329 739Z"></path><path id="MJX-1-TEX-N-32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mover"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D462"></use></g><g data-mml-node="TeXAtom" transform="translate(572, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D458"></use></g></g></g><g data-mml-node="mo" transform="translate(273, -14)"><use xlink:href="#MJX-1-TEX-N-20D7"></use></g></g></g><g data-mml-node="mo" transform="translate(1268.2, 0)"><use xlink:href="#MJX-1-TEX-N-3D"></use></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(2324, 0)"><g data-mml-node="mover"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D463"></use></g><g data-mml-node="TeXAtom" transform="translate(485, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D458"></use></g></g></g><g data-mml-node="mo" transform="translate(229.5, -13)"><use xlink:href="#MJX-1-TEX-N-20D7"></use></g></g></g><g data-mml-node="mo" transform="translate(3449.6, 0)"><use xlink:href="#MJX-1-TEX-N-2212"></use></g><g data-mml-node="munderover" transform="translate(4449.8, 0)"><g data-mml-node="mo"><use xlink:href="#MJX-1-TEX-SO-2211"></use></g><g data-mml-node="TeXAtom" transform="translate(1056, 477.1) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D458"></use></g><g data-mml-node="mo" transform="translate(521, 0)"><use xlink:href="#MJX-1-TEX-N-2212"></use></g><g data-mml-node="mn" transform="translate(1299, 0)"><use xlink:href="#MJX-1-TEX-N-31"></use></g></g><g data-mml-node="TeXAtom" transform="translate(1056, -285.4) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D456"></use></g><g data-mml-node="mo" transform="translate(345, 0)"><use xlink:href="#MJX-1-TEX-N-3D"></use></g><g data-mml-node="mn" transform="translate(1123, 0)"><use xlink:href="#MJX-1-TEX-N-31"></use></g></g></g><g data-mml-node="mfrac" transform="translate(6994.6, 0)"><g data-mml-node="mrow" transform="translate(298.5, 451.6) scale(0.707)"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mover"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D463"></use></g><g data-mml-node="TeXAtom" transform="translate(485, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D458"></use></g></g></g><g data-mml-node="mo" transform="translate(229.5, -13)"><use xlink:href="#MJX-1-TEX-N-20D7"></use></g></g></g><g data-mml-node="mo" transform="translate(903.4, 0)"><use xlink:href="#MJX-1-TEX-N-22C5"></use></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(1181.4, 0)"><g data-mml-node="mover"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D462"></use></g><g data-mml-node="TeXAtom" transform="translate(572, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D456"></use></g></g></g><g data-mml-node="mo" transform="translate(210.8, -14)"><use xlink:href="#MJX-1-TEX-N-20D7"></use></g></g></g></g><g data-mml-node="mrow" transform="translate(220, -405.7) scale(0.707)"><g data-mml-node="mo"><use xlink:href="#MJX-1-TEX-N-2016"></use></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(500, 0)"><g data-mml-node="mover"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D462"></use></g><g data-mml-node="TeXAtom" transform="translate(572, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D456"></use></g></g></g><g data-mml-node="mo" transform="translate(210.8, -14)"><use xlink:href="#MJX-1-TEX-N-20D7"></use></g></g></g><g data-mml-node="msup" transform="translate(1366, 0)"><g data-mml-node="mo"><use xlink:href="#MJX-1-TEX-N-2016"></use></g><g data-mml-node="mn" transform="translate(500, 289) scale(0.707)"><use xlink:href="#MJX-1-TEX-N-32"></use></g></g></g><rect width="1804.8" height="60" x="120" y="220"></rect></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(9039.3, 0)"><g data-mml-node="mover"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D462"></use></g><g data-mml-node="TeXAtom" transform="translate(572, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D456"></use></g></g></g><g data-mml-node="mo" transform="translate(210.8, -14)"><use xlink:href="#MJX-1-TEX-N-20D7"></use></g></g></g></g></g></svg></mjx-container>, it is computed as:</p><div style="display: flex;justify-content: center;">    <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -11.559ex" xmlns="http://www.w3.org/2000/svg" width="34.177ex" height="24.25ex" role="img" focusable="false" viewBox="0 -5609.2 15106.4 10718.4" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-1-TEX-I-1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path><path id="MJX-1-TEX-N-20D7" d="M377 694Q377 702 382 708T397 714Q404 714 409 709Q414 705 419 690Q429 653 460 633Q471 626 471 615Q471 606 468 603T454 594Q411 572 379 531Q377 529 374 525T369 519T364 517T357 516Q350 516 344 521T337 536Q337 555 384 595H213L42 596Q29 605 29 615Q29 622 42 635H401Q377 673 377 694Z"></path><path id="MJX-1-TEX-N-28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path><path id="MJX-1-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-1-TEX-N-29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path><path id="MJX-1-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-1-TEX-I-1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path><path id="MJX-1-TEX-N-2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path><path id="MJX-1-TEX-I-1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"></path><path id="MJX-1-TEX-I-1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-1-TEX-I-1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path><path id="MJX-1-TEX-I-1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path><path id="MJX-1-TEX-N-32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path><path id="MJX-1-TEX-N-22EE" d="M78 30Q78 54 95 72T138 90Q162 90 180 74T199 31Q199 6 182 -12T139 -30T96 -13T78 30ZM78 440Q78 464 95 482T138 500Q162 500 180 484T199 441Q199 416 182 398T139 380T96 397T78 440ZM78 840Q78 864 95 882T138 900Q162 900 180 884T199 841Q199 816 182 798T139 780T96 797T78 840Z"></path><path id="MJX-1-TEX-N-2016" d="M133 736Q138 750 153 750Q164 750 170 739Q172 735 172 250T170 -239Q164 -250 152 -250Q144 -250 138 -244L137 -243Q133 -241 133 -179T132 250Q132 731 133 736ZM329 739Q334 750 346 750Q353 750 361 744L362 743Q366 741 366 679T367 250T367 -178T362 -243L361 -244Q355 -250 347 -250Q335 -250 329 -239Q327 -235 327 250T329 739Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mtable"><g data-mml-node="mtr"><g data-mml-node="mtd"><g data-mml-node="mtable"><g data-mml-node="mtr" transform="translate(0, 4551.8)"><g data-mml-node="mtd" transform="translate(459.3, 0)"><g data-mml-node="msup"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mover"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D462"></use></g><g data-mml-node="TeXAtom" transform="translate(572, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D458"></use></g></g></g><g data-mml-node="mo" transform="translate(273, -14)"><use xlink:href="#MJX-1-TEX-N-20D7"></use></g></g></g><g data-mml-node="TeXAtom" transform="translate(990.4, 527.1) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mo"><use xlink:href="#MJX-1-TEX-N-28"></use></g><g data-mml-node="mn" transform="translate(389, 0)"><use xlink:href="#MJX-1-TEX-N-31"></use></g><g data-mml-node="mo" transform="translate(889, 0)"><use xlink:href="#MJX-1-TEX-N-29"></use></g></g></g></g><g data-mml-node="mtd" transform="translate(6146.6, 0)"><g data-mml-node="mo"><use xlink:href="#MJX-1-TEX-N-3D"></use></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(1055.8, 0)"><g data-mml-node="mover"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D463"></use></g><g data-mml-node="TeXAtom" transform="translate(485, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D458"></use></g></g></g><g data-mml-node="mo" transform="translate(229.5, -13)"><use xlink:href="#MJX-1-TEX-N-20D7"></use></g></g></g><g data-mml-node="mo" transform="translate(2181.4, 0)"><use xlink:href="#MJX-1-TEX-N-2212"></use></g><g data-mml-node="mi" transform="translate(3181.6, 0)"><use xlink:href="#MJX-1-TEX-I-1D45D"></use></g><g data-mml-node="mi" transform="translate(3684.6, 0)"><use xlink:href="#MJX-1-TEX-I-1D45F"></use></g><g data-mml-node="mi" transform="translate(4135.6, 0)"><use xlink:href="#MJX-1-TEX-I-1D45C"></use></g><g data-mml-node="msub" transform="translate(4620.6, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D457"></use></g><g data-mml-node="TeXAtom" transform="translate(412, -212.1) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mover"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D462"></use></g><g data-mml-node="TeXAtom" transform="translate(572, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-31"></use></g></g></g><g data-mml-node="mo" transform="translate(265.6, -14)"><use xlink:href="#MJX-1-TEX-N-20D7"></use></g></g></g></g></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(5772.4, 0)"><g data-mml-node="mover"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D463"></use></g><g data-mml-node="TeXAtom" transform="translate(485, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D458"></use></g></g></g><g data-mml-node="mo" transform="translate(229.5, -13)"><use xlink:href="#MJX-1-TEX-N-20D7"></use></g></g></g></g></g><g data-mml-node="mtr" transform="translate(0, 2676.3)"><g data-mml-node="mtd" transform="translate(459.3, 0)"><g data-mml-node="msup"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mover"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D462"></use></g><g data-mml-node="TeXAtom" transform="translate(572, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D458"></use></g></g></g><g data-mml-node="mo" transform="translate(273, -14)"><use xlink:href="#MJX-1-TEX-N-20D7"></use></g></g></g><g data-mml-node="TeXAtom" transform="translate(990.4, 527.1) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mo"><use xlink:href="#MJX-1-TEX-N-28"></use></g><g data-mml-node="mn" transform="translate(389, 0)"><use xlink:href="#MJX-1-TEX-N-32"></use></g><g data-mml-node="mo" transform="translate(889, 0)"><use xlink:href="#MJX-1-TEX-N-29"></use></g></g></g></g><g data-mml-node="mtd" transform="translate(5105.9, 0)"><g data-mml-node="mo"><use xlink:href="#MJX-1-TEX-N-3D"></use></g><g data-mml-node="msup" transform="translate(1055.8, 0)"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mover"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D462"></use></g><g data-mml-node="TeXAtom" transform="translate(572, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D458"></use></g></g></g><g data-mml-node="mo" transform="translate(273, -14)"><use xlink:href="#MJX-1-TEX-N-20D7"></use></g></g></g><g data-mml-node="TeXAtom" transform="translate(990.4, 527.1) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mo"><use xlink:href="#MJX-1-TEX-N-28"></use></g><g data-mml-node="mn" transform="translate(389, 0)"><use xlink:href="#MJX-1-TEX-N-31"></use></g><g data-mml-node="mo" transform="translate(889, 0)"><use xlink:href="#MJX-1-TEX-N-29"></use></g></g></g><g data-mml-node="mo" transform="translate(3222.1, 0)"><use xlink:href="#MJX-1-TEX-N-2212"></use></g><g data-mml-node="mi" transform="translate(4222.3, 0)"><use xlink:href="#MJX-1-TEX-I-1D45D"></use></g><g data-mml-node="mi" transform="translate(4725.3, 0)"><use xlink:href="#MJX-1-TEX-I-1D45F"></use></g><g data-mml-node="mi" transform="translate(5176.3, 0)"><use xlink:href="#MJX-1-TEX-I-1D45C"></use></g><g data-mml-node="msub" transform="translate(5661.3, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D457"></use></g><g data-mml-node="TeXAtom" transform="translate(412, -212.1) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mover"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D462"></use></g><g data-mml-node="TeXAtom" transform="translate(572, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-32"></use></g></g></g><g data-mml-node="mo" transform="translate(265.6, -14)"><use xlink:href="#MJX-1-TEX-N-20D7"></use></g></g></g></g></g><g data-mml-node="msup" transform="translate(6813.1, 0)"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mover"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D462"></use></g><g data-mml-node="TeXAtom" transform="translate(572, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D458"></use></g></g></g><g data-mml-node="mo" transform="translate(273, -14)"><use xlink:href="#MJX-1-TEX-N-20D7"></use></g></g></g><g data-mml-node="TeXAtom" transform="translate(990.4, 527.1) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mo"><use xlink:href="#MJX-1-TEX-N-28"></use></g><g data-mml-node="mn" transform="translate(389, 0)"><use xlink:href="#MJX-1-TEX-N-31"></use></g><g data-mml-node="mo" transform="translate(889, 0)"><use xlink:href="#MJX-1-TEX-N-29"></use></g></g></g></g></g><g data-mml-node="mtr" transform="translate(0, 558.1)"><g data-mml-node="mtd" transform="translate(1292.3, 0)"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mo"><use xlink:href="#MJX-1-TEX-N-22EE"></use></g></g></g></g><g data-mml-node="mtr" transform="translate(0, -1249.3)"><g data-mml-node="mtd"><g data-mml-node="msup"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mover"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D462"></use></g><g data-mml-node="TeXAtom" transform="translate(572, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D458"></use></g></g></g><g data-mml-node="mo" transform="translate(273, -14)"><use xlink:href="#MJX-1-TEX-N-20D7"></use></g></g></g><g data-mml-node="TeXAtom" transform="translate(990.4, 527.1) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mo"><use xlink:href="#MJX-1-TEX-N-28"></use></g><g data-mml-node="mi" transform="translate(389, 0)"><use xlink:href="#MJX-1-TEX-I-1D458"></use></g><g data-mml-node="mo" transform="translate(910, 0)"><use xlink:href="#MJX-1-TEX-N-2212"></use></g><g data-mml-node="mn" transform="translate(1688, 0)"><use xlink:href="#MJX-1-TEX-N-31"></use></g><g data-mml-node="mo" transform="translate(2188, 0)"><use xlink:href="#MJX-1-TEX-N-29"></use></g></g></g></g><g data-mml-node="mtd" transform="translate(3862.6, 0)"><g data-mml-node="mo"><use xlink:href="#MJX-1-TEX-N-3D"></use></g><g data-mml-node="msup" transform="translate(1055.8, 0)"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mover"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D462"></use></g><g data-mml-node="TeXAtom" transform="translate(572, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D458"></use></g></g></g><g data-mml-node="mo" transform="translate(273, -14)"><use xlink:href="#MJX-1-TEX-N-20D7"></use></g></g></g><g data-mml-node="TeXAtom" transform="translate(990.4, 527.1) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mo"><use xlink:href="#MJX-1-TEX-N-28"></use></g><g data-mml-node="mi" transform="translate(389, 0)"><use xlink:href="#MJX-1-TEX-I-1D458"></use></g><g data-mml-node="mo" transform="translate(910, 0)"><use xlink:href="#MJX-1-TEX-N-2212"></use></g><g data-mml-node="mn" transform="translate(1688, 0)"><use xlink:href="#MJX-1-TEX-N-32"></use></g><g data-mml-node="mo" transform="translate(2188, 0)"><use xlink:href="#MJX-1-TEX-N-29"></use></g></g></g><g data-mml-node="mo" transform="translate(4140.6, 0)"><use xlink:href="#MJX-1-TEX-N-2212"></use></g><g data-mml-node="mi" transform="translate(5140.8, 0)"><use xlink:href="#MJX-1-TEX-I-1D45D"></use></g><g data-mml-node="mi" transform="translate(5643.8, 0)"><use xlink:href="#MJX-1-TEX-I-1D45F"></use></g><g data-mml-node="mi" transform="translate(6094.8, 0)"><use xlink:href="#MJX-1-TEX-I-1D45C"></use></g><g data-mml-node="msub" transform="translate(6579.8, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D457"></use></g><g data-mml-node="TeXAtom" transform="translate(412, -212.1) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mover"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D462"></use></g><g data-mml-node="TeXAtom" transform="translate(572, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D458"></use></g><g data-mml-node="mo" transform="translate(521, 0)"><use xlink:href="#MJX-1-TEX-N-2212"></use></g><g data-mml-node="mn" transform="translate(1299, 0)"><use xlink:href="#MJX-1-TEX-N-31"></use></g></g></g><g data-mml-node="mo" transform="translate(724.8, -14)"><use xlink:href="#MJX-1-TEX-N-20D7"></use></g></g></g></g></g><g data-mml-node="msup" transform="translate(8381.2, 0)"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mover"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D462"></use></g><g data-mml-node="TeXAtom" transform="translate(572, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D458"></use></g></g></g><g data-mml-node="mo" transform="translate(273, -14)"><use xlink:href="#MJX-1-TEX-N-20D7"></use></g></g></g><g data-mml-node="TeXAtom" transform="translate(990.4, 527.1) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mo"><use xlink:href="#MJX-1-TEX-N-28"></use></g><g data-mml-node="mi" transform="translate(389, 0)"><use xlink:href="#MJX-1-TEX-I-1D458"></use></g><g data-mml-node="mo" transform="translate(910, 0)"><use xlink:href="#MJX-1-TEX-N-2212"></use></g><g data-mml-node="mn" transform="translate(1688, 0)"><use xlink:href="#MJX-1-TEX-N-32"></use></g><g data-mml-node="mo" transform="translate(2188, 0)"><use xlink:href="#MJX-1-TEX-N-29"></use></g></g></g></g></g><g data-mml-node="mtr" transform="translate(0, -3841.8)"><g data-mml-node="mtd" transform="translate(936.1, 0)"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mover"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D462"></use></g><g data-mml-node="TeXAtom" transform="translate(572, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D458"></use></g></g></g><g data-mml-node="mo" transform="translate(273, -14)"><use xlink:href="#MJX-1-TEX-N-20D7"></use></g></g></g></g><g data-mml-node="mtd" transform="translate(6805.3, 0)"><g data-mml-node="mo"><use xlink:href="#MJX-1-TEX-N-3D"></use></g><g data-mml-node="mfrac" transform="translate(1055.8, 0)"><g data-mml-node="msup" transform="translate(720, 676)"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mover"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D462"></use></g><g data-mml-node="TeXAtom" transform="translate(572, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D458"></use></g></g></g><g data-mml-node="mo" transform="translate(273, -14)"><use xlink:href="#MJX-1-TEX-N-20D7"></use></g></g></g><g data-mml-node="TeXAtom" transform="translate(990.4, 527.1) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mo"><use xlink:href="#MJX-1-TEX-N-28"></use></g><g data-mml-node="mi" transform="translate(389, 0)"><use xlink:href="#MJX-1-TEX-I-1D458"></use></g><g data-mml-node="mo" transform="translate(910, 0)"><use xlink:href="#MJX-1-TEX-N-2212"></use></g><g data-mml-node="mn" transform="translate(1688, 0)"><use xlink:href="#MJX-1-TEX-N-31"></use></g><g data-mml-node="mo" transform="translate(2188, 0)"><use xlink:href="#MJX-1-TEX-N-29"></use></g></g></g><g data-mml-node="mrow" transform="translate(220, -1017.4)"><g data-mml-node="mo"><use xlink:href="#MJX-1-TEX-N-2016"></use></g><g data-mml-node="msup" transform="translate(500, 0)"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mover"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D462"></use></g><g data-mml-node="TeXAtom" transform="translate(572, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D458"></use></g></g></g><g data-mml-node="mo" transform="translate(273, -14)"><use xlink:href="#MJX-1-TEX-N-20D7"></use></g></g></g><g data-mml-node="TeXAtom" transform="translate(990.4, 527.1) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mo"><use xlink:href="#MJX-1-TEX-N-28"></use></g><g data-mml-node="mi" transform="translate(389, 0)"><use xlink:href="#MJX-1-TEX-I-1D458"></use></g><g data-mml-node="mo" transform="translate(910, 0)"><use xlink:href="#MJX-1-TEX-N-2212"></use></g><g data-mml-node="mn" transform="translate(1688, 0)"><use xlink:href="#MJX-1-TEX-N-31"></use></g><g data-mml-node="mo" transform="translate(2188, 0)"><use xlink:href="#MJX-1-TEX-N-29"></use></g></g></g><g data-mml-node="mo" transform="translate(3362.6, 0)"><use xlink:href="#MJX-1-TEX-N-2016"></use></g></g><rect width="4062.6" height="60" x="120" y="220"></rect></g></g></g></g></g></g></g></g></g></svg></mjx-container></div><p>where <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -1.318ex" xmlns="http://www.w3.org/2000/svg" width="14.877ex" height="3.491ex" role="img" focusable="false" viewBox="0 -960.4 6575.7 1542.9" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"></path><path id="MJX-1-TEX-I-1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-1-TEX-I-1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path><path id="MJX-1-TEX-I-1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path><path id="MJX-1-TEX-I-1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-1-TEX-N-20D7" d="M377 694Q377 702 382 708T397 714Q404 714 409 709Q414 705 419 690Q429 653 460 633Q471 626 471 615Q471 606 468 603T454 594Q411 572 379 531Q377 529 374 525T369 519T364 517T357 516Q350 516 344 521T337 536Q337 555 384 595H213L42 596Q29 605 29 615Q29 622 42 635H401Q377 673 377 694Z"></path><path id="MJX-1-TEX-I-1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path><path id="MJX-1-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-1-TEX-N-22C5" d="M78 250Q78 274 95 292T138 310Q162 310 180 294T199 251Q199 226 182 208T139 190T96 207T78 250Z"></path><path id="MJX-1-TEX-N-2016" d="M133 736Q138 750 153 750Q164 750 170 739Q172 735 172 250T170 -239Q164 -250 152 -250Q144 -250 138 -244L137 -243Q133 -241 133 -179T132 250Q132 731 133 736ZM329 739Q334 750 346 750Q353 750 361 744L362 743Q366 741 366 679T367 250T367 -178T362 -243L361 -244Q355 -250 347 -250Q335 -250 329 -239Q327 -235 327 250T329 739Z"></path><path id="MJX-1-TEX-N-32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D45D"></use></g><g data-mml-node="mi" transform="translate(503, 0)"><use xlink:href="#MJX-1-TEX-I-1D45F"></use></g><g data-mml-node="mi" transform="translate(954, 0)"><use xlink:href="#MJX-1-TEX-I-1D45C"></use></g><g data-mml-node="msub" transform="translate(1439, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D457"></use></g><g data-mml-node="TeXAtom" transform="translate(412, -212.1) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mover"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D462"></use></g><g data-mml-node="mo" transform="translate(63.8, -14)"><use xlink:href="#MJX-1-TEX-N-20D7"></use></g></g></g></g></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(2305.5, 0)"><g data-mml-node="mover"><g data-mml-node="mi" transform="translate(7.5, 0)"><use xlink:href="#MJX-1-TEX-I-1D463"></use></g><g data-mml-node="mo" transform="translate(27.8, -13)"><use xlink:href="#MJX-1-TEX-N-20D7"></use></g></g></g><g data-mml-node="mo" transform="translate(3111, 0)"><use xlink:href="#MJX-1-TEX-N-3D"></use></g><g data-mml-node="mfrac" transform="translate(4166.8, 0)"><g data-mml-node="mrow" transform="translate(431.3, 394) scale(0.707)"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mover"><g data-mml-node="mi" transform="translate(7.5, 0)"><use xlink:href="#MJX-1-TEX-I-1D463"></use></g><g data-mml-node="mo" transform="translate(27.8, -13)"><use xlink:href="#MJX-1-TEX-N-20D7"></use></g></g></g><g data-mml-node="mo" transform="translate(527.8, 0)"><use xlink:href="#MJX-1-TEX-N-22C5"></use></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(805.8, 0)"><g data-mml-node="mover"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D462"></use></g><g data-mml-node="mo" transform="translate(63.8, -14)"><use xlink:href="#MJX-1-TEX-N-20D7"></use></g></g></g></g><g data-mml-node="mrow" transform="translate(220, -405.7) scale(0.707)"><g data-mml-node="mo"><use xlink:href="#MJX-1-TEX-N-2016"></use></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(500, 0)"><g data-mml-node="mover"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D462"></use></g><g data-mml-node="mo" transform="translate(63.8, -14)"><use xlink:href="#MJX-1-TEX-N-20D7"></use></g></g></g><g data-mml-node="msup" transform="translate(1072, 0)"><g data-mml-node="mo"><use xlink:href="#MJX-1-TEX-N-2016"></use></g><g data-mml-node="mn" transform="translate(500, 289) scale(0.707)"><use xlink:href="#MJX-1-TEX-N-32"></use></g></g></g><rect width="1596.9" height="60" x="120" y="220"></rect></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(6003.7, 0)"><g data-mml-node="mover"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D462"></use></g><g data-mml-node="mo" transform="translate(63.8, -14)"><use xlink:href="#MJX-1-TEX-N-20D7"></use></g></g></g></g></g></svg></mjx-container>.</p><h4 id="Matlab-code-1"><a href="#Matlab-code-1" class="headerlink" title="Matlab code"></a>Matlab code</h4><pre class="line-numbers language-matlab" data-language="matlab"><code class="language-matlab"><span class="token keyword">function</span> <span class="token punctuation">[</span>Q<span class="token punctuation">,</span> R<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token function">modified_gram_schmidt_qr</span><span class="token punctuation">(</span>A<span class="token punctuation">)</span>    <span class="token punctuation">[</span>m<span class="token punctuation">,</span> n<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token function">size</span><span class="token punctuation">(</span>A<span class="token punctuation">)</span><span class="token punctuation">;</span>    Q <span class="token operator">=</span> A<span class="token punctuation">;</span>    R <span class="token operator">=</span> <span class="token function">zeros</span><span class="token punctuation">(</span>n<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">for</span> k <span class="token operator">=</span> <span class="token number">1</span><span class="token operator">:</span>n        <span class="token function">R</span><span class="token punctuation">(</span>k<span class="token punctuation">,</span> k<span class="token punctuation">)</span> <span class="token operator">=</span> <span class="token function">norm</span><span class="token punctuation">(</span><span class="token function">Q</span><span class="token punctuation">(</span><span class="token operator">:</span><span class="token punctuation">,</span> k<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">Q</span><span class="token punctuation">(</span><span class="token operator">:</span><span class="token punctuation">,</span> k<span class="token punctuation">)</span> <span class="token operator">=</span> <span class="token function">Q</span><span class="token punctuation">(</span><span class="token operator">:</span><span class="token punctuation">,</span> k<span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token function">R</span><span class="token punctuation">(</span>k<span class="token punctuation">,</span> k<span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">R</span><span class="token punctuation">(</span>k<span class="token punctuation">,</span> k<span class="token operator">+</span><span class="token number">1</span><span class="token operator">:</span>n<span class="token punctuation">)</span> <span class="token operator">=</span> <span class="token function">Q</span><span class="token punctuation">(</span><span class="token operator">:</span><span class="token punctuation">,</span>k<span class="token punctuation">)</span><span class="token operator">'</span> <span class="token operator">*</span> <span class="token function">Q</span><span class="token punctuation">(</span><span class="token operator">:</span><span class="token punctuation">,</span> k<span class="token operator">+</span><span class="token number">1</span><span class="token operator">:</span>n<span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">Q</span><span class="token punctuation">(</span><span class="token operator">:</span><span class="token punctuation">,</span> k<span class="token operator">+</span><span class="token number">1</span><span class="token operator">:</span>n<span class="token punctuation">)</span> <span class="token operator">=</span> <span class="token function">Q</span><span class="token punctuation">(</span><span class="token operator">:</span><span class="token punctuation">,</span> k<span class="token operator">+</span><span class="token number">1</span><span class="token operator">:</span>n<span class="token punctuation">)</span> <span class="token operator">-</span> <span class="token function">Q</span><span class="token punctuation">(</span><span class="token operator">:</span><span class="token punctuation">,</span> k<span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token function">R</span><span class="token punctuation">(</span>k<span class="token punctuation">,</span> k<span class="token operator">+</span><span class="token number">1</span><span class="token operator">:</span>n<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">end</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="Python-code-1"><a href="#Python-code-1" class="headerlink" title="Python code"></a>Python code</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Decomposition</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">pass</span>    <span class="token keyword">def</span> <span class="token function">modified_gram_schmidt_qr</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> A<span class="token punctuation">)</span><span class="token punctuation">:</span>        m<span class="token punctuation">,</span> n <span class="token operator">=</span> A<span class="token punctuation">.</span>shape        Q <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>shape<span class="token operator">=</span><span class="token punctuation">(</span>m<span class="token punctuation">,</span> n<span class="token punctuation">)</span><span class="token punctuation">)</span>        R <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>shape<span class="token operator">=</span><span class="token punctuation">(</span>n<span class="token punctuation">,</span> n<span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token keyword">for</span> j <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> n<span class="token punctuation">)</span><span class="token punctuation">:</span>            R<span class="token punctuation">[</span>j<span class="token punctuation">,</span> j<span class="token punctuation">]</span> <span class="token operator">=</span> np<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>A<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> j<span class="token punctuation">]</span><span class="token punctuation">,</span> A<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> j<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>            Q<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> j<span class="token punctuation">]</span> <span class="token operator">=</span> A<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> j<span class="token punctuation">]</span> <span class="token operator">/</span> R<span class="token punctuation">[</span>j<span class="token punctuation">,</span> j<span class="token punctuation">]</span>            <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>j<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">,</span> n<span class="token punctuation">)</span><span class="token punctuation">:</span>                R<span class="token punctuation">[</span>j<span class="token punctuation">,</span> i<span class="token punctuation">]</span> <span class="token operator">=</span> np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>Q<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> j<span class="token punctuation">]</span><span class="token punctuation">,</span> A<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> i<span class="token punctuation">]</span><span class="token punctuation">)</span>                A<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> i<span class="token punctuation">]</span> <span class="token operator">=</span> A<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> i<span class="token punctuation">]</span> <span class="token operator">-</span> R<span class="token punctuation">[</span>j<span class="token punctuation">,</span> i<span class="token punctuation">]</span> <span class="token operator">*</span> Q<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> j<span class="token punctuation">]</span>        <span class="token keyword">return</span> Q<span class="token punctuation">,</span> R<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="Householder-Transformations"><a href="#Householder-Transformations" class="headerlink" title="Householder Transformations"></a>Householder Transformations</h3><p>Householder transformations are simple orthogonal transformations corresponding to reflection through a plane. Reflection across the plane orthogonal to a unit normal vector <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="1.097ex" height="1.027ex" role="img" focusable="false" viewBox="0 -443 485 454" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D463"></use></g></g></g></svg></mjx-container> can be expressed in matrix form as</p><div style="display: flex;justify-content: center;">    <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.186ex" xmlns="http://www.w3.org/2000/svg" width="13.497ex" height="2.09ex" role="img" focusable="false" viewBox="0 -841.7 5965.8 923.7" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D43B" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 219 683Q260 681 355 681Q389 681 418 681T463 682T483 682Q499 682 499 672Q499 670 497 658Q492 641 487 638H485Q483 638 480 638T473 638T464 637T455 637Q416 636 405 634T387 623Q384 619 355 500Q348 474 340 442T328 395L324 380Q324 378 469 378H614L615 381Q615 384 646 504Q674 619 674 627T617 637Q594 637 587 639T580 648Q580 650 582 660Q586 677 588 679T604 682Q609 682 646 681T740 680Q802 680 835 681T871 682Q888 682 888 672Q888 645 876 638H874Q872 638 869 638T862 638T853 637T844 637Q805 636 794 634T776 623Q773 618 704 340T634 58Q634 51 638 51Q646 48 692 46H723Q729 38 729 37T726 19Q722 6 716 0H701Q664 2 567 2Q533 2 504 2T458 2T437 1Q420 1 420 10Q420 15 423 24Q428 43 433 45Q437 46 448 46H454Q481 46 514 49Q520 50 522 50T528 55T534 64T540 82T547 110T558 153Q565 181 569 198Q602 330 602 331T457 332H312L279 197Q245 63 245 58Q245 51 253 49T303 46H334Q340 38 340 37T337 19Q333 6 327 0H312Q275 2 178 2Q144 2 115 2T69 2T48 1Q31 1 31 10Q31 12 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path><path id="MJX-1-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-1-TEX-I-1D43C" d="M43 1Q26 1 26 10Q26 12 29 24Q34 43 39 45Q42 46 54 46H60Q120 46 136 53Q137 53 138 54Q143 56 149 77T198 273Q210 318 216 344Q286 624 286 626Q284 630 284 631Q274 637 213 637H193Q184 643 189 662Q193 677 195 680T209 683H213Q285 681 359 681Q481 681 487 683H497Q504 676 504 672T501 655T494 639Q491 637 471 637Q440 637 407 634Q393 631 388 623Q381 609 337 432Q326 385 315 341Q245 65 245 59Q245 52 255 50T307 46H339Q345 38 345 37T342 19Q338 6 332 0H316Q279 2 179 2Q143 2 113 2T65 2T43 1Z"></path><path id="MJX-1-TEX-N-2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path><path id="MJX-1-TEX-N-32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path><path id="MJX-1-TEX-I-1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path><path id="MJX-1-TEX-I-1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D43B"></use></g><g data-mml-node="mo" transform="translate(1165.8, 0)"><use xlink:href="#MJX-1-TEX-N-3D"></use></g><g data-mml-node="mi" transform="translate(2221.6, 0)"><use xlink:href="#MJX-1-TEX-I-1D43C"></use></g><g data-mml-node="mo" transform="translate(2947.8, 0)"><use xlink:href="#MJX-1-TEX-N-2212"></use></g><g data-mml-node="mn" transform="translate(3948, 0)"><use xlink:href="#MJX-1-TEX-N-32"></use></g><g data-mml-node="mi" transform="translate(4448, 0)"><use xlink:href="#MJX-1-TEX-I-1D463"></use></g><g data-mml-node="msup" transform="translate(4933, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D463"></use></g><g data-mml-node="mi" transform="translate(485, 363) scale(0.707)"><use xlink:href="#MJX-1-TEX-I-1D447"></use></g></g></g></g></svg></mjx-container></div><p>In particular, if we take <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="14.956ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 6610.6 1000" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-1-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-1-TEX-I-1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path><path id="MJX-1-TEX-N-2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path><path id="MJX-1-TEX-I-1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path><path id="MJX-1-TEX-N-2016" d="M133 736Q138 750 153 750Q164 750 170 739Q172 735 172 250T170 -239Q164 -250 152 -250Q144 -250 138 -244L137 -243Q133 -241 133 -179T132 250Q132 731 133 736ZM329 739Q334 750 346 750Q353 750 361 744L362 743Q366 741 366 679T367 250T367 -178T362 -243L361 -244Q355 -250 347 -250Q335 -250 329 -239Q327 -235 327 250T329 739Z"></path><path id="MJX-1-TEX-I-1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path><path id="MJX-1-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D462"></use></g><g data-mml-node="mo" transform="translate(849.8, 0)"><use xlink:href="#MJX-1-TEX-N-3D"></use></g><g data-mml-node="mi" transform="translate(1905.6, 0)"><use xlink:href="#MJX-1-TEX-I-1D465"></use></g><g data-mml-node="mo" transform="translate(2699.8, 0)"><use xlink:href="#MJX-1-TEX-N-2212"></use></g><g data-mml-node="mi" transform="translate(3700, 0)"><use xlink:href="#MJX-1-TEX-I-1D460"></use></g><g data-mml-node="mo" transform="translate(4169, 0)"><use xlink:href="#MJX-1-TEX-N-2016"></use></g><g data-mml-node="mi" transform="translate(4669, 0)"><use xlink:href="#MJX-1-TEX-I-1D465"></use></g><g data-mml-node="mo" transform="translate(5241, 0)"><use xlink:href="#MJX-1-TEX-N-2016"></use></g><g data-mml-node="msub" transform="translate(5741, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D452"></use></g><g data-mml-node="TeXAtom" transform="translate(466, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-31"></use></g></g></g></g></g></svg></mjx-container> where <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.186ex" xmlns="http://www.w3.org/2000/svg" width="6.97ex" height="1.692ex" role="img" focusable="false" viewBox="0 -666 3080.6 748" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path><path id="MJX-1-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-1-TEX-N-B1" d="M56 320T56 333T70 353H369V502Q369 651 371 655Q376 666 388 666Q402 666 405 654T409 596V500V353H707Q722 345 722 333Q722 320 707 313H409V40H707Q722 32 722 20T707 0H70Q56 7 56 20T70 40H369V313H70Q56 320 56 333Z"></path><path id="MJX-1-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D460"></use></g><g data-mml-node="mo" transform="translate(746.8, 0)"><use xlink:href="#MJX-1-TEX-N-3D"></use></g><g data-mml-node="mo" transform="translate(1802.6, 0)"><use xlink:href="#MJX-1-TEX-N-B1"></use></g><g data-mml-node="mn" transform="translate(2580.6, 0)"><use xlink:href="#MJX-1-TEX-N-31"></use></g></g></g></svg></mjx-container> and <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="10.096ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 4462.6 1000" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path><path id="MJX-1-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-1-TEX-I-1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-1-TEX-N-2F" d="M423 750Q432 750 438 744T444 730Q444 725 271 248T92 -240Q85 -250 75 -250Q68 -250 62 -245T56 -231Q56 -221 230 257T407 740Q411 750 423 750Z"></path><path id="MJX-1-TEX-N-2016" d="M133 736Q138 750 153 750Q164 750 170 739Q172 735 172 250T170 -239Q164 -250 152 -250Q144 -250 138 -244L137 -243Q133 -241 133 -179T132 250Q132 731 133 736ZM329 739Q334 750 346 750Q353 750 361 744L362 743Q366 741 366 679T367 250T367 -178T362 -243L361 -244Q355 -250 347 -250Q335 -250 329 -239Q327 -235 327 250T329 739Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D463"></use></g><g data-mml-node="mo" transform="translate(762.8, 0)"><use xlink:href="#MJX-1-TEX-N-3D"></use></g><g data-mml-node="mi" transform="translate(1818.6, 0)"><use xlink:href="#MJX-1-TEX-I-1D462"></use></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(2390.6, 0)"><g data-mml-node="mo"><use xlink:href="#MJX-1-TEX-N-2F"></use></g></g><g data-mml-node="mo" transform="translate(2890.6, 0)"><use xlink:href="#MJX-1-TEX-N-2016"></use></g><g data-mml-node="mi" transform="translate(3390.6, 0)"><use xlink:href="#MJX-1-TEX-I-1D462"></use></g><g data-mml-node="mo" transform="translate(3962.6, 0)"><use xlink:href="#MJX-1-TEX-N-2016"></use></g></g></g></svg></mjx-container> then</p><div style="display: flex;justify-content: center;">    <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.884ex" xmlns="http://www.w3.org/2000/svg" width="27.716ex" height="3.122ex" role="img" focusable="false" viewBox="0 -989.2 12250.4 1379.8" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D43B" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 219 683Q260 681 355 681Q389 681 418 681T463 682T483 682Q499 682 499 672Q499 670 497 658Q492 641 487 638H485Q483 638 480 638T473 638T464 637T455 637Q416 636 405 634T387 623Q384 619 355 500Q348 474 340 442T328 395L324 380Q324 378 469 378H614L615 381Q615 384 646 504Q674 619 674 627T617 637Q594 637 587 639T580 648Q580 650 582 660Q586 677 588 679T604 682Q609 682 646 681T740 680Q802 680 835 681T871 682Q888 682 888 672Q888 645 876 638H874Q872 638 869 638T862 638T853 637T844 637Q805 636 794 634T776 623Q773 618 704 340T634 58Q634 51 638 51Q646 48 692 46H723Q729 38 729 37T726 19Q722 6 716 0H701Q664 2 567 2Q533 2 504 2T458 2T437 1Q420 1 420 10Q420 15 423 24Q428 43 433 45Q437 46 448 46H454Q481 46 514 49Q520 50 522 50T528 55T534 64T540 82T547 110T558 153Q565 181 569 198Q602 330 602 331T457 332H312L279 197Q245 63 245 58Q245 51 253 49T303 46H334Q340 38 340 37T337 19Q333 6 327 0H312Q275 2 178 2Q144 2 115 2T69 2T48 1Q31 1 31 10Q31 12 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path><path id="MJX-1-TEX-I-1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path><path id="MJX-1-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-1-TEX-N-28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path><path id="MJX-1-TEX-I-1D43C" d="M43 1Q26 1 26 10Q26 12 29 24Q34 43 39 45Q42 46 54 46H60Q120 46 136 53Q137 53 138 54Q143 56 149 77T198 273Q210 318 216 344Q286 624 286 626Q284 630 284 631Q274 637 213 637H193Q184 643 189 662Q193 677 195 680T209 683H213Q285 681 359 681Q481 681 487 683H497Q504 676 504 672T501 655T494 639Q491 637 471 637Q440 637 407 634Q393 631 388 623Q381 609 337 432Q326 385 315 341Q245 65 245 59Q245 52 255 50T307 46H339Q345 38 345 37T342 19Q338 6 332 0H316Q279 2 179 2Q143 2 113 2T65 2T43 1Z"></path><path id="MJX-1-TEX-N-2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path><path id="MJX-1-TEX-N-32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path><path id="MJX-1-TEX-I-1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-1-TEX-I-1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path><path id="MJX-1-TEX-N-29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path><path id="MJX-1-TEX-I-1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path><path id="MJX-1-TEX-N-2016" d="M133 736Q138 750 153 750Q164 750 170 739Q172 735 172 250T170 -239Q164 -250 152 -250Q144 -250 138 -244L137 -243Q133 -241 133 -179T132 250Q132 731 133 736ZM329 739Q334 750 346 750Q353 750 361 744L362 743Q366 741 366 679T367 250T367 -178T362 -243L361 -244Q355 -250 347 -250Q335 -250 329 -239Q327 -235 327 250T329 739Z"></path><path id="MJX-1-TEX-I-1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path><path id="MJX-1-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D43B"></use></g><g data-mml-node="mi" transform="translate(888, 0)"><use xlink:href="#MJX-1-TEX-I-1D465"></use></g><g data-mml-node="mo" transform="translate(1737.8, 0)"><use xlink:href="#MJX-1-TEX-N-3D"></use></g><g data-mml-node="mo" transform="translate(2793.6, 0)"><use xlink:href="#MJX-1-TEX-N-28"></use></g><g data-mml-node="mi" transform="translate(3182.6, 0)"><use xlink:href="#MJX-1-TEX-I-1D43C"></use></g><g data-mml-node="mo" transform="translate(3908.8, 0)"><use xlink:href="#MJX-1-TEX-N-2212"></use></g><g data-mml-node="mn" transform="translate(4909, 0)"><use xlink:href="#MJX-1-TEX-N-32"></use></g><g data-mml-node="mfrac" transform="translate(5409, 0)"><g data-mml-node="mrow" transform="translate(220, 394) scale(0.707)"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D462"></use></g><g data-mml-node="msup" transform="translate(572, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D462"></use></g><g data-mml-node="mi" transform="translate(572, 363) scale(0.707)"><use xlink:href="#MJX-1-TEX-I-1D447"></use></g></g></g><g data-mml-node="mrow" transform="translate(220, -382.9) scale(0.707)"><g data-mml-node="msup"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D462"></use></g><g data-mml-node="mi" transform="translate(572, 289) scale(0.707)"><use xlink:href="#MJX-1-TEX-I-1D447"></use></g></g><g data-mml-node="mi" transform="translate(1119.8, 0)"><use xlink:href="#MJX-1-TEX-I-1D462"></use></g></g><rect width="1396.3" height="60" x="120" y="220"></rect></g><g data-mml-node="mo" transform="translate(7045.3, 0)"><use xlink:href="#MJX-1-TEX-N-29"></use></g><g data-mml-node="mi" transform="translate(7434.3, 0)"><use xlink:href="#MJX-1-TEX-I-1D465"></use></g><g data-mml-node="mo" transform="translate(8284.1, 0)"><use xlink:href="#MJX-1-TEX-N-3D"></use></g><g data-mml-node="mi" transform="translate(9339.8, 0)"><use xlink:href="#MJX-1-TEX-I-1D460"></use></g><g data-mml-node="mo" transform="translate(9808.8, 0)"><use xlink:href="#MJX-1-TEX-N-2016"></use></g><g data-mml-node="mi" transform="translate(10308.8, 0)"><use xlink:href="#MJX-1-TEX-I-1D465"></use></g><g data-mml-node="mo" transform="translate(10880.8, 0)"><use xlink:href="#MJX-1-TEX-N-2016"></use></g><g data-mml-node="msub" transform="translate(11380.8, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D452"></use></g><g data-mml-node="TeXAtom" transform="translate(466, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-31"></use></g></g></g></g></g></svg></mjx-container></div><p>Let us first verify that this works:</p><div style="display: flex;justify-content: center;">    <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="38.86ex" height="2.47ex" role="img" focusable="false" viewBox="0 -841.7 17176.3 1091.7" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-1-TEX-I-1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path><path id="MJX-1-TEX-I-1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path><path id="MJX-1-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-1-TEX-N-28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path><path id="MJX-1-TEX-N-2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path><path id="MJX-1-TEX-I-1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path><path id="MJX-1-TEX-N-2016" d="M133 736Q138 750 153 750Q164 750 170 739Q172 735 172 250T170 -239Q164 -250 152 -250Q144 -250 138 -244L137 -243Q133 -241 133 -179T132 250Q132 731 133 736ZM329 739Q334 750 346 750Q353 750 361 744L362 743Q366 741 366 679T367 250T367 -178T362 -243L361 -244Q355 -250 347 -250Q335 -250 329 -239Q327 -235 327 250T329 739Z"></path><path id="MJX-1-TEX-I-1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path><path id="MJX-1-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-1-TEX-N-29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path><path id="MJX-1-TEX-N-32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D462"></use></g><g data-mml-node="mi" transform="translate(572, 363) scale(0.707)"><use xlink:href="#MJX-1-TEX-I-1D447"></use></g></g><g data-mml-node="mi" transform="translate(1119.8, 0)"><use xlink:href="#MJX-1-TEX-I-1D465"></use></g><g data-mml-node="mo" transform="translate(1969.6, 0)"><use xlink:href="#MJX-1-TEX-N-3D"></use></g><g data-mml-node="mo" transform="translate(3025.4, 0)"><use xlink:href="#MJX-1-TEX-N-28"></use></g><g data-mml-node="mi" transform="translate(3414.4, 0)"><use xlink:href="#MJX-1-TEX-I-1D465"></use></g><g data-mml-node="mo" transform="translate(4208.6, 0)"><use xlink:href="#MJX-1-TEX-N-2212"></use></g><g data-mml-node="mi" transform="translate(5208.8, 0)"><use xlink:href="#MJX-1-TEX-I-1D460"></use></g><g data-mml-node="mo" transform="translate(5677.8, 0)"><use xlink:href="#MJX-1-TEX-N-2016"></use></g><g data-mml-node="mi" transform="translate(6177.8, 0)"><use xlink:href="#MJX-1-TEX-I-1D465"></use></g><g data-mml-node="mo" transform="translate(6749.8, 0)"><use xlink:href="#MJX-1-TEX-N-2016"></use></g><g data-mml-node="msub" transform="translate(7249.8, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D452"></use></g><g data-mml-node="TeXAtom" transform="translate(466, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-31"></use></g></g></g><g data-mml-node="msup" transform="translate(8119.4, 0)"><g data-mml-node="mo"><use xlink:href="#MJX-1-TEX-N-29"></use></g><g data-mml-node="mi" transform="translate(389, 363) scale(0.707)"><use xlink:href="#MJX-1-TEX-I-1D447"></use></g></g><g data-mml-node="mi" transform="translate(9056.2, 0)"><use xlink:href="#MJX-1-TEX-I-1D465"></use></g><g data-mml-node="mo" transform="translate(9905.9, 0)"><use xlink:href="#MJX-1-TEX-N-3D"></use></g><g data-mml-node="mo" transform="translate(10961.7, 0)"><use xlink:href="#MJX-1-TEX-N-2016"></use></g><g data-mml-node="mi" transform="translate(11461.7, 0)"><use xlink:href="#MJX-1-TEX-I-1D465"></use></g><g data-mml-node="msup" transform="translate(12033.7, 0)"><g data-mml-node="mo"><use xlink:href="#MJX-1-TEX-N-2016"></use></g><g data-mml-node="mn" transform="translate(500, 363) scale(0.707)"><use xlink:href="#MJX-1-TEX-N-32"></use></g></g><g data-mml-node="mo" transform="translate(13159.5, 0)"><use xlink:href="#MJX-1-TEX-N-2212"></use></g><g data-mml-node="mi" transform="translate(14159.7, 0)"><use xlink:href="#MJX-1-TEX-I-1D460"></use></g><g data-mml-node="msub" transform="translate(14628.7, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D465"></use></g><g data-mml-node="TeXAtom" transform="translate(572, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-31"></use></g></g></g><g data-mml-node="mo" transform="translate(15604.3, 0)"><use xlink:href="#MJX-1-TEX-N-2016"></use></g><g data-mml-node="mi" transform="translate(16104.3, 0)"><use xlink:href="#MJX-1-TEX-I-1D465"></use></g><g data-mml-node="mo" transform="translate(16676.3, 0)"><use xlink:href="#MJX-1-TEX-N-2016"></use></g></g></g></svg></mjx-container></div><p>and </p><div style="display: flex;justify-content: center;">    <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="83.449ex" height="2.47ex" role="img" focusable="false" viewBox="0 -841.7 36884.5 1091.7" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-1-TEX-I-1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path><path id="MJX-1-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-1-TEX-N-28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path><path id="MJX-1-TEX-I-1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path><path id="MJX-1-TEX-N-2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path><path id="MJX-1-TEX-I-1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path><path id="MJX-1-TEX-N-2016" d="M133 736Q138 750 153 750Q164 750 170 739Q172 735 172 250T170 -239Q164 -250 152 -250Q144 -250 138 -244L137 -243Q133 -241 133 -179T132 250Q132 731 133 736ZM329 739Q334 750 346 750Q353 750 361 744L362 743Q366 741 366 679T367 250T367 -178T362 -243L361 -244Q355 -250 347 -250Q335 -250 329 -239Q327 -235 327 250T329 739Z"></path><path id="MJX-1-TEX-I-1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path><path id="MJX-1-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-1-TEX-N-29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path><path id="MJX-1-TEX-N-32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path><path id="MJX-1-TEX-N-2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D462"></use></g><g data-mml-node="mi" transform="translate(572, 363) scale(0.707)"><use xlink:href="#MJX-1-TEX-I-1D447"></use></g></g><g data-mml-node="mi" transform="translate(1119.8, 0)"><use xlink:href="#MJX-1-TEX-I-1D462"></use></g><g data-mml-node="mo" transform="translate(1969.6, 0)"><use xlink:href="#MJX-1-TEX-N-3D"></use></g><g data-mml-node="mo" transform="translate(3025.4, 0)"><use xlink:href="#MJX-1-TEX-N-28"></use></g><g data-mml-node="mi" transform="translate(3414.4, 0)"><use xlink:href="#MJX-1-TEX-I-1D465"></use></g><g data-mml-node="mo" transform="translate(4208.6, 0)"><use xlink:href="#MJX-1-TEX-N-2212"></use></g><g data-mml-node="mi" transform="translate(5208.8, 0)"><use xlink:href="#MJX-1-TEX-I-1D460"></use></g><g data-mml-node="mo" transform="translate(5677.8, 0)"><use xlink:href="#MJX-1-TEX-N-2016"></use></g><g data-mml-node="mi" transform="translate(6177.8, 0)"><use xlink:href="#MJX-1-TEX-I-1D465"></use></g><g data-mml-node="mo" transform="translate(6749.8, 0)"><use xlink:href="#MJX-1-TEX-N-2016"></use></g><g data-mml-node="msub" transform="translate(7249.8, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D452"></use></g><g data-mml-node="TeXAtom" transform="translate(466, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-31"></use></g></g></g><g data-mml-node="msup" transform="translate(8119.4, 0)"><g data-mml-node="mo"><use xlink:href="#MJX-1-TEX-N-29"></use></g><g data-mml-node="mi" transform="translate(389, 363) scale(0.707)"><use xlink:href="#MJX-1-TEX-I-1D447"></use></g></g><g data-mml-node="mo" transform="translate(9056.2, 0)"><use xlink:href="#MJX-1-TEX-N-28"></use></g><g data-mml-node="mi" transform="translate(9445.2, 0)"><use xlink:href="#MJX-1-TEX-I-1D465"></use></g><g data-mml-node="mo" transform="translate(10239.4, 0)"><use xlink:href="#MJX-1-TEX-N-2212"></use></g><g data-mml-node="mi" transform="translate(11239.6, 0)"><use xlink:href="#MJX-1-TEX-I-1D460"></use></g><g data-mml-node="mo" transform="translate(11708.6, 0)"><use xlink:href="#MJX-1-TEX-N-2016"></use></g><g data-mml-node="mi" transform="translate(12208.6, 0)"><use xlink:href="#MJX-1-TEX-I-1D465"></use></g><g data-mml-node="mo" transform="translate(12780.6, 0)"><use xlink:href="#MJX-1-TEX-N-2016"></use></g><g data-mml-node="msub" transform="translate(13280.6, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D452"></use></g><g data-mml-node="TeXAtom" transform="translate(466, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-31"></use></g></g></g><g data-mml-node="mo" transform="translate(14150.2, 0)"><use xlink:href="#MJX-1-TEX-N-29"></use></g><g data-mml-node="mo" transform="translate(14816.9, 0)"><use xlink:href="#MJX-1-TEX-N-3D"></use></g><g data-mml-node="mo" transform="translate(15872.7, 0)"><use xlink:href="#MJX-1-TEX-N-2016"></use></g><g data-mml-node="mi" transform="translate(16372.7, 0)"><use xlink:href="#MJX-1-TEX-I-1D465"></use></g><g data-mml-node="msup" transform="translate(16944.7, 0)"><g data-mml-node="mo"><use xlink:href="#MJX-1-TEX-N-2016"></use></g><g data-mml-node="mn" transform="translate(500, 363) scale(0.707)"><use xlink:href="#MJX-1-TEX-N-32"></use></g></g><g data-mml-node="mo" transform="translate(18070.5, 0)"><use xlink:href="#MJX-1-TEX-N-2212"></use></g><g data-mml-node="mn" transform="translate(19070.7, 0)"><use xlink:href="#MJX-1-TEX-N-32"></use></g><g data-mml-node="mi" transform="translate(19570.7, 0)"><use xlink:href="#MJX-1-TEX-I-1D460"></use></g><g data-mml-node="msub" transform="translate(20039.7, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D465"></use></g><g data-mml-node="TeXAtom" transform="translate(572, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-31"></use></g></g></g><g data-mml-node="mo" transform="translate(21015.3, 0)"><use xlink:href="#MJX-1-TEX-N-2016"></use></g><g data-mml-node="mi" transform="translate(21515.3, 0)"><use xlink:href="#MJX-1-TEX-I-1D465"></use></g><g data-mml-node="mo" transform="translate(22087.3, 0)"><use xlink:href="#MJX-1-TEX-N-2016"></use></g><g data-mml-node="mo" transform="translate(22809.5, 0)"><use xlink:href="#MJX-1-TEX-N-2B"></use></g><g data-mml-node="mo" transform="translate(23809.7, 0)"><use xlink:href="#MJX-1-TEX-N-2016"></use></g><g data-mml-node="mi" transform="translate(24309.7, 0)"><use xlink:href="#MJX-1-TEX-I-1D465"></use></g><g data-mml-node="msup" transform="translate(24881.7, 0)"><g data-mml-node="mo"><use xlink:href="#MJX-1-TEX-N-2016"></use></g><g data-mml-node="mn" transform="translate(500, 363) scale(0.707)"><use xlink:href="#MJX-1-TEX-N-32"></use></g></g><g data-mml-node="mo" transform="translate(25785.3, 0)"><use xlink:href="#MJX-1-TEX-N-2016"></use></g><g data-mml-node="msub" transform="translate(26285.3, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D452"></use></g><g data-mml-node="mn" transform="translate(466, -150) scale(0.707)"><use xlink:href="#MJX-1-TEX-N-31"></use></g></g><g data-mml-node="msup" transform="translate(27154.8, 0)"><g data-mml-node="mo"><use xlink:href="#MJX-1-TEX-N-2016"></use></g><g data-mml-node="mn" transform="translate(500, 363) scale(0.707)"><use xlink:href="#MJX-1-TEX-N-32"></use></g></g><g data-mml-node="mo" transform="translate(28336.1, 0)"><use xlink:href="#MJX-1-TEX-N-3D"></use></g><g data-mml-node="mn" transform="translate(29391.9, 0)"><use xlink:href="#MJX-1-TEX-N-32"></use></g><g data-mml-node="mo" transform="translate(29891.9, 0)"><use xlink:href="#MJX-1-TEX-N-28"></use></g><g data-mml-node="mo" transform="translate(30280.9, 0)"><use xlink:href="#MJX-1-TEX-N-2016"></use></g><g data-mml-node="mi" transform="translate(30780.9, 0)"><use xlink:href="#MJX-1-TEX-I-1D465"></use></g><g data-mml-node="msup" transform="translate(31352.9, 0)"><g data-mml-node="mo"><use xlink:href="#MJX-1-TEX-N-2016"></use></g><g data-mml-node="mn" transform="translate(500, 363) scale(0.707)"><use xlink:href="#MJX-1-TEX-N-32"></use></g></g><g data-mml-node="mo" transform="translate(32478.7, 0)"><use xlink:href="#MJX-1-TEX-N-2212"></use></g><g data-mml-node="mi" transform="translate(33478.9, 0)"><use xlink:href="#MJX-1-TEX-I-1D460"></use></g><g data-mml-node="msub" transform="translate(33947.9, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D465"></use></g><g data-mml-node="mn" transform="translate(572, -150) scale(0.707)"><use xlink:href="#MJX-1-TEX-N-31"></use></g></g><g data-mml-node="mo" transform="translate(34923.5, 0)"><use xlink:href="#MJX-1-TEX-N-2016"></use></g><g data-mml-node="mi" transform="translate(35423.5, 0)"><use xlink:href="#MJX-1-TEX-I-1D465"></use></g><g data-mml-node="mo" transform="translate(35995.5, 0)"><use xlink:href="#MJX-1-TEX-N-2016"></use></g><g data-mml-node="mo" transform="translate(36495.5, 0)"><use xlink:href="#MJX-1-TEX-N-29"></use></g></g></g></svg></mjx-container></div><p>so </p><div style="display: flex;justify-content: center;">    <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.186ex" xmlns="http://www.w3.org/2000/svg" width="11.804ex" height="2.09ex" role="img" focusable="false" viewBox="0 -841.7 5217.2 923.7" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-1-TEX-I-1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path><path id="MJX-1-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-1-TEX-N-32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path><path id="MJX-1-TEX-I-1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D462"></use></g><g data-mml-node="mi" transform="translate(572, 363) scale(0.707)"><use xlink:href="#MJX-1-TEX-I-1D447"></use></g></g><g data-mml-node="mi" transform="translate(1119.8, 0)"><use xlink:href="#MJX-1-TEX-I-1D462"></use></g><g data-mml-node="mo" transform="translate(1969.6, 0)"><use xlink:href="#MJX-1-TEX-N-3D"></use></g><g data-mml-node="mn" transform="translate(3025.4, 0)"><use xlink:href="#MJX-1-TEX-N-32"></use></g><g data-mml-node="msup" transform="translate(3525.4, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D462"></use></g><g data-mml-node="mi" transform="translate(572, 363) scale(0.707)"><use xlink:href="#MJX-1-TEX-I-1D447"></use></g></g><g data-mml-node="mi" transform="translate(4645.2, 0)"><use xlink:href="#MJX-1-TEX-I-1D465"></use></g></g></g></svg></mjx-container></div><p>finally</p><div style="display: flex;justify-content: center;">    <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.884ex" xmlns="http://www.w3.org/2000/svg" width="34.481ex" height="3.122ex" role="img" focusable="false" viewBox="0 -989.2 15240.4 1379.8" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D43B" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 219 683Q260 681 355 681Q389 681 418 681T463 682T483 682Q499 682 499 672Q499 670 497 658Q492 641 487 638H485Q483 638 480 638T473 638T464 637T455 637Q416 636 405 634T387 623Q384 619 355 500Q348 474 340 442T328 395L324 380Q324 378 469 378H614L615 381Q615 384 646 504Q674 619 674 627T617 637Q594 637 587 639T580 648Q580 650 582 660Q586 677 588 679T604 682Q609 682 646 681T740 680Q802 680 835 681T871 682Q888 682 888 672Q888 645 876 638H874Q872 638 869 638T862 638T853 637T844 637Q805 636 794 634T776 623Q773 618 704 340T634 58Q634 51 638 51Q646 48 692 46H723Q729 38 729 37T726 19Q722 6 716 0H701Q664 2 567 2Q533 2 504 2T458 2T437 1Q420 1 420 10Q420 15 423 24Q428 43 433 45Q437 46 448 46H454Q481 46 514 49Q520 50 522 50T528 55T534 64T540 82T547 110T558 153Q565 181 569 198Q602 330 602 331T457 332H312L279 197Q245 63 245 58Q245 51 253 49T303 46H334Q340 38 340 37T337 19Q333 6 327 0H312Q275 2 178 2Q144 2 115 2T69 2T48 1Q31 1 31 10Q31 12 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path><path id="MJX-1-TEX-I-1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path><path id="MJX-1-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-1-TEX-N-2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path><path id="MJX-1-TEX-N-32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path><path id="MJX-1-TEX-I-1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-1-TEX-I-1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path><path id="MJX-1-TEX-I-1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path><path id="MJX-1-TEX-N-2016" d="M133 736Q138 750 153 750Q164 750 170 739Q172 735 172 250T170 -239Q164 -250 152 -250Q144 -250 138 -244L137 -243Q133 -241 133 -179T132 250Q132 731 133 736ZM329 739Q334 750 346 750Q353 750 361 744L362 743Q366 741 366 679T367 250T367 -178T362 -243L361 -244Q355 -250 347 -250Q335 -250 329 -239Q327 -235 327 250T329 739Z"></path><path id="MJX-1-TEX-I-1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path><path id="MJX-1-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D43B"></use></g><g data-mml-node="mi" transform="translate(888, 0)"><use xlink:href="#MJX-1-TEX-I-1D465"></use></g><g data-mml-node="mo" transform="translate(1737.8, 0)"><use xlink:href="#MJX-1-TEX-N-3D"></use></g><g data-mml-node="mi" transform="translate(2793.6, 0)"><use xlink:href="#MJX-1-TEX-I-1D465"></use></g><g data-mml-node="mo" transform="translate(3587.8, 0)"><use xlink:href="#MJX-1-TEX-N-2212"></use></g><g data-mml-node="mn" transform="translate(4588, 0)"><use xlink:href="#MJX-1-TEX-N-32"></use></g><g data-mml-node="mi" transform="translate(5088, 0)"><use xlink:href="#MJX-1-TEX-I-1D462"></use></g><g data-mml-node="mfrac" transform="translate(5660, 0)"><g data-mml-node="mrow" transform="translate(220, 394) scale(0.707)"><g data-mml-node="msup"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D462"></use></g><g data-mml-node="mi" transform="translate(572, 363) scale(0.707)"><use xlink:href="#MJX-1-TEX-I-1D447"></use></g></g><g data-mml-node="mi" transform="translate(1119.8, 0)"><use xlink:href="#MJX-1-TEX-I-1D465"></use></g></g><g data-mml-node="mrow" transform="translate(220, -382.9) scale(0.707)"><g data-mml-node="msup"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D462"></use></g><g data-mml-node="mi" transform="translate(572, 289) scale(0.707)"><use xlink:href="#MJX-1-TEX-I-1D447"></use></g></g><g data-mml-node="mi" transform="translate(1119.8, 0)"><use xlink:href="#MJX-1-TEX-I-1D462"></use></g></g><rect width="1396.3" height="60" x="120" y="220"></rect></g><g data-mml-node="mo" transform="translate(7574.1, 0)"><use xlink:href="#MJX-1-TEX-N-3D"></use></g><g data-mml-node="mi" transform="translate(8629.8, 0)"><use xlink:href="#MJX-1-TEX-I-1D465"></use></g><g data-mml-node="mo" transform="translate(9424.1, 0)"><use xlink:href="#MJX-1-TEX-N-2212"></use></g><g data-mml-node="mi" transform="translate(10424.3, 0)"><use xlink:href="#MJX-1-TEX-I-1D462"></use></g><g data-mml-node="mo" transform="translate(11274.1, 0)"><use xlink:href="#MJX-1-TEX-N-3D"></use></g><g data-mml-node="mi" transform="translate(12329.8, 0)"><use xlink:href="#MJX-1-TEX-I-1D460"></use></g><g data-mml-node="mo" transform="translate(12798.8, 0)"><use xlink:href="#MJX-1-TEX-N-2016"></use></g><g data-mml-node="mi" transform="translate(13298.8, 0)"><use xlink:href="#MJX-1-TEX-I-1D465"></use></g><g data-mml-node="mo" transform="translate(13870.8, 0)"><use xlink:href="#MJX-1-TEX-N-2016"></use></g><g data-mml-node="msub" transform="translate(14370.8, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D452"></use></g><g data-mml-node="mn" transform="translate(466, -150) scale(0.707)"><use xlink:href="#MJX-1-TEX-N-31"></use></g></g></g></g></svg></mjx-container></div><p>As a byproduct of this calculation, note that we have</p><div style="display: flex;justify-content: center;">    <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="16.561ex" height="2.47ex" role="img" focusable="false" viewBox="0 -841.7 7319.9 1091.7" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-1-TEX-I-1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path><path id="MJX-1-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-1-TEX-N-2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path><path id="MJX-1-TEX-N-32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path><path id="MJX-1-TEX-I-1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path><path id="MJX-1-TEX-N-2016" d="M133 736Q138 750 153 750Q164 750 170 739Q172 735 172 250T170 -239Q164 -250 152 -250Q144 -250 138 -244L137 -243Q133 -241 133 -179T132 250Q132 731 133 736ZM329 739Q334 750 346 750Q353 750 361 744L362 743Q366 741 366 679T367 250T367 -178T362 -243L361 -244Q355 -250 347 -250Q335 -250 329 -239Q327 -235 327 250T329 739Z"></path><path id="MJX-1-TEX-I-1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path><path id="MJX-1-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D462"></use></g><g data-mml-node="mi" transform="translate(572, 363) scale(0.707)"><use xlink:href="#MJX-1-TEX-I-1D447"></use></g></g><g data-mml-node="mi" transform="translate(1119.8, 0)"><use xlink:href="#MJX-1-TEX-I-1D462"></use></g><g data-mml-node="mo" transform="translate(1969.6, 0)"><use xlink:href="#MJX-1-TEX-N-3D"></use></g><g data-mml-node="mo" transform="translate(3025.4, 0)"><use xlink:href="#MJX-1-TEX-N-2212"></use></g><g data-mml-node="mn" transform="translate(3803.4, 0)"><use xlink:href="#MJX-1-TEX-N-32"></use></g><g data-mml-node="mi" transform="translate(4303.4, 0)"><use xlink:href="#MJX-1-TEX-I-1D460"></use></g><g data-mml-node="mo" transform="translate(4772.4, 0)"><use xlink:href="#MJX-1-TEX-N-2016"></use></g><g data-mml-node="mi" transform="translate(5272.4, 0)"><use xlink:href="#MJX-1-TEX-I-1D465"></use></g><g data-mml-node="mo" transform="translate(5844.4, 0)"><use xlink:href="#MJX-1-TEX-N-2016"></use></g><g data-mml-node="msub" transform="translate(6344.4, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D462"></use></g><g data-mml-node="mn" transform="translate(572, -150) scale(0.707)"><use xlink:href="#MJX-1-TEX-N-31"></use></g></g></g></g></svg></mjx-container></div><p>where <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="14.815ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 6548.1 1000" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-1-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-1-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-1-TEX-I-1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path><path id="MJX-1-TEX-N-2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path><path id="MJX-1-TEX-I-1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path><path id="MJX-1-TEX-N-2016" d="M133 736Q138 750 153 750Q164 750 170 739Q172 735 172 250T170 -239Q164 -250 152 -250Q144 -250 138 -244L137 -243Q133 -241 133 -179T132 250Q132 731 133 736ZM329 739Q334 750 346 750Q353 750 361 744L362 743Q366 741 366 679T367 250T367 -178T362 -243L361 -244Q355 -250 347 -250Q335 -250 329 -239Q327 -235 327 250T329 739Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D462"></use></g><g data-mml-node="mn" transform="translate(572, -150) scale(0.707)"><use xlink:href="#MJX-1-TEX-N-31"></use></g></g><g data-mml-node="mo" transform="translate(1253.3, 0)"><use xlink:href="#MJX-1-TEX-N-3D"></use></g><g data-mml-node="msub" transform="translate(2309.1, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D465"></use></g><g data-mml-node="mn" transform="translate(572, -150) scale(0.707)"><use xlink:href="#MJX-1-TEX-N-31"></use></g></g><g data-mml-node="mo" transform="translate(3506.9, 0)"><use xlink:href="#MJX-1-TEX-N-2212"></use></g><g data-mml-node="mi" transform="translate(4507.1, 0)"><use xlink:href="#MJX-1-TEX-I-1D460"></use></g><g data-mml-node="mo" transform="translate(4976.1, 0)"><use xlink:href="#MJX-1-TEX-N-2016"></use></g><g data-mml-node="mi" transform="translate(5476.1, 0)"><use xlink:href="#MJX-1-TEX-I-1D465"></use></g><g data-mml-node="mo" transform="translate(6048.1, 0)"><use xlink:href="#MJX-1-TEX-N-2016"></use></g></g></g></svg></mjx-container>; and if we define <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="9.269ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 4097.1 1000" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path><path id="MJX-1-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-1-TEX-I-1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-1-TEX-N-2F" d="M423 750Q432 750 438 744T444 730Q444 725 271 248T92 -240Q85 -250 75 -250Q68 -250 62 -245T56 -231Q56 -221 230 257T407 740Q411 750 423 750Z"></path><path id="MJX-1-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D464"></use></g><g data-mml-node="mo" transform="translate(993.8, 0)"><use xlink:href="#MJX-1-TEX-N-3D"></use></g><g data-mml-node="mi" transform="translate(2049.6, 0)"><use xlink:href="#MJX-1-TEX-I-1D462"></use></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(2621.6, 0)"><g data-mml-node="mo"><use xlink:href="#MJX-1-TEX-N-2F"></use></g></g><g data-mml-node="msub" transform="translate(3121.6, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D462"></use></g><g data-mml-node="mn" transform="translate(572, -150) scale(0.707)"><use xlink:href="#MJX-1-TEX-N-31"></use></g></g></g></g></svg></mjx-container>, we have</p><div style="display: flex;justify-content: center;">    <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -1.238ex" xmlns="http://www.w3.org/2000/svg" width="41.711ex" height="3.476ex" role="img" focusable="false" viewBox="0 -989.2 18436.1 1536.3" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D43B" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 219 683Q260 681 355 681Q389 681 418 681T463 682T483 682Q499 682 499 672Q499 670 497 658Q492 641 487 638H485Q483 638 480 638T473 638T464 637T455 637Q416 636 405 634T387 623Q384 619 355 500Q348 474 340 442T328 395L324 380Q324 378 469 378H614L615 381Q615 384 646 504Q674 619 674 627T617 637Q594 637 587 639T580 648Q580 650 582 660Q586 677 588 679T604 682Q609 682 646 681T740 680Q802 680 835 681T871 682Q888 682 888 672Q888 645 876 638H874Q872 638 869 638T862 638T853 637T844 637Q805 636 794 634T776 623Q773 618 704 340T634 58Q634 51 638 51Q646 48 692 46H723Q729 38 729 37T726 19Q722 6 716 0H701Q664 2 567 2Q533 2 504 2T458 2T437 1Q420 1 420 10Q420 15 423 24Q428 43 433 45Q437 46 448 46H454Q481 46 514 49Q520 50 522 50T528 55T534 64T540 82T547 110T558 153Q565 181 569 198Q602 330 602 331T457 332H312L279 197Q245 63 245 58Q245 51 253 49T303 46H334Q340 38 340 37T337 19Q333 6 327 0H312Q275 2 178 2Q144 2 115 2T69 2T48 1Q31 1 31 10Q31 12 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path><path id="MJX-1-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-1-TEX-I-1D43C" d="M43 1Q26 1 26 10Q26 12 29 24Q34 43 39 45Q42 46 54 46H60Q120 46 136 53Q137 53 138 54Q143 56 149 77T198 273Q210 318 216 344Q286 624 286 626Q284 630 284 631Q274 637 213 637H193Q184 643 189 662Q193 677 195 680T209 683H213Q285 681 359 681Q481 681 487 683H497Q504 676 504 672T501 655T494 639Q491 637 471 637Q440 637 407 634Q393 631 388 623Q381 609 337 432Q326 385 315 341Q245 65 245 59Q245 52 255 50T307 46H339Q345 38 345 37T342 19Q338 6 332 0H316Q279 2 179 2Q143 2 113 2T65 2T43 1Z"></path><path id="MJX-1-TEX-N-2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path><path id="MJX-1-TEX-N-32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path><path id="MJX-1-TEX-I-1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path><path id="MJX-1-TEX-I-1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path><path id="MJX-1-TEX-N-2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path><path id="MJX-1-TEX-I-1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path><path id="MJX-1-TEX-I-1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-1-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-1-TEX-N-2016" d="M133 736Q138 750 153 750Q164 750 170 739Q172 735 172 250T170 -239Q164 -250 152 -250Q144 -250 138 -244L137 -243Q133 -241 133 -179T132 250Q132 731 133 736ZM329 739Q334 750 346 750Q353 750 361 744L362 743Q366 741 366 679T367 250T367 -178T362 -243L361 -244Q355 -250 347 -250Q335 -250 329 -239Q327 -235 327 250T329 739Z"></path><path id="MJX-1-TEX-I-1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path><path id="MJX-1-TEX-I-1D70F" d="M39 284Q18 284 18 294Q18 301 45 338T99 398Q134 425 164 429Q170 431 332 431Q492 431 497 429Q517 424 517 402Q517 388 508 376T485 360Q479 358 389 358T299 356Q298 355 283 274T251 109T233 20Q228 5 215 -4T186 -13Q153 -13 153 20V30L203 192Q214 228 227 272T248 336L254 357Q254 358 208 358Q206 358 197 358T183 359Q105 359 61 295Q56 287 53 286T39 284Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D43B"></use></g><g data-mml-node="mo" transform="translate(1165.8, 0)"><use xlink:href="#MJX-1-TEX-N-3D"></use></g><g data-mml-node="mi" transform="translate(2221.6, 0)"><use xlink:href="#MJX-1-TEX-I-1D43C"></use></g><g data-mml-node="mo" transform="translate(2947.8, 0)"><use xlink:href="#MJX-1-TEX-N-2212"></use></g><g data-mml-node="mn" transform="translate(3948, 0)"><use xlink:href="#MJX-1-TEX-N-32"></use></g><g data-mml-node="mfrac" transform="translate(4448, 0)"><g data-mml-node="mrow" transform="translate(220, 394) scale(0.707)"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D464"></use></g><g data-mml-node="msup" transform="translate(716, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D464"></use></g><g data-mml-node="mi" transform="translate(716, 363) scale(0.707)"><use xlink:href="#MJX-1-TEX-I-1D447"></use></g></g></g><g data-mml-node="mrow" transform="translate(220, -382.9) scale(0.707)"><g data-mml-node="msup"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D464"></use></g><g data-mml-node="mi" transform="translate(716, 289) scale(0.707)"><use xlink:href="#MJX-1-TEX-I-1D447"></use></g></g><g data-mml-node="mi" transform="translate(1263.8, 0)"><use xlink:href="#MJX-1-TEX-I-1D464"></use></g></g><rect width="1599.9" height="60" x="120" y="220"></rect></g><g data-mml-node="mo" transform="translate(6565.7, 0)"><use xlink:href="#MJX-1-TEX-N-3D"></use></g><g data-mml-node="mi" transform="translate(7621.5, 0)"><use xlink:href="#MJX-1-TEX-I-1D43C"></use></g><g data-mml-node="mo" transform="translate(8347.7, 0)"><use xlink:href="#MJX-1-TEX-N-2B"></use></g><g data-mml-node="mfrac" transform="translate(9347.9, 0)"><g data-mml-node="mrow" transform="translate(265.1, 446.1) scale(0.707)"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D460"></use></g><g data-mml-node="msub" transform="translate(469, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D462"></use></g><g data-mml-node="mn" transform="translate(572, -150) scale(0.707)"><use xlink:href="#MJX-1-TEX-N-31"></use></g></g></g><g data-mml-node="mrow" transform="translate(220, -370.3) scale(0.707)"><g data-mml-node="mo"><use xlink:href="#MJX-1-TEX-N-2016"></use></g><g data-mml-node="mi" transform="translate(500, 0)"><use xlink:href="#MJX-1-TEX-I-1D465"></use></g><g data-mml-node="mo" transform="translate(1072, 0)"><use xlink:href="#MJX-1-TEX-N-2016"></use></g></g><rect width="1311.6" height="60" x="120" y="220"></rect></g><g data-mml-node="mi" transform="translate(10899.5, 0)"><use xlink:href="#MJX-1-TEX-I-1D464"></use></g><g data-mml-node="msup" transform="translate(11615.5, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D464"></use></g><g data-mml-node="mi" transform="translate(716, 363) scale(0.707)"><use xlink:href="#MJX-1-TEX-I-1D447"></use></g></g><g data-mml-node="mo" transform="translate(13157.1, 0)"><use xlink:href="#MJX-1-TEX-N-3D"></use></g><g data-mml-node="mi" transform="translate(14212.9, 0)"><use xlink:href="#MJX-1-TEX-I-1D43C"></use></g><g data-mml-node="mo" transform="translate(14939.1, 0)"><use xlink:href="#MJX-1-TEX-N-2212"></use></g><g data-mml-node="mi" transform="translate(15939.3, 0)"><use xlink:href="#MJX-1-TEX-I-1D70F"></use></g><g data-mml-node="mi" transform="translate(16456.3, 0)"><use xlink:href="#MJX-1-TEX-I-1D464"></use></g><g data-mml-node="msup" transform="translate(17172.3, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D464"></use></g><g data-mml-node="mi" transform="translate(716, 363) scale(0.707)"><use xlink:href="#MJX-1-TEX-I-1D447"></use></g></g></g></g></svg></mjx-container></div><p>where <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="13.903ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 6145.1 1000" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D70F" d="M39 284Q18 284 18 294Q18 301 45 338T99 398Q134 425 164 429Q170 431 332 431Q492 431 497 429Q517 424 517 402Q517 388 508 376T485 360Q479 358 389 358T299 356Q298 355 283 274T251 109T233 20Q228 5 215 -4T186 -13Q153 -13 153 20V30L203 192Q214 228 227 272T248 336L254 357Q254 358 208 358Q206 358 197 358T183 359Q105 359 61 295Q56 287 53 286T39 284Z"></path><path id="MJX-1-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-1-TEX-N-2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path><path id="MJX-1-TEX-I-1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path><path id="MJX-1-TEX-I-1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-1-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-1-TEX-N-2F" d="M423 750Q432 750 438 744T444 730Q444 725 271 248T92 -240Q85 -250 75 -250Q68 -250 62 -245T56 -231Q56 -221 230 257T407 740Q411 750 423 750Z"></path><path id="MJX-1-TEX-N-2016" d="M133 736Q138 750 153 750Q164 750 170 739Q172 735 172 250T170 -239Q164 -250 152 -250Q144 -250 138 -244L137 -243Q133 -241 133 -179T132 250Q132 731 133 736ZM329 739Q334 750 346 750Q353 750 361 744L362 743Q366 741 366 679T367 250T367 -178T362 -243L361 -244Q355 -250 347 -250Q335 -250 329 -239Q327 -235 327 250T329 739Z"></path><path id="MJX-1-TEX-I-1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D70F"></use></g><g data-mml-node="mo" transform="translate(794.8, 0)"><use xlink:href="#MJX-1-TEX-N-3D"></use></g><g data-mml-node="mo" transform="translate(1850.6, 0)"><use xlink:href="#MJX-1-TEX-N-2212"></use></g><g data-mml-node="mi" transform="translate(2628.6, 0)"><use xlink:href="#MJX-1-TEX-I-1D460"></use></g><g data-mml-node="msub" transform="translate(3097.6, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D462"></use></g><g data-mml-node="mn" transform="translate(572, -150) scale(0.707)"><use xlink:href="#MJX-1-TEX-N-31"></use></g></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(4073.1, 0)"><g data-mml-node="mo"><use xlink:href="#MJX-1-TEX-N-2F"></use></g></g><g data-mml-node="mo" transform="translate(4573.1, 0)"><use xlink:href="#MJX-1-TEX-N-2016"></use></g><g data-mml-node="mi" transform="translate(5073.1, 0)"><use xlink:href="#MJX-1-TEX-I-1D465"></use></g><g data-mml-node="mo" transform="translate(5645.1, 0)"><use xlink:href="#MJX-1-TEX-N-2016"></use></g></g></g></svg></mjx-container>.</p><h4 id="Matlab-code-2"><a href="#Matlab-code-2" class="headerlink" title="Matlab code"></a>Matlab code</h4><pre class="line-numbers language-matlab" data-language="matlab"><code class="language-matlab"><span class="token keyword">function</span> <span class="token punctuation">[</span>Q<span class="token punctuation">,</span>R<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token function">householder_qr</span><span class="token punctuation">(</span>A<span class="token punctuation">)</span>    <span class="token punctuation">[</span>m<span class="token punctuation">,</span> n<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token function">size</span><span class="token punctuation">(</span>A<span class="token punctuation">)</span><span class="token punctuation">;</span>    Q <span class="token operator">=</span> <span class="token function">eye</span><span class="token punctuation">(</span>m<span class="token punctuation">)</span><span class="token punctuation">;</span>    R <span class="token operator">=</span> A<span class="token punctuation">;</span>    I <span class="token operator">=</span> <span class="token function">eye</span><span class="token punctuation">(</span>n<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">for</span> <span class="token number">j</span> <span class="token operator">=</span> <span class="token number">1</span><span class="token operator">:</span>n<span class="token operator">-</span><span class="token number">1</span>        x <span class="token operator">=</span> <span class="token function">R</span><span class="token punctuation">(</span><span class="token number">j</span><span class="token operator">:</span>n<span class="token punctuation">,</span> <span class="token number">j</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        v <span class="token operator">=</span> <span class="token operator">-</span><span class="token function">sign</span><span class="token punctuation">(</span><span class="token function">x</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token function">norm</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token function">eye</span><span class="token punctuation">(</span>n<span class="token operator">-</span><span class="token number">j</span><span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">-</span> x<span class="token punctuation">;</span>        <span class="token keyword">if</span> <span class="token function">norm</span><span class="token punctuation">(</span>v<span class="token punctuation">)</span> <span class="token operator">></span> <span class="token number">0</span>            v <span class="token operator">=</span> v <span class="token operator">/</span> <span class="token function">norm</span><span class="token punctuation">(</span>v<span class="token punctuation">)</span><span class="token punctuation">;</span>            P <span class="token operator">=</span> I<span class="token punctuation">;</span>            <span class="token function">P</span><span class="token punctuation">(</span><span class="token number">j</span><span class="token operator">:</span>n<span class="token punctuation">,</span> <span class="token number">j</span><span class="token operator">:</span>n<span class="token punctuation">)</span> <span class="token operator">=</span> <span class="token function">P</span><span class="token punctuation">(</span><span class="token number">j</span><span class="token operator">:</span>n<span class="token punctuation">,</span> <span class="token number">j</span><span class="token operator">:</span>n<span class="token punctuation">)</span> <span class="token operator">-</span> <span class="token number">2</span><span class="token operator">*</span>v<span class="token operator">*</span>v<span class="token operator">'</span><span class="token punctuation">;</span>            R <span class="token operator">=</span> P <span class="token operator">*</span> R<span class="token punctuation">;</span>            Q <span class="token operator">=</span> Q <span class="token operator">*</span> P<span class="token punctuation">;</span>        <span class="token keyword">end</span>    <span class="token keyword">end</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="Python-code-2"><a href="#Python-code-2" class="headerlink" title="Python code"></a>Python code</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Decomposition</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""    https://stackoverflow.com/a/53493770/15048366    """</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">pass</span>    <span class="token keyword">def</span> <span class="token function">householder_vectorised</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> arr<span class="token punctuation">)</span><span class="token punctuation">:</span>        v <span class="token operator">=</span> arr <span class="token operator">/</span> <span class="token punctuation">(</span>arr<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">+</span> np<span class="token punctuation">.</span>copysign<span class="token punctuation">(</span>np<span class="token punctuation">.</span>linalg<span class="token punctuation">.</span>norm<span class="token punctuation">(</span>arr<span class="token punctuation">)</span><span class="token punctuation">,</span> arr<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        v<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">1</span>        tau <span class="token operator">=</span> <span class="token number">2</span> <span class="token operator">/</span> <span class="token punctuation">(</span>v<span class="token punctuation">.</span>T @ v<span class="token punctuation">)</span>        <span class="token keyword">return</span> v<span class="token punctuation">,</span> tau    <span class="token keyword">def</span> <span class="token function">householder_qr</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> A<span class="token punctuation">)</span><span class="token punctuation">:</span>        m<span class="token punctuation">,</span> n <span class="token operator">=</span> A<span class="token punctuation">.</span>shape        Q <span class="token operator">=</span> np<span class="token punctuation">.</span>identity<span class="token punctuation">(</span>m<span class="token punctuation">)</span>        R <span class="token operator">=</span> A<span class="token punctuation">.</span>copy<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">for</span> j <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> n<span class="token punctuation">)</span><span class="token punctuation">:</span>            v<span class="token punctuation">,</span> tau <span class="token operator">=</span> self<span class="token punctuation">.</span>householder_vectorised<span class="token punctuation">(</span>R<span class="token punctuation">[</span>j<span class="token punctuation">:</span><span class="token punctuation">,</span> j<span class="token punctuation">,</span> np<span class="token punctuation">.</span>newaxis<span class="token punctuation">]</span><span class="token punctuation">)</span>            H <span class="token operator">=</span> np<span class="token punctuation">.</span>identity<span class="token punctuation">(</span>m<span class="token punctuation">)</span>            H<span class="token punctuation">[</span>j<span class="token punctuation">:</span><span class="token punctuation">,</span> j<span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">-=</span> tau <span class="token operator">*</span> <span class="token punctuation">(</span>v @ v<span class="token punctuation">.</span>T<span class="token punctuation">)</span>            R <span class="token operator">=</span> H @ R            Q <span class="token operator">=</span> H @ Q        Q<span class="token punctuation">,</span> R <span class="token operator">=</span> Q<span class="token punctuation">[</span><span class="token punctuation">:</span>n<span class="token punctuation">]</span><span class="token punctuation">.</span>T<span class="token punctuation">,</span> np<span class="token punctuation">.</span>triu<span class="token punctuation">(</span>R<span class="token punctuation">[</span><span class="token punctuation">:</span>n<span class="token punctuation">]</span><span class="token punctuation">)</span>        <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>n<span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token keyword">if</span> R<span class="token punctuation">[</span>i<span class="token punctuation">,</span> i<span class="token punctuation">]</span> <span class="token operator">&lt;</span> <span class="token number">0</span><span class="token punctuation">:</span>                Q<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> i<span class="token punctuation">]</span> <span class="token operator">*=</span> <span class="token operator">-</span><span class="token number">1</span>                R<span class="token punctuation">[</span>i<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">*=</span> <span class="token operator">-</span><span class="token number">1</span>                        <span class="token keyword">return</span> Q<span class="token punctuation">,</span> R<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>if <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.09ex" xmlns="http://www.w3.org/2000/svg" width="6.361ex" height="1.312ex" role="img" focusable="false" viewBox="0 -540 2811.6 580" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-1-TEX-N-3E" d="M84 520Q84 528 88 533T96 539L99 540Q106 540 253 471T544 334L687 265Q694 260 694 250T687 235Q685 233 395 96L107 -40H101Q83 -38 83 -20Q83 -19 83 -17Q82 -10 98 -1Q117 9 248 71Q326 108 378 132L626 250L378 368Q90 504 86 509Q84 513 84 520Z"></path><path id="MJX-1-TEX-I-1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D45A"></use></g><g data-mml-node="mo" transform="translate(1155.8, 0)"><use xlink:href="#MJX-1-TEX-N-3E"></use></g><g data-mml-node="mi" transform="translate(2211.6, 0)"><use xlink:href="#MJX-1-TEX-I-1D45B"></use></g></g></g></svg></mjx-container>:</p><ul><li><p>Full QR factorisation</p><img src="/Hexo-Blog/2021/01/29/2021-01-29-matrix-decomposition/full_qr.png" class=""></li><li><p>Reduced QR factorisation</p><img src="/Hexo-Blog/2021/01/29/2021-01-29-matrix-decomposition/reduced_qr.png" class=""></li></ul><h1 id="Linear-Function"><a href="#Linear-Function" class="headerlink" title="Linear Function"></a>Linear Function</h1><p>After decomposing matrix <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0" xmlns="http://www.w3.org/2000/svg" width="1.697ex" height="1.62ex" role="img" focusable="false" viewBox="0 -716 750 716" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D434"></use></g></g></g></svg></mjx-container>, you can write a function in python to solve a system</p><div style="display: flex;justify-content: center;">    <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.186ex" xmlns="http://www.w3.org/2000/svg" width="6.979ex" height="1.805ex" role="img" focusable="false" viewBox="0 -716 3084.6 798" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"></path><path id="MJX-1-TEX-I-1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path><path id="MJX-1-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-1-TEX-I-1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D434"></use></g><g data-mml-node="mi" transform="translate(750, 0)"><use xlink:href="#MJX-1-TEX-I-1D465"></use></g><g data-mml-node="mo" transform="translate(1599.8, 0)"><use xlink:href="#MJX-1-TEX-N-3D"></use></g><g data-mml-node="mi" transform="translate(2655.6, 0)"><use xlink:href="#MJX-1-TEX-I-1D44F"></use></g></g></g></svg></mjx-container></div><p>using LU decomposition and QR decomposition. Your function should take <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0" xmlns="http://www.w3.org/2000/svg" width="1.697ex" height="1.62ex" role="img" focusable="false" viewBox="0 -716 750 716" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D434"></use></g></g></g></svg></mjx-container> and <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="0.971ex" height="1.595ex" role="img" focusable="false" viewBox="0 -694 429 705" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D44F"></use></g></g></g></svg></mjx-container> as input and return <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="1.294ex" height="1.025ex" role="img" focusable="false" viewBox="0 -442 572 453" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D465"></use></g></g></g></svg></mjx-container>.</p><p>Function should include the following:</p><ul><li>Check that <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0" xmlns="http://www.w3.org/2000/svg" width="1.697ex" height="1.62ex" role="img" focusable="false" viewBox="0 -716 750 716" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D434"></use></g></g></g></svg></mjx-container> is not a singular matrix, that is, <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0" xmlns="http://www.w3.org/2000/svg" width="1.697ex" height="1.62ex" role="img" focusable="false" viewBox="0 -716 750 716" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D434"></use></g></g></g></svg></mjx-container> is invertible.</li><li>Invert <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0" xmlns="http://www.w3.org/2000/svg" width="1.697ex" height="1.62ex" role="img" focusable="false" viewBox="0 -716 750 716" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D434"></use></g></g></g></svg></mjx-container> using different decomposition methods and solve</li><li>Return <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="1.294ex" height="1.025ex" role="img" focusable="false" viewBox="0 -442 572 453" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D465"></use></g></g></g></svg></mjx-container>.</li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Decomposition</span><span class="token punctuation">:</span>        <span class="token keyword">def</span> <span class="token function">plu</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> A<span class="token punctuation">)</span><span class="token punctuation">:</span>        n <span class="token operator">=</span> A<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>        P <span class="token operator">=</span> np<span class="token punctuation">.</span>identity<span class="token punctuation">(</span>n<span class="token punctuation">)</span>        L <span class="token operator">=</span> np<span class="token punctuation">.</span>identity<span class="token punctuation">(</span>n<span class="token punctuation">)</span>        U <span class="token operator">=</span> A<span class="token punctuation">.</span>copy<span class="token punctuation">(</span><span class="token punctuation">)</span>                PF <span class="token operator">=</span> np<span class="token punctuation">.</span>identity<span class="token punctuation">(</span>n<span class="token punctuation">)</span>        LF <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>shape<span class="token operator">=</span><span class="token punctuation">(</span>n<span class="token punctuation">,</span> n<span class="token punctuation">)</span><span class="token punctuation">)</span>                <span class="token comment"># Loop over rows</span>        <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>n<span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            index <span class="token operator">=</span> np<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span><span class="token builtin">abs</span><span class="token punctuation">(</span>U<span class="token punctuation">[</span>i<span class="token punctuation">:</span><span class="token punctuation">,</span> i<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>            index <span class="token operator">=</span> index <span class="token operator">+</span> i            <span class="token keyword">if</span> index <span class="token operator">!=</span> i<span class="token punctuation">:</span>                P <span class="token operator">=</span> np<span class="token punctuation">.</span>identity<span class="token punctuation">(</span>n<span class="token punctuation">)</span>                P<span class="token punctuation">[</span><span class="token punctuation">[</span>index<span class="token punctuation">,</span> i<span class="token punctuation">]</span><span class="token punctuation">,</span> i<span class="token punctuation">:</span>n<span class="token punctuation">]</span> <span class="token operator">=</span> P<span class="token punctuation">[</span><span class="token punctuation">[</span>i<span class="token punctuation">,</span> index<span class="token punctuation">]</span><span class="token punctuation">,</span> i<span class="token punctuation">:</span>n<span class="token punctuation">]</span>                U<span class="token punctuation">[</span><span class="token punctuation">[</span>index<span class="token punctuation">,</span> i<span class="token punctuation">]</span><span class="token punctuation">,</span> i<span class="token punctuation">:</span>n<span class="token punctuation">]</span> <span class="token operator">=</span> U<span class="token punctuation">[</span><span class="token punctuation">[</span>i<span class="token punctuation">,</span> index<span class="token punctuation">]</span><span class="token punctuation">,</span> i<span class="token punctuation">:</span>n<span class="token punctuation">]</span>                 PF <span class="token operator">=</span> np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>P<span class="token punctuation">,</span> PF<span class="token punctuation">)</span>                LF <span class="token operator">=</span> np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>P<span class="token punctuation">,</span> LF<span class="token punctuation">)</span>            L <span class="token operator">=</span> np<span class="token punctuation">.</span>identity<span class="token punctuation">(</span>n<span class="token punctuation">)</span>            <span class="token keyword">for</span> j <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>i<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">,</span> n<span class="token punctuation">)</span><span class="token punctuation">:</span>                L<span class="token punctuation">[</span>j<span class="token punctuation">,</span> i<span class="token punctuation">]</span>  <span class="token operator">=</span> <span class="token operator">-</span><span class="token punctuation">(</span>U<span class="token punctuation">[</span>j<span class="token punctuation">,</span> i<span class="token punctuation">]</span> <span class="token operator">/</span> U<span class="token punctuation">[</span>i<span class="token punctuation">,</span> i<span class="token punctuation">]</span><span class="token punctuation">)</span>                LF<span class="token punctuation">[</span>j<span class="token punctuation">,</span> i<span class="token punctuation">]</span> <span class="token operator">=</span>  <span class="token punctuation">(</span>U<span class="token punctuation">[</span>j<span class="token punctuation">,</span> i<span class="token punctuation">]</span> <span class="token operator">/</span> U<span class="token punctuation">[</span>i<span class="token punctuation">,</span> i<span class="token punctuation">]</span><span class="token punctuation">)</span>            U <span class="token operator">=</span> np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>L<span class="token punctuation">,</span> U<span class="token punctuation">)</span>        np<span class="token punctuation">.</span>fill_diagonal<span class="token punctuation">(</span>LF<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> PF<span class="token punctuation">,</span> LF<span class="token punctuation">,</span> U        <span class="token keyword">def</span> <span class="token function">gram_schmidt_qr</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> A<span class="token punctuation">)</span><span class="token punctuation">:</span>        m<span class="token punctuation">,</span> n <span class="token operator">=</span> A<span class="token punctuation">.</span>shape        Q <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>shape<span class="token operator">=</span><span class="token punctuation">(</span>m<span class="token punctuation">,</span> n<span class="token punctuation">)</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span><span class="token string">'float64'</span><span class="token punctuation">)</span>        R <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>shape<span class="token operator">=</span><span class="token punctuation">(</span>n<span class="token punctuation">,</span> n<span class="token punctuation">)</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span><span class="token string">'float64'</span><span class="token punctuation">)</span>        <span class="token keyword">for</span> j <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>n<span class="token punctuation">)</span><span class="token punctuation">:</span>            v <span class="token operator">=</span> A<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> j<span class="token punctuation">]</span>            <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>j<span class="token punctuation">)</span><span class="token punctuation">:</span>                q <span class="token operator">=</span> Q<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> i<span class="token punctuation">]</span>                R<span class="token punctuation">[</span>i<span class="token punctuation">,</span> j<span class="token punctuation">]</span> <span class="token operator">=</span> q<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>v<span class="token punctuation">)</span>                v <span class="token operator">=</span> v <span class="token operator">-</span> R<span class="token punctuation">[</span>i<span class="token punctuation">,</span> j<span class="token punctuation">]</span> <span class="token operator">*</span> q            Q<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> j<span class="token punctuation">]</span> <span class="token operator">=</span> v <span class="token operator">/</span> np<span class="token punctuation">.</span>linalg<span class="token punctuation">.</span>norm<span class="token punctuation">(</span>v<span class="token punctuation">)</span>            R<span class="token punctuation">[</span>j<span class="token punctuation">,</span> j<span class="token punctuation">]</span> <span class="token operator">=</span> np<span class="token punctuation">.</span>linalg<span class="token punctuation">.</span>norm<span class="token punctuation">(</span>v<span class="token punctuation">)</span>        <span class="token keyword">return</span> Q<span class="token punctuation">,</span> R        <span class="token keyword">def</span> <span class="token function">modified_gram_schmidt_qr</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> A<span class="token punctuation">)</span><span class="token punctuation">:</span>        n <span class="token operator">=</span> A<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>        Q <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>A<span class="token punctuation">,</span> dtype<span class="token operator">=</span><span class="token string">'float64'</span><span class="token punctuation">)</span>        R <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">(</span>n<span class="token punctuation">,</span> n<span class="token punctuation">)</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span><span class="token string">'float64'</span><span class="token punctuation">)</span>        <span class="token keyword">for</span> k <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>n<span class="token punctuation">)</span><span class="token punctuation">:</span>            a_k <span class="token operator">=</span> Q<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> k<span class="token punctuation">]</span>            R<span class="token punctuation">[</span>k<span class="token punctuation">,</span>k<span class="token punctuation">]</span> <span class="token operator">=</span> np<span class="token punctuation">.</span>linalg<span class="token punctuation">.</span>norm<span class="token punctuation">(</span>a_k<span class="token punctuation">)</span>            a_k <span class="token operator">/=</span> R<span class="token punctuation">[</span>k<span class="token punctuation">,</span> k<span class="token punctuation">]</span>            <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>k<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">,</span> n<span class="token punctuation">)</span><span class="token punctuation">:</span>                a_i <span class="token operator">=</span> Q<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> i<span class="token punctuation">]</span>                R<span class="token punctuation">[</span>k<span class="token punctuation">,</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> np<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span>a_k<span class="token punctuation">)</span> @ a_i                a_i <span class="token operator">-=</span> R<span class="token punctuation">[</span>k<span class="token punctuation">,</span> i<span class="token punctuation">]</span> <span class="token operator">*</span> a_k        <span class="token keyword">return</span> Q<span class="token punctuation">,</span> R        <span class="token keyword">def</span> <span class="token function">householder_vectorised</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> arr<span class="token punctuation">)</span><span class="token punctuation">:</span>        v <span class="token operator">=</span> arr <span class="token operator">/</span> <span class="token punctuation">(</span>arr<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">+</span> np<span class="token punctuation">.</span>copysign<span class="token punctuation">(</span>np<span class="token punctuation">.</span>linalg<span class="token punctuation">.</span>norm<span class="token punctuation">(</span>arr<span class="token punctuation">)</span><span class="token punctuation">,</span> arr<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        v<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">1</span>        tau <span class="token operator">=</span> <span class="token number">2</span> <span class="token operator">/</span> <span class="token punctuation">(</span>v<span class="token punctuation">.</span>T @ v<span class="token punctuation">)</span>        <span class="token keyword">return</span> v<span class="token punctuation">,</span> tau    <span class="token keyword">def</span> <span class="token function">householder_qr</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> A<span class="token punctuation">)</span><span class="token punctuation">:</span>        m<span class="token punctuation">,</span> n <span class="token operator">=</span> A<span class="token punctuation">.</span>shape        Q <span class="token operator">=</span> np<span class="token punctuation">.</span>identity<span class="token punctuation">(</span>m<span class="token punctuation">)</span>        R <span class="token operator">=</span> A<span class="token punctuation">.</span>copy<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">for</span> j <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> n<span class="token punctuation">)</span><span class="token punctuation">:</span>            v<span class="token punctuation">,</span> tau <span class="token operator">=</span> self<span class="token punctuation">.</span>householder_vectorised<span class="token punctuation">(</span>R<span class="token punctuation">[</span>j<span class="token punctuation">:</span><span class="token punctuation">,</span> j<span class="token punctuation">,</span> np<span class="token punctuation">.</span>newaxis<span class="token punctuation">]</span><span class="token punctuation">)</span>            H <span class="token operator">=</span> np<span class="token punctuation">.</span>identity<span class="token punctuation">(</span>m<span class="token punctuation">)</span>            H<span class="token punctuation">[</span>j<span class="token punctuation">:</span><span class="token punctuation">,</span> j<span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">-=</span> tau <span class="token operator">*</span> <span class="token punctuation">(</span>v @ v<span class="token punctuation">.</span>T<span class="token punctuation">)</span>            R <span class="token operator">=</span> H @ R            Q <span class="token operator">=</span> H @ Q        Q<span class="token punctuation">,</span> R <span class="token operator">=</span> Q<span class="token punctuation">[</span><span class="token punctuation">:</span>n<span class="token punctuation">]</span><span class="token punctuation">.</span>T<span class="token punctuation">,</span> np<span class="token punctuation">.</span>triu<span class="token punctuation">(</span>R<span class="token punctuation">[</span><span class="token punctuation">:</span>n<span class="token punctuation">]</span><span class="token punctuation">)</span>        <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>n<span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token keyword">if</span> R<span class="token punctuation">[</span>i<span class="token punctuation">,</span> i<span class="token punctuation">]</span> <span class="token operator">&lt;</span> <span class="token number">0</span><span class="token punctuation">:</span>                Q<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> i<span class="token punctuation">]</span> <span class="token operator">*=</span> <span class="token operator">-</span><span class="token number">1</span>                R<span class="token punctuation">[</span>i<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">*=</span> <span class="token operator">-</span><span class="token number">1</span>                        <span class="token keyword">return</span> Q<span class="token punctuation">,</span> R<span class="token keyword">def</span> <span class="token function">linear_function_solver</span><span class="token punctuation">(</span>A<span class="token punctuation">,</span> b<span class="token punctuation">,</span> method<span class="token operator">=</span><span class="token string">"LU"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    det <span class="token operator">=</span> ChioDeterminants<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>calculate<span class="token punctuation">(</span>A<span class="token punctuation">)</span>    factoriser <span class="token operator">=</span> Decomposition<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">if</span> det <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Matrix is singular!"</span><span class="token punctuation">)</span>        <span class="token keyword">return</span>    <span class="token keyword">if</span> method <span class="token operator">==</span> <span class="token string">"LU"</span><span class="token punctuation">:</span>        P<span class="token punctuation">,</span> L<span class="token punctuation">,</span> U <span class="token operator">=</span> factoriser<span class="token punctuation">.</span>plu<span class="token punctuation">(</span>A<span class="token punctuation">)</span>        z_1 <span class="token operator">=</span> P<span class="token punctuation">.</span>T @ b        z_2 <span class="token operator">=</span> np<span class="token punctuation">.</span>linalg<span class="token punctuation">.</span>inv<span class="token punctuation">(</span>L<span class="token punctuation">)</span> @ z_1        x <span class="token operator">=</span> np<span class="token punctuation">.</span>linalg<span class="token punctuation">.</span>inv<span class="token punctuation">(</span>U<span class="token punctuation">)</span> @ z_2        <span class="token keyword">return</span> x    <span class="token keyword">elif</span> method <span class="token operator">==</span> <span class="token string">"CGS"</span><span class="token punctuation">:</span>        Q<span class="token punctuation">,</span> R <span class="token operator">=</span> factoriser<span class="token punctuation">.</span>gram_schmidt_qr<span class="token punctuation">(</span>A<span class="token punctuation">)</span>        x <span class="token operator">=</span> np<span class="token punctuation">.</span>linalg<span class="token punctuation">.</span>inv<span class="token punctuation">(</span>R<span class="token punctuation">)</span> @ Q<span class="token punctuation">.</span>T @ b        <span class="token keyword">return</span> x    <span class="token keyword">elif</span> method <span class="token operator">==</span> <span class="token string">"MGS"</span><span class="token punctuation">:</span>        Q<span class="token punctuation">,</span> R <span class="token operator">=</span> factoriser<span class="token punctuation">.</span>modified_gram_schmidt_qr<span class="token punctuation">(</span>A<span class="token punctuation">)</span>        x <span class="token operator">=</span> np<span class="token punctuation">.</span>linalg<span class="token punctuation">.</span>inv<span class="token punctuation">(</span>R<span class="token punctuation">)</span> @ Q<span class="token punctuation">.</span>T @ b        <span class="token keyword">return</span> x    <span class="token keyword">elif</span> method <span class="token operator">==</span> <span class="token string">"HHT"</span><span class="token punctuation">:</span>        Q<span class="token punctuation">,</span> R <span class="token operator">=</span> factoriser<span class="token punctuation">.</span>householder_qr<span class="token punctuation">(</span>A<span class="token punctuation">)</span>        x <span class="token operator">=</span> np<span class="token punctuation">.</span>linalg<span class="token punctuation">.</span>inv<span class="token punctuation">(</span>R<span class="token punctuation">)</span> @ Q<span class="token punctuation">.</span>T @ b        <span class="token keyword">return</span> x<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>Let’s check on four different approachs.</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">A <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span>    <span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>     <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>     <span class="token punctuation">[</span><span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">,</span>     <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>b <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">20</span><span class="token punctuation">,</span> <span class="token number">12</span><span class="token punctuation">,</span> <span class="token number">23</span><span class="token punctuation">,</span> <span class="token number">19</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"NP:  "</span><span class="token punctuation">,</span> np<span class="token punctuation">.</span>linalg<span class="token punctuation">.</span>solve<span class="token punctuation">(</span>A<span class="token punctuation">,</span> b<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"LU:  "</span><span class="token punctuation">,</span> linear_function_solver<span class="token punctuation">(</span>A<span class="token punctuation">,</span> b<span class="token punctuation">,</span> method<span class="token operator">=</span><span class="token string">"LU"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"CGS: "</span><span class="token punctuation">,</span> linear_function_solver<span class="token punctuation">(</span>A<span class="token punctuation">,</span> b<span class="token punctuation">,</span> method<span class="token operator">=</span><span class="token string">"CGS"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"MGS: "</span><span class="token punctuation">,</span> linear_function_solver<span class="token punctuation">(</span>A<span class="token punctuation">,</span> b<span class="token punctuation">,</span> method<span class="token operator">=</span><span class="token string">"MGS"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"HHT: "</span><span class="token punctuation">,</span> linear_function_solver<span class="token punctuation">(</span>A<span class="token punctuation">,</span> b<span class="token punctuation">,</span> method<span class="token operator">=</span><span class="token string">"HHT"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><pre class="line-numbers language-python" data-language="python"><code class="language-python">NP<span class="token punctuation">:</span>   <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">.</span> <span class="token number">1</span><span class="token punctuation">.</span> <span class="token number">1</span><span class="token punctuation">.</span> <span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">]</span>LU<span class="token punctuation">:</span>   <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">.</span> <span class="token number">1</span><span class="token punctuation">.</span> <span class="token number">1</span><span class="token punctuation">.</span> <span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">]</span>CGS<span class="token punctuation">:</span>  <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">.</span> <span class="token number">1</span><span class="token punctuation">.</span> <span class="token number">1</span><span class="token punctuation">.</span> <span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">]</span>MGS<span class="token punctuation">:</span>  <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">.</span> <span class="token number">1</span><span class="token punctuation">.</span> <span class="token number">1</span><span class="token punctuation">.</span> <span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">]</span>HHT<span class="token punctuation">:</span>  <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">.</span> <span class="token number">1</span><span class="token punctuation">.</span> <span class="token number">1</span><span class="token punctuation">.</span> <span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>In this article, I implement different matrix decomposition methods, named LU decomposition and QR decomposition (Gram-Schmidt process, Modified Gram-Schmidt process, Householder transformations). In the future, I may apply matrix decomposition algorithm to neural networks. I hope it will be much more efficient than the regularisers methods.</p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ol><li><a href="https://ece.uwaterloo.ca/~dwharder/NumericalAnalysis/04LinearAlgebra/lup/">https://ece.uwaterloo.ca/~dwharder/NumericalAnalysis/04LinearAlgebra/lup/</a></li><li><a href="https://math.unm.edu/~loring/links/linear_s08/LU.pdf">https://math.unm.edu/~loring/links/linear_s08/LU.pdf</a></li><li><a href="https://johnfoster.pge.utexas.edu/numerical-methods-book/">https://johnfoster.pge.utexas.edu/numerical-methods-book/</a></li><li><a href="https://web.cs.ucdavis.edu/~bai/publications/andersonbaidongarra92.pdf">https://web.cs.ucdavis.edu/~bai/publications/andersonbaidongarra92.pdf</a></li><li><a href="https://deepai.org/machine-learning-glossary-and-terms/qr-decomposition">https://deepai.org/machine-learning-glossary-and-terms/qr-decomposition</a></li><li><a href="https://en.wikipedia.org/wiki/Matrix_decomposition">https://en.wikipedia.org/wiki/Matrix_decomposition</a></li><li><a href="http://homepages.math.uic.edu/~jan/mcs507f13/">http://homepages.math.uic.edu/~jan/mcs507f13/</a></li><li><a href="https://www.cis.upenn.edu/~cis610/Gram-Schmidt-Bjorck.pdf">https://www.cis.upenn.edu/~cis610/Gram-Schmidt-Bjorck.pdf</a></li><li><a href="https://wikivisually.com/wiki/Gram%E2%80%93Schmidt_process">https://wikivisually.com/wiki/Gram%E2%80%93Schmidt_process</a></li><li><a href="https://rpubs.com/aaronsc32/qr-decomposition-householder">https://rpubs.com/aaronsc32/qr-decomposition-householder</a></li><li><a href="https://www.cs.cornell.edu/~bindel/class/cs6210-f09/lec18.pdf">https://www.cs.cornell.edu/~bindel/class/cs6210-f09/lec18.pdf</a></li><li><a href="https://homel.vsb.cz/~dom033/predmety/parisLA/05_orthogonalization.pdf">https://homel.vsb.cz/~dom033/predmety/parisLA/05_orthogonalization.pdf</a></li><li><a href="https://core.ac.uk/download/pdf/82066579.pdf">https://core.ac.uk/download/pdf/82066579.pdf</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Math Programming </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> MATLAB </tag>
            
            <tag> Linear Algebra </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Laplace Expansion and Chiò Condensation</title>
      <link href="/Hexo-Blog/2021/01/28/2021-01-28-laplace-expansion-and-chio-algorithm/"/>
      <url>/Hexo-Blog/2021/01/28/2021-01-28-laplace-expansion-and-chio-algorithm/</url>
      
        <content type="html"><![CDATA[<p>Determinants are mathematical objects which have applications in engineering mathematics. For example, they can be used in the solution of simultaneous equations, and to evaluate vector products. Determinants can also be used to see if a system of <em>n</em> linear equations in <em>n</em> variables has a unique solution. There are several ways to calculate determinant, however, today I’m going to introduce another way of computing determinants: Chiò Identity.</p><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>In 1853 Felice (Félix) Chiò (1813–1871) published his short “Mémoire sur les fonctions connues sous le noms De Résultantes Ou De Déterminans”. In this article, I first give a way of evaluating determinants by Laplace Expansion, and explicitly comparing Chiò Identity to this.</p><h1 id="Laplace-Expansion-Method"><a href="#Laplace-Expansion-Method" class="headerlink" title="Laplace Expansion Method"></a>Laplace Expansion Method</h1><p>In linear algebra, the Laplace expansion, named after Pierre-Simon Laplace, also called cofactor expansion, is an expression for the determinant of an <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="5.481ex" height="1.136ex" role="img" focusable="false" viewBox="0 -491 2422.4 502" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-3-TEX-I-1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-3-TEX-N-D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-3-TEX-I-1D45B"></use></g><g data-mml-node="mo" transform="translate(822.2, 0)"><use xlink:href="#MJX-3-TEX-N-D7"></use></g><g data-mml-node="mi" transform="translate(1822.4, 0)"><use xlink:href="#MJX-3-TEX-I-1D45B"></use></g></g></g></svg></mjx-container> matrix <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0" xmlns="http://www.w3.org/2000/svg" width="1.697ex" height="1.62ex" role="img" focusable="false" viewBox="0 -716 750 716" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-3-TEX-I-1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-3-TEX-I-1D434"></use></g></g></g></svg></mjx-container> that is a weighted sum of the determinants of <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="1.357ex" height="1.025ex" role="img" focusable="false" viewBox="0 -442 600 453" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-3-TEX-I-1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-3-TEX-I-1D45B"></use></g></g></g></svg></mjx-container> sub-matrices (or minors) of <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0" xmlns="http://www.w3.org/2000/svg" width="2.378ex" height="1.545ex" role="img" focusable="false" viewBox="0 -683 1051 683" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-3-TEX-I-1D440" d="M289 629Q289 635 232 637Q208 637 201 638T194 648Q194 649 196 659Q197 662 198 666T199 671T201 676T203 679T207 681T212 683T220 683T232 684Q238 684 262 684T307 683Q386 683 398 683T414 678Q415 674 451 396L487 117L510 154Q534 190 574 254T662 394Q837 673 839 675Q840 676 842 678T846 681L852 683H948Q965 683 988 683T1017 684Q1051 684 1051 673Q1051 668 1048 656T1045 643Q1041 637 1008 637Q968 636 957 634T939 623Q936 618 867 340T797 59Q797 55 798 54T805 50T822 48T855 46H886Q892 37 892 35Q892 19 885 5Q880 0 869 0Q864 0 828 1T736 2Q675 2 644 2T609 1Q592 1 592 11Q592 13 594 25Q598 41 602 43T625 46Q652 46 685 49Q699 52 704 61Q706 65 742 207T813 490T848 631L654 322Q458 10 453 5Q451 4 449 3Q444 0 433 0Q418 0 415 7Q413 11 374 317L335 624L267 354Q200 88 200 79Q206 46 272 46H282Q288 41 289 37T286 19Q282 3 278 1Q274 0 267 0Q265 0 255 0T221 1T157 2Q127 2 95 1T58 0Q43 0 39 2T35 11Q35 13 38 25T43 40Q45 46 65 46Q135 46 154 86Q158 92 223 354T289 629Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-3-TEX-I-1D440"></use></g></g></g></svg></mjx-container>, each of size <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="16.795ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 7423.3 1000" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-3-TEX-N-28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path><path id="MJX-3-TEX-I-1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-3-TEX-N-2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path><path id="MJX-3-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-3-TEX-N-29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path><path id="MJX-3-TEX-N-D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mo"><use xlink:href="#MJX-3-TEX-N-28"></use></g><g data-mml-node="mi" transform="translate(389, 0)"><use xlink:href="#MJX-3-TEX-I-1D45B"></use></g><g data-mml-node="mo" transform="translate(1211.2, 0)"><use xlink:href="#MJX-3-TEX-N-2212"></use></g><g data-mml-node="mn" transform="translate(2211.4, 0)"><use xlink:href="#MJX-3-TEX-N-31"></use></g><g data-mml-node="mo" transform="translate(2711.4, 0)"><use xlink:href="#MJX-3-TEX-N-29"></use></g><g data-mml-node="mo" transform="translate(3322.7, 0)"><use xlink:href="#MJX-3-TEX-N-D7"></use></g><g data-mml-node="mo" transform="translate(4322.9, 0)"><use xlink:href="#MJX-3-TEX-N-28"></use></g><g data-mml-node="mi" transform="translate(4711.9, 0)"><use xlink:href="#MJX-3-TEX-I-1D45B"></use></g><g data-mml-node="mo" transform="translate(5534.1, 0)"><use xlink:href="#MJX-3-TEX-N-2212"></use></g><g data-mml-node="mn" transform="translate(6534.3, 0)"><use xlink:href="#MJX-3-TEX-N-31"></use></g><g data-mml-node="mo" transform="translate(7034.3, 0)"><use xlink:href="#MJX-3-TEX-N-29"></use></g></g></g></svg></mjx-container>. The <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="0.781ex" height="1.52ex" role="img" focusable="false" viewBox="0 -661 345 672" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-3-TEX-I-1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-3-TEX-I-1D456"></use></g></g></g></svg></mjx-container>, <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.462ex" xmlns="http://www.w3.org/2000/svg" width="0.932ex" height="1.957ex" role="img" focusable="false" viewBox="0 -661 412 865" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-3-TEX-I-1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-3-TEX-I-1D457"></use></g></g></g></svg></mjx-container> cofactor of the matrix <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0" xmlns="http://www.w3.org/2000/svg" width="1.697ex" height="1.62ex" role="img" focusable="false" viewBox="0 -716 750 716" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-3-TEX-I-1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-3-TEX-I-1D434"></use></g></g></g></svg></mjx-container> is the scalar <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.666ex" xmlns="http://www.w3.org/2000/svg" width="2.942ex" height="2.261ex" role="img" focusable="false" viewBox="0 -705 1300.3 999.2" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-3-TEX-I-1D436" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z"></path><path id="MJX-3-TEX-I-1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path><path id="MJX-3-TEX-I-1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-3-TEX-I-1D436"></use></g><g data-mml-node="TeXAtom" transform="translate(715, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-3-TEX-I-1D456"></use></g><g data-mml-node="mi" transform="translate(345, 0)"><use xlink:href="#MJX-3-TEX-I-1D457"></use></g></g></g></g></g></svg></mjx-container> defined by <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.666ex" xmlns="http://www.w3.org/2000/svg" width="16.698ex" height="2.544ex" role="img" focusable="false" viewBox="0 -830.4 7380.5 1124.6" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-3-TEX-I-1D436" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z"></path><path id="MJX-3-TEX-I-1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path><path id="MJX-3-TEX-I-1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path><path id="MJX-3-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-3-TEX-N-28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path><path id="MJX-3-TEX-N-2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path><path id="MJX-3-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-3-TEX-N-29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path><path id="MJX-3-TEX-N-2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path><path id="MJX-3-TEX-I-1D440" d="M289 629Q289 635 232 637Q208 637 201 638T194 648Q194 649 196 659Q197 662 198 666T199 671T201 676T203 679T207 681T212 683T220 683T232 684Q238 684 262 684T307 683Q386 683 398 683T414 678Q415 674 451 396L487 117L510 154Q534 190 574 254T662 394Q837 673 839 675Q840 676 842 678T846 681L852 683H948Q965 683 988 683T1017 684Q1051 684 1051 673Q1051 668 1048 656T1045 643Q1041 637 1008 637Q968 636 957 634T939 623Q936 618 867 340T797 59Q797 55 798 54T805 50T822 48T855 46H886Q892 37 892 35Q892 19 885 5Q880 0 869 0Q864 0 828 1T736 2Q675 2 644 2T609 1Q592 1 592 11Q592 13 594 25Q598 41 602 43T625 46Q652 46 685 49Q699 52 704 61Q706 65 742 207T813 490T848 631L654 322Q458 10 453 5Q451 4 449 3Q444 0 433 0Q418 0 415 7Q413 11 374 317L335 624L267 354Q200 88 200 79Q206 46 272 46H282Q288 41 289 37T286 19Q282 3 278 1Q274 0 267 0Q265 0 255 0T221 1T157 2Q127 2 95 1T58 0Q43 0 39 2T35 11Q35 13 38 25T43 40Q45 46 65 46Q135 46 154 86Q158 92 223 354T289 629Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-3-TEX-I-1D436"></use></g><g data-mml-node="TeXAtom" transform="translate(715, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-3-TEX-I-1D456"></use></g><g data-mml-node="mi" transform="translate(345, 0)"><use xlink:href="#MJX-3-TEX-I-1D457"></use></g></g></g><g data-mml-node="mo" transform="translate(1578.1, 0)"><use xlink:href="#MJX-3-TEX-N-3D"></use></g><g data-mml-node="mo" transform="translate(2633.8, 0)"><use xlink:href="#MJX-3-TEX-N-28"></use></g><g data-mml-node="mo" transform="translate(3022.8, 0)"><use xlink:href="#MJX-3-TEX-N-2212"></use></g><g data-mml-node="mn" transform="translate(3800.8, 0)"><use xlink:href="#MJX-3-TEX-N-31"></use></g><g data-mml-node="msup" transform="translate(4300.8, 0)"><g data-mml-node="mo"><use xlink:href="#MJX-3-TEX-N-29"></use></g><g data-mml-node="TeXAtom" transform="translate(389, 363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-3-TEX-I-1D456"></use></g><g data-mml-node="mo" transform="translate(345, 0)"><use xlink:href="#MJX-3-TEX-N-2B"></use></g><g data-mml-node="mi" transform="translate(1123, 0)"><use xlink:href="#MJX-3-TEX-I-1D457"></use></g></g></g><g data-mml-node="msub" transform="translate(5825.2, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-3-TEX-I-1D440"></use></g><g data-mml-node="TeXAtom" transform="translate(970, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-3-TEX-I-1D456"></use></g><g data-mml-node="mi" transform="translate(345, 0)"><use xlink:href="#MJX-3-TEX-I-1D457"></use></g></g></g></g></g></svg></mjx-container>, where <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.666ex" xmlns="http://www.w3.org/2000/svg" width="3.519ex" height="2.211ex" role="img" focusable="false" viewBox="0 -683 1555.3 977.2" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-3-TEX-I-1D440" d="M289 629Q289 635 232 637Q208 637 201 638T194 648Q194 649 196 659Q197 662 198 666T199 671T201 676T203 679T207 681T212 683T220 683T232 684Q238 684 262 684T307 683Q386 683 398 683T414 678Q415 674 451 396L487 117L510 154Q534 190 574 254T662 394Q837 673 839 675Q840 676 842 678T846 681L852 683H948Q965 683 988 683T1017 684Q1051 684 1051 673Q1051 668 1048 656T1045 643Q1041 637 1008 637Q968 636 957 634T939 623Q936 618 867 340T797 59Q797 55 798 54T805 50T822 48T855 46H886Q892 37 892 35Q892 19 885 5Q880 0 869 0Q864 0 828 1T736 2Q675 2 644 2T609 1Q592 1 592 11Q592 13 594 25Q598 41 602 43T625 46Q652 46 685 49Q699 52 704 61Q706 65 742 207T813 490T848 631L654 322Q458 10 453 5Q451 4 449 3Q444 0 433 0Q418 0 415 7Q413 11 374 317L335 624L267 354Q200 88 200 79Q206 46 272 46H282Q288 41 289 37T286 19Q282 3 278 1Q274 0 267 0Q265 0 255 0T221 1T157 2Q127 2 95 1T58 0Q43 0 39 2T35 11Q35 13 38 25T43 40Q45 46 65 46Q135 46 154 86Q158 92 223 354T289 629Z"></path><path id="MJX-3-TEX-I-1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path><path id="MJX-3-TEX-I-1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-3-TEX-I-1D440"></use></g><g data-mml-node="TeXAtom" transform="translate(970, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-3-TEX-I-1D456"></use></g><g data-mml-node="mi" transform="translate(345, 0)"><use xlink:href="#MJX-3-TEX-I-1D457"></use></g></g></g></g></g></svg></mjx-container> is the <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="0.781ex" height="1.52ex" role="img" focusable="false" viewBox="0 -661 345 672" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-3-TEX-I-1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-3-TEX-I-1D456"></use></g></g></g></svg></mjx-container>, <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.462ex" xmlns="http://www.w3.org/2000/svg" width="0.932ex" height="1.957ex" role="img" focusable="false" viewBox="0 -661 412 865" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-3-TEX-I-1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-3-TEX-I-1D457"></use></g></g></g></svg></mjx-container> minor of <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0" xmlns="http://www.w3.org/2000/svg" width="1.697ex" height="1.62ex" role="img" focusable="false" viewBox="0 -716 750 716" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-3-TEX-I-1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-3-TEX-I-1D434"></use></g></g></g></svg></mjx-container>, that is, the determinant of the <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="16.795ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 7423.3 1000" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-3-TEX-N-28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path><path id="MJX-3-TEX-I-1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-3-TEX-N-2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path><path id="MJX-3-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-3-TEX-N-29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path><path id="MJX-3-TEX-N-D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mo"><use xlink:href="#MJX-3-TEX-N-28"></use></g><g data-mml-node="mi" transform="translate(389, 0)"><use xlink:href="#MJX-3-TEX-I-1D45B"></use></g><g data-mml-node="mo" transform="translate(1211.2, 0)"><use xlink:href="#MJX-3-TEX-N-2212"></use></g><g data-mml-node="mn" transform="translate(2211.4, 0)"><use xlink:href="#MJX-3-TEX-N-31"></use></g><g data-mml-node="mo" transform="translate(2711.4, 0)"><use xlink:href="#MJX-3-TEX-N-29"></use></g><g data-mml-node="mo" transform="translate(3322.7, 0)"><use xlink:href="#MJX-3-TEX-N-D7"></use></g><g data-mml-node="mo" transform="translate(4322.9, 0)"><use xlink:href="#MJX-3-TEX-N-28"></use></g><g data-mml-node="mi" transform="translate(4711.9, 0)"><use xlink:href="#MJX-3-TEX-I-1D45B"></use></g><g data-mml-node="mo" transform="translate(5534.1, 0)"><use xlink:href="#MJX-3-TEX-N-2212"></use></g><g data-mml-node="mn" transform="translate(6534.3, 0)"><use xlink:href="#MJX-3-TEX-N-31"></use></g><g data-mml-node="mo" transform="translate(7034.3, 0)"><use xlink:href="#MJX-3-TEX-N-29"></use></g></g></g></svg></mjx-container> matrix that results from deleting the <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="0.781ex" height="1.52ex" role="img" focusable="false" viewBox="0 -661 345 672" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-3-TEX-I-1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-3-TEX-I-1D456"></use></g></g></g></svg></mjx-container>-th row and the <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.462ex" xmlns="http://www.w3.org/2000/svg" width="0.932ex" height="1.957ex" role="img" focusable="false" viewBox="0 -661 412 865" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-3-TEX-I-1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-3-TEX-I-1D457"></use></g></g></g></svg></mjx-container>-th column of <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0" xmlns="http://www.w3.org/2000/svg" width="1.697ex" height="1.62ex" role="img" focusable="false" viewBox="0 -716 750 716" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-3-TEX-I-1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-3-TEX-I-1D434"></use></g></g></g></svg></mjx-container>.</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">LaplaceDeterminants</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">pass</span>        <span class="token keyword">def</span> <span class="token function">minor_matrix</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> A<span class="token punctuation">,</span> i<span class="token punctuation">,</span> j<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment"># Delete i-th row</span>        sub_A <span class="token operator">=</span> np<span class="token punctuation">.</span>delete<span class="token punctuation">(</span>A<span class="token punctuation">,</span> i<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span>        <span class="token comment"># Delete j-th column</span>        sub_A <span class="token operator">=</span> np<span class="token punctuation">.</span>delete<span class="token punctuation">(</span>sub_A<span class="token punctuation">,</span> j<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> sub_A        <span class="token keyword">def</span> <span class="token function">calculate</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> A<span class="token punctuation">)</span><span class="token punctuation">:</span>        n<span class="token punctuation">,</span> m <span class="token operator">=</span> A<span class="token punctuation">.</span>shape        <span class="token keyword">if</span> <span class="token keyword">not</span> n <span class="token operator">==</span> m<span class="token punctuation">:</span>             <span class="token keyword">raise</span> Exception<span class="token punctuation">(</span><span class="token string">"Must be a square matrix!"</span><span class="token punctuation">)</span>        <span class="token keyword">if</span> n <span class="token operator">==</span> <span class="token number">2</span><span class="token punctuation">:</span>            <span class="token keyword">return</span> A<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token operator">*</span>A<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">-</span> A<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token operator">*</span>A<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>        det <span class="token operator">=</span> <span class="token number">0</span>        <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>n<span class="token punctuation">)</span><span class="token punctuation">:</span>            M <span class="token operator">=</span> self<span class="token punctuation">.</span>minor_matrix<span class="token punctuation">(</span>A<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> i<span class="token punctuation">)</span>            det <span class="token operator">+=</span> <span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token operator">**</span>i <span class="token operator">*</span> A<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">*</span> self<span class="token punctuation">.</span>calculate<span class="token punctuation">(</span>M<span class="token punctuation">)</span>        <span class="token keyword">return</span> det<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="Chio-Condensation-Method"><a href="#Chio-Condensation-Method" class="headerlink" title="Chiò Condensation Method"></a>Chiò Condensation Method</h1><p>The statement of Chiò Condensation is: let <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.666ex" xmlns="http://www.w3.org/2000/svg" width="8.995ex" height="2.363ex" role="img" focusable="false" viewBox="0 -750 3975.8 1044.2" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-3-TEX-I-1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"></path><path id="MJX-3-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-3-TEX-N-28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path><path id="MJX-3-TEX-I-1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path><path id="MJX-3-TEX-I-1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path><path id="MJX-3-TEX-I-1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path><path id="MJX-3-TEX-N-29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-3-TEX-I-1D434"></use></g><g data-mml-node="mo" transform="translate(1027.8, 0)"><use xlink:href="#MJX-3-TEX-N-3D"></use></g><g data-mml-node="mo" transform="translate(2083.6, 0)"><use xlink:href="#MJX-3-TEX-N-28"></use></g><g data-mml-node="msub" transform="translate(2472.6, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-3-TEX-I-1D44E"></use></g><g data-mml-node="TeXAtom" transform="translate(529, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-3-TEX-I-1D456"></use></g><g data-mml-node="mi" transform="translate(345, 0)"><use xlink:href="#MJX-3-TEX-I-1D457"></use></g></g></g><g data-mml-node="mo" transform="translate(3586.8, 0)"><use xlink:href="#MJX-3-TEX-N-29"></use></g></g></g></svg></mjx-container> be an <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="5.481ex" height="1.136ex" role="img" focusable="false" viewBox="0 -491 2422.4 502" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-3-TEX-I-1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-3-TEX-N-D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-3-TEX-I-1D45B"></use></g><g data-mml-node="mo" transform="translate(822.2, 0)"><use xlink:href="#MJX-3-TEX-N-D7"></use></g><g data-mml-node="mi" transform="translate(1822.4, 0)"><use xlink:href="#MJX-3-TEX-I-1D45B"></use></g></g></g></svg></mjx-container> matrix, and let <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.486ex" xmlns="http://www.w3.org/2000/svg" width="7.058ex" height="2.106ex" role="img" focusable="false" viewBox="0 -716 3119.7 931" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-3-TEX-I-1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path><path id="MJX-3-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-3-TEX-N-2260" d="M166 -215T159 -215T147 -212T141 -204T139 -197Q139 -190 144 -183L306 133H70Q56 140 56 153Q56 168 72 173H327L406 327H72Q56 332 56 347Q56 360 70 367H426Q597 702 602 707Q605 716 618 716Q625 716 630 712T636 703T638 696Q638 692 471 367H707Q722 359 722 347Q722 336 708 328L451 327L371 173H708Q722 163 722 153Q722 140 707 133H351Q175 -210 170 -212Q166 -215 159 -215Z"></path><path id="MJX-3-TEX-N-30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-3-TEX-I-1D44E"></use></g><g data-mml-node="TeXAtom" transform="translate(529, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><use xlink:href="#MJX-3-TEX-N-31"></use><use xlink:href="#MJX-3-TEX-N-31" transform="translate(500, 0)"></use></g></g></g><g data-mml-node="mo" transform="translate(1563.9, 0)"><use xlink:href="#MJX-3-TEX-N-2260"></use></g><g data-mml-node="mn" transform="translate(2619.7, 0)"><use xlink:href="#MJX-3-TEX-N-30"></use></g></g></g></svg></mjx-container>. Replace each element <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.666ex" xmlns="http://www.w3.org/2000/svg" width="2.521ex" height="1.663ex" role="img" focusable="false" viewBox="0 -441 1114.3 735.2" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-3-TEX-I-1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path><path id="MJX-3-TEX-I-1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path><path id="MJX-3-TEX-I-1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-3-TEX-I-1D44E"></use></g><g data-mml-node="TeXAtom" transform="translate(529, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-3-TEX-I-1D456"></use></g><g data-mml-node="mi" transform="translate(345, 0)"><use xlink:href="#MJX-3-TEX-I-1D457"></use></g></g></g></g></g></svg></mjx-container> in the <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="16.795ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 7423.3 1000" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-3-TEX-N-28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path><path id="MJX-3-TEX-I-1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-3-TEX-N-2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path><path id="MJX-3-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-3-TEX-N-29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path><path id="MJX-3-TEX-N-D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mo"><use xlink:href="#MJX-3-TEX-N-28"></use></g><g data-mml-node="mi" transform="translate(389, 0)"><use xlink:href="#MJX-3-TEX-I-1D45B"></use></g><g data-mml-node="mo" transform="translate(1211.2, 0)"><use xlink:href="#MJX-3-TEX-N-2212"></use></g><g data-mml-node="mn" transform="translate(2211.4, 0)"><use xlink:href="#MJX-3-TEX-N-31"></use></g><g data-mml-node="mo" transform="translate(2711.4, 0)"><use xlink:href="#MJX-3-TEX-N-29"></use></g><g data-mml-node="mo" transform="translate(3322.7, 0)"><use xlink:href="#MJX-3-TEX-N-D7"></use></g><g data-mml-node="mo" transform="translate(4322.9, 0)"><use xlink:href="#MJX-3-TEX-N-28"></use></g><g data-mml-node="mi" transform="translate(4711.9, 0)"><use xlink:href="#MJX-3-TEX-I-1D45B"></use></g><g data-mml-node="mo" transform="translate(5534.1, 0)"><use xlink:href="#MJX-3-TEX-N-2212"></use></g><g data-mml-node="mn" transform="translate(6534.3, 0)"><use xlink:href="#MJX-3-TEX-N-31"></use></g><g data-mml-node="mo" transform="translate(7034.3, 0)"><use xlink:href="#MJX-3-TEX-N-29"></use></g></g></g></svg></mjx-container> sub-matrix, let’s called it <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0" xmlns="http://www.w3.org/2000/svg" width="1.873ex" height="1.545ex" role="img" focusable="false" viewBox="0 -683 828 683" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-3-TEX-I-1D437" d="M287 628Q287 635 230 637Q207 637 200 638T193 647Q193 655 197 667T204 682Q206 683 403 683Q570 682 590 682T630 676Q702 659 752 597T803 431Q803 275 696 151T444 3L430 1L236 0H125H72Q48 0 41 2T33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM703 469Q703 507 692 537T666 584T629 613T590 629T555 636Q553 636 541 636T512 636T479 637H436Q392 637 386 627Q384 623 313 339T242 52Q242 48 253 48T330 47Q335 47 349 47T373 46Q499 46 581 128Q617 164 640 212T683 339T703 469Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-3-TEX-I-1D437"></use></g></g></g></svg></mjx-container>, of <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0" xmlns="http://www.w3.org/2000/svg" width="1.697ex" height="1.62ex" role="img" focusable="false" viewBox="0 -716 750 716" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-3-TEX-I-1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-3-TEX-I-1D434"></use></g></g></g></svg></mjx-container> obtained by deleting the <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="0.781ex" height="1.52ex" role="img" focusable="false" viewBox="0 -661 345 672" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-3-TEX-I-1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-3-TEX-I-1D456"></use></g></g></g></svg></mjx-container>th row and <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.462ex" xmlns="http://www.w3.org/2000/svg" width="0.932ex" height="1.957ex" role="img" focusable="false" viewBox="0 -661 412 865" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-3-TEX-I-1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-3-TEX-I-1D457"></use></g></g></g></svg></mjx-container>th column of <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0" xmlns="http://www.w3.org/2000/svg" width="1.697ex" height="1.62ex" role="img" focusable="false" viewBox="0 -716 750 716" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-3-TEX-I-1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-3-TEX-I-1D434"></use></g></g></g></svg></mjx-container> by:</p><div style="display: flex;justify-content: center;">    <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -2.249ex" xmlns="http://www.w3.org/2000/svg" width="15.701ex" height="5.63ex" role="img" focusable="false" viewBox="0 -1494.2 6939.7 2488.5" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-3-TEX-I-1D437" d="M287 628Q287 635 230 637Q207 637 200 638T193 647Q193 655 197 667T204 682Q206 683 403 683Q570 682 590 682T630 676Q702 659 752 597T803 431Q803 275 696 151T444 3L430 1L236 0H125H72Q48 0 41 2T33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM703 469Q703 507 692 537T666 584T629 613T590 629T555 636Q553 636 541 636T512 636T479 637H436Q392 637 386 627Q384 623 313 339T242 52Q242 48 253 48T330 47Q335 47 349 47T373 46Q499 46 581 128Q617 164 640 212T683 339T703 469Z"></path><path id="MJX-3-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-3-TEX-S3-5B" d="M247 -949V1450H516V1388H309V-887H516V-949H247Z"></path><path id="MJX-3-TEX-I-1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path><path id="MJX-3-TEX-I-1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path><path id="MJX-3-TEX-I-1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path><path id="MJX-3-TEX-I-1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-3-TEX-S3-5D" d="M11 1388V1450H280V-949H11V-887H218V1388H11Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-3-TEX-I-1D437"></use></g><g data-mml-node="mo" transform="translate(1105.8, 0)"><use xlink:href="#MJX-3-TEX-N-3D"></use></g><g data-mml-node="mrow" transform="translate(2161.6, 0)"><g data-mml-node="mo"><use xlink:href="#MJX-3-TEX-S3-5B"></use></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(528, 0)"><g data-mml-node="mtable"><g data-mml-node="mtr" transform="translate(0, 744.2)"><g data-mml-node="mtd" transform="translate(90.2, 0)"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-3-TEX-I-1D44E"></use></g><g data-mml-node="TeXAtom" transform="translate(529, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-3-TEX-I-1D456"></use></g><g data-mml-node="mi" transform="translate(345, 0)"><use xlink:href="#MJX-3-TEX-I-1D457"></use></g></g></g></g><g data-mml-node="mtd" transform="translate(2384.7, 0)"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-3-TEX-I-1D44E"></use></g><g data-mml-node="TeXAtom" transform="translate(529, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-3-TEX-I-1D456"></use></g><g data-mml-node="mi" transform="translate(345, 0)"><use xlink:href="#MJX-3-TEX-I-1D45B"></use></g></g></g></g></g><g data-mml-node="mtr" transform="translate(0, -700)"><g data-mml-node="mtd"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-3-TEX-I-1D44E"></use></g><g data-mml-node="TeXAtom" transform="translate(529, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-3-TEX-I-1D45B"></use></g><g data-mml-node="mi" transform="translate(600, 0)"><use xlink:href="#MJX-3-TEX-I-1D457"></use></g></g></g></g><g data-mml-node="mtd" transform="translate(2294.6, 0)"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-3-TEX-I-1D44E"></use></g><g data-mml-node="TeXAtom" transform="translate(529, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-3-TEX-I-1D45B"></use></g><g data-mml-node="mi" transform="translate(600, 0)"><use xlink:href="#MJX-3-TEX-I-1D45B"></use></g></g></g></g></g></g></g><g data-mml-node="mo" transform="translate(4250.1, 0)"><use xlink:href="#MJX-3-TEX-S3-5D"></use></g></g></g></g></svg></mjx-container></div><p>Then <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -1.285ex" xmlns="http://www.w3.org/2000/svg" width="21.883ex" height="3.241ex" role="img" focusable="false" viewBox="0 -864.9 9672.4 1432.7" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-3-TEX-I-1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path><path id="MJX-3-TEX-I-1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path><path id="MJX-3-TEX-I-1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path><path id="MJX-3-TEX-N-28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path><path id="MJX-3-TEX-I-1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"></path><path id="MJX-3-TEX-N-29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path><path id="MJX-3-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-3-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-3-TEX-I-1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path><path id="MJX-3-TEX-I-1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-3-TEX-N-2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path><path id="MJX-3-TEX-N-32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path><path id="MJX-3-TEX-N-22C5" d="M78 250Q78 274 95 292T138 310Q162 310 180 294T199 251Q199 226 182 208T139 190T96 207T78 250Z"></path><path id="MJX-3-TEX-I-1D437" d="M287 628Q287 635 230 637Q207 637 200 638T193 647Q193 655 197 667T204 682Q206 683 403 683Q570 682 590 682T630 676Q702 659 752 597T803 431Q803 275 696 151T444 3L430 1L236 0H125H72Q48 0 41 2T33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM703 469Q703 507 692 537T666 584T629 613T590 629T555 636Q553 636 541 636T512 636T479 637H436Q392 637 386 627Q384 623 313 339T242 52Q242 48 253 48T330 47Q335 47 349 47T373 46Q499 46 581 128Q617 164 640 212T683 339T703 469Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-3-TEX-I-1D451"></use></g><g data-mml-node="mi" transform="translate(520, 0)"><use xlink:href="#MJX-3-TEX-I-1D452"></use></g><g data-mml-node="mi" transform="translate(986, 0)"><use xlink:href="#MJX-3-TEX-I-1D461"></use></g><g data-mml-node="mo" transform="translate(1347, 0)"><use xlink:href="#MJX-3-TEX-N-28"></use></g><g data-mml-node="mi" transform="translate(1736, 0)"><use xlink:href="#MJX-3-TEX-I-1D434"></use></g><g data-mml-node="mo" transform="translate(2486, 0)"><use xlink:href="#MJX-3-TEX-N-29"></use></g><g data-mml-node="mo" transform="translate(3152.8, 0)"><use xlink:href="#MJX-3-TEX-N-3D"></use></g><g data-mml-node="mfrac" transform="translate(4208.6, 0)"><g data-mml-node="mn" transform="translate(717.4, 394) scale(0.707)"><use xlink:href="#MJX-3-TEX-N-31"></use></g><g data-mml-node="msubsup" transform="translate(220, -464) scale(0.707)"><g data-mml-node="mi"><use xlink:href="#MJX-3-TEX-I-1D44E"></use></g><g data-mml-node="TeXAtom" transform="translate(529, 411.6) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-3-TEX-I-1D45B"></use></g><g data-mml-node="mo" transform="translate(600, 0)"><use xlink:href="#MJX-3-TEX-N-2212"></use></g><g data-mml-node="mn" transform="translate(1378, 0)"><use xlink:href="#MJX-3-TEX-N-32"></use></g></g><g data-mml-node="TeXAtom" transform="translate(529, -138.9) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-3-TEX-I-1D45B"></use></g><g data-mml-node="mi" transform="translate(600, 0)"><use xlink:href="#MJX-3-TEX-I-1D45B"></use></g></g></g><rect width="1548.4" height="60" x="120" y="220"></rect></g><g data-mml-node="mo" transform="translate(6219.2, 0)"><use xlink:href="#MJX-3-TEX-N-22C5"></use></g><g data-mml-node="mi" transform="translate(6719.4, 0)"><use xlink:href="#MJX-3-TEX-I-1D451"></use></g><g data-mml-node="mi" transform="translate(7239.4, 0)"><use xlink:href="#MJX-3-TEX-I-1D452"></use></g><g data-mml-node="mi" transform="translate(7705.4, 0)"><use xlink:href="#MJX-3-TEX-I-1D461"></use></g><g data-mml-node="mo" transform="translate(8066.4, 0)"><use xlink:href="#MJX-3-TEX-N-28"></use></g><g data-mml-node="mi" transform="translate(8455.4, 0)"><use xlink:href="#MJX-3-TEX-I-1D437"></use></g><g data-mml-node="mo" transform="translate(9283.4, 0)"><use xlink:href="#MJX-3-TEX-N-29"></use></g></g></g></svg></mjx-container>.</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">ChioDeterminants</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">pass</span>        <span class="token keyword">def</span> <span class="token function">calculate</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> A<span class="token punctuation">)</span><span class="token punctuation">:</span>        n<span class="token punctuation">,</span> m <span class="token operator">=</span> A<span class="token punctuation">.</span>shape        <span class="token keyword">if</span> <span class="token keyword">not</span> n <span class="token operator">==</span> m<span class="token punctuation">:</span>             <span class="token keyword">raise</span> Exception<span class="token punctuation">(</span><span class="token string">"Must be a square matrix!"</span><span class="token punctuation">)</span>        <span class="token keyword">if</span> n <span class="token operator">==</span> <span class="token number">2</span><span class="token punctuation">:</span>            <span class="token keyword">return</span> A<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token operator">*</span>A<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">-</span> A<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token operator">*</span>A<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>        <span class="token keyword">if</span> A<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>            <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>n<span class="token punctuation">)</span><span class="token punctuation">:</span>                <span class="token keyword">if</span> A<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">!=</span> <span class="token number">0</span><span class="token punctuation">:</span>                    A<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">[</span>i<span class="token punctuation">,</span> n<span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> A<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">[</span>n<span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> i<span class="token punctuation">]</span><span class="token punctuation">]</span>                    A<span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> n<span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">=</span> A<span class="token punctuation">[</span><span class="token punctuation">[</span>n<span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span>                    <span class="token keyword">break</span>                <span class="token keyword">else</span><span class="token punctuation">:</span>                    <span class="token keyword">return</span> <span class="token number">0</span>        D <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>shape<span class="token operator">=</span><span class="token punctuation">(</span>n<span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> n<span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>n<span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token keyword">for</span> j <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>n<span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>                D<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span>j<span class="token punctuation">]</span> <span class="token operator">=</span> A<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token operator">*</span>A<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">-</span> A<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token operator">*</span>A<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span>        det <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token number">1</span><span class="token operator">/</span>A<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token operator">**</span><span class="token punctuation">(</span>n<span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">*</span> self<span class="token punctuation">.</span>calculate<span class="token punctuation">(</span>D<span class="token punctuation">)</span>        <span class="token keyword">return</span> det<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="Performance"><a href="#Performance" class="headerlink" title="Performance"></a>Performance</h1><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">test_laplace</span><span class="token punctuation">(</span>n_samples<span class="token operator">=</span><span class="token number">50000</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    algo <span class="token operator">=</span> LaplaceDeterminants<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>n_samples<span class="token punctuation">)</span><span class="token punctuation">:</span>        A <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span>        det <span class="token operator">=</span> algo<span class="token punctuation">.</span>calculate<span class="token punctuation">(</span>A<span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">test_chio</span><span class="token punctuation">(</span>n_samples<span class="token operator">=</span><span class="token number">50000</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    algo <span class="token operator">=</span> ChioDeterminants<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>n_samples<span class="token punctuation">)</span><span class="token punctuation">:</span>        A <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span>        det <span class="token operator">=</span> algo<span class="token punctuation">.</span>calculate<span class="token punctuation">(</span>A<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><img src="/Hexo-Blog/2021/01/28/2021-01-28-laplace-expansion-and-chio-algorithm/perf1.png" class=""><p>What if we also compare Chiò Condensation Method to <code>numpy</code> and <code>scipy</code>? They both computed determinants via LU factorization, relying on BLAS and <a href="http://www.netlib.org/lapack/explore-html/dd/d9a/group__double_g_ecomputational_ga0019443faea08275ca60a734d0593e60.html">LAPACK</a> to provide efficient low level implementations of standard linear algebra algorithms.</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">test_numpy</span><span class="token punctuation">(</span>n_samples<span class="token operator">=</span><span class="token number">50000</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>n_samples<span class="token punctuation">)</span><span class="token punctuation">:</span>        A <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span>        det <span class="token operator">=</span> np<span class="token punctuation">.</span>linalg<span class="token punctuation">.</span>det<span class="token punctuation">(</span>A<span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">test_scipy</span><span class="token punctuation">(</span>n_samples<span class="token operator">=</span><span class="token number">50000</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>n_samples<span class="token punctuation">)</span><span class="token punctuation">:</span>        A <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span>        det <span class="token operator">=</span> linalg<span class="token punctuation">.</span>det<span class="token punctuation">(</span>A<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><img src="/Hexo-Blog/2021/01/28/2021-01-28-laplace-expansion-and-chio-algorithm/perf2.png" class=""><p>Quantstart has implemented an LU Decomposition directly over <a href="https://www.quantstart.com/articles/LU-Decomposition-in-Python-and-NumPy/">here</a>, which does not rely on any external libraries.</p><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>Clearly, Chiò Condensation Method is much quicker than Laplace Expansion Method by minors, which yeilds complexity computation of <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="5.473ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 2419 1000" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-3-TEX-I-1D442" d="M740 435Q740 320 676 213T511 42T304 -22Q207 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435ZM637 476Q637 565 591 615T476 665Q396 665 322 605Q242 542 200 428T157 216Q157 126 200 73T314 19Q404 19 485 98T608 313Q637 408 637 476Z"></path><path id="MJX-3-TEX-N-28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path><path id="MJX-3-TEX-I-1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-3-TEX-N-21" d="M78 661Q78 682 96 699T138 716T180 700T199 661Q199 654 179 432T158 206Q156 198 139 198Q121 198 119 206Q118 209 98 431T78 661ZM79 61Q79 89 97 105T141 121Q164 119 181 104T198 61Q198 31 181 16T139 1Q114 1 97 16T79 61Z"></path><path id="MJX-3-TEX-N-29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-3-TEX-I-1D442"></use></g><g data-mml-node="mo" transform="translate(763, 0)"><use xlink:href="#MJX-3-TEX-N-28"></use></g><g data-mml-node="mi" transform="translate(1152, 0)"><use xlink:href="#MJX-3-TEX-I-1D45B"></use></g><g data-mml-node="mo" transform="translate(1752, 0)"><use xlink:href="#MJX-3-TEX-N-21"></use></g><g data-mml-node="mo" transform="translate(2030, 0)"><use xlink:href="#MJX-3-TEX-N-29"></use></g></g></g></svg></mjx-container>. As an alternative method for hand-calculating determinants, therefore, Chiò’s method is quite effective. For numerical computations of large determinants on a computer, however, Chiò’s method is not so efficient as other methods such as, for example, Gaussian elimination, because of certain difficulties with round-off errors. In addition, Chiò’s method requires approximately <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.816ex" xmlns="http://www.w3.org/2000/svg" width="4.066ex" height="2.773ex" role="img" focusable="false" viewBox="0 -864.9 1797.1 1225.5" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-3-TEX-N-32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path><path id="MJX-3-TEX-N-33" d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z"></path><path id="MJX-3-TEX-I-1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mfrac"><g data-mml-node="mn" transform="translate(220, 394) scale(0.707)"><use xlink:href="#MJX-3-TEX-N-32"></use></g><g data-mml-node="mn" transform="translate(220, -345) scale(0.707)"><use xlink:href="#MJX-3-TEX-N-33"></use></g><rect width="553.6" height="60" x="120" y="220"></rect></g><g data-mml-node="msup" transform="translate(793.6, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-3-TEX-I-1D45B"></use></g><g data-mml-node="mn" transform="translate(600, 363) scale(0.707)"><use xlink:href="#MJX-3-TEX-N-33"></use></g></g></g></g></svg></mjx-container> multiplications, whereas Gaussian elimination requires approximately <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.816ex" xmlns="http://www.w3.org/2000/svg" width="4.066ex" height="2.773ex" role="img" focusable="false" viewBox="0 -864.9 1797.1 1225.5" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-3-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-3-TEX-N-33" d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z"></path><path id="MJX-3-TEX-I-1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mfrac"><g data-mml-node="mn" transform="translate(220, 394) scale(0.707)"><use xlink:href="#MJX-3-TEX-N-31"></use></g><g data-mml-node="mn" transform="translate(220, -345) scale(0.707)"><use xlink:href="#MJX-3-TEX-N-33"></use></g><rect width="553.6" height="60" x="120" y="220"></rect></g><g data-mml-node="msup" transform="translate(793.6, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-3-TEX-I-1D45B"></use></g><g data-mml-node="mn" transform="translate(600, 363) scale(0.707)"><use xlink:href="#MJX-3-TEX-N-33"></use></g></g></g></g></svg></mjx-container>. </p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ol><li><a href="https://www.sciencedirect.com/science/article/pii/S0024379514002249">https://www.sciencedirect.com/science/article/pii/S0024379514002249</a></li><li><a href="https://www.codeformech.com/determinant-linear-algebra-using-python/">https://www.codeformech.com/determinant-linear-algebra-using-python/</a></li><li><a href="https://en.wikipedia.org/wiki/Laplace_expansion">https://en.wikipedia.org/wiki/Laplace_expansion</a></li><li><a href="https://stackoverflow.com/questions/16636858/complexity-computation-of-a-determinant-recursive-algorithm">https://stackoverflow.com/questions/16636858/complexity-computation-of-a-determinant-recursive-algorithm</a></li><li><a href="https://www.youtube.com/watch?v=UlWcofkUDDU">https://www.youtube.com/watch?v=UlWcofkUDDU</a>.</li><li>Fuller, L. E., &amp; Logan, J. D. On the Evaluation of Determinants by Chiò Method, 1975</li></ol>]]></content>
      
      
      <categories>
          
          <category> Math Programming </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> Linear Algebra </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Missionaries and Cannibals Problem</title>
      <link href="/Hexo-Blog/2021/01/27/2021-01-27-missionaries-and-cannibals-problem/"/>
      <url>/Hexo-Blog/2021/01/27/2021-01-27-missionaries-and-cannibals-problem/</url>
      
        <content type="html"><![CDATA[<p>This is actually one of my project when I was in college during my “Mathematical Programming” class. I used R programming language to solve missionaries and cannibals problem with matrix analysis and graph theory.</p><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>The missionaries and cannibals problem, and the closely related jealous husbands problem, are classic river-crossing logic puzzles. In the missionaries and cannibals problem, three missionaries and three cannibals must cross a river using a boat which can carry at most two people, under the constraint that, for both banks, if there are missionaries present on the bank, they cannot be outnumbered by cannibals (if they were, the cannibals would eat the missionaries). The boat cannot cross the river by itself with no people on board.</p><img src="/Hexo-Blog/2021/01/27/2021-01-27-missionaries-and-cannibals-problem/mc-search-space.png" class=""><h1 id="Solving"><a href="#Solving" class="headerlink" title="Solving"></a>Solving</h1><p>A system for solving the Missionaries and Cannibals problem whereby the current state is represented by a simple vector <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="7.709ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 3407.3 1000" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-2-TEX-N-28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path><path id="MJX-2-TEX-I-1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-2-TEX-N-2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path><path id="MJX-2-TEX-I-1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z"></path><path id="MJX-2-TEX-I-1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"></path><path id="MJX-2-TEX-N-29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mo"><use xlink:href="#MJX-2-TEX-N-28"></use></g><g data-mml-node="mi" transform="translate(389, 0)"><use xlink:href="#MJX-2-TEX-I-1D45A"></use></g><g data-mml-node="mo" transform="translate(1267, 0)"><use xlink:href="#MJX-2-TEX-N-2C"></use></g><g data-mml-node="mi" transform="translate(1711.7, 0)"><use xlink:href="#MJX-2-TEX-I-1D450"></use></g><g data-mml-node="mo" transform="translate(2144.7, 0)"><use xlink:href="#MJX-2-TEX-N-2C"></use></g><g data-mml-node="mi" transform="translate(2589.3, 0)"><use xlink:href="#MJX-2-TEX-I-1D44F"></use></g><g data-mml-node="mo" transform="translate(3018.3, 0)"><use xlink:href="#MJX-2-TEX-N-29"></use></g></g></g></svg></mjx-container>. The vector’s elements represent the number of missionaries, cannibals, and whether the boat is on the the east side or on the west side, respectively. Suppose the boat and all of the missionaries and cannibals start on the east side, the vector is initialized to <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="7.089ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 3133.3 1000" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-2-TEX-N-28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path><path id="MJX-2-TEX-N-33" d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z"></path><path id="MJX-2-TEX-N-2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path><path id="MJX-2-TEX-I-1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path><path id="MJX-2-TEX-N-29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mo"><use xlink:href="#MJX-2-TEX-N-28"></use></g><g data-mml-node="mn" transform="translate(389, 0)"><use xlink:href="#MJX-2-TEX-N-33"></use></g><g data-mml-node="mo" transform="translate(889, 0)"><use xlink:href="#MJX-2-TEX-N-2C"></use></g><g data-mml-node="mn" transform="translate(1333.7, 0)"><use xlink:href="#MJX-2-TEX-N-33"></use></g><g data-mml-node="mo" transform="translate(1833.7, 0)"><use xlink:href="#MJX-2-TEX-N-2C"></use></g><g data-mml-node="mi" transform="translate(2278.3, 0)"><use xlink:href="#MJX-2-TEX-I-1D452"></use></g><g data-mml-node="mo" transform="translate(2744.3, 0)"><use xlink:href="#MJX-2-TEX-N-29"></use></g></g></g></svg></mjx-container>. According to the problem, <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.439ex" xmlns="http://www.w3.org/2000/svg" width="12.547ex" height="1.946ex" role="img" focusable="false" viewBox="0 -666 5545.6 860" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-2-TEX-I-1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-2-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-2-TEX-N-30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path><path id="MJX-2-TEX-N-2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path><path id="MJX-2-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-2-TEX-N-32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path><path id="MJX-2-TEX-N-33" d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D45A"></use></g><g data-mml-node="mo" transform="translate(1155.8, 0)"><use xlink:href="#MJX-2-TEX-N-3D"></use></g><g data-mml-node="mn" transform="translate(2211.6, 0)"><use xlink:href="#MJX-2-TEX-N-30"></use></g><g data-mml-node="mo" transform="translate(2711.6, 0)"><use xlink:href="#MJX-2-TEX-N-2C"></use></g><g data-mml-node="mn" transform="translate(3156.2, 0)"><use xlink:href="#MJX-2-TEX-N-31"></use></g><g data-mml-node="mo" transform="translate(3656.2, 0)"><use xlink:href="#MJX-2-TEX-N-2C"></use></g><g data-mml-node="mn" transform="translate(4100.9, 0)"><use xlink:href="#MJX-2-TEX-N-32"></use></g><g data-mml-node="mo" transform="translate(4600.9, 0)"><use xlink:href="#MJX-2-TEX-N-2C"></use></g><g data-mml-node="mn" transform="translate(5045.6, 0)"><use xlink:href="#MJX-2-TEX-N-33"></use></g></g></g></svg></mjx-container> and <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.439ex" xmlns="http://www.w3.org/2000/svg" width="11.54ex" height="1.946ex" role="img" focusable="false" viewBox="0 -666 5100.6 860" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-2-TEX-I-1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z"></path><path id="MJX-2-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-2-TEX-N-30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path><path id="MJX-2-TEX-N-2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path><path id="MJX-2-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-2-TEX-N-32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path><path id="MJX-2-TEX-N-33" d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D450"></use></g><g data-mml-node="mo" transform="translate(710.8, 0)"><use xlink:href="#MJX-2-TEX-N-3D"></use></g><g data-mml-node="mn" transform="translate(1766.6, 0)"><use xlink:href="#MJX-2-TEX-N-30"></use></g><g data-mml-node="mo" transform="translate(2266.6, 0)"><use xlink:href="#MJX-2-TEX-N-2C"></use></g><g data-mml-node="mn" transform="translate(2711.2, 0)"><use xlink:href="#MJX-2-TEX-N-31"></use></g><g data-mml-node="mo" transform="translate(3211.2, 0)"><use xlink:href="#MJX-2-TEX-N-2C"></use></g><g data-mml-node="mn" transform="translate(3655.9, 0)"><use xlink:href="#MJX-2-TEX-N-32"></use></g><g data-mml-node="mo" transform="translate(4155.9, 0)"><use xlink:href="#MJX-2-TEX-N-2C"></use></g><g data-mml-node="mn" transform="translate(4600.6, 0)"><use xlink:href="#MJX-2-TEX-N-33"></use></g></g></g></svg></mjx-container>, therefore, there are <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.186ex" xmlns="http://www.w3.org/2000/svg" width="14.205ex" height="1.717ex" role="img" focusable="false" viewBox="0 -677 6278.4 759" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-2-TEX-N-34" d="M462 0Q444 3 333 3Q217 3 199 0H190V46H221Q241 46 248 46T265 48T279 53T286 61Q287 63 287 115V165H28V211L179 442Q332 674 334 675Q336 677 355 677H373L379 671V211H471V165H379V114Q379 73 379 66T385 54Q393 47 442 46H471V0H462ZM293 211V545L74 212L183 211H293Z"></path><path id="MJX-2-TEX-N-D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path><path id="MJX-2-TEX-N-32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path><path id="MJX-2-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-2-TEX-N-33" d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-34"></use></g><g data-mml-node="mo" transform="translate(722.2, 0)"><use xlink:href="#MJX-2-TEX-N-D7"></use></g><g data-mml-node="mn" transform="translate(1722.4, 0)"><use xlink:href="#MJX-2-TEX-N-34"></use></g><g data-mml-node="mo" transform="translate(2444.7, 0)"><use xlink:href="#MJX-2-TEX-N-D7"></use></g><g data-mml-node="mn" transform="translate(3444.9, 0)"><use xlink:href="#MJX-2-TEX-N-32"></use></g><g data-mml-node="mo" transform="translate(4222.7, 0)"><use xlink:href="#MJX-2-TEX-N-3D"></use></g><g data-mml-node="mn" transform="translate(5278.4, 0)"><use xlink:href="#MJX-2-TEX-N-33"></use><use xlink:href="#MJX-2-TEX-N-32" transform="translate(500, 0)"></use></g></g></g></svg></mjx-container> different status. However, the nummber of missionaries must be greater than the number of cannibals, and there should always be people on the shoreside while the boat is on that side, so the following states do not exist:</p><div style="display: flex;justify-content: center;">  <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -2.262ex" xmlns="http://www.w3.org/2000/svg" width="57.922ex" height="5.656ex" role="img" focusable="false" viewBox="0 -1500 25601.3 2500" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-2-TEX-N-28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path><path id="MJX-2-TEX-N-30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path><path id="MJX-2-TEX-N-2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path><path id="MJX-2-TEX-I-1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path><path id="MJX-2-TEX-N-29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path><path id="MJX-2-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-2-TEX-N-32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path><path id="MJX-2-TEX-I-1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path><path id="MJX-2-TEX-N-33" d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mtable"><g data-mml-node="mtr" transform="translate(0, 750)"><g data-mml-node="mtd" transform="translate(125, 0)"><g data-mml-node="mo"><use xlink:href="#MJX-2-TEX-N-28"></use></g><g data-mml-node="mn" transform="translate(389, 0)"><use xlink:href="#MJX-2-TEX-N-30"></use></g><g data-mml-node="mo" transform="translate(889, 0)"><use xlink:href="#MJX-2-TEX-N-2C"></use></g><g data-mml-node="mn" transform="translate(1333.7, 0)"><use xlink:href="#MJX-2-TEX-N-30"></use></g><g data-mml-node="mo" transform="translate(1833.7, 0)"><use xlink:href="#MJX-2-TEX-N-2C"></use></g><g data-mml-node="mi" transform="translate(2278.3, 0)"><use xlink:href="#MJX-2-TEX-I-1D452"></use></g><g data-mml-node="mo" transform="translate(2744.3, 0)"><use xlink:href="#MJX-2-TEX-N-29"></use></g><g data-mml-node="mo" transform="translate(3133.3, 0)"><use xlink:href="#MJX-2-TEX-N-2C"></use></g><g data-mml-node="mo" transform="translate(3578, 0)"><use xlink:href="#MJX-2-TEX-N-28"></use></g><g data-mml-node="mn" transform="translate(3967, 0)"><use xlink:href="#MJX-2-TEX-N-31"></use></g><g data-mml-node="mo" transform="translate(4467, 0)"><use xlink:href="#MJX-2-TEX-N-2C"></use></g><g data-mml-node="mn" transform="translate(4911.7, 0)"><use xlink:href="#MJX-2-TEX-N-32"></use></g><g data-mml-node="mo" transform="translate(5411.7, 0)"><use xlink:href="#MJX-2-TEX-N-2C"></use></g><g data-mml-node="mi" transform="translate(5856.3, 0)"><use xlink:href="#MJX-2-TEX-I-1D452"></use></g><g data-mml-node="mo" transform="translate(6322.3, 0)"><use xlink:href="#MJX-2-TEX-N-29"></use></g><g data-mml-node="mo" transform="translate(6711.3, 0)"><use xlink:href="#MJX-2-TEX-N-2C"></use></g><g data-mml-node="mo" transform="translate(7156, 0)"><use xlink:href="#MJX-2-TEX-N-28"></use></g><g data-mml-node="mn" transform="translate(7545, 0)"><use xlink:href="#MJX-2-TEX-N-31"></use></g><g data-mml-node="mo" transform="translate(8045, 0)"><use xlink:href="#MJX-2-TEX-N-2C"></use></g><g data-mml-node="mn" transform="translate(8489.7, 0)"><use xlink:href="#MJX-2-TEX-N-32"></use></g><g data-mml-node="mo" transform="translate(8989.7, 0)"><use xlink:href="#MJX-2-TEX-N-2C"></use></g><g data-mml-node="mi" transform="translate(9434.3, 0)"><use xlink:href="#MJX-2-TEX-I-1D464"></use></g><g data-mml-node="mo" transform="translate(10150.3, 0)"><use xlink:href="#MJX-2-TEX-N-29"></use></g><g data-mml-node="mo" transform="translate(10539.3, 0)"><use xlink:href="#MJX-2-TEX-N-2C"></use></g><g data-mml-node="mo" transform="translate(10984, 0)"><use xlink:href="#MJX-2-TEX-N-28"></use></g><g data-mml-node="mn" transform="translate(11373, 0)"><use xlink:href="#MJX-2-TEX-N-31"></use></g><g data-mml-node="mo" transform="translate(11873, 0)"><use xlink:href="#MJX-2-TEX-N-2C"></use></g><g data-mml-node="mn" transform="translate(12317.7, 0)"><use xlink:href="#MJX-2-TEX-N-33"></use></g><g data-mml-node="mo" transform="translate(12817.7, 0)"><use xlink:href="#MJX-2-TEX-N-2C"></use></g><g data-mml-node="mi" transform="translate(13262.3, 0)"><use xlink:href="#MJX-2-TEX-I-1D452"></use></g><g data-mml-node="mo" transform="translate(13728.3, 0)"><use xlink:href="#MJX-2-TEX-N-29"></use></g><g data-mml-node="mo" transform="translate(14117.3, 0)"><use xlink:href="#MJX-2-TEX-N-2C"></use></g><g data-mml-node="mo" transform="translate(14562, 0)"><use xlink:href="#MJX-2-TEX-N-28"></use></g><g data-mml-node="mn" transform="translate(14951, 0)"><use xlink:href="#MJX-2-TEX-N-31"></use></g><g data-mml-node="mo" transform="translate(15451, 0)"><use xlink:href="#MJX-2-TEX-N-2C"></use></g><g data-mml-node="mn" transform="translate(15895.7, 0)"><use xlink:href="#MJX-2-TEX-N-33"></use></g><g data-mml-node="mo" transform="translate(16395.7, 0)"><use xlink:href="#MJX-2-TEX-N-2C"></use></g><g data-mml-node="mi" transform="translate(16840.3, 0)"><use xlink:href="#MJX-2-TEX-I-1D464"></use></g><g data-mml-node="mo" transform="translate(17556.3, 0)"><use xlink:href="#MJX-2-TEX-N-29"></use></g><g data-mml-node="mo" transform="translate(17945.3, 0)"><use xlink:href="#MJX-2-TEX-N-2C"></use></g><g data-mml-node="mo" transform="translate(18390, 0)"><use xlink:href="#MJX-2-TEX-N-28"></use></g><g data-mml-node="mn" transform="translate(18779, 0)"><use xlink:href="#MJX-2-TEX-N-32"></use></g><g data-mml-node="mo" transform="translate(19279, 0)"><use xlink:href="#MJX-2-TEX-N-2C"></use></g><g data-mml-node="mn" transform="translate(19723.7, 0)"><use xlink:href="#MJX-2-TEX-N-33"></use></g><g data-mml-node="mo" transform="translate(20223.7, 0)"><use xlink:href="#MJX-2-TEX-N-2C"></use></g><g data-mml-node="mi" transform="translate(20668.3, 0)"><use xlink:href="#MJX-2-TEX-I-1D452"></use></g><g data-mml-node="mo" transform="translate(21134.3, 0)"><use xlink:href="#MJX-2-TEX-N-29"></use></g><g data-mml-node="mo" transform="translate(21523.3, 0)"><use xlink:href="#MJX-2-TEX-N-2C"></use></g><g data-mml-node="mo" transform="translate(21968, 0)"><use xlink:href="#MJX-2-TEX-N-28"></use></g><g data-mml-node="mn" transform="translate(22357, 0)"><use xlink:href="#MJX-2-TEX-N-32"></use></g><g data-mml-node="mo" transform="translate(22857, 0)"><use xlink:href="#MJX-2-TEX-N-2C"></use></g><g data-mml-node="mn" transform="translate(23301.7, 0)"><use xlink:href="#MJX-2-TEX-N-33"></use></g><g data-mml-node="mo" transform="translate(23801.7, 0)"><use xlink:href="#MJX-2-TEX-N-2C"></use></g><g data-mml-node="mi" transform="translate(24246.3, 0)"><use xlink:href="#MJX-2-TEX-I-1D464"></use></g><g data-mml-node="mo" transform="translate(24962.3, 0)"><use xlink:href="#MJX-2-TEX-N-29"></use></g></g></g><g data-mml-node="mtr" transform="translate(0, -750)"><g data-mml-node="mtd"><g data-mml-node="mo"><use xlink:href="#MJX-2-TEX-N-28"></use></g><g data-mml-node="mn" transform="translate(389, 0)"><use xlink:href="#MJX-2-TEX-N-30"></use></g><g data-mml-node="mo" transform="translate(889, 0)"><use xlink:href="#MJX-2-TEX-N-2C"></use></g><g data-mml-node="mn" transform="translate(1333.7, 0)"><use xlink:href="#MJX-2-TEX-N-30"></use></g><g data-mml-node="mo" transform="translate(1833.7, 0)"><use xlink:href="#MJX-2-TEX-N-2C"></use></g><g data-mml-node="mi" transform="translate(2278.3, 0)"><use xlink:href="#MJX-2-TEX-I-1D464"></use></g><g data-mml-node="mo" transform="translate(2994.3, 0)"><use xlink:href="#MJX-2-TEX-N-29"></use></g><g data-mml-node="mo" transform="translate(3383.3, 0)"><use xlink:href="#MJX-2-TEX-N-2C"></use></g><g data-mml-node="mo" transform="translate(3828, 0)"><use xlink:href="#MJX-2-TEX-N-28"></use></g><g data-mml-node="mn" transform="translate(4217, 0)"><use xlink:href="#MJX-2-TEX-N-32"></use></g><g data-mml-node="mo" transform="translate(4717, 0)"><use xlink:href="#MJX-2-TEX-N-2C"></use></g><g data-mml-node="mn" transform="translate(5161.7, 0)"><use xlink:href="#MJX-2-TEX-N-31"></use></g><g data-mml-node="mo" transform="translate(5661.7, 0)"><use xlink:href="#MJX-2-TEX-N-2C"></use></g><g data-mml-node="mi" transform="translate(6106.3, 0)"><use xlink:href="#MJX-2-TEX-I-1D452"></use></g><g data-mml-node="mo" transform="translate(6572.3, 0)"><use xlink:href="#MJX-2-TEX-N-29"></use></g><g data-mml-node="mo" transform="translate(6961.3, 0)"><use xlink:href="#MJX-2-TEX-N-2C"></use></g><g data-mml-node="mo" transform="translate(7406, 0)"><use xlink:href="#MJX-2-TEX-N-28"></use></g><g data-mml-node="mn" transform="translate(7795, 0)"><use xlink:href="#MJX-2-TEX-N-32"></use></g><g data-mml-node="mo" transform="translate(8295, 0)"><use xlink:href="#MJX-2-TEX-N-2C"></use></g><g data-mml-node="mn" transform="translate(8739.7, 0)"><use xlink:href="#MJX-2-TEX-N-31"></use></g><g data-mml-node="mo" transform="translate(9239.7, 0)"><use xlink:href="#MJX-2-TEX-N-2C"></use></g><g data-mml-node="mi" transform="translate(9684.3, 0)"><use xlink:href="#MJX-2-TEX-I-1D464"></use></g><g data-mml-node="mo" transform="translate(10400.3, 0)"><use xlink:href="#MJX-2-TEX-N-29"></use></g><g data-mml-node="mo" transform="translate(10789.3, 0)"><use xlink:href="#MJX-2-TEX-N-2C"></use></g><g data-mml-node="mo" transform="translate(11234, 0)"><use xlink:href="#MJX-2-TEX-N-28"></use></g><g data-mml-node="mn" transform="translate(11623, 0)"><use xlink:href="#MJX-2-TEX-N-32"></use></g><g data-mml-node="mo" transform="translate(12123, 0)"><use xlink:href="#MJX-2-TEX-N-2C"></use></g><g data-mml-node="mn" transform="translate(12567.7, 0)"><use xlink:href="#MJX-2-TEX-N-30"></use></g><g data-mml-node="mo" transform="translate(13067.7, 0)"><use xlink:href="#MJX-2-TEX-N-2C"></use></g><g data-mml-node="mi" transform="translate(13512.3, 0)"><use xlink:href="#MJX-2-TEX-I-1D452"></use></g><g data-mml-node="mo" transform="translate(13978.3, 0)"><use xlink:href="#MJX-2-TEX-N-29"></use></g><g data-mml-node="mo" transform="translate(14367.3, 0)"><use xlink:href="#MJX-2-TEX-N-2C"></use></g><g data-mml-node="mo" transform="translate(14812, 0)"><use xlink:href="#MJX-2-TEX-N-28"></use></g><g data-mml-node="mn" transform="translate(15201, 0)"><use xlink:href="#MJX-2-TEX-N-32"></use></g><g data-mml-node="mo" transform="translate(15701, 0)"><use xlink:href="#MJX-2-TEX-N-2C"></use></g><g data-mml-node="mn" transform="translate(16145.7, 0)"><use xlink:href="#MJX-2-TEX-N-30"></use></g><g data-mml-node="mo" transform="translate(16645.7, 0)"><use xlink:href="#MJX-2-TEX-N-2C"></use></g><g data-mml-node="mi" transform="translate(17090.3, 0)"><use xlink:href="#MJX-2-TEX-I-1D464"></use></g><g data-mml-node="mo" transform="translate(17806.3, 0)"><use xlink:href="#MJX-2-TEX-N-29"></use></g><g data-mml-node="mo" transform="translate(18195.3, 0)"><use xlink:href="#MJX-2-TEX-N-2C"></use></g><g data-mml-node="mo" transform="translate(18640, 0)"><use xlink:href="#MJX-2-TEX-N-28"></use></g><g data-mml-node="mn" transform="translate(19029, 0)"><use xlink:href="#MJX-2-TEX-N-31"></use></g><g data-mml-node="mo" transform="translate(19529, 0)"><use xlink:href="#MJX-2-TEX-N-2C"></use></g><g data-mml-node="mn" transform="translate(19973.7, 0)"><use xlink:href="#MJX-2-TEX-N-30"></use></g><g data-mml-node="mo" transform="translate(20473.7, 0)"><use xlink:href="#MJX-2-TEX-N-2C"></use></g><g data-mml-node="mi" transform="translate(20918.3, 0)"><use xlink:href="#MJX-2-TEX-I-1D452"></use></g><g data-mml-node="mo" transform="translate(21384.3, 0)"><use xlink:href="#MJX-2-TEX-N-29"></use></g><g data-mml-node="mo" transform="translate(21773.3, 0)"><use xlink:href="#MJX-2-TEX-N-2C"></use></g><g data-mml-node="mo" transform="translate(22218, 0)"><use xlink:href="#MJX-2-TEX-N-28"></use></g><g data-mml-node="mn" transform="translate(22607, 0)"><use xlink:href="#MJX-2-TEX-N-31"></use></g><g data-mml-node="mo" transform="translate(23107, 0)"><use xlink:href="#MJX-2-TEX-N-2C"></use></g><g data-mml-node="mn" transform="translate(23551.7, 0)"><use xlink:href="#MJX-2-TEX-N-30"></use></g><g data-mml-node="mo" transform="translate(24051.7, 0)"><use xlink:href="#MJX-2-TEX-N-2C"></use></g><g data-mml-node="mi" transform="translate(24496.3, 0)"><use xlink:href="#MJX-2-TEX-I-1D464"></use></g><g data-mml-node="mo" transform="translate(25212.3, 0)"><use xlink:href="#MJX-2-TEX-N-29"></use></g></g></g></g></g></g></svg></mjx-container></div><p>There are only 18 states left, and we use <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.439ex" xmlns="http://www.w3.org/2000/svg" width="12.878ex" height="1.441ex" role="img" focusable="false" viewBox="0 -443 5691.9 637" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-2-TEX-I-1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path><path id="MJX-2-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-2-TEX-N-2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path><path id="MJX-2-TEX-N-32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path><path id="MJX-2-TEX-N-2026" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60ZM525 60Q525 84 542 102T585 120Q609 120 627 104T646 61Q646 36 629 18T586 0T543 17T525 60ZM972 60Q972 84 989 102T1032 120Q1056 120 1074 104T1093 61Q1093 36 1076 18T1033 0T990 17T972 60Z"></path><path id="MJX-2-TEX-N-38" d="M70 417T70 494T124 618T248 666Q319 666 374 624T429 515Q429 485 418 459T392 417T361 389T335 371T324 363L338 354Q352 344 366 334T382 323Q457 264 457 174Q457 95 399 37T249 -22Q159 -22 101 29T43 155Q43 263 172 335L154 348Q133 361 127 368Q70 417 70 494ZM286 386L292 390Q298 394 301 396T311 403T323 413T334 425T345 438T355 454T364 471T369 491T371 513Q371 556 342 586T275 624Q268 625 242 625Q201 625 165 599T128 534Q128 511 141 492T167 463T217 431Q224 426 228 424L286 386ZM250 21Q308 21 350 55T392 137Q392 154 387 169T375 194T353 216T330 234T301 253T274 270Q260 279 244 289T218 306L210 311Q204 311 181 294T133 239T107 157Q107 98 150 60T250 21Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D463"></use></g><g data-mml-node="mn" transform="translate(485, -150) scale(0.707)"><use xlink:href="#MJX-2-TEX-N-31"></use></g></g><g data-mml-node="mo" transform="translate(888.6, 0)"><use xlink:href="#MJX-2-TEX-N-2C"></use></g><g data-mml-node="msub" transform="translate(1333.2, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D463"></use></g><g data-mml-node="mn" transform="translate(485, -150) scale(0.707)"><use xlink:href="#MJX-2-TEX-N-32"></use></g></g><g data-mml-node="mo" transform="translate(2221.8, 0)"><use xlink:href="#MJX-2-TEX-N-2C"></use></g><g data-mml-node="mo" transform="translate(2666.4, 0)"><use xlink:href="#MJX-2-TEX-N-2026"></use></g><g data-mml-node="mo" transform="translate(4005.1, 0)"><use xlink:href="#MJX-2-TEX-N-2C"></use></g><g data-mml-node="msub" transform="translate(4449.8, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D463"></use></g><g data-mml-node="TeXAtom" transform="translate(485, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-31"></use><use xlink:href="#MJX-2-TEX-N-38" transform="translate(500, 0)"></use></g></g></g></g></g></svg></mjx-container> to represent a set of vertexs as following:</p><div style="display: flex;justify-content: center;">    <span>        <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -14.14ex" xmlns="http://www.w3.org/2000/svg" width="27.861ex" height="29.412ex" role="img" focusable="false" viewBox="0 -6750 12314.4 13000" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-2-TEX-I-1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path><path id="MJX-2-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-2-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-2-TEX-N-28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path><path id="MJX-2-TEX-N-33" d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z"></path><path id="MJX-2-TEX-N-2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path><path id="MJX-2-TEX-I-1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path><path id="MJX-2-TEX-N-29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path><path id="MJX-2-TEX-N-30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path><path id="MJX-2-TEX-I-1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path><path id="MJX-2-TEX-N-32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path><path id="MJX-2-TEX-N-34" d="M462 0Q444 3 333 3Q217 3 199 0H190V46H221Q241 46 248 46T265 48T279 53T286 61Q287 63 287 115V165H28V211L179 442Q332 674 334 675Q336 677 355 677H373L379 671V211H471V165H379V114Q379 73 379 66T385 54Q393 47 442 46H471V0H462ZM293 211V545L74 212L183 211H293Z"></path><path id="MJX-2-TEX-N-35" d="M164 157Q164 133 148 117T109 101H102Q148 22 224 22Q294 22 326 82Q345 115 345 210Q345 313 318 349Q292 382 260 382H254Q176 382 136 314Q132 307 129 306T114 304Q97 304 95 310Q93 314 93 485V614Q93 664 98 664Q100 666 102 666Q103 666 123 658T178 642T253 634Q324 634 389 662Q397 666 402 666Q410 666 410 648V635Q328 538 205 538Q174 538 149 544L139 546V374Q158 388 169 396T205 412T256 420Q337 420 393 355T449 201Q449 109 385 44T229 -22Q148 -22 99 32T50 154Q50 178 61 192T84 210T107 214Q132 214 148 197T164 157Z"></path><path id="MJX-2-TEX-N-36" d="M42 313Q42 476 123 571T303 666Q372 666 402 630T432 550Q432 525 418 510T379 495Q356 495 341 509T326 548Q326 592 373 601Q351 623 311 626Q240 626 194 566Q147 500 147 364L148 360Q153 366 156 373Q197 433 263 433H267Q313 433 348 414Q372 400 396 374T435 317Q456 268 456 210V192Q456 169 451 149Q440 90 387 34T253 -22Q225 -22 199 -14T143 16T92 75T56 172T42 313ZM257 397Q227 397 205 380T171 335T154 278T148 216Q148 133 160 97T198 39Q222 21 251 21Q302 21 329 59Q342 77 347 104T352 209Q352 289 347 316T329 361Q302 397 257 397Z"></path><path id="MJX-2-TEX-N-37" d="M55 458Q56 460 72 567L88 674Q88 676 108 676H128V672Q128 662 143 655T195 646T364 644H485V605L417 512Q408 500 387 472T360 435T339 403T319 367T305 330T292 284T284 230T278 162T275 80Q275 66 275 52T274 28V19Q270 2 255 -10T221 -22Q210 -22 200 -19T179 0T168 40Q168 198 265 368Q285 400 349 489L395 552H302Q128 552 119 546Q113 543 108 522T98 479L95 458V455H55V458Z"></path><path id="MJX-2-TEX-N-38" d="M70 417T70 494T124 618T248 666Q319 666 374 624T429 515Q429 485 418 459T392 417T361 389T335 371T324 363L338 354Q352 344 366 334T382 323Q457 264 457 174Q457 95 399 37T249 -22Q159 -22 101 29T43 155Q43 263 172 335L154 348Q133 361 127 368Q70 417 70 494ZM286 386L292 390Q298 394 301 396T311 403T323 413T334 425T345 438T355 454T364 471T369 491T371 513Q371 556 342 586T275 624Q268 625 242 625Q201 625 165 599T128 534Q128 511 141 492T167 463T217 431Q224 426 228 424L286 386ZM250 21Q308 21 350 55T392 137Q392 154 387 169T375 194T353 216T330 234T301 253T274 270Q260 279 244 289T218 306L210 311Q204 311 181 294T133 239T107 157Q107 98 150 60T250 21Z"></path><path id="MJX-2-TEX-N-39" d="M352 287Q304 211 232 211Q154 211 104 270T44 396Q42 412 42 436V444Q42 537 111 606Q171 666 243 666Q245 666 249 666T257 665H261Q273 665 286 663T323 651T370 619T413 560Q456 472 456 334Q456 194 396 97Q361 41 312 10T208 -22Q147 -22 108 7T68 93T121 149Q143 149 158 135T173 96Q173 78 164 65T148 49T135 44L131 43Q131 41 138 37T164 27T206 22H212Q272 22 313 86Q352 142 352 280V287ZM244 248Q292 248 321 297T351 430Q351 508 343 542Q341 552 337 562T323 588T293 615T246 625Q208 625 181 598Q160 576 154 546T147 441Q147 358 152 329T172 282Q197 248 244 248Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mtable"><g data-mml-node="mtr" transform="translate(0, 6000)"><g data-mml-node="mtd"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D463"></use></g><g data-mml-node="mn" transform="translate(485, -150) scale(0.707)"><use xlink:href="#MJX-2-TEX-N-31"></use></g></g><g data-mml-node="mo" transform="translate(1166.3, 0)"><use xlink:href="#MJX-2-TEX-N-3D"></use></g><g data-mml-node="mo" transform="translate(2222.1, 0)"><use xlink:href="#MJX-2-TEX-N-28"></use></g><g data-mml-node="mn" transform="translate(2611.1, 0)"><use xlink:href="#MJX-2-TEX-N-33"></use></g><g data-mml-node="mo" transform="translate(3111.1, 0)"><use xlink:href="#MJX-2-TEX-N-2C"></use></g><g data-mml-node="mn" transform="translate(3555.8, 0)"><use xlink:href="#MJX-2-TEX-N-33"></use></g><g data-mml-node="mo" transform="translate(4055.8, 0)"><use xlink:href="#MJX-2-TEX-N-2C"></use></g><g data-mml-node="mi" transform="translate(4500.4, 0)"><use xlink:href="#MJX-2-TEX-I-1D452"></use></g><g data-mml-node="mo" transform="translate(4966.4, 0)"><use xlink:href="#MJX-2-TEX-N-29"></use></g><g data-mml-node="mstyle" transform="translate(5355.4, 0)"><g data-mml-node="mspace"></g></g><g data-mml-node="msub" transform="translate(6355.4, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D463"></use></g><g data-mml-node="TeXAtom" transform="translate(485, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-31"></use><use xlink:href="#MJX-2-TEX-N-30" transform="translate(500, 0)"></use></g></g></g><g data-mml-node="mo" transform="translate(7875.3, 0)"><use xlink:href="#MJX-2-TEX-N-3D"></use></g><g data-mml-node="mo" transform="translate(8931.1, 0)"><use xlink:href="#MJX-2-TEX-N-28"></use></g><g data-mml-node="mn" transform="translate(9320.1, 0)"><use xlink:href="#MJX-2-TEX-N-33"></use></g><g data-mml-node="mo" transform="translate(9820.1, 0)"><use xlink:href="#MJX-2-TEX-N-2C"></use></g><g data-mml-node="mn" transform="translate(10264.8, 0)"><use xlink:href="#MJX-2-TEX-N-33"></use></g><g data-mml-node="mo" transform="translate(10764.8, 0)"><use xlink:href="#MJX-2-TEX-N-2C"></use></g><g data-mml-node="mi" transform="translate(11209.4, 0)"><use xlink:href="#MJX-2-TEX-I-1D464"></use></g><g data-mml-node="mo" transform="translate(11925.4, 0)"><use xlink:href="#MJX-2-TEX-N-29"></use></g></g></g><g data-mml-node="mtr" transform="translate(0, 4500)"><g data-mml-node="mtd"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D463"></use></g><g data-mml-node="mn" transform="translate(485, -150) scale(0.707)"><use xlink:href="#MJX-2-TEX-N-32"></use></g></g><g data-mml-node="mo" transform="translate(1166.3, 0)"><use xlink:href="#MJX-2-TEX-N-3D"></use></g><g data-mml-node="mo" transform="translate(2222.1, 0)"><use xlink:href="#MJX-2-TEX-N-28"></use></g><g data-mml-node="mn" transform="translate(2611.1, 0)"><use xlink:href="#MJX-2-TEX-N-33"></use></g><g data-mml-node="mo" transform="translate(3111.1, 0)"><use xlink:href="#MJX-2-TEX-N-2C"></use></g><g data-mml-node="mn" transform="translate(3555.8, 0)"><use xlink:href="#MJX-2-TEX-N-32"></use></g><g data-mml-node="mo" transform="translate(4055.8, 0)"><use xlink:href="#MJX-2-TEX-N-2C"></use></g><g data-mml-node="mi" transform="translate(4500.4, 0)"><use xlink:href="#MJX-2-TEX-I-1D452"></use></g><g data-mml-node="mo" transform="translate(4966.4, 0)"><use xlink:href="#MJX-2-TEX-N-29"></use></g><g data-mml-node="mstyle" transform="translate(5355.4, 0)"><g data-mml-node="mspace"></g></g><g data-mml-node="msub" transform="translate(6355.4, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D463"></use></g><g data-mml-node="TeXAtom" transform="translate(485, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-31"></use><use xlink:href="#MJX-2-TEX-N-31" transform="translate(500, 0)"></use></g></g></g><g data-mml-node="mo" transform="translate(7875.3, 0)"><use xlink:href="#MJX-2-TEX-N-3D"></use></g><g data-mml-node="mo" transform="translate(8931.1, 0)"><use xlink:href="#MJX-2-TEX-N-28"></use></g><g data-mml-node="mn" transform="translate(9320.1, 0)"><use xlink:href="#MJX-2-TEX-N-33"></use></g><g data-mml-node="mo" transform="translate(9820.1, 0)"><use xlink:href="#MJX-2-TEX-N-2C"></use></g><g data-mml-node="mn" transform="translate(10264.8, 0)"><use xlink:href="#MJX-2-TEX-N-32"></use></g><g data-mml-node="mo" transform="translate(10764.8, 0)"><use xlink:href="#MJX-2-TEX-N-2C"></use></g><g data-mml-node="mi" transform="translate(11209.4, 0)"><use xlink:href="#MJX-2-TEX-I-1D464"></use></g><g data-mml-node="mo" transform="translate(11925.4, 0)"><use xlink:href="#MJX-2-TEX-N-29"></use></g></g></g><g data-mml-node="mtr" transform="translate(0, 3000)"><g data-mml-node="mtd"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D463"></use></g><g data-mml-node="mn" transform="translate(485, -150) scale(0.707)"><use xlink:href="#MJX-2-TEX-N-33"></use></g></g><g data-mml-node="mo" transform="translate(1166.3, 0)"><use xlink:href="#MJX-2-TEX-N-3D"></use></g><g data-mml-node="mo" transform="translate(2222.1, 0)"><use xlink:href="#MJX-2-TEX-N-28"></use></g><g data-mml-node="mn" transform="translate(2611.1, 0)"><use xlink:href="#MJX-2-TEX-N-33"></use></g><g data-mml-node="mo" transform="translate(3111.1, 0)"><use xlink:href="#MJX-2-TEX-N-2C"></use></g><g data-mml-node="mn" transform="translate(3555.8, 0)"><use xlink:href="#MJX-2-TEX-N-31"></use></g><g data-mml-node="mo" transform="translate(4055.8, 0)"><use xlink:href="#MJX-2-TEX-N-2C"></use></g><g data-mml-node="mi" transform="translate(4500.4, 0)"><use xlink:href="#MJX-2-TEX-I-1D452"></use></g><g data-mml-node="mo" transform="translate(4966.4, 0)"><use xlink:href="#MJX-2-TEX-N-29"></use></g><g data-mml-node="mstyle" transform="translate(5355.4, 0)"><g data-mml-node="mspace"></g></g><g data-mml-node="msub" transform="translate(6355.4, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D463"></use></g><g data-mml-node="TeXAtom" transform="translate(485, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-31"></use><use xlink:href="#MJX-2-TEX-N-32" transform="translate(500, 0)"></use></g></g></g><g data-mml-node="mo" transform="translate(7875.3, 0)"><use xlink:href="#MJX-2-TEX-N-3D"></use></g><g data-mml-node="mo" transform="translate(8931.1, 0)"><use xlink:href="#MJX-2-TEX-N-28"></use></g><g data-mml-node="mn" transform="translate(9320.1, 0)"><use xlink:href="#MJX-2-TEX-N-33"></use></g><g data-mml-node="mo" transform="translate(9820.1, 0)"><use xlink:href="#MJX-2-TEX-N-2C"></use></g><g data-mml-node="mn" transform="translate(10264.8, 0)"><use xlink:href="#MJX-2-TEX-N-31"></use></g><g data-mml-node="mo" transform="translate(10764.8, 0)"><use xlink:href="#MJX-2-TEX-N-2C"></use></g><g data-mml-node="mi" transform="translate(11209.4, 0)"><use xlink:href="#MJX-2-TEX-I-1D464"></use></g><g data-mml-node="mo" transform="translate(11925.4, 0)"><use xlink:href="#MJX-2-TEX-N-29"></use></g></g></g><g data-mml-node="mtr" transform="translate(0, 1500)"><g data-mml-node="mtd"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D463"></use></g><g data-mml-node="mn" transform="translate(485, -150) scale(0.707)"><use xlink:href="#MJX-2-TEX-N-34"></use></g></g><g data-mml-node="mo" transform="translate(1166.3, 0)"><use xlink:href="#MJX-2-TEX-N-3D"></use></g><g data-mml-node="mo" transform="translate(2222.1, 0)"><use xlink:href="#MJX-2-TEX-N-28"></use></g><g data-mml-node="mn" transform="translate(2611.1, 0)"><use xlink:href="#MJX-2-TEX-N-33"></use></g><g data-mml-node="mo" transform="translate(3111.1, 0)"><use xlink:href="#MJX-2-TEX-N-2C"></use></g><g data-mml-node="mn" transform="translate(3555.8, 0)"><use xlink:href="#MJX-2-TEX-N-30"></use></g><g data-mml-node="mo" transform="translate(4055.8, 0)"><use xlink:href="#MJX-2-TEX-N-2C"></use></g><g data-mml-node="mi" transform="translate(4500.4, 0)"><use xlink:href="#MJX-2-TEX-I-1D452"></use></g><g data-mml-node="mo" transform="translate(4966.4, 0)"><use xlink:href="#MJX-2-TEX-N-29"></use></g><g data-mml-node="mstyle" transform="translate(5355.4, 0)"><g data-mml-node="mspace"></g></g><g data-mml-node="msub" transform="translate(6355.4, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D463"></use></g><g data-mml-node="TeXAtom" transform="translate(485, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-31"></use><use xlink:href="#MJX-2-TEX-N-33" transform="translate(500, 0)"></use></g></g></g><g data-mml-node="mo" transform="translate(7875.3, 0)"><use xlink:href="#MJX-2-TEX-N-3D"></use></g><g data-mml-node="mo" transform="translate(8931.1, 0)"><use xlink:href="#MJX-2-TEX-N-28"></use></g><g data-mml-node="mn" transform="translate(9320.1, 0)"><use xlink:href="#MJX-2-TEX-N-33"></use></g><g data-mml-node="mo" transform="translate(9820.1, 0)"><use xlink:href="#MJX-2-TEX-N-2C"></use></g><g data-mml-node="mn" transform="translate(10264.8, 0)"><use xlink:href="#MJX-2-TEX-N-30"></use></g><g data-mml-node="mo" transform="translate(10764.8, 0)"><use xlink:href="#MJX-2-TEX-N-2C"></use></g><g data-mml-node="mi" transform="translate(11209.4, 0)"><use xlink:href="#MJX-2-TEX-I-1D464"></use></g><g data-mml-node="mo" transform="translate(11925.4, 0)"><use xlink:href="#MJX-2-TEX-N-29"></use></g></g></g><g data-mml-node="mtr"><g data-mml-node="mtd"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D463"></use></g><g data-mml-node="mn" transform="translate(485, -150) scale(0.707)"><use xlink:href="#MJX-2-TEX-N-35"></use></g></g><g data-mml-node="mo" transform="translate(1166.3, 0)"><use xlink:href="#MJX-2-TEX-N-3D"></use></g><g data-mml-node="mo" transform="translate(2222.1, 0)"><use xlink:href="#MJX-2-TEX-N-28"></use></g><g data-mml-node="mn" transform="translate(2611.1, 0)"><use xlink:href="#MJX-2-TEX-N-32"></use></g><g data-mml-node="mo" transform="translate(3111.1, 0)"><use xlink:href="#MJX-2-TEX-N-2C"></use></g><g data-mml-node="mn" transform="translate(3555.8, 0)"><use xlink:href="#MJX-2-TEX-N-32"></use></g><g data-mml-node="mo" transform="translate(4055.8, 0)"><use xlink:href="#MJX-2-TEX-N-2C"></use></g><g data-mml-node="mi" transform="translate(4500.4, 0)"><use xlink:href="#MJX-2-TEX-I-1D452"></use></g><g data-mml-node="mo" transform="translate(4966.4, 0)"><use xlink:href="#MJX-2-TEX-N-29"></use></g><g data-mml-node="mstyle" transform="translate(5355.4, 0)"><g data-mml-node="mspace"></g></g><g data-mml-node="msub" transform="translate(6355.4, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D463"></use></g><g data-mml-node="TeXAtom" transform="translate(485, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-31"></use><use xlink:href="#MJX-2-TEX-N-34" transform="translate(500, 0)"></use></g></g></g><g data-mml-node="mo" transform="translate(7875.3, 0)"><use xlink:href="#MJX-2-TEX-N-3D"></use></g><g data-mml-node="mo" transform="translate(8931.1, 0)"><use xlink:href="#MJX-2-TEX-N-28"></use></g><g data-mml-node="mn" transform="translate(9320.1, 0)"><use xlink:href="#MJX-2-TEX-N-32"></use></g><g data-mml-node="mo" transform="translate(9820.1, 0)"><use xlink:href="#MJX-2-TEX-N-2C"></use></g><g data-mml-node="mn" transform="translate(10264.8, 0)"><use xlink:href="#MJX-2-TEX-N-32"></use></g><g data-mml-node="mo" transform="translate(10764.8, 0)"><use xlink:href="#MJX-2-TEX-N-2C"></use></g><g data-mml-node="mi" transform="translate(11209.4, 0)"><use xlink:href="#MJX-2-TEX-I-1D464"></use></g><g data-mml-node="mo" transform="translate(11925.4, 0)"><use xlink:href="#MJX-2-TEX-N-29"></use></g></g></g><g data-mml-node="mtr" transform="translate(0, -1500)"><g data-mml-node="mtd"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D463"></use></g><g data-mml-node="mn" transform="translate(485, -150) scale(0.707)"><use xlink:href="#MJX-2-TEX-N-36"></use></g></g><g data-mml-node="mo" transform="translate(1166.3, 0)"><use xlink:href="#MJX-2-TEX-N-3D"></use></g><g data-mml-node="mo" transform="translate(2222.1, 0)"><use xlink:href="#MJX-2-TEX-N-28"></use></g><g data-mml-node="mn" transform="translate(2611.1, 0)"><use xlink:href="#MJX-2-TEX-N-32"></use></g><g data-mml-node="mo" transform="translate(3111.1, 0)"><use xlink:href="#MJX-2-TEX-N-2C"></use></g><g data-mml-node="mn" transform="translate(3555.8, 0)"><use xlink:href="#MJX-2-TEX-N-33"></use></g><g data-mml-node="mo" transform="translate(4055.8, 0)"><use xlink:href="#MJX-2-TEX-N-2C"></use></g><g data-mml-node="mi" transform="translate(4500.4, 0)"><use xlink:href="#MJX-2-TEX-I-1D452"></use></g><g data-mml-node="mo" transform="translate(4966.4, 0)"><use xlink:href="#MJX-2-TEX-N-29"></use></g><g data-mml-node="mstyle" transform="translate(5355.4, 0)"><g data-mml-node="mspace"></g></g><g data-mml-node="msub" transform="translate(6355.4, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D463"></use></g><g data-mml-node="TeXAtom" transform="translate(485, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-31"></use><use xlink:href="#MJX-2-TEX-N-35" transform="translate(500, 0)"></use></g></g></g><g data-mml-node="mo" transform="translate(7875.3, 0)"><use xlink:href="#MJX-2-TEX-N-3D"></use></g><g data-mml-node="mo" transform="translate(8931.1, 0)"><use xlink:href="#MJX-2-TEX-N-28"></use></g><g data-mml-node="mn" transform="translate(9320.1, 0)"><use xlink:href="#MJX-2-TEX-N-31"></use></g><g data-mml-node="mo" transform="translate(9820.1, 0)"><use xlink:href="#MJX-2-TEX-N-2C"></use></g><g data-mml-node="mn" transform="translate(10264.8, 0)"><use xlink:href="#MJX-2-TEX-N-31"></use></g><g data-mml-node="mo" transform="translate(10764.8, 0)"><use xlink:href="#MJX-2-TEX-N-2C"></use></g><g data-mml-node="mi" transform="translate(11209.4, 0)"><use xlink:href="#MJX-2-TEX-I-1D464"></use></g><g data-mml-node="mo" transform="translate(11925.4, 0)"><use xlink:href="#MJX-2-TEX-N-29"></use></g></g></g><g data-mml-node="mtr" transform="translate(0, -3000)"><g data-mml-node="mtd"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D463"></use></g><g data-mml-node="mn" transform="translate(485, -150) scale(0.707)"><use xlink:href="#MJX-2-TEX-N-37"></use></g></g><g data-mml-node="mo" transform="translate(1166.3, 0)"><use xlink:href="#MJX-2-TEX-N-3D"></use></g><g data-mml-node="mo" transform="translate(2222.1, 0)"><use xlink:href="#MJX-2-TEX-N-28"></use></g><g data-mml-node="mn" transform="translate(2611.1, 0)"><use xlink:href="#MJX-2-TEX-N-30"></use></g><g data-mml-node="mo" transform="translate(3111.1, 0)"><use xlink:href="#MJX-2-TEX-N-2C"></use></g><g data-mml-node="mn" transform="translate(3555.8, 0)"><use xlink:href="#MJX-2-TEX-N-33"></use></g><g data-mml-node="mo" transform="translate(4055.8, 0)"><use xlink:href="#MJX-2-TEX-N-2C"></use></g><g data-mml-node="mi" transform="translate(4500.4, 0)"><use xlink:href="#MJX-2-TEX-I-1D452"></use></g><g data-mml-node="mo" transform="translate(4966.4, 0)"><use xlink:href="#MJX-2-TEX-N-29"></use></g><g data-mml-node="mstyle" transform="translate(5355.4, 0)"><g data-mml-node="mspace"></g></g><g data-mml-node="msub" transform="translate(6355.4, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D463"></use></g><g data-mml-node="TeXAtom" transform="translate(485, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-31"></use><use xlink:href="#MJX-2-TEX-N-36" transform="translate(500, 0)"></use></g></g></g><g data-mml-node="mo" transform="translate(7875.3, 0)"><use xlink:href="#MJX-2-TEX-N-3D"></use></g><g data-mml-node="mo" transform="translate(8931.1, 0)"><use xlink:href="#MJX-2-TEX-N-28"></use></g><g data-mml-node="mn" transform="translate(9320.1, 0)"><use xlink:href="#MJX-2-TEX-N-30"></use></g><g data-mml-node="mo" transform="translate(9820.1, 0)"><use xlink:href="#MJX-2-TEX-N-2C"></use></g><g data-mml-node="mn" transform="translate(10264.8, 0)"><use xlink:href="#MJX-2-TEX-N-33"></use></g><g data-mml-node="mo" transform="translate(10764.8, 0)"><use xlink:href="#MJX-2-TEX-N-2C"></use></g><g data-mml-node="mi" transform="translate(11209.4, 0)"><use xlink:href="#MJX-2-TEX-I-1D464"></use></g><g data-mml-node="mo" transform="translate(11925.4, 0)"><use xlink:href="#MJX-2-TEX-N-29"></use></g></g></g><g data-mml-node="mtr" transform="translate(0, -4500)"><g data-mml-node="mtd"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D463"></use></g><g data-mml-node="mn" transform="translate(485, -150) scale(0.707)"><use xlink:href="#MJX-2-TEX-N-38"></use></g></g><g data-mml-node="mo" transform="translate(1166.3, 0)"><use xlink:href="#MJX-2-TEX-N-3D"></use></g><g data-mml-node="mo" transform="translate(2222.1, 0)"><use xlink:href="#MJX-2-TEX-N-28"></use></g><g data-mml-node="mn" transform="translate(2611.1, 0)"><use xlink:href="#MJX-2-TEX-N-30"></use></g><g data-mml-node="mo" transform="translate(3111.1, 0)"><use xlink:href="#MJX-2-TEX-N-2C"></use></g><g data-mml-node="mn" transform="translate(3555.8, 0)"><use xlink:href="#MJX-2-TEX-N-32"></use></g><g data-mml-node="mo" transform="translate(4055.8, 0)"><use xlink:href="#MJX-2-TEX-N-2C"></use></g><g data-mml-node="mi" transform="translate(4500.4, 0)"><use xlink:href="#MJX-2-TEX-I-1D452"></use></g><g data-mml-node="mo" transform="translate(4966.4, 0)"><use xlink:href="#MJX-2-TEX-N-29"></use></g><g data-mml-node="mstyle" transform="translate(5355.4, 0)"><g data-mml-node="mspace"></g></g><g data-mml-node="msub" transform="translate(6355.4, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D463"></use></g><g data-mml-node="TeXAtom" transform="translate(485, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-31"></use><use xlink:href="#MJX-2-TEX-N-37" transform="translate(500, 0)"></use></g></g></g><g data-mml-node="mo" transform="translate(7875.3, 0)"><use xlink:href="#MJX-2-TEX-N-3D"></use></g><g data-mml-node="mo" transform="translate(8931.1, 0)"><use xlink:href="#MJX-2-TEX-N-28"></use></g><g data-mml-node="mn" transform="translate(9320.1, 0)"><use xlink:href="#MJX-2-TEX-N-30"></use></g><g data-mml-node="mo" transform="translate(9820.1, 0)"><use xlink:href="#MJX-2-TEX-N-2C"></use></g><g data-mml-node="mn" transform="translate(10264.8, 0)"><use xlink:href="#MJX-2-TEX-N-32"></use></g><g data-mml-node="mo" transform="translate(10764.8, 0)"><use xlink:href="#MJX-2-TEX-N-2C"></use></g><g data-mml-node="mi" transform="translate(11209.4, 0)"><use xlink:href="#MJX-2-TEX-I-1D464"></use></g><g data-mml-node="mo" transform="translate(11925.4, 0)"><use xlink:href="#MJX-2-TEX-N-29"></use></g></g></g><g data-mml-node="mtr" transform="translate(0, -6000)"><g data-mml-node="mtd"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D463"></use></g><g data-mml-node="mn" transform="translate(485, -150) scale(0.707)"><use xlink:href="#MJX-2-TEX-N-39"></use></g></g><g data-mml-node="mo" transform="translate(1166.3, 0)"><use xlink:href="#MJX-2-TEX-N-3D"></use></g><g data-mml-node="mo" transform="translate(2222.1, 0)"><use xlink:href="#MJX-2-TEX-N-28"></use></g><g data-mml-node="mn" transform="translate(2611.1, 0)"><use xlink:href="#MJX-2-TEX-N-30"></use></g><g data-mml-node="mo" transform="translate(3111.1, 0)"><use xlink:href="#MJX-2-TEX-N-2C"></use></g><g data-mml-node="mn" transform="translate(3555.8, 0)"><use xlink:href="#MJX-2-TEX-N-31"></use></g><g data-mml-node="mo" transform="translate(4055.8, 0)"><use xlink:href="#MJX-2-TEX-N-2C"></use></g><g data-mml-node="mi" transform="translate(4500.4, 0)"><use xlink:href="#MJX-2-TEX-I-1D452"></use></g><g data-mml-node="mo" transform="translate(4966.4, 0)"><use xlink:href="#MJX-2-TEX-N-29"></use></g><g data-mml-node="mstyle" transform="translate(5355.4, 0)"><g data-mml-node="mspace"></g></g><g data-mml-node="msub" transform="translate(6355.4, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D463"></use></g><g data-mml-node="TeXAtom" transform="translate(485, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-31"></use><use xlink:href="#MJX-2-TEX-N-38" transform="translate(500, 0)"></use></g></g></g><g data-mml-node="mo" transform="translate(7875.3, 0)"><use xlink:href="#MJX-2-TEX-N-3D"></use></g><g data-mml-node="mo" transform="translate(8931.1, 0)"><use xlink:href="#MJX-2-TEX-N-28"></use></g><g data-mml-node="mn" transform="translate(9320.1, 0)"><use xlink:href="#MJX-2-TEX-N-30"></use></g><g data-mml-node="mo" transform="translate(9820.1, 0)"><use xlink:href="#MJX-2-TEX-N-2C"></use></g><g data-mml-node="mn" transform="translate(10264.8, 0)"><use xlink:href="#MJX-2-TEX-N-31"></use></g><g data-mml-node="mo" transform="translate(10764.8, 0)"><use xlink:href="#MJX-2-TEX-N-2C"></use></g><g data-mml-node="mi" transform="translate(11209.4, 0)"><use xlink:href="#MJX-2-TEX-I-1D464"></use></g><g data-mml-node="mo" transform="translate(11925.4, 0)"><use xlink:href="#MJX-2-TEX-N-29"></use></g></g></g></g></g></g></svg></mjx-container>    </span></div><p>We know that everyone departs from east side, that is, the initial vertex is <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.339ex" xmlns="http://www.w3.org/2000/svg" width="2.01ex" height="1.342ex" role="img" focusable="false" viewBox="0 -443 888.6 593" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-2-TEX-I-1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path><path id="MJX-2-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D463"></use></g><g data-mml-node="mn" transform="translate(485, -150) scale(0.707)"><use xlink:href="#MJX-2-TEX-N-31"></use></g></g></g></g></svg></mjx-container>. Suppose 1 missionary and 1 cannibal tend to go from east side to west side, then it should be represented as <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.339ex" xmlns="http://www.w3.org/2000/svg" width="2.01ex" height="1.342ex" role="img" focusable="false" viewBox="0 -443 888.6 593" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-2-TEX-I-1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path><path id="MJX-2-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D463"></use></g><g data-mml-node="mn" transform="translate(485, -150) scale(0.707)"><use xlink:href="#MJX-2-TEX-N-31"></use></g></g></g></g></svg></mjx-container> to <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.375ex" xmlns="http://www.w3.org/2000/svg" width="2.81ex" height="1.377ex" role="img" focusable="false" viewBox="0 -443 1242.1 608.6" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-2-TEX-I-1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path><path id="MJX-2-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-2-TEX-N-35" d="M164 157Q164 133 148 117T109 101H102Q148 22 224 22Q294 22 326 82Q345 115 345 210Q345 313 318 349Q292 382 260 382H254Q176 382 136 314Q132 307 129 306T114 304Q97 304 95 310Q93 314 93 485V614Q93 664 98 664Q100 666 102 666Q103 666 123 658T178 642T253 634Q324 634 389 662Q397 666 402 666Q410 666 410 648V635Q328 538 205 538Q174 538 149 544L139 546V374Q158 388 169 396T205 412T256 420Q337 420 393 355T449 201Q449 109 385 44T229 -22Q148 -22 99 32T50 154Q50 178 61 192T84 210T107 214Q132 214 148 197T164 157Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D463"></use></g><g data-mml-node="TeXAtom" transform="translate(485, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-31"></use><use xlink:href="#MJX-2-TEX-N-35" transform="translate(500, 0)"></use></g></g></g></g></g></svg></mjx-container>, and we will say <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.339ex" xmlns="http://www.w3.org/2000/svg" width="2.01ex" height="1.342ex" role="img" focusable="false" viewBox="0 -443 888.6 593" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-2-TEX-I-1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path><path id="MJX-2-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D463"></use></g><g data-mml-node="mn" transform="translate(485, -150) scale(0.707)"><use xlink:href="#MJX-2-TEX-N-31"></use></g></g></g></g></svg></mjx-container> and <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.375ex" xmlns="http://www.w3.org/2000/svg" width="2.81ex" height="1.377ex" role="img" focusable="false" viewBox="0 -443 1242.1 608.6" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-2-TEX-I-1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path><path id="MJX-2-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-2-TEX-N-35" d="M164 157Q164 133 148 117T109 101H102Q148 22 224 22Q294 22 326 82Q345 115 345 210Q345 313 318 349Q292 382 260 382H254Q176 382 136 314Q132 307 129 306T114 304Q97 304 95 310Q93 314 93 485V614Q93 664 98 664Q100 666 102 666Q103 666 123 658T178 642T253 634Q324 634 389 662Q397 666 402 666Q410 666 410 648V635Q328 538 205 538Q174 538 149 544L139 546V374Q158 388 169 396T205 412T256 420Q337 420 393 355T449 201Q449 109 385 44T229 -22Q148 -22 99 32T50 154Q50 178 61 192T84 210T107 214Q132 214 148 197T164 157Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D463"></use></g><g data-mml-node="TeXAtom" transform="translate(485, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-31"></use><use xlink:href="#MJX-2-TEX-N-35" transform="translate(500, 0)"></use></g></g></g></g></g></svg></mjx-container> are adjacent to each other: </p><div style="display: flex;justify-content: center;">  <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -1.769ex" xmlns="http://www.w3.org/2000/svg" width="9.943ex" height="4.593ex" role="img" focusable="false" viewBox="0 -1248 4394.6 2030" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-2-TEX-I-1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path><path id="MJX-2-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-2-TEX-S4-2192" d="M56 237T56 250T70 270H835Q719 357 692 493Q692 494 692 496T691 499Q691 511 708 511H711Q720 511 723 510T729 506T732 497T735 481T743 456Q765 389 816 336T935 261Q944 258 944 250Q944 244 939 241T915 231T877 212Q836 186 806 152T761 85T740 35T732 4Q730 -6 727 -8T711 -11Q691 -11 691 0Q691 7 696 25Q728 151 835 230H70Q56 237 56 250Z"></path><path id="MJX-2-TEX-S4-2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path><path id="MJX-2-TEX-N-6D" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q351 442 364 440T387 434T406 426T421 417T432 406T441 395T448 384T452 374T455 366L457 361L460 365Q463 369 466 373T475 384T488 397T503 410T523 422T546 432T572 439T603 442Q729 442 740 329Q741 322 741 190V104Q741 66 743 59T754 49Q775 46 803 46H819V0H811L788 1Q764 2 737 2T699 3Q596 3 587 0H579V46H595Q656 46 656 62Q657 64 657 200Q656 335 655 343Q649 371 635 385T611 402T585 404Q540 404 506 370Q479 343 472 315T464 232V168V108Q464 78 465 68T468 55T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z"></path><path id="MJX-2-TEX-N-2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path><path id="MJX-2-TEX-N-20" d=""></path><path id="MJX-2-TEX-N-63" d="M370 305T349 305T313 320T297 358Q297 381 312 396Q317 401 317 402T307 404Q281 408 258 408Q209 408 178 376Q131 329 131 219Q131 137 162 90Q203 29 272 29Q313 29 338 55T374 117Q376 125 379 127T395 129H409Q415 123 415 120Q415 116 411 104T395 71T366 33T318 2T249 -11Q163 -11 99 53T34 214Q34 318 99 383T250 448T370 421T404 357Q404 334 387 320Z"></path><path id="MJX-2-TEX-N-35" d="M164 157Q164 133 148 117T109 101H102Q148 22 224 22Q294 22 326 82Q345 115 345 210Q345 313 318 349Q292 382 260 382H254Q176 382 136 314Q132 307 129 306T114 304Q97 304 95 310Q93 314 93 485V614Q93 664 98 664Q100 666 102 666Q103 666 123 658T178 642T253 634Q324 634 389 662Q397 666 402 666Q410 666 410 648V635Q328 538 205 538Q174 538 149 544L139 546V374Q158 388 169 396T205 412T256 420Q337 420 393 355T449 201Q449 109 385 44T229 -22Q148 -22 99 32T50 154Q50 178 61 192T84 210T107 214Q132 214 148 197T164 157Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D463"></use></g><g data-mml-node="mn" transform="translate(485, -150) scale(0.707)"><use xlink:href="#MJX-2-TEX-N-31"></use></g></g><g data-mml-node="munderover" transform="translate(1166.3, 0)"><g data-mml-node="mo"><use xlink:href="#MJX-2-TEX-S4-2192" transform="translate(708.4, 0)"></use><svg width="808.4" height="865" x="0" y="-182" viewBox="202.1 -182 808.4 865"><use xlink:href="#MJX-2-TEX-S4-2212" transform="scale(1.559, 1)"></use></svg></g><g data-mml-node="mpadded" transform="translate(638.2, -682) scale(0.707)"><g transform="translate(278, -240)"></g></g><g data-mml-node="mpadded" transform="translate(0, 831.2) scale(0.707)"><g transform="translate(278, 150)"><g data-mml-node="mtext"><use xlink:href="#MJX-2-TEX-N-6D"></use><use xlink:href="#MJX-2-TEX-N-2C" transform="translate(833, 0)"></use><use xlink:href="#MJX-2-TEX-N-20" transform="translate(1111, 0)"></use><use xlink:href="#MJX-2-TEX-N-63" transform="translate(1361, 0)"></use></g></g></g></g><g data-mml-node="msub" transform="translate(3152.5, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D463"></use></g><g data-mml-node="TeXAtom" transform="translate(485, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-31"></use><use xlink:href="#MJX-2-TEX-N-35" transform="translate(500, 0)"></use></g></g></g></g></g></svg></mjx-container></div><p>Obviously, the procedure is reversible:</p><div style="display: flex;justify-content: center;">  <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -1.769ex" xmlns="http://www.w3.org/2000/svg" width="9.943ex" height="4.593ex" role="img" focusable="false" viewBox="0 -1248 4394.6 2030" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-2-TEX-I-1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path><path id="MJX-2-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-2-TEX-N-35" d="M164 157Q164 133 148 117T109 101H102Q148 22 224 22Q294 22 326 82Q345 115 345 210Q345 313 318 349Q292 382 260 382H254Q176 382 136 314Q132 307 129 306T114 304Q97 304 95 310Q93 314 93 485V614Q93 664 98 664Q100 666 102 666Q103 666 123 658T178 642T253 634Q324 634 389 662Q397 666 402 666Q410 666 410 648V635Q328 538 205 538Q174 538 149 544L139 546V374Q158 388 169 396T205 412T256 420Q337 420 393 355T449 201Q449 109 385 44T229 -22Q148 -22 99 32T50 154Q50 178 61 192T84 210T107 214Q132 214 148 197T164 157Z"></path><path id="MJX-2-TEX-S4-2192" d="M56 237T56 250T70 270H835Q719 357 692 493Q692 494 692 496T691 499Q691 511 708 511H711Q720 511 723 510T729 506T732 497T735 481T743 456Q765 389 816 336T935 261Q944 258 944 250Q944 244 939 241T915 231T877 212Q836 186 806 152T761 85T740 35T732 4Q730 -6 727 -8T711 -11Q691 -11 691 0Q691 7 696 25Q728 151 835 230H70Q56 237 56 250Z"></path><path id="MJX-2-TEX-S4-2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path><path id="MJX-2-TEX-N-6D" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q351 442 364 440T387 434T406 426T421 417T432 406T441 395T448 384T452 374T455 366L457 361L460 365Q463 369 466 373T475 384T488 397T503 410T523 422T546 432T572 439T603 442Q729 442 740 329Q741 322 741 190V104Q741 66 743 59T754 49Q775 46 803 46H819V0H811L788 1Q764 2 737 2T699 3Q596 3 587 0H579V46H595Q656 46 656 62Q657 64 657 200Q656 335 655 343Q649 371 635 385T611 402T585 404Q540 404 506 370Q479 343 472 315T464 232V168V108Q464 78 465 68T468 55T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z"></path><path id="MJX-2-TEX-N-2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path><path id="MJX-2-TEX-N-20" d=""></path><path id="MJX-2-TEX-N-63" d="M370 305T349 305T313 320T297 358Q297 381 312 396Q317 401 317 402T307 404Q281 408 258 408Q209 408 178 376Q131 329 131 219Q131 137 162 90Q203 29 272 29Q313 29 338 55T374 117Q376 125 379 127T395 129H409Q415 123 415 120Q415 116 411 104T395 71T366 33T318 2T249 -11Q163 -11 99 53T34 214Q34 318 99 383T250 448T370 421T404 357Q404 334 387 320Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D463"></use></g><g data-mml-node="TeXAtom" transform="translate(485, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-31"></use><use xlink:href="#MJX-2-TEX-N-35" transform="translate(500, 0)"></use></g></g></g><g data-mml-node="munderover" transform="translate(1519.9, 0)"><g data-mml-node="mo"><use xlink:href="#MJX-2-TEX-S4-2192" transform="translate(708.4, 0)"></use><svg width="808.4" height="865" x="0" y="-182" viewBox="202.1 -182 808.4 865"><use xlink:href="#MJX-2-TEX-S4-2212" transform="scale(1.559, 1)"></use></svg></g><g data-mml-node="mpadded" transform="translate(638.2, -682) scale(0.707)"><g transform="translate(278, -240)"></g></g><g data-mml-node="mpadded" transform="translate(0, 831.2) scale(0.707)"><g transform="translate(278, 150)"><g data-mml-node="mtext"><use xlink:href="#MJX-2-TEX-N-6D"></use><use xlink:href="#MJX-2-TEX-N-2C" transform="translate(833, 0)"></use><use xlink:href="#MJX-2-TEX-N-20" transform="translate(1111, 0)"></use><use xlink:href="#MJX-2-TEX-N-63" transform="translate(1361, 0)"></use></g></g></g></g><g data-mml-node="msub" transform="translate(3506, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D463"></use></g><g data-mml-node="TeXAtom" transform="translate(485, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-31"></use></g></g></g></g></g></svg></mjx-container></div><p>This means the two vertexs has symmetric relation, we then use undirected arrow to connect them. For example, let’s say we make missionaries and cannibals start from <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.339ex" xmlns="http://www.w3.org/2000/svg" width="2.01ex" height="1.342ex" role="img" focusable="false" viewBox="0 -443 888.6 593" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-2-TEX-I-1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path><path id="MJX-2-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D463"></use></g><g data-mml-node="mn" transform="translate(485, -150) scale(0.707)"><use xlink:href="#MJX-2-TEX-N-31"></use></g></g></g></g></svg></mjx-container>, then there will be three possible conditions:</p><div style="display: flex;justify-content: center;">    <span>        <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -7.238ex" xmlns="http://www.w3.org/2000/svg" width="9.943ex" height="15.608ex" role="img" focusable="false" viewBox="0 -3699.4 4394.6 6898.7" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-2-TEX-I-1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path><path id="MJX-2-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-2-TEX-S4-2192" d="M56 237T56 250T70 270H835Q719 357 692 493Q692 494 692 496T691 499Q691 511 708 511H711Q720 511 723 510T729 506T732 497T735 481T743 456Q765 389 816 336T935 261Q944 258 944 250Q944 244 939 241T915 231T877 212Q836 186 806 152T761 85T740 35T732 4Q730 -6 727 -8T711 -11Q691 -11 691 0Q691 7 696 25Q728 151 835 230H70Q56 237 56 250Z"></path><path id="MJX-2-TEX-S4-2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path><path id="MJX-2-TEX-N-6D" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q351 442 364 440T387 434T406 426T421 417T432 406T441 395T448 384T452 374T455 366L457 361L460 365Q463 369 466 373T475 384T488 397T503 410T523 422T546 432T572 439T603 442Q729 442 740 329Q741 322 741 190V104Q741 66 743 59T754 49Q775 46 803 46H819V0H811L788 1Q764 2 737 2T699 3Q596 3 587 0H579V46H595Q656 46 656 62Q657 64 657 200Q656 335 655 343Q649 371 635 385T611 402T585 404Q540 404 506 370Q479 343 472 315T464 232V168V108Q464 78 465 68T468 55T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z"></path><path id="MJX-2-TEX-N-2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path><path id="MJX-2-TEX-N-20" d=""></path><path id="MJX-2-TEX-N-63" d="M370 305T349 305T313 320T297 358Q297 381 312 396Q317 401 317 402T307 404Q281 408 258 408Q209 408 178 376Q131 329 131 219Q131 137 162 90Q203 29 272 29Q313 29 338 55T374 117Q376 125 379 127T395 129H409Q415 123 415 120Q415 116 411 104T395 71T366 33T318 2T249 -11Q163 -11 99 53T34 214Q34 318 99 383T250 448T370 421T404 357Q404 334 387 320Z"></path><path id="MJX-2-TEX-N-35" d="M164 157Q164 133 148 117T109 101H102Q148 22 224 22Q294 22 326 82Q345 115 345 210Q345 313 318 349Q292 382 260 382H254Q176 382 136 314Q132 307 129 306T114 304Q97 304 95 310Q93 314 93 485V614Q93 664 98 664Q100 666 102 666Q103 666 123 658T178 642T253 634Q324 634 389 662Q397 666 402 666Q410 666 410 648V635Q328 538 205 538Q174 538 149 544L139 546V374Q158 388 169 396T205 412T256 420Q337 420 393 355T449 201Q449 109 385 44T229 -22Q148 -22 99 32T50 154Q50 178 61 192T84 210T107 214Q132 214 148 197T164 157Z"></path><path id="MJX-2-TEX-N-37" d="M55 458Q56 460 72 567L88 674Q88 676 108 676H128V672Q128 662 143 655T195 646T364 644H485V605L417 512Q408 500 387 472T360 435T339 403T319 367T305 330T292 284T284 230T278 162T275 80Q275 66 275 52T274 28V19Q270 2 255 -10T221 -22Q210 -22 200 -19T179 0T168 40Q168 198 265 368Q285 400 349 489L395 552H302Q128 552 119 546Q113 543 108 522T98 479L95 458V455H55V458Z"></path><path id="MJX-2-TEX-N-2192" d="M56 237T56 250T70 270H835Q719 357 692 493Q692 494 692 496T691 499Q691 511 708 511H711Q720 511 723 510T729 506T732 497T735 481T743 456Q765 389 816 336T935 261Q944 258 944 250Q944 244 939 241T915 231T877 212Q836 186 806 152T761 85T740 35T732 4Q730 -6 727 -8T711 -11Q691 -11 691 0Q691 7 696 25Q728 151 835 230H70Q56 237 56 250Z"></path><path id="MJX-2-TEX-N-38" d="M70 417T70 494T124 618T248 666Q319 666 374 624T429 515Q429 485 418 459T392 417T361 389T335 371T324 363L338 354Q352 344 366 334T382 323Q457 264 457 174Q457 95 399 37T249 -22Q159 -22 101 29T43 155Q43 263 172 335L154 348Q133 361 127 368Q70 417 70 494ZM286 386L292 390Q298 394 301 396T311 403T323 413T334 425T345 438T355 454T364 471T369 491T371 513Q371 556 342 586T275 624Q268 625 242 625Q201 625 165 599T128 534Q128 511 141 492T167 463T217 431Q224 426 228 424L286 386ZM250 21Q308 21 350 55T392 137Q392 154 387 169T375 194T353 216T330 234T301 253T274 270Q260 279 244 289T218 306L210 311Q204 311 181 294T133 239T107 157Q107 98 150 60T250 21Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mtable"><g data-mml-node="mtr" transform="translate(0, 2451.4)"><g data-mml-node="mtd"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D463"></use></g><g data-mml-node="TeXAtom" transform="translate(485, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-31"></use></g></g></g><g data-mml-node="munderover" transform="translate(1166.3, 0)"><g data-mml-node="mo"><use xlink:href="#MJX-2-TEX-S4-2192" transform="translate(708.4, 0)"></use><svg width="808.4" height="865" x="0" y="-182" viewBox="202.1 -182 808.4 865"><use xlink:href="#MJX-2-TEX-S4-2212" transform="scale(1.559, 1)"></use></svg></g><g data-mml-node="mpadded" transform="translate(638.2, -682) scale(0.707)"><g transform="translate(278, -240)"></g></g><g data-mml-node="mpadded" transform="translate(0, 831.2) scale(0.707)"><g transform="translate(278, 150)"><g data-mml-node="mtext"><use xlink:href="#MJX-2-TEX-N-6D"></use><use xlink:href="#MJX-2-TEX-N-2C" transform="translate(833, 0)"></use><use xlink:href="#MJX-2-TEX-N-20" transform="translate(1111, 0)"></use><use xlink:href="#MJX-2-TEX-N-63" transform="translate(1361, 0)"></use></g></g></g></g><g data-mml-node="msub" transform="translate(3152.5, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D463"></use></g><g data-mml-node="TeXAtom" transform="translate(485, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-31"></use><use xlink:href="#MJX-2-TEX-N-35" transform="translate(500, 0)"></use></g></g></g></g></g><g data-mml-node="mtr" transform="translate(0, -78.6)"><g data-mml-node="mtd" transform="translate(137.5, 0)"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D463"></use></g><g data-mml-node="TeXAtom" transform="translate(485, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-31"></use></g></g></g><g data-mml-node="munderover" transform="translate(1166.3, 0)"><g data-mml-node="mo"><use xlink:href="#MJX-2-TEX-S4-2192" transform="translate(433.3, 0)"></use><svg width="533.3" height="865" x="0" y="-182" viewBox="133.3 -182 533.3 865"><use xlink:href="#MJX-2-TEX-S4-2212" transform="scale(1.028, 1)"></use></svg></g><g data-mml-node="mpadded" transform="translate(500.6, -682) scale(0.707)"><g transform="translate(278, -240)"></g></g><g data-mml-node="mpadded" transform="translate(0, 831.2) scale(0.707)"><g transform="translate(278, 150)"><g data-mml-node="mtext"><use xlink:href="#MJX-2-TEX-N-63"></use><use xlink:href="#MJX-2-TEX-N-2C" transform="translate(444, 0)"></use><use xlink:href="#MJX-2-TEX-N-20" transform="translate(722, 0)"></use><use xlink:href="#MJX-2-TEX-N-63" transform="translate(972, 0)"></use></g></g></g></g><g data-mml-node="msub" transform="translate(2877.4, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D463"></use></g><g data-mml-node="TeXAtom" transform="translate(485, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-31"></use><use xlink:href="#MJX-2-TEX-N-37" transform="translate(500, 0)"></use></g></g></g></g></g><g data-mml-node="mtr" transform="translate(0, -2488.4)"><g data-mml-node="mtd" transform="translate(354.2, 0)"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D463"></use></g><g data-mml-node="TeXAtom" transform="translate(485, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-31"></use></g></g></g><g data-mml-node="munderover" transform="translate(1166.3, 0)"><g data-mml-node="mo"><use xlink:href="#MJX-2-TEX-N-2192"></use></g><g data-mml-node="mpadded" transform="translate(284, -611) scale(0.707)"><g transform="translate(278, -240)"></g></g><g data-mml-node="mpadded" transform="translate(127, 711) scale(0.707)"><g transform="translate(278, 150)"><g data-mml-node="mtext"><use xlink:href="#MJX-2-TEX-N-63"></use></g></g></g></g><g data-mml-node="msub" transform="translate(2444.1, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D463"></use></g><g data-mml-node="TeXAtom" transform="translate(485, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-31"></use><use xlink:href="#MJX-2-TEX-N-38" transform="translate(500, 0)"></use></g></g></g></g></g></g></g></g></svg></mjx-container>    </span></div><p>By repeating the above algorithm, we can get an undirected graph.</p><img src="/Hexo-Blog/2021/01/27/2021-01-27-missionaries-and-cannibals-problem/undirected-graph-of-river-crossing-problem1.jpg" class=""><p>In order to compute this in matrix, I define a <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex" xmlns="http://www.w3.org/2000/svg" width="7.291ex" height="1.557ex" role="img" focusable="false" viewBox="0 -666 3222.4 688" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-2-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-2-TEX-N-38" d="M70 417T70 494T124 618T248 666Q319 666 374 624T429 515Q429 485 418 459T392 417T361 389T335 371T324 363L338 354Q352 344 366 334T382 323Q457 264 457 174Q457 95 399 37T249 -22Q159 -22 101 29T43 155Q43 263 172 335L154 348Q133 361 127 368Q70 417 70 494ZM286 386L292 390Q298 394 301 396T311 403T323 413T334 425T345 438T355 454T364 471T369 491T371 513Q371 556 342 586T275 624Q268 625 242 625Q201 625 165 599T128 534Q128 511 141 492T167 463T217 431Q224 426 228 424L286 386ZM250 21Q308 21 350 55T392 137Q392 154 387 169T375 194T353 216T330 234T301 253T274 270Q260 279 244 289T218 306L210 311Q204 311 181 294T133 239T107 157Q107 98 150 60T250 21Z"></path><path id="MJX-2-TEX-N-D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-31"></use><use xlink:href="#MJX-2-TEX-N-38" transform="translate(500, 0)"></use></g><g data-mml-node="mo" transform="translate(1222.2, 0)"><use xlink:href="#MJX-2-TEX-N-D7"></use></g><g data-mml-node="mn" transform="translate(2222.4, 0)"><use xlink:href="#MJX-2-TEX-N-31"></use><use xlink:href="#MJX-2-TEX-N-38" transform="translate(500, 0)"></use></g></g></g></svg></mjx-container> adjacency matrix <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.666ex" xmlns="http://www.w3.org/2000/svg" width="8.493ex" height="2.363ex" role="img" focusable="false" viewBox="0 -750 3753.8 1044.2" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-2-TEX-I-1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"></path><path id="MJX-2-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-2-TEX-N-5B" d="M118 -250V750H255V710H158V-210H255V-250H118Z"></path><path id="MJX-2-TEX-I-1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path><path id="MJX-2-TEX-I-1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path><path id="MJX-2-TEX-I-1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path><path id="MJX-2-TEX-N-5D" d="M22 710V750H159V-250H22V-210H119V710H22Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D434"></use></g><g data-mml-node="mo" transform="translate(1027.8, 0)"><use xlink:href="#MJX-2-TEX-N-3D"></use></g><g data-mml-node="mo" transform="translate(2083.6, 0)"><use xlink:href="#MJX-2-TEX-N-5B"></use></g><g data-mml-node="msub" transform="translate(2361.6, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D44E"></use></g><g data-mml-node="TeXAtom" transform="translate(529, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D456"></use></g><g data-mml-node="mi" transform="translate(345, 0)"><use xlink:href="#MJX-2-TEX-I-1D457"></use></g></g></g><g data-mml-node="mo" transform="translate(3475.8, 0)"><use xlink:href="#MJX-2-TEX-N-5D"></use></g></g></g></svg></mjx-container> to represent the undirected graph: if <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex" xmlns="http://www.w3.org/2000/svg" width="1.762ex" height="1.359ex" role="img" focusable="false" viewBox="0 -443 779 600.8" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-2-TEX-I-1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path><path id="MJX-2-TEX-I-1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D463"></use></g><g data-mml-node="mi" transform="translate(485, -150) scale(0.707)"><use xlink:href="#MJX-2-TEX-I-1D456"></use></g></g></g></g></svg></mjx-container> and <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.666ex" xmlns="http://www.w3.org/2000/svg" width="1.87ex" height="1.668ex" role="img" focusable="false" viewBox="0 -443 826.3 737.2" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-2-TEX-I-1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path><path id="MJX-2-TEX-I-1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D463"></use></g><g data-mml-node="mi" transform="translate(485, -150) scale(0.707)"><use xlink:href="#MJX-2-TEX-I-1D457"></use></g></g></g></g></svg></mjx-container> are adjacent to each other, then 1, otherwise 0. Moreover, for every <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="0.781ex" height="1.52ex" role="img" focusable="false" viewBox="0 -661 345 672" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-2-TEX-I-1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D456"></use></g></g></g></svg></mjx-container>, we let the value be 0, that is, <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex" xmlns="http://www.w3.org/2000/svg" width="6.562ex" height="1.864ex" role="img" focusable="false" viewBox="0 -666 2900.5 823.8" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-2-TEX-I-1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path><path id="MJX-2-TEX-I-1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path><path id="MJX-2-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-2-TEX-N-30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D44E"></use></g><g data-mml-node="TeXAtom" transform="translate(529, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D456"></use></g><g data-mml-node="mi" transform="translate(345, 0)"><use xlink:href="#MJX-2-TEX-I-1D456"></use></g></g></g><g data-mml-node="mo" transform="translate(1344.7, 0)"><use xlink:href="#MJX-2-TEX-N-3D"></use></g><g data-mml-node="mn" transform="translate(2400.5, 0)"><use xlink:href="#MJX-2-TEX-N-30"></use></g></g></g></svg></mjx-container>.</p><div style="display: flex;justify-content: center;">  <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -2.149ex" xmlns="http://www.w3.org/2000/svg" width="18.715ex" height="5.43ex" role="img" focusable="false" viewBox="0 -1450 8272 2400" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-2-TEX-I-1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"></path><path id="MJX-2-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-2-TEX-S3-5B" d="M247 -949V1450H516V1388H309V-887H516V-949H247Z"></path><path id="MJX-2-TEX-N-30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path><path id="MJX-2-TEX-N-36" d="M42 313Q42 476 123 571T303 666Q372 666 402 630T432 550Q432 525 418 510T379 495Q356 495 341 509T326 548Q326 592 373 601Q351 623 311 626Q240 626 194 566Q147 500 147 364L148 360Q153 366 156 373Q197 433 263 433H267Q313 433 348 414Q372 400 396 374T435 317Q456 268 456 210V192Q456 169 451 149Q440 90 387 34T253 -22Q225 -22 199 -14T143 16T92 75T56 172T42 313ZM257 397Q227 397 205 380T171 335T154 278T148 216Q148 133 160 97T198 39Q222 21 251 21Q302 21 329 59Q342 77 347 104T352 209Q352 289 347 316T329 361Q302 397 257 397Z"></path><path id="MJX-2-TEX-N-D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path><path id="MJX-2-TEX-I-1D435" d="M231 637Q204 637 199 638T194 649Q194 676 205 682Q206 683 335 683Q594 683 608 681Q671 671 713 636T756 544Q756 480 698 429T565 360L555 357Q619 348 660 311T702 219Q702 146 630 78T453 1Q446 0 242 0Q42 0 39 2Q35 5 35 10Q35 17 37 24Q42 43 47 45Q51 46 62 46H68Q95 46 128 49Q142 52 147 61Q150 65 219 339T288 628Q288 635 231 637ZM649 544Q649 574 634 600T585 634Q578 636 493 637Q473 637 451 637T416 636H403Q388 635 384 626Q382 622 352 506Q352 503 351 500L320 374H401Q482 374 494 376Q554 386 601 434T649 544ZM595 229Q595 273 572 302T512 336Q506 337 429 337Q311 337 310 336Q310 334 293 263T258 122L240 52Q240 48 252 48T333 46Q422 46 429 47Q491 54 543 105T595 229Z"></path><path id="MJX-2-TEX-N-39" d="M352 287Q304 211 232 211Q154 211 104 270T44 396Q42 412 42 436V444Q42 537 111 606Q171 666 243 666Q245 666 249 666T257 665H261Q273 665 286 663T323 651T370 619T413 560Q456 472 456 334Q456 194 396 97Q361 41 312 10T208 -22Q147 -22 108 7T68 93T121 149Q143 149 158 135T173 96Q173 78 164 65T148 49T135 44L131 43Q131 41 138 37T164 27T206 22H212Q272 22 313 86Q352 142 352 280V287ZM244 248Q292 248 321 297T351 430Q351 508 343 542Q341 552 337 562T323 588T293 615T246 625Q208 625 181 598Q160 576 154 546T147 441Q147 358 152 329T172 282Q197 248 244 248Z"></path><path id="MJX-2-TEX-S3-5D" d="M11 1388V1450H280V-949H11V-887H218V1388H11Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D434"></use></g><g data-mml-node="mo" transform="translate(1027.8, 0)"><use xlink:href="#MJX-2-TEX-N-3D"></use></g><g data-mml-node="mrow" transform="translate(2083.6, 0)"><g data-mml-node="mo"><use xlink:href="#MJX-2-TEX-S3-5B"></use></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(528, 0)"><g data-mml-node="mtable"><g data-mml-node="mtr" transform="translate(0, 700)"><g data-mml-node="mtd" transform="translate(129.5, 0)"><g data-mml-node="msub"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-30"></use></g><g data-mml-node="TeXAtom" transform="translate(500, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-36"></use></g><g data-mml-node="mo" transform="translate(500, 0)"><use xlink:href="#MJX-2-TEX-N-D7"></use></g><g data-mml-node="mn" transform="translate(1278, 0)"><use xlink:href="#MJX-2-TEX-N-36"></use></g></g></g></g><g data-mml-node="mtd" transform="translate(3066.2, 0)"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D435"></use></g><g data-mml-node="TeXAtom" transform="translate(759, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-39"></use></g><g data-mml-node="mo" transform="translate(500, 0)"><use xlink:href="#MJX-2-TEX-N-D7"></use></g><g data-mml-node="mn" transform="translate(1278, 0)"><use xlink:href="#MJX-2-TEX-N-39"></use></g></g></g></g></g><g data-mml-node="mtr" transform="translate(0, -700)"><g data-mml-node="mtd"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D435"></use></g><g data-mml-node="TeXAtom" transform="translate(759, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-39"></use></g><g data-mml-node="mo" transform="translate(500, 0)"><use xlink:href="#MJX-2-TEX-N-D7"></use></g><g data-mml-node="mn" transform="translate(1278, 0)"><use xlink:href="#MJX-2-TEX-N-39"></use></g></g></g></g><g data-mml-node="mtd" transform="translate(3195.7, 0)"><g data-mml-node="msub"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-30"></use></g><g data-mml-node="TeXAtom" transform="translate(500, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-36"></use></g><g data-mml-node="mo" transform="translate(500, 0)"><use xlink:href="#MJX-2-TEX-N-D7"></use></g><g data-mml-node="mn" transform="translate(1278, 0)"><use xlink:href="#MJX-2-TEX-N-36"></use></g></g></g></g></g></g></g><g data-mml-node="mo" transform="translate(5660.5, 0)"><use xlink:href="#MJX-2-TEX-S3-5D"></use></g></g></g></g></svg></mjx-container></div><p>where <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.666ex" xmlns="http://www.w3.org/2000/svg" width="8.287ex" height="2.363ex" role="img" focusable="false" viewBox="0 -750 3662.8 1044.2" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-2-TEX-I-1D435" d="M231 637Q204 637 199 638T194 649Q194 676 205 682Q206 683 335 683Q594 683 608 681Q671 671 713 636T756 544Q756 480 698 429T565 360L555 357Q619 348 660 311T702 219Q702 146 630 78T453 1Q446 0 242 0Q42 0 39 2Q35 5 35 10Q35 17 37 24Q42 43 47 45Q51 46 62 46H68Q95 46 128 49Q142 52 147 61Q150 65 219 339T288 628Q288 635 231 637ZM649 544Q649 574 634 600T585 634Q578 636 493 637Q473 637 451 637T416 636H403Q388 635 384 626Q382 622 352 506Q352 503 351 500L320 374H401Q482 374 494 376Q554 386 601 434T649 544ZM595 229Q595 273 572 302T512 336Q506 337 429 337Q311 337 310 336Q310 334 293 263T258 122L240 52Q240 48 252 48T333 46Q422 46 429 47Q491 54 543 105T595 229Z"></path><path id="MJX-2-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-2-TEX-N-5B" d="M118 -250V750H255V710H158V-210H255V-250H118Z"></path><path id="MJX-2-TEX-I-1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"></path><path id="MJX-2-TEX-I-1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path><path id="MJX-2-TEX-I-1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path><path id="MJX-2-TEX-N-5D" d="M22 710V750H159V-250H22V-210H119V710H22Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D435"></use></g><g data-mml-node="mo" transform="translate(1036.8, 0)"><use xlink:href="#MJX-2-TEX-N-3D"></use></g><g data-mml-node="mo" transform="translate(2092.6, 0)"><use xlink:href="#MJX-2-TEX-N-5B"></use></g><g data-mml-node="msub" transform="translate(2370.6, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D44F"></use></g><g data-mml-node="TeXAtom" transform="translate(429, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D456"></use></g><g data-mml-node="mi" transform="translate(345, 0)"><use xlink:href="#MJX-2-TEX-I-1D457"></use></g></g></g><g data-mml-node="mo" transform="translate(3384.8, 0)"><use xlink:href="#MJX-2-TEX-N-5D"></use></g></g></g></svg></mjx-container> is a <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex" xmlns="http://www.w3.org/2000/svg" width="5.028ex" height="1.557ex" role="img" focusable="false" viewBox="0 -666 2222.4 688" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-2-TEX-N-39" d="M352 287Q304 211 232 211Q154 211 104 270T44 396Q42 412 42 436V444Q42 537 111 606Q171 666 243 666Q245 666 249 666T257 665H261Q273 665 286 663T323 651T370 619T413 560Q456 472 456 334Q456 194 396 97Q361 41 312 10T208 -22Q147 -22 108 7T68 93T121 149Q143 149 158 135T173 96Q173 78 164 65T148 49T135 44L131 43Q131 41 138 37T164 27T206 22H212Q272 22 313 86Q352 142 352 280V287ZM244 248Q292 248 321 297T351 430Q351 508 343 542Q341 552 337 562T323 588T293 615T246 625Q208 625 181 598Q160 576 154 546T147 441Q147 358 152 329T172 282Q197 248 244 248Z"></path><path id="MJX-2-TEX-N-D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-39"></use></g><g data-mml-node="mo" transform="translate(722.2, 0)"><use xlink:href="#MJX-2-TEX-N-D7"></use></g><g data-mml-node="mn" transform="translate(1722.4, 0)"><use xlink:href="#MJX-2-TEX-N-39"></use></g></g></g></svg></mjx-container> symmetric matrix.</p><div style="display: flex;justify-content: center;">  <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -13.235ex" xmlns="http://www.w3.org/2000/svg" width="36.033ex" height="27.602ex" role="img" focusable="false" viewBox="0 -6350 15926.6 12200" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-2-TEX-I-1D435" d="M231 637Q204 637 199 638T194 649Q194 676 205 682Q206 683 335 683Q594 683 608 681Q671 671 713 636T756 544Q756 480 698 429T565 360L555 357Q619 348 660 311T702 219Q702 146 630 78T453 1Q446 0 242 0Q42 0 39 2Q35 5 35 10Q35 17 37 24Q42 43 47 45Q51 46 62 46H68Q95 46 128 49Q142 52 147 61Q150 65 219 339T288 628Q288 635 231 637ZM649 544Q649 574 634 600T585 634Q578 636 493 637Q473 637 451 637T416 636H403Q388 635 384 626Q382 622 352 506Q352 503 351 500L320 374H401Q482 374 494 376Q554 386 601 434T649 544ZM595 229Q595 273 572 302T512 336Q506 337 429 337Q311 337 310 336Q310 334 293 263T258 122L240 52Q240 48 252 48T333 46Q422 46 429 47Q491 54 543 105T595 229Z"></path><path id="MJX-2-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-2-TEX-S4-23A1" d="M319 -645V1154H666V1070H403V-645H319Z"></path><path id="MJX-2-TEX-S4-23A3" d="M319 -644V1155H403V-560H666V-644H319Z"></path><path id="MJX-2-TEX-S4-23A2" d="M319 0V602H403V0H319Z"></path><path id="MJX-2-TEX-N-30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path><path id="MJX-2-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-2-TEX-S4-23A4" d="M0 1070V1154H347V-645H263V1070H0Z"></path><path id="MJX-2-TEX-S4-23A6" d="M263 -560V1155H347V-644H0V-560H263Z"></path><path id="MJX-2-TEX-S4-23A5" d="M263 0V602H347V0H263Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D435"></use></g><g data-mml-node="mo" transform="translate(1036.8, 0)"><use xlink:href="#MJX-2-TEX-N-3D"></use></g><g data-mml-node="mrow" transform="translate(2092.6, 0)"><g data-mml-node="mo"><use xlink:href="#MJX-2-TEX-S4-23A1" transform="translate(0, 5196)"></use><use xlink:href="#MJX-2-TEX-S4-23A3" transform="translate(0, -5206)"></use><svg width="667" height="8802" y="-4151" x="0" viewBox="0 2200.5 667 8802"><use xlink:href="#MJX-2-TEX-S4-23A2" transform="scale(1, 21.932)"></use></svg></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(667, 0)"><g data-mml-node="mtable"><g data-mml-node="mtr" transform="translate(0, 5600)"><g data-mml-node="mtd"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(1500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(3000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(4500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(6000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(7500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-31"></use></g></g><g data-mml-node="mtd" transform="translate(9000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(10500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-31"></use></g></g><g data-mml-node="mtd" transform="translate(12000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-31"></use></g></g></g><g data-mml-node="mtr" transform="translate(0, 4200)"><g data-mml-node="mtd"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(1500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(3000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(4500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(6000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(7500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-31"></use></g></g><g data-mml-node="mtd" transform="translate(9000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-31"></use></g></g><g data-mml-node="mtd" transform="translate(10500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-31"></use></g></g><g data-mml-node="mtd" transform="translate(12000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-30"></use></g></g></g><g data-mml-node="mtr" transform="translate(0, 2800)"><g data-mml-node="mtd"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(1500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(3000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(4500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(6000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-31"></use></g></g><g data-mml-node="mtd" transform="translate(7500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(9000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-31"></use></g></g><g data-mml-node="mtd" transform="translate(10500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(12000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-30"></use></g></g></g><g data-mml-node="mtr" transform="translate(0, 1400)"><g data-mml-node="mtd"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(1500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(3000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(4500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(6000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(7500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(9000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(10500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(12000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-30"></use></g></g></g><g data-mml-node="mtr" transform="translate(0, 0)"><g data-mml-node="mtd"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(1500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(3000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-31"></use></g></g><g data-mml-node="mtd" transform="translate(4500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(6000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-31"></use></g></g><g data-mml-node="mtd" transform="translate(7500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(9000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(10500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(12000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-30"></use></g></g></g><g data-mml-node="mtr" transform="translate(0, -1400)"><g data-mml-node="mtd"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-31"></use></g></g><g data-mml-node="mtd" transform="translate(1500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-31"></use></g></g><g data-mml-node="mtd" transform="translate(3000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(4500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(6000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(7500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(9000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(10500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(12000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-30"></use></g></g></g><g data-mml-node="mtr" transform="translate(0, -2800)"><g data-mml-node="mtd"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(1500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-31"></use></g></g><g data-mml-node="mtd" transform="translate(3000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-31"></use></g></g><g data-mml-node="mtd" transform="translate(4500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(6000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(7500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(9000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(10500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(12000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-30"></use></g></g></g><g data-mml-node="mtr" transform="translate(0, -4200)"><g data-mml-node="mtd"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-31"></use></g></g><g data-mml-node="mtd" transform="translate(1500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-31"></use></g></g><g data-mml-node="mtd" transform="translate(3000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(4500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(6000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(7500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(9000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(10500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(12000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-30"></use></g></g></g><g data-mml-node="mtr" transform="translate(0, -5600)"><g data-mml-node="mtd"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-31"></use></g></g><g data-mml-node="mtd" transform="translate(1500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(3000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(4500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(6000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(7500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(9000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(10500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(12000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-30"></use></g></g></g></g></g><g data-mml-node="mo" transform="translate(13167, 0)"><use xlink:href="#MJX-2-TEX-S4-23A4" transform="translate(0, 5196)"></use><use xlink:href="#MJX-2-TEX-S4-23A6" transform="translate(0, -5206)"></use><svg width="667" height="8802" y="-4151" x="0" viewBox="0 2200.5 667 8802"><use xlink:href="#MJX-2-TEX-S4-23A5" transform="scale(1, 21.932)"></use></svg></g></g></g></g></svg></mjx-container></div><p>From above, we can tell that <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.65ex" xmlns="http://www.w3.org/2000/svg" width="7.277ex" height="2.22ex" role="img" focusable="false" viewBox="0 -694 3216.2 981.2" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-2-TEX-I-1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"></path><path id="MJX-2-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-2-TEX-N-2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path><path id="MJX-2-TEX-N-36" d="M42 313Q42 476 123 571T303 666Q372 666 402 630T432 550Q432 525 418 510T379 495Q356 495 341 509T326 548Q326 592 373 601Q351 623 311 626Q240 626 194 566Q147 500 147 364L148 360Q153 366 156 373Q197 433 263 433H267Q313 433 348 414Q372 400 396 374T435 317Q456 268 456 210V192Q456 169 451 149Q440 90 387 34T253 -22Q225 -22 199 -14T143 16T92 75T56 172T42 313ZM257 397Q227 397 205 380T171 335T154 278T148 216Q148 133 160 97T198 39Q222 21 251 21Q302 21 329 59Q342 77 347 104T352 209Q352 289 347 316T329 361Q302 397 257 397Z"></path><path id="MJX-2-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D44F"></use></g><g data-mml-node="TeXAtom" transform="translate(429, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-31"></use></g><g data-mml-node="mo" transform="translate(500, 0)"><use xlink:href="#MJX-2-TEX-N-2C"></use></g><g data-mml-node="mn" transform="translate(778, 0)"><use xlink:href="#MJX-2-TEX-N-36"></use></g></g></g><g data-mml-node="mo" transform="translate(1660.5, 0)"><use xlink:href="#MJX-2-TEX-N-3D"></use></g><g data-mml-node="mn" transform="translate(2716.2, 0)"><use xlink:href="#MJX-2-TEX-N-31"></use></g></g></g></svg></mjx-container> illustrate <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.339ex" xmlns="http://www.w3.org/2000/svg" width="2.01ex" height="1.342ex" role="img" focusable="false" viewBox="0 -443 888.6 593" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-2-TEX-I-1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path><path id="MJX-2-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D463"></use></g><g data-mml-node="mn" transform="translate(485, -150) scale(0.707)"><use xlink:href="#MJX-2-TEX-N-31"></use></g></g></g></g></svg></mjx-container> and <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.375ex" xmlns="http://www.w3.org/2000/svg" width="2.81ex" height="1.377ex" role="img" focusable="false" viewBox="0 -443 1242.1 608.6" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-2-TEX-I-1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path><path id="MJX-2-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-2-TEX-N-35" d="M164 157Q164 133 148 117T109 101H102Q148 22 224 22Q294 22 326 82Q345 115 345 210Q345 313 318 349Q292 382 260 382H254Q176 382 136 314Q132 307 129 306T114 304Q97 304 95 310Q93 314 93 485V614Q93 664 98 664Q100 666 102 666Q103 666 123 658T178 642T253 634Q324 634 389 662Q397 666 402 666Q410 666 410 648V635Q328 538 205 538Q174 538 149 544L139 546V374Q158 388 169 396T205 412T256 420Q337 420 393 355T449 201Q449 109 385 44T229 -22Q148 -22 99 32T50 154Q50 178 61 192T84 210T107 214Q132 214 148 197T164 157Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D463"></use></g><g data-mml-node="TeXAtom" transform="translate(485, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-31"></use><use xlink:href="#MJX-2-TEX-N-35" transform="translate(500, 0)"></use></g></g></g></g></g></svg></mjx-container> are adjacent to each other. We define the <code>journey</code> starts from <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex" xmlns="http://www.w3.org/2000/svg" width="1.762ex" height="1.359ex" role="img" focusable="false" viewBox="0 -443 779 600.8" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-2-TEX-I-1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path><path id="MJX-2-TEX-I-1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D463"></use></g><g data-mml-node="mi" transform="translate(485, -150) scale(0.707)"><use xlink:href="#MJX-2-TEX-I-1D456"></use></g></g></g></g></svg></mjx-container> to <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.666ex" xmlns="http://www.w3.org/2000/svg" width="1.87ex" height="1.668ex" role="img" focusable="false" viewBox="0 -443 826.3 737.2" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-2-TEX-I-1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path><path id="MJX-2-TEX-I-1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D463"></use></g><g data-mml-node="mi" transform="translate(485, -150) scale(0.707)"><use xlink:href="#MJX-2-TEX-I-1D457"></use></g></g></g></g></svg></mjx-container> is a series of paths:</p><div style="display: flex;justify-content: center;">  <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.666ex" xmlns="http://www.w3.org/2000/svg" width="29.531ex" height="1.985ex" role="img" focusable="false" viewBox="0 -583 13052.6 877.2" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-2-TEX-I-1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path><path id="MJX-2-TEX-I-1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path><path id="MJX-2-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-2-TEX-I-1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-2-TEX-N-30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path><path id="MJX-2-TEX-N-2192" d="M56 237T56 250T70 270H835Q719 357 692 493Q692 494 692 496T691 499Q691 511 708 511H711Q720 511 723 510T729 506T732 497T735 481T743 456Q765 389 816 336T935 261Q944 258 944 250Q944 244 939 241T915 231T877 212Q836 186 806 152T761 85T740 35T732 4Q730 -6 727 -8T711 -11Q691 -11 691 0Q691 7 696 25Q728 151 835 230H70Q56 237 56 250Z"></path><path id="MJX-2-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-2-TEX-N-2026" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60ZM525 60Q525 84 542 102T585 120Q609 120 627 104T646 61Q646 36 629 18T586 0T543 17T525 60ZM972 60Q972 84 989 102T1032 120Q1056 120 1074 104T1093 61Q1093 36 1076 18T1033 0T990 17T972 60Z"></path><path id="MJX-2-TEX-I-1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path><path id="MJX-2-TEX-I-1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D463"></use></g><g data-mml-node="mi" transform="translate(485, -150) scale(0.707)"><use xlink:href="#MJX-2-TEX-I-1D456"></use></g></g><g data-mml-node="mo" transform="translate(1056.7, 0)"><use xlink:href="#MJX-2-TEX-N-3D"></use></g><g data-mml-node="msub" transform="translate(2112.5, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D462"></use></g><g data-mml-node="mn" transform="translate(572, -150) scale(0.707)"><use xlink:href="#MJX-2-TEX-N-30"></use></g></g><g data-mml-node="mo" transform="translate(3365.8, 0)"><use xlink:href="#MJX-2-TEX-N-2192"></use></g><g data-mml-node="msub" transform="translate(4643.6, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D462"></use></g><g data-mml-node="mn" transform="translate(572, -150) scale(0.707)"><use xlink:href="#MJX-2-TEX-N-31"></use></g></g><g data-mml-node="mo" transform="translate(5896.9, 0)"><use xlink:href="#MJX-2-TEX-N-2192"></use></g><g data-mml-node="mo" transform="translate(7174.7, 0)"><use xlink:href="#MJX-2-TEX-N-2026"></use></g><g data-mml-node="mo" transform="translate(8624.5, 0)"><use xlink:href="#MJX-2-TEX-N-2192"></use></g><g data-mml-node="msub" transform="translate(9902.3, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D462"></use></g><g data-mml-node="mi" transform="translate(572, -150) scale(0.707)"><use xlink:href="#MJX-2-TEX-I-1D458"></use></g></g><g data-mml-node="mo" transform="translate(11170.5, 0)"><use xlink:href="#MJX-2-TEX-N-3D"></use></g><g data-mml-node="msub" transform="translate(12226.2, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D463"></use></g><g data-mml-node="mi" transform="translate(485, -150) scale(0.707)"><use xlink:href="#MJX-2-TEX-I-1D457"></use></g></g></g></g></svg></mjx-container></div><p>If we do not pass through the vertex of repeat visits, which means that <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex" xmlns="http://www.w3.org/2000/svg" width="2.241ex" height="1.357ex" role="img" focusable="false" viewBox="0 -442 990.4 599.8" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-2-TEX-I-1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-2-TEX-I-1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D462"></use></g><g data-mml-node="mi" transform="translate(572, -150) scale(0.707)"><use xlink:href="#MJX-2-TEX-I-1D458"></use></g></g></g></g></svg></mjx-container> are different from each other, then we call it is a path of length <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="1.179ex" height="1.595ex" role="img" focusable="false" viewBox="0 -694 521 705" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-2-TEX-I-1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D458"></use></g></g></g></svg></mjx-container>. Our goal is to start at <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.339ex" xmlns="http://www.w3.org/2000/svg" width="2.01ex" height="1.342ex" role="img" focusable="false" viewBox="0 -443 888.6 593" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-2-TEX-I-1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path><path id="MJX-2-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D463"></use></g><g data-mml-node="mn" transform="translate(485, -150) scale(0.707)"><use xlink:href="#MJX-2-TEX-N-31"></use></g></g></g></g></svg></mjx-container> and end at <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.375ex" xmlns="http://www.w3.org/2000/svg" width="2.81ex" height="1.377ex" role="img" focusable="false" viewBox="0 -443 1242.1 608.6" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-2-TEX-I-1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path><path id="MJX-2-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-2-TEX-N-30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D463"></use></g><g data-mml-node="TeXAtom" transform="translate(485, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-31"></use><use xlink:href="#MJX-2-TEX-N-30" transform="translate(500, 0)"></use></g></g></g></g></g></svg></mjx-container>, because <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.339ex" xmlns="http://www.w3.org/2000/svg" width="2.01ex" height="1.342ex" role="img" focusable="false" viewBox="0 -443 888.6 593" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-2-TEX-I-1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path><path id="MJX-2-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D463"></use></g><g data-mml-node="mn" transform="translate(485, -150) scale(0.707)"><use xlink:href="#MJX-2-TEX-N-31"></use></g></g></g></g></svg></mjx-container> part of the east side, and <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.375ex" xmlns="http://www.w3.org/2000/svg" width="2.81ex" height="1.377ex" role="img" focusable="false" viewBox="0 -443 1242.1 608.6" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-2-TEX-I-1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path><path id="MJX-2-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-2-TEX-N-30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D463"></use></g><g data-mml-node="TeXAtom" transform="translate(485, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-31"></use><use xlink:href="#MJX-2-TEX-N-30" transform="translate(500, 0)"></use></g></g></g></g></g></svg></mjx-container> belongs to the west side, so the length of the path <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="1.179ex" height="1.595ex" role="img" focusable="false" viewBox="0 -694 521 705" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-2-TEX-I-1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D458"></use></g></g></g></svg></mjx-container> must be an odd number. For odd numbers <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="1.179ex" height="1.595ex" role="img" focusable="false" viewBox="0 -694 521 705" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-2-TEX-I-1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D458"></use></g></g></g></svg></mjx-container>, it is not difficult to confirm:</p><div style="display: flex;justify-content: center;">  <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -2.827ex" xmlns="http://www.w3.org/2000/svg" width="19.91ex" height="6.785ex" role="img" focusable="false" viewBox="0 -1749.5 8800.4 2999" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-2-TEX-I-1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"></path><path id="MJX-2-TEX-I-1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path><path id="MJX-2-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-2-TEX-S4-5B" d="M269 -1249V1750H577V1677H342V-1176H577V-1249H269Z"></path><path id="MJX-2-TEX-N-30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path><path id="MJX-2-TEX-N-36" d="M42 313Q42 476 123 571T303 666Q372 666 402 630T432 550Q432 525 418 510T379 495Q356 495 341 509T326 548Q326 592 373 601Q351 623 311 626Q240 626 194 566Q147 500 147 364L148 360Q153 366 156 373Q197 433 263 433H267Q313 433 348 414Q372 400 396 374T435 317Q456 268 456 210V192Q456 169 451 149Q440 90 387 34T253 -22Q225 -22 199 -14T143 16T92 75T56 172T42 313ZM257 397Q227 397 205 380T171 335T154 278T148 216Q148 133 160 97T198 39Q222 21 251 21Q302 21 329 59Q342 77 347 104T352 209Q352 289 347 316T329 361Q302 397 257 397Z"></path><path id="MJX-2-TEX-N-D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path><path id="MJX-2-TEX-I-1D435" d="M231 637Q204 637 199 638T194 649Q194 676 205 682Q206 683 335 683Q594 683 608 681Q671 671 713 636T756 544Q756 480 698 429T565 360L555 357Q619 348 660 311T702 219Q702 146 630 78T453 1Q446 0 242 0Q42 0 39 2Q35 5 35 10Q35 17 37 24Q42 43 47 45Q51 46 62 46H68Q95 46 128 49Q142 52 147 61Q150 65 219 339T288 628Q288 635 231 637ZM649 544Q649 574 634 600T585 634Q578 636 493 637Q473 637 451 637T416 636H403Q388 635 384 626Q382 622 352 506Q352 503 351 500L320 374H401Q482 374 494 376Q554 386 601 434T649 544ZM595 229Q595 273 572 302T512 336Q506 337 429 337Q311 337 310 336Q310 334 293 263T258 122L240 52Q240 48 252 48T333 46Q422 46 429 47Q491 54 543 105T595 229Z"></path><path id="MJX-2-TEX-N-39" d="M352 287Q304 211 232 211Q154 211 104 270T44 396Q42 412 42 436V444Q42 537 111 606Q171 666 243 666Q245 666 249 666T257 665H261Q273 665 286 663T323 651T370 619T413 560Q456 472 456 334Q456 194 396 97Q361 41 312 10T208 -22Q147 -22 108 7T68 93T121 149Q143 149 158 135T173 96Q173 78 164 65T148 49T135 44L131 43Q131 41 138 37T164 27T206 22H212Q272 22 313 86Q352 142 352 280V287ZM244 248Q292 248 321 297T351 430Q351 508 343 542Q341 552 337 562T323 588T293 615T246 625Q208 625 181 598Q160 576 154 546T147 441Q147 358 152 329T172 282Q197 248 244 248Z"></path><path id="MJX-2-TEX-S4-5D" d="M5 1677V1750H313V-1249H5V-1176H240V1677H5Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D434"></use></g><g data-mml-node="mi" transform="translate(750, 363) scale(0.707)"><use xlink:href="#MJX-2-TEX-I-1D458"></use></g></g><g data-mml-node="mo" transform="translate(1446.2, 0)"><use xlink:href="#MJX-2-TEX-N-3D"></use></g><g data-mml-node="mrow" transform="translate(2502, 0)"><g data-mml-node="mo"><use xlink:href="#MJX-2-TEX-S4-5B"></use></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(583, 0)"><g data-mml-node="mtable"><g data-mml-node="mtr"><g data-mml-node="mtd"><g data-mml-node="mtable"><g data-mml-node="mtr" transform="translate(0, 762.6)"><g data-mml-node="mtd" transform="translate(129.5, 0)"><g data-mml-node="msub"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-30"></use></g><g data-mml-node="TeXAtom" transform="translate(500, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-36"></use></g><g data-mml-node="mo" transform="translate(500, 0)"><use xlink:href="#MJX-2-TEX-N-D7"></use></g><g data-mml-node="mn" transform="translate(1278, 0)"><use xlink:href="#MJX-2-TEX-N-36"></use></g></g></g></g><g data-mml-node="mtd" transform="translate(3066.2, 0)"><g data-mml-node="msubsup"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D435"></use></g><g data-mml-node="mi" transform="translate(759, 413) scale(0.707)"><use xlink:href="#MJX-2-TEX-I-1D458"></use></g><g data-mml-node="TeXAtom" transform="translate(759, -247) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-39"></use></g><g data-mml-node="mo" transform="translate(500, 0)"><use xlink:href="#MJX-2-TEX-N-D7"></use></g><g data-mml-node="mn" transform="translate(1278, 0)"><use xlink:href="#MJX-2-TEX-N-39"></use></g></g></g></g></g><g data-mml-node="mtr" transform="translate(0, -903.7)"><g data-mml-node="mtd"><g data-mml-node="msubsup"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D435"></use></g><g data-mml-node="mi" transform="translate(759, 413) scale(0.707)"><use xlink:href="#MJX-2-TEX-I-1D458"></use></g><g data-mml-node="TeXAtom" transform="translate(759, -247) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-39"></use></g><g data-mml-node="mo" transform="translate(500, 0)"><use xlink:href="#MJX-2-TEX-N-D7"></use></g><g data-mml-node="mn" transform="translate(1278, 0)"><use xlink:href="#MJX-2-TEX-N-39"></use></g></g></g></g><g data-mml-node="mtd" transform="translate(3195.7, 0)"><g data-mml-node="msub"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-30"></use></g><g data-mml-node="TeXAtom" transform="translate(500, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><use xlink:href="#MJX-2-TEX-N-36"></use></g><g data-mml-node="mo" transform="translate(500, 0)"><use xlink:href="#MJX-2-TEX-N-D7"></use></g><g data-mml-node="mn" transform="translate(1278, 0)"><use xlink:href="#MJX-2-TEX-N-36"></use></g></g></g></g></g></g></g></g></g></g><g data-mml-node="mo" transform="translate(5715.5, 0)"><use xlink:href="#MJX-2-TEX-S4-5D"></use></g></g></g></g></svg></mjx-container></div><p>The power of the adjacency matrix <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0" xmlns="http://www.w3.org/2000/svg" width="1.697ex" height="1.62ex" role="img" focusable="false" viewBox="0 -716 750 716" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-2-TEX-I-1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D434"></use></g></g></g></svg></mjx-container>, represented as <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.987ex" xmlns="http://www.w3.org/2000/svg" width="3.021ex" height="2.919ex" role="img" focusable="false" viewBox="0 -853.7 1335.3 1290.2" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-2-TEX-I-1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"></path><path id="MJX-2-TEX-I-1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path><path id="MJX-2-TEX-I-1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path><path id="MJX-2-TEX-I-1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msubsup"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D434"></use></g><g data-mml-node="mi" transform="translate(750, 363) scale(0.707)"><use xlink:href="#MJX-2-TEX-I-1D458"></use></g><g data-mml-node="TeXAtom" transform="translate(750, -292.2) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D456"></use></g><g data-mml-node="mi" transform="translate(345, 0)"><use xlink:href="#MJX-2-TEX-I-1D457"></use></g></g></g></g></g></svg></mjx-container>, gives the number of paths of length k between vertices <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex" xmlns="http://www.w3.org/2000/svg" width="1.762ex" height="1.359ex" role="img" focusable="false" viewBox="0 -443 779 600.8" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-2-TEX-I-1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path><path id="MJX-2-TEX-I-1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D463"></use></g><g data-mml-node="mi" transform="translate(485, -150) scale(0.707)"><use xlink:href="#MJX-2-TEX-I-1D456"></use></g></g></g></g></svg></mjx-container> and <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.666ex" xmlns="http://www.w3.org/2000/svg" width="1.87ex" height="1.668ex" role="img" focusable="false" viewBox="0 -443 826.3 737.2" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-2-TEX-I-1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path><path id="MJX-2-TEX-I-1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D463"></use></g><g data-mml-node="mi" transform="translate(485, -150) scale(0.707)"><use xlink:href="#MJX-2-TEX-I-1D457"></use></g></g></g></g></svg></mjx-container>. (<a href="https://mathworld.wolfram.com/GraphPower.html">mathematical proof</a>)</p><p>This property provides an easy method to find the shortest path: calculated continuously <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0" xmlns="http://www.w3.org/2000/svg" width="2.643ex" height="1.932ex" role="img" focusable="false" viewBox="0 -853.7 1168.4 853.7" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-2-TEX-I-1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"></path><path id="MJX-2-TEX-I-1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-1D434"></use></g><g data-mml-node="mi" transform="translate(750, 363) scale(0.707)"><use xlink:href="#MJX-2-TEX-I-1D458"></use></g></g></g></g></svg></mjx-container>, and <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.439ex" xmlns="http://www.w3.org/2000/svg" width="12.63ex" height="2.009ex" role="img" focusable="false" viewBox="0 -694 5582.6 888" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path><path id="MJX-1-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-1-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-1-TEX-N-2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path><path id="MJX-1-TEX-N-33" d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z"></path><path id="MJX-1-TEX-N-35" d="M164 157Q164 133 148 117T109 101H102Q148 22 224 22Q294 22 326 82Q345 115 345 210Q345 313 318 349Q292 382 260 382H254Q176 382 136 314Q132 307 129 306T114 304Q97 304 95 310Q93 314 93 485V614Q93 664 98 664Q100 666 102 666Q103 666 123 658T178 642T253 634Q324 634 389 662Q397 666 402 666Q410 666 410 648V635Q328 538 205 538Q174 538 149 544L139 546V374Q158 388 169 396T205 412T256 420Q337 420 393 355T449 201Q449 109 385 44T229 -22Q148 -22 99 32T50 154Q50 178 61 192T84 210T107 214Q132 214 148 197T164 157Z"></path><path id="MJX-1-TEX-N-2026" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60ZM525 60Q525 84 542 102T585 120Q609 120 627 104T646 61Q646 36 629 18T586 0T543 17T525 60ZM972 60Q972 84 989 102T1032 120Q1056 120 1074 104T1093 61Q1093 36 1076 18T1033 0T990 17T972 60Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D458"></use></g><g data-mml-node="mo" transform="translate(798.8, 0)"><use xlink:href="#MJX-1-TEX-N-3D"></use></g><g data-mml-node="mn" transform="translate(1854.6, 0)"><use xlink:href="#MJX-1-TEX-N-31"></use></g><g data-mml-node="mo" transform="translate(2354.6, 0)"><use xlink:href="#MJX-1-TEX-N-2C"></use></g><g data-mml-node="mn" transform="translate(2799.2, 0)"><use xlink:href="#MJX-1-TEX-N-33"></use></g><g data-mml-node="mo" transform="translate(3299.2, 0)"><use xlink:href="#MJX-1-TEX-N-2C"></use></g><g data-mml-node="mn" transform="translate(3743.9, 0)"><use xlink:href="#MJX-1-TEX-N-35"></use></g><g data-mml-node="mo" transform="translate(4410.6, 0)"><use xlink:href="#MJX-1-TEX-N-2026"></use></g></g></g></svg></mjx-container> until <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="8.802ex" height="2.497ex" role="img" focusable="false" viewBox="0 -853.7 3890.4 1103.7" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"></path><path id="MJX-1-TEX-I-1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path><path id="MJX-1-TEX-N-28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path><path id="MJX-1-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-1-TEX-N-2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path><path id="MJX-1-TEX-N-30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path><path id="MJX-1-TEX-N-29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D434"></use></g><g data-mml-node="mi" transform="translate(750, 363) scale(0.707)"><use xlink:href="#MJX-1-TEX-I-1D451"></use></g></g><g data-mml-node="mo" transform="translate(1167.7, 0)"><use xlink:href="#MJX-1-TEX-N-28"></use></g><g data-mml-node="mn" transform="translate(1556.7, 0)"><use xlink:href="#MJX-1-TEX-N-31"></use></g><g data-mml-node="mo" transform="translate(2056.7, 0)"><use xlink:href="#MJX-1-TEX-N-2C"></use></g><g data-mml-node="mn" transform="translate(2501.4, 0)"><use xlink:href="#MJX-1-TEX-N-31"></use><use xlink:href="#MJX-1-TEX-N-30" transform="translate(500, 0)"></use></g><g data-mml-node="mo" transform="translate(3501.4, 0)"><use xlink:href="#MJX-1-TEX-N-29"></use></g></g></g></svg></mjx-container> is greater than zero, that is, <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="11.839ex" height="2.497ex" role="img" focusable="false" viewBox="0 -853.7 5232.9 1103.7" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D435" d="M231 637Q204 637 199 638T194 649Q194 676 205 682Q206 683 335 683Q594 683 608 681Q671 671 713 636T756 544Q756 480 698 429T565 360L555 357Q619 348 660 311T702 219Q702 146 630 78T453 1Q446 0 242 0Q42 0 39 2Q35 5 35 10Q35 17 37 24Q42 43 47 45Q51 46 62 46H68Q95 46 128 49Q142 52 147 61Q150 65 219 339T288 628Q288 635 231 637ZM649 544Q649 574 634 600T585 634Q578 636 493 637Q473 637 451 637T416 636H403Q388 635 384 626Q382 622 352 506Q352 503 351 500L320 374H401Q482 374 494 376Q554 386 601 434T649 544ZM595 229Q595 273 572 302T512 336Q506 337 429 337Q311 337 310 336Q310 334 293 263T258 122L240 52Q240 48 252 48T333 46Q422 46 429 47Q491 54 543 105T595 229Z"></path><path id="MJX-1-TEX-I-1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path><path id="MJX-1-TEX-N-28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path><path id="MJX-1-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-1-TEX-N-2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path><path id="MJX-1-TEX-N-29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path><path id="MJX-1-TEX-N-3E" d="M84 520Q84 528 88 533T96 539L99 540Q106 540 253 471T544 334L687 265Q694 260 694 250T687 235Q685 233 395 96L107 -40H101Q83 -38 83 -20Q83 -19 83 -17Q82 -10 98 -1Q117 9 248 71Q326 108 378 132L626 250L378 368Q90 504 86 509Q84 513 84 520Z"></path><path id="MJX-1-TEX-N-30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D435"></use></g><g data-mml-node="mi" transform="translate(759, 363) scale(0.707)"><use xlink:href="#MJX-1-TEX-I-1D451"></use></g></g><g data-mml-node="mo" transform="translate(1176.7, 0)"><use xlink:href="#MJX-1-TEX-N-28"></use></g><g data-mml-node="mn" transform="translate(1565.7, 0)"><use xlink:href="#MJX-1-TEX-N-31"></use></g><g data-mml-node="mo" transform="translate(2065.7, 0)"><use xlink:href="#MJX-1-TEX-N-2C"></use></g><g data-mml-node="mn" transform="translate(2510.4, 0)"><use xlink:href="#MJX-1-TEX-N-31"></use></g><g data-mml-node="mo" transform="translate(3010.4, 0)"><use xlink:href="#MJX-1-TEX-N-29"></use></g><g data-mml-node="mo" transform="translate(3677.1, 0)"><use xlink:href="#MJX-1-TEX-N-3E"></use></g><g data-mml-node="mn" transform="translate(4732.9, 0)"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g></g></svg></mjx-container> which means that we have arrived at the destination <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.375ex" xmlns="http://www.w3.org/2000/svg" width="2.81ex" height="1.377ex" role="img" focusable="false" viewBox="0 -443 1242.1 608.6" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path><path id="MJX-1-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-1-TEX-N-30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D463"></use></g><g data-mml-node="TeXAtom" transform="translate(485, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-31"></use><use xlink:href="#MJX-1-TEX-N-30" transform="translate(500, 0)"></use></g></g></g></g></g></svg></mjx-container>, but also know the shortest path length is equal to <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.023ex" xmlns="http://www.w3.org/2000/svg" width="1.176ex" height="1.593ex" role="img" focusable="false" viewBox="0 -694 520 704" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D451"></use></g></g></g></svg></mjx-container>, and the total number of path is equal to <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="7.691ex" height="2.497ex" role="img" focusable="false" viewBox="0 -853.7 3399.4 1103.7" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D435" d="M231 637Q204 637 199 638T194 649Q194 676 205 682Q206 683 335 683Q594 683 608 681Q671 671 713 636T756 544Q756 480 698 429T565 360L555 357Q619 348 660 311T702 219Q702 146 630 78T453 1Q446 0 242 0Q42 0 39 2Q35 5 35 10Q35 17 37 24Q42 43 47 45Q51 46 62 46H68Q95 46 128 49Q142 52 147 61Q150 65 219 339T288 628Q288 635 231 637ZM649 544Q649 574 634 600T585 634Q578 636 493 637Q473 637 451 637T416 636H403Q388 635 384 626Q382 622 352 506Q352 503 351 500L320 374H401Q482 374 494 376Q554 386 601 434T649 544ZM595 229Q595 273 572 302T512 336Q506 337 429 337Q311 337 310 336Q310 334 293 263T258 122L240 52Q240 48 252 48T333 46Q422 46 429 47Q491 54 543 105T595 229Z"></path><path id="MJX-1-TEX-I-1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path><path id="MJX-1-TEX-N-28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path><path id="MJX-1-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-1-TEX-N-2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path><path id="MJX-1-TEX-N-29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D435"></use></g><g data-mml-node="mi" transform="translate(759, 363) scale(0.707)"><use xlink:href="#MJX-1-TEX-I-1D451"></use></g></g><g data-mml-node="mo" transform="translate(1176.7, 0)"><use xlink:href="#MJX-1-TEX-N-28"></use></g><g data-mml-node="mn" transform="translate(1565.7, 0)"><use xlink:href="#MJX-1-TEX-N-31"></use></g><g data-mml-node="mo" transform="translate(2065.7, 0)"><use xlink:href="#MJX-1-TEX-N-2C"></use></g><g data-mml-node="mn" transform="translate(2510.4, 0)"><use xlink:href="#MJX-1-TEX-N-31"></use></g><g data-mml-node="mo" transform="translate(3010.4, 0)"><use xlink:href="#MJX-1-TEX-N-29"></use></g></g></g></svg></mjx-container>.</p><h1 id="Computation-in-R"><a href="#Computation-in-R" class="headerlink" title="Computation in R"></a>Computation in R</h1><p>I define a variable B representing the adjacency matrix mentioned above and build a function to compute the power to a matrix.</p><pre class="line-numbers language-r" data-language="r"><code class="language-r">B <span class="token operator">&lt;-</span> matrix<span class="token punctuation">(</span>c<span class="token punctuation">(</span>    <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span>     <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span>     <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span>      <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span>      <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span>      <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span>      <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span>      <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span>     <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nrow<span class="token operator">=</span><span class="token number">9</span><span class="token punctuation">)</span>power <span class="token operator">=</span> <span class="token keyword">function</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> n<span class="token punctuation">)</span> Reduce<span class="token punctuation">(</span>`<span class="token percent-operator operator">%*%</span>`<span class="token punctuation">,</span> replicate<span class="token punctuation">(</span>n<span class="token punctuation">,</span> x<span class="token punctuation">,</span> simplify<span class="token operator">=</span><span class="token boolean">FALSE</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><p>For <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.186ex" xmlns="http://www.w3.org/2000/svg" width="5.327ex" height="1.756ex" role="img" focusable="false" viewBox="0 -694 2354.6 776" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path><path id="MJX-1-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-1-TEX-N-33" d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D458"></use></g><g data-mml-node="mo" transform="translate(798.8, 0)"><use xlink:href="#MJX-1-TEX-N-3D"></use></g><g data-mml-node="mn" transform="translate(1854.6, 0)"><use xlink:href="#MJX-1-TEX-N-33"></use></g></g></g></svg></mjx-container>:</p><pre class="line-numbers language-r" data-language="r"><code class="language-r">power<span class="token punctuation">(</span>B<span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>     <span class="token punctuation">[</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token punctuation">[</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token punctuation">[</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">]</span> <span class="token punctuation">[</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">]</span> <span class="token punctuation">[</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">]</span> <span class="token punctuation">[</span><span class="token punctuation">,</span><span class="token number">6</span><span class="token punctuation">]</span> <span class="token punctuation">[</span><span class="token punctuation">,</span><span class="token number">7</span><span class="token punctuation">]</span> <span class="token punctuation">[</span><span class="token punctuation">,</span><span class="token number">8</span><span class="token punctuation">]</span> <span class="token punctuation">[</span><span class="token punctuation">,</span><span class="token number">9</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token punctuation">]</span>    <span class="token number">0</span>    <span class="token number">0</span>    <span class="token number">0</span>    <span class="token number">0</span>    <span class="token number">0</span>    <span class="token number">5</span>    <span class="token number">2</span>    <span class="token number">5</span>    <span class="token number">3</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token punctuation">]</span>    <span class="token number">0</span>    <span class="token number">0</span>    <span class="token number">0</span>    <span class="token number">0</span>    <span class="token number">1</span>    <span class="token number">5</span>    <span class="token number">4</span>    <span class="token number">5</span>    <span class="token number">2</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token punctuation">]</span>    <span class="token number">0</span>    <span class="token number">0</span>    <span class="token number">1</span>    <span class="token number">0</span>    <span class="token number">3</span>    <span class="token number">1</span>    <span class="token number">3</span>    <span class="token number">1</span>    <span class="token number">0</span><span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token punctuation">]</span>    <span class="token number">0</span>    <span class="token number">0</span>    <span class="token number">0</span>    <span class="token number">0</span>    <span class="token number">0</span>    <span class="token number">0</span>    <span class="token number">0</span>    <span class="token number">0</span>    <span class="token number">0</span><span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token punctuation">]</span>    <span class="token number">0</span>    <span class="token number">1</span>    <span class="token number">3</span>    <span class="token number">0</span>    <span class="token number">3</span>    <span class="token number">0</span>    <span class="token number">1</span>    <span class="token number">0</span>    <span class="token number">0</span><span class="token punctuation">[</span><span class="token number">6</span><span class="token punctuation">,</span><span class="token punctuation">]</span>    <span class="token number">5</span>    <span class="token number">5</span>    <span class="token number">1</span>    <span class="token number">0</span>    <span class="token number">0</span>    <span class="token number">0</span>    <span class="token number">0</span>    <span class="token number">0</span>    <span class="token number">0</span><span class="token punctuation">[</span><span class="token number">7</span><span class="token punctuation">,</span><span class="token punctuation">]</span>    <span class="token number">2</span>    <span class="token number">4</span>    <span class="token number">3</span>    <span class="token number">0</span>    <span class="token number">1</span>    <span class="token number">0</span>    <span class="token number">0</span>    <span class="token number">0</span>    <span class="token number">0</span><span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">,</span><span class="token punctuation">]</span>    <span class="token number">5</span>    <span class="token number">5</span>    <span class="token number">1</span>    <span class="token number">0</span>    <span class="token number">0</span>    <span class="token number">0</span>    <span class="token number">0</span>    <span class="token number">0</span>    <span class="token number">0</span><span class="token punctuation">[</span><span class="token number">9</span><span class="token punctuation">,</span><span class="token punctuation">]</span>    <span class="token number">3</span>    <span class="token number">2</span>    <span class="token number">0</span>    <span class="token number">0</span>    <span class="token number">0</span>    <span class="token number">0</span>    <span class="token number">0</span>    <span class="token number">0</span>    <span class="token number">0</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>Vertex first visited: <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.471ex" xmlns="http://www.w3.org/2000/svg" width="9.882ex" height="1.79ex" role="img" focusable="false" viewBox="0 -583 4367.9 791" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path><path id="MJX-1-TEX-N-37" d="M55 458Q56 460 72 567L88 674Q88 676 108 676H128V672Q128 662 143 655T195 646T364 644H485V605L417 512Q408 500 387 472T360 435T339 403T319 367T305 330T292 284T284 230T278 162T275 80Q275 66 275 52T274 28V19Q270 2 255 -10T221 -22Q210 -22 200 -19T179 0T168 40Q168 198 265 368Q285 400 349 489L395 552H302Q128 552 119 546Q113 543 108 522T98 479L95 458V455H55V458Z"></path><path id="MJX-1-TEX-N-2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path><path id="MJX-1-TEX-N-39" d="M352 287Q304 211 232 211Q154 211 104 270T44 396Q42 412 42 436V444Q42 537 111 606Q171 666 243 666Q245 666 249 666T257 665H261Q273 665 286 663T323 651T370 619T413 560Q456 472 456 334Q456 194 396 97Q361 41 312 10T208 -22Q147 -22 108 7T68 93T121 149Q143 149 158 135T173 96Q173 78 164 65T148 49T135 44L131 43Q131 41 138 37T164 27T206 22H212Q272 22 313 86Q352 142 352 280V287ZM244 248Q292 248 321 297T351 430Q351 508 343 542Q341 552 337 562T323 588T293 615T246 625Q208 625 181 598Q160 576 154 546T147 441Q147 358 152 329T172 282Q197 248 244 248Z"></path><path id="MJX-1-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-1-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-1-TEX-N-36" d="M42 313Q42 476 123 571T303 666Q372 666 402 630T432 550Q432 525 418 510T379 495Q356 495 341 509T326 548Q326 592 373 601Q351 623 311 626Q240 626 194 566Q147 500 147 364L148 360Q153 366 156 373Q197 433 263 433H267Q313 433 348 414Q372 400 396 374T435 317Q456 268 456 210V192Q456 169 451 149Q440 90 387 34T253 -22Q225 -22 199 -14T143 16T92 75T56 172T42 313ZM257 397Q227 397 205 380T171 335T154 278T148 216Q148 133 160 97T198 39Q222 21 251 21Q302 21 329 59Q342 77 347 104T352 209Q352 289 347 316T329 361Q302 397 257 397Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D463"></use></g><g data-mml-node="TeXAtom" transform="translate(485, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-37"></use></g><g data-mml-node="mo" transform="translate(500, 0)"><use xlink:href="#MJX-1-TEX-N-2B"></use></g><g data-mml-node="mn" transform="translate(1278, 0)"><use xlink:href="#MJX-1-TEX-N-39"></use></g></g></g><g data-mml-node="mo" transform="translate(2070, 0)"><use xlink:href="#MJX-1-TEX-N-3D"></use></g><g data-mml-node="msub" transform="translate(3125.8, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D463"></use></g><g data-mml-node="TeXAtom" transform="translate(485, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-31"></use><use xlink:href="#MJX-1-TEX-N-36" transform="translate(500, 0)"></use></g></g></g></g></g></svg></mjx-container></p><p>Shortest path length: <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="11.485ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 5076.2 1000" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path><path id="MJX-1-TEX-N-28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path><path id="MJX-1-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-1-TEX-N-2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path><path id="MJX-1-TEX-N-36" d="M42 313Q42 476 123 571T303 666Q372 666 402 630T432 550Q432 525 418 510T379 495Q356 495 341 509T326 548Q326 592 373 601Q351 623 311 626Q240 626 194 566Q147 500 147 364L148 360Q153 366 156 373Q197 433 263 433H267Q313 433 348 414Q372 400 396 374T435 317Q456 268 456 210V192Q456 169 451 149Q440 90 387 34T253 -22Q225 -22 199 -14T143 16T92 75T56 172T42 313ZM257 397Q227 397 205 380T171 335T154 278T148 216Q148 133 160 97T198 39Q222 21 251 21Q302 21 329 59Q342 77 347 104T352 209Q352 289 347 316T329 361Q302 397 257 397Z"></path><path id="MJX-1-TEX-N-29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path><path id="MJX-1-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-1-TEX-N-33" d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D451"></use></g><g data-mml-node="mo" transform="translate(520, 0)"><use xlink:href="#MJX-1-TEX-N-28"></use></g><g data-mml-node="mn" transform="translate(909, 0)"><use xlink:href="#MJX-1-TEX-N-31"></use></g><g data-mml-node="mo" transform="translate(1409, 0)"><use xlink:href="#MJX-1-TEX-N-2C"></use></g><g data-mml-node="mn" transform="translate(1853.7, 0)"><use xlink:href="#MJX-1-TEX-N-31"></use><use xlink:href="#MJX-1-TEX-N-36" transform="translate(500, 0)"></use></g><g data-mml-node="mo" transform="translate(2853.7, 0)"><use xlink:href="#MJX-1-TEX-N-29"></use></g><g data-mml-node="mo" transform="translate(3520.4, 0)"><use xlink:href="#MJX-1-TEX-N-3D"></use></g><g data-mml-node="mn" transform="translate(4576.2, 0)"><use xlink:href="#MJX-1-TEX-N-33"></use></g></g></g></svg></mjx-container></p><p>Total number of path: 2</p><hr><p>For <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.186ex" xmlns="http://www.w3.org/2000/svg" width="5.327ex" height="1.756ex" role="img" focusable="false" viewBox="0 -694 2354.6 776" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path><path id="MJX-1-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-1-TEX-N-35" d="M164 157Q164 133 148 117T109 101H102Q148 22 224 22Q294 22 326 82Q345 115 345 210Q345 313 318 349Q292 382 260 382H254Q176 382 136 314Q132 307 129 306T114 304Q97 304 95 310Q93 314 93 485V614Q93 664 98 664Q100 666 102 666Q103 666 123 658T178 642T253 634Q324 634 389 662Q397 666 402 666Q410 666 410 648V635Q328 538 205 538Q174 538 149 544L139 546V374Q158 388 169 396T205 412T256 420Q337 420 393 355T449 201Q449 109 385 44T229 -22Q148 -22 99 32T50 154Q50 178 61 192T84 210T107 214Q132 214 148 197T164 157Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D458"></use></g><g data-mml-node="mo" transform="translate(798.8, 0)"><use xlink:href="#MJX-1-TEX-N-3D"></use></g><g data-mml-node="mn" transform="translate(1854.6, 0)"><use xlink:href="#MJX-1-TEX-N-35"></use></g></g></g></svg></mjx-container>:</p><pre class="line-numbers language-r" data-language="r"><code class="language-r">power<span class="token punctuation">(</span>B<span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span>     <span class="token punctuation">[</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token punctuation">[</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token punctuation">[</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">]</span> <span class="token punctuation">[</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">]</span> <span class="token punctuation">[</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">]</span> <span class="token punctuation">[</span><span class="token punctuation">,</span><span class="token number">6</span><span class="token punctuation">]</span> <span class="token punctuation">[</span><span class="token punctuation">,</span><span class="token number">7</span><span class="token punctuation">]</span> <span class="token punctuation">[</span><span class="token punctuation">,</span><span class="token number">8</span><span class="token punctuation">]</span> <span class="token punctuation">[</span><span class="token punctuation">,</span><span class="token number">9</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token punctuation">]</span>    <span class="token number">0</span>    <span class="token number">0</span>    <span class="token number">0</span>    <span class="token number">0</span>    <span class="token number">2</span>   <span class="token number">25</span>   <span class="token number">14</span>   <span class="token number">25</span>   <span class="token number">13</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token punctuation">]</span>    <span class="token number">0</span>    <span class="token number">0</span>    <span class="token number">1</span>    <span class="token number">0</span>    <span class="token number">6</span>   <span class="token number">26</span>   <span class="token number">19</span>   <span class="token number">26</span>   <span class="token number">12</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token punctuation">]</span>    <span class="token number">0</span>    <span class="token number">1</span>    <span class="token number">5</span>    <span class="token number">0</span>   <span class="token number">10</span>    <span class="token number">7</span>   <span class="token number">11</span>    <span class="token number">7</span>    <span class="token number">2</span><span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token punctuation">]</span>    <span class="token number">0</span>    <span class="token number">0</span>    <span class="token number">0</span>    <span class="token number">0</span>    <span class="token number">0</span>    <span class="token number">0</span>    <span class="token number">0</span>    <span class="token number">0</span>    <span class="token number">0</span><span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token punctuation">]</span>    <span class="token number">2</span>    <span class="token number">6</span>   <span class="token number">10</span>    <span class="token number">0</span>   <span class="token number">10</span>    <span class="token number">1</span>    <span class="token number">5</span>    <span class="token number">1</span>    <span class="token number">0</span><span class="token punctuation">[</span><span class="token number">6</span><span class="token punctuation">,</span><span class="token punctuation">]</span>   <span class="token number">25</span>   <span class="token number">26</span>    <span class="token number">7</span>    <span class="token number">0</span>    <span class="token number">1</span>    <span class="token number">0</span>    <span class="token number">0</span>    <span class="token number">0</span>    <span class="token number">0</span><span class="token punctuation">[</span><span class="token number">7</span><span class="token punctuation">,</span><span class="token punctuation">]</span>   <span class="token number">14</span>   <span class="token number">19</span>   <span class="token number">11</span>    <span class="token number">0</span>    <span class="token number">5</span>    <span class="token number">0</span>    <span class="token number">1</span>    <span class="token number">0</span>    <span class="token number">0</span><span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">,</span><span class="token punctuation">]</span>   <span class="token number">25</span>   <span class="token number">26</span>    <span class="token number">7</span>    <span class="token number">0</span>    <span class="token number">1</span>    <span class="token number">0</span>    <span class="token number">0</span>    <span class="token number">0</span>    <span class="token number">0</span><span class="token punctuation">[</span><span class="token number">9</span><span class="token punctuation">,</span><span class="token punctuation">]</span>   <span class="token number">13</span>   <span class="token number">12</span>    <span class="token number">2</span>    <span class="token number">0</span>    <span class="token number">0</span>    <span class="token number">0</span>    <span class="token number">0</span>    <span class="token number">0</span>    <span class="token number">0</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>Vertex first visited: <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.471ex" xmlns="http://www.w3.org/2000/svg" width="9.882ex" height="1.79ex" role="img" focusable="false" viewBox="0 -583 4367.9 791" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path><path id="MJX-1-TEX-N-35" d="M164 157Q164 133 148 117T109 101H102Q148 22 224 22Q294 22 326 82Q345 115 345 210Q345 313 318 349Q292 382 260 382H254Q176 382 136 314Q132 307 129 306T114 304Q97 304 95 310Q93 314 93 485V614Q93 664 98 664Q100 666 102 666Q103 666 123 658T178 642T253 634Q324 634 389 662Q397 666 402 666Q410 666 410 648V635Q328 538 205 538Q174 538 149 544L139 546V374Q158 388 169 396T205 412T256 420Q337 420 393 355T449 201Q449 109 385 44T229 -22Q148 -22 99 32T50 154Q50 178 61 192T84 210T107 214Q132 214 148 197T164 157Z"></path><path id="MJX-1-TEX-N-2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path><path id="MJX-1-TEX-N-39" d="M352 287Q304 211 232 211Q154 211 104 270T44 396Q42 412 42 436V444Q42 537 111 606Q171 666 243 666Q245 666 249 666T257 665H261Q273 665 286 663T323 651T370 619T413 560Q456 472 456 334Q456 194 396 97Q361 41 312 10T208 -22Q147 -22 108 7T68 93T121 149Q143 149 158 135T173 96Q173 78 164 65T148 49T135 44L131 43Q131 41 138 37T164 27T206 22H212Q272 22 313 86Q352 142 352 280V287ZM244 248Q292 248 321 297T351 430Q351 508 343 542Q341 552 337 562T323 588T293 615T246 625Q208 625 181 598Q160 576 154 546T147 441Q147 358 152 329T172 282Q197 248 244 248Z"></path><path id="MJX-1-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-1-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-1-TEX-N-34" d="M462 0Q444 3 333 3Q217 3 199 0H190V46H221Q241 46 248 46T265 48T279 53T286 61Q287 63 287 115V165H28V211L179 442Q332 674 334 675Q336 677 355 677H373L379 671V211H471V165H379V114Q379 73 379 66T385 54Q393 47 442 46H471V0H462ZM293 211V545L74 212L183 211H293Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D463"></use></g><g data-mml-node="TeXAtom" transform="translate(485, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-35"></use></g><g data-mml-node="mo" transform="translate(500, 0)"><use xlink:href="#MJX-1-TEX-N-2B"></use></g><g data-mml-node="mn" transform="translate(1278, 0)"><use xlink:href="#MJX-1-TEX-N-39"></use></g></g></g><g data-mml-node="mo" transform="translate(2070, 0)"><use xlink:href="#MJX-1-TEX-N-3D"></use></g><g data-mml-node="msub" transform="translate(3125.8, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D463"></use></g><g data-mml-node="TeXAtom" transform="translate(485, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-31"></use><use xlink:href="#MJX-1-TEX-N-34" transform="translate(500, 0)"></use></g></g></g></g></g></svg></mjx-container></p><p>Shortest path length: <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="11.485ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 5076.2 1000" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path><path id="MJX-1-TEX-N-28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path><path id="MJX-1-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-1-TEX-N-2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path><path id="MJX-1-TEX-N-34" d="M462 0Q444 3 333 3Q217 3 199 0H190V46H221Q241 46 248 46T265 48T279 53T286 61Q287 63 287 115V165H28V211L179 442Q332 674 334 675Q336 677 355 677H373L379 671V211H471V165H379V114Q379 73 379 66T385 54Q393 47 442 46H471V0H462ZM293 211V545L74 212L183 211H293Z"></path><path id="MJX-1-TEX-N-29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path><path id="MJX-1-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-1-TEX-N-35" d="M164 157Q164 133 148 117T109 101H102Q148 22 224 22Q294 22 326 82Q345 115 345 210Q345 313 318 349Q292 382 260 382H254Q176 382 136 314Q132 307 129 306T114 304Q97 304 95 310Q93 314 93 485V614Q93 664 98 664Q100 666 102 666Q103 666 123 658T178 642T253 634Q324 634 389 662Q397 666 402 666Q410 666 410 648V635Q328 538 205 538Q174 538 149 544L139 546V374Q158 388 169 396T205 412T256 420Q337 420 393 355T449 201Q449 109 385 44T229 -22Q148 -22 99 32T50 154Q50 178 61 192T84 210T107 214Q132 214 148 197T164 157Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D451"></use></g><g data-mml-node="mo" transform="translate(520, 0)"><use xlink:href="#MJX-1-TEX-N-28"></use></g><g data-mml-node="mn" transform="translate(909, 0)"><use xlink:href="#MJX-1-TEX-N-31"></use></g><g data-mml-node="mo" transform="translate(1409, 0)"><use xlink:href="#MJX-1-TEX-N-2C"></use></g><g data-mml-node="mn" transform="translate(1853.7, 0)"><use xlink:href="#MJX-1-TEX-N-31"></use><use xlink:href="#MJX-1-TEX-N-34" transform="translate(500, 0)"></use></g><g data-mml-node="mo" transform="translate(2853.7, 0)"><use xlink:href="#MJX-1-TEX-N-29"></use></g><g data-mml-node="mo" transform="translate(3520.4, 0)"><use xlink:href="#MJX-1-TEX-N-3D"></use></g><g data-mml-node="mn" transform="translate(4576.2, 0)"><use xlink:href="#MJX-1-TEX-N-35"></use></g></g></g></svg></mjx-container></p><p>Total number of path: 2</p><hr><p>For <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.186ex" xmlns="http://www.w3.org/2000/svg" width="5.327ex" height="1.756ex" role="img" focusable="false" viewBox="0 -694 2354.6 776" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path><path id="MJX-1-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-1-TEX-N-37" d="M55 458Q56 460 72 567L88 674Q88 676 108 676H128V672Q128 662 143 655T195 646T364 644H485V605L417 512Q408 500 387 472T360 435T339 403T319 367T305 330T292 284T284 230T278 162T275 80Q275 66 275 52T274 28V19Q270 2 255 -10T221 -22Q210 -22 200 -19T179 0T168 40Q168 198 265 368Q285 400 349 489L395 552H302Q128 552 119 546Q113 543 108 522T98 479L95 458V455H55V458Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D458"></use></g><g data-mml-node="mo" transform="translate(798.8, 0)"><use xlink:href="#MJX-1-TEX-N-3D"></use></g><g data-mml-node="mn" transform="translate(1854.6, 0)"><use xlink:href="#MJX-1-TEX-N-37"></use></g></g></g></svg></mjx-container>:</p><pre class="line-numbers language-r" data-language="r"><code class="language-r">power<span class="token punctuation">(</span>B<span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">)</span>     <span class="token punctuation">[</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token punctuation">[</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token punctuation">[</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">]</span> <span class="token punctuation">[</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">]</span> <span class="token punctuation">[</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">]</span> <span class="token punctuation">[</span><span class="token punctuation">,</span><span class="token number">6</span><span class="token punctuation">]</span> <span class="token punctuation">[</span><span class="token punctuation">,</span><span class="token number">7</span><span class="token punctuation">]</span> <span class="token punctuation">[</span><span class="token punctuation">,</span><span class="token number">8</span><span class="token punctuation">]</span> <span class="token punctuation">[</span><span class="token punctuation">,</span><span class="token number">9</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token punctuation">]</span>    <span class="token number">0</span>    <span class="token number">0</span>    <span class="token number">2</span>    <span class="token number">0</span>   <span class="token number">18</span>  <span class="token number">127</span>   <span class="token number">80</span>  <span class="token number">127</span>   <span class="token number">63</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token punctuation">]</span>    <span class="token number">0</span>    <span class="token number">1</span>    <span class="token number">8</span>    <span class="token number">0</span>   <span class="token number">32</span>  <span class="token number">135</span>   <span class="token number">96</span>  <span class="token number">135</span>   <span class="token number">64</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token punctuation">]</span>    <span class="token number">2</span>    <span class="token number">8</span>   <span class="token number">21</span>    <span class="token number">0</span>   <span class="token number">36</span>   <span class="token number">41</span>   <span class="token number">46</span>   <span class="token number">41</span>   <span class="token number">16</span><span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token punctuation">]</span>    <span class="token number">0</span>    <span class="token number">0</span>    <span class="token number">0</span>    <span class="token number">0</span>    <span class="token number">0</span>    <span class="token number">0</span>    <span class="token number">0</span>    <span class="token number">0</span>    <span class="token number">0</span><span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token punctuation">]</span>   <span class="token number">18</span>   <span class="token number">32</span>   <span class="token number">36</span>    <span class="token number">0</span>   <span class="token number">35</span>    <span class="token number">9</span>   <span class="token number">22</span>    <span class="token number">9</span>    <span class="token number">2</span><span class="token punctuation">[</span><span class="token number">6</span><span class="token punctuation">,</span><span class="token punctuation">]</span>  <span class="token number">127</span>  <span class="token number">135</span>   <span class="token number">41</span>    <span class="token number">0</span>    <span class="token number">9</span>    <span class="token number">0</span>    <span class="token number">1</span>    <span class="token number">0</span>    <span class="token number">0</span><span class="token punctuation">[</span><span class="token number">7</span><span class="token punctuation">,</span><span class="token punctuation">]</span>   <span class="token number">80</span>   <span class="token number">96</span>   <span class="token number">46</span>    <span class="token number">0</span>   <span class="token number">22</span>    <span class="token number">1</span>    <span class="token number">7</span>    <span class="token number">1</span>    <span class="token number">0</span><span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">,</span><span class="token punctuation">]</span>  <span class="token number">127</span>  <span class="token number">135</span>   <span class="token number">41</span>    <span class="token number">0</span>    <span class="token number">9</span>    <span class="token number">0</span>    <span class="token number">1</span>    <span class="token number">0</span>    <span class="token number">0</span><span class="token punctuation">[</span><span class="token number">9</span><span class="token punctuation">,</span><span class="token punctuation">]</span>   <span class="token number">63</span>   <span class="token number">64</span>   <span class="token number">16</span>    <span class="token number">0</span>    <span class="token number">2</span>    <span class="token number">0</span>    <span class="token number">0</span>    <span class="token number">0</span>    <span class="token number">0</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>Vertex first visited: <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.471ex" xmlns="http://www.w3.org/2000/svg" width="9.882ex" height="1.79ex" role="img" focusable="false" viewBox="0 -583 4367.9 791" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path><path id="MJX-1-TEX-N-33" d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z"></path><path id="MJX-1-TEX-N-2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path><path id="MJX-1-TEX-N-39" d="M352 287Q304 211 232 211Q154 211 104 270T44 396Q42 412 42 436V444Q42 537 111 606Q171 666 243 666Q245 666 249 666T257 665H261Q273 665 286 663T323 651T370 619T413 560Q456 472 456 334Q456 194 396 97Q361 41 312 10T208 -22Q147 -22 108 7T68 93T121 149Q143 149 158 135T173 96Q173 78 164 65T148 49T135 44L131 43Q131 41 138 37T164 27T206 22H212Q272 22 313 86Q352 142 352 280V287ZM244 248Q292 248 321 297T351 430Q351 508 343 542Q341 552 337 562T323 588T293 615T246 625Q208 625 181 598Q160 576 154 546T147 441Q147 358 152 329T172 282Q197 248 244 248Z"></path><path id="MJX-1-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-1-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-1-TEX-N-32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D463"></use></g><g data-mml-node="TeXAtom" transform="translate(485, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-33"></use></g><g data-mml-node="mo" transform="translate(500, 0)"><use xlink:href="#MJX-1-TEX-N-2B"></use></g><g data-mml-node="mn" transform="translate(1278, 0)"><use xlink:href="#MJX-1-TEX-N-39"></use></g></g></g><g data-mml-node="mo" transform="translate(2070, 0)"><use xlink:href="#MJX-1-TEX-N-3D"></use></g><g data-mml-node="msub" transform="translate(3125.8, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D463"></use></g><g data-mml-node="TeXAtom" transform="translate(485, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-31"></use><use xlink:href="#MJX-1-TEX-N-32" transform="translate(500, 0)"></use></g></g></g></g></g></svg></mjx-container></p><p>Shortest path length: <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="11.485ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 5076.2 1000" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path><path id="MJX-1-TEX-N-28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path><path id="MJX-1-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-1-TEX-N-2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path><path id="MJX-1-TEX-N-32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path><path id="MJX-1-TEX-N-29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path><path id="MJX-1-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-1-TEX-N-37" d="M55 458Q56 460 72 567L88 674Q88 676 108 676H128V672Q128 662 143 655T195 646T364 644H485V605L417 512Q408 500 387 472T360 435T339 403T319 367T305 330T292 284T284 230T278 162T275 80Q275 66 275 52T274 28V19Q270 2 255 -10T221 -22Q210 -22 200 -19T179 0T168 40Q168 198 265 368Q285 400 349 489L395 552H302Q128 552 119 546Q113 543 108 522T98 479L95 458V455H55V458Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D451"></use></g><g data-mml-node="mo" transform="translate(520, 0)"><use xlink:href="#MJX-1-TEX-N-28"></use></g><g data-mml-node="mn" transform="translate(909, 0)"><use xlink:href="#MJX-1-TEX-N-31"></use></g><g data-mml-node="mo" transform="translate(1409, 0)"><use xlink:href="#MJX-1-TEX-N-2C"></use></g><g data-mml-node="mn" transform="translate(1853.7, 0)"><use xlink:href="#MJX-1-TEX-N-31"></use><use xlink:href="#MJX-1-TEX-N-32" transform="translate(500, 0)"></use></g><g data-mml-node="mo" transform="translate(2853.7, 0)"><use xlink:href="#MJX-1-TEX-N-29"></use></g><g data-mml-node="mo" transform="translate(3520.4, 0)"><use xlink:href="#MJX-1-TEX-N-3D"></use></g><g data-mml-node="mn" transform="translate(4576.2, 0)"><use xlink:href="#MJX-1-TEX-N-37"></use></g></g></g></svg></mjx-container></p><p>Total number of path: 2</p><hr><p>For <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.186ex" xmlns="http://www.w3.org/2000/svg" width="5.327ex" height="1.756ex" role="img" focusable="false" viewBox="0 -694 2354.6 776" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path><path id="MJX-1-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-1-TEX-N-39" d="M352 287Q304 211 232 211Q154 211 104 270T44 396Q42 412 42 436V444Q42 537 111 606Q171 666 243 666Q245 666 249 666T257 665H261Q273 665 286 663T323 651T370 619T413 560Q456 472 456 334Q456 194 396 97Q361 41 312 10T208 -22Q147 -22 108 7T68 93T121 149Q143 149 158 135T173 96Q173 78 164 65T148 49T135 44L131 43Q131 41 138 37T164 27T206 22H212Q272 22 313 86Q352 142 352 280V287ZM244 248Q292 248 321 297T351 430Q351 508 343 542Q341 552 337 562T323 588T293 615T246 625Q208 625 181 598Q160 576 154 546T147 441Q147 358 152 329T172 282Q197 248 244 248Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D458"></use></g><g data-mml-node="mo" transform="translate(798.8, 0)"><use xlink:href="#MJX-1-TEX-N-3D"></use></g><g data-mml-node="mn" transform="translate(1854.6, 0)"><use xlink:href="#MJX-1-TEX-N-39"></use></g></g></g></svg></mjx-container>:</p><pre class="line-numbers language-r" data-language="r"><code class="language-r">power<span class="token punctuation">(</span>B<span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">)</span>     <span class="token punctuation">[</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token punctuation">[</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token punctuation">[</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">]</span> <span class="token punctuation">[</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">]</span> <span class="token punctuation">[</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">]</span> <span class="token punctuation">[</span><span class="token punctuation">,</span><span class="token number">6</span><span class="token punctuation">]</span> <span class="token punctuation">[</span><span class="token punctuation">,</span><span class="token number">7</span><span class="token punctuation">]</span> <span class="token punctuation">[</span><span class="token punctuation">,</span><span class="token number">8</span><span class="token punctuation">]</span> <span class="token punctuation">[</span><span class="token punctuation">,</span><span class="token number">9</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token punctuation">]</span>    <span class="token number">0</span>    <span class="token number">2</span>   <span class="token number">22</span>    <span class="token number">0</span>  <span class="token number">118</span>  <span class="token number">651</span>  <span class="token number">432</span>  <span class="token number">651</span>  <span class="token number">317</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token punctuation">]</span>    <span class="token number">2</span>   <span class="token number">11</span>   <span class="token number">49</span>    <span class="token number">0</span>  <span class="token number">168</span>  <span class="token number">700</span>  <span class="token number">494</span>  <span class="token number">700</span>  <span class="token number">334</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token punctuation">]</span>   <span class="token number">22</span>   <span class="token number">49</span>   <span class="token number">86</span>    <span class="token number">0</span>  <span class="token number">139</span>  <span class="token number">226</span>  <span class="token number">210</span>  <span class="token number">226</span>   <span class="token number">98</span><span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token punctuation">]</span>    <span class="token number">0</span>    <span class="token number">0</span>    <span class="token number">0</span>    <span class="token number">0</span>    <span class="token number">0</span>    <span class="token number">0</span>    <span class="token number">0</span>    <span class="token number">0</span>    <span class="token number">0</span><span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token punctuation">]</span>  <span class="token number">118</span>  <span class="token number">168</span>  <span class="token number">139</span>    <span class="token number">0</span>  <span class="token number">128</span>   <span class="token number">60</span>   <span class="token number">97</span>   <span class="token number">60</span>   <span class="token number">20</span><span class="token punctuation">[</span><span class="token number">6</span><span class="token punctuation">,</span><span class="token punctuation">]</span>  <span class="token number">651</span>  <span class="token number">700</span>  <span class="token number">226</span>    <span class="token number">0</span>   <span class="token number">60</span>    <span class="token number">1</span>   <span class="token number">11</span>    <span class="token number">1</span>    <span class="token number">0</span><span class="token punctuation">[</span><span class="token number">7</span><span class="token punctuation">,</span><span class="token punctuation">]</span>  <span class="token number">432</span>  <span class="token number">494</span>  <span class="token number">210</span>    <span class="token number">0</span>   <span class="token number">97</span>   <span class="token number">11</span>   <span class="token number">38</span>   <span class="token number">11</span>    <span class="token number">2</span><span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">,</span><span class="token punctuation">]</span>  <span class="token number">651</span>  <span class="token number">700</span>  <span class="token number">226</span>    <span class="token number">0</span>   <span class="token number">60</span>    <span class="token number">1</span>   <span class="token number">11</span>    <span class="token number">1</span>    <span class="token number">0</span><span class="token punctuation">[</span><span class="token number">9</span><span class="token punctuation">,</span><span class="token punctuation">]</span>  <span class="token number">317</span>  <span class="token number">334</span>   <span class="token number">98</span>    <span class="token number">0</span>   <span class="token number">20</span>    <span class="token number">0</span>    <span class="token number">2</span>    <span class="token number">0</span>    <span class="token number">0</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>Vertex first visited: <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.471ex" xmlns="http://www.w3.org/2000/svg" width="9.882ex" height="1.79ex" role="img" focusable="false" viewBox="0 -583 4367.9 791" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path><path id="MJX-1-TEX-N-32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path><path id="MJX-1-TEX-N-2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path><path id="MJX-1-TEX-N-39" d="M352 287Q304 211 232 211Q154 211 104 270T44 396Q42 412 42 436V444Q42 537 111 606Q171 666 243 666Q245 666 249 666T257 665H261Q273 665 286 663T323 651T370 619T413 560Q456 472 456 334Q456 194 396 97Q361 41 312 10T208 -22Q147 -22 108 7T68 93T121 149Q143 149 158 135T173 96Q173 78 164 65T148 49T135 44L131 43Q131 41 138 37T164 27T206 22H212Q272 22 313 86Q352 142 352 280V287ZM244 248Q292 248 321 297T351 430Q351 508 343 542Q341 552 337 562T323 588T293 615T246 625Q208 625 181 598Q160 576 154 546T147 441Q147 358 152 329T172 282Q197 248 244 248Z"></path><path id="MJX-1-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-1-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D463"></use></g><g data-mml-node="TeXAtom" transform="translate(485, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-32"></use></g><g data-mml-node="mo" transform="translate(500, 0)"><use xlink:href="#MJX-1-TEX-N-2B"></use></g><g data-mml-node="mn" transform="translate(1278, 0)"><use xlink:href="#MJX-1-TEX-N-39"></use></g></g></g><g data-mml-node="mo" transform="translate(2070, 0)"><use xlink:href="#MJX-1-TEX-N-3D"></use></g><g data-mml-node="msub" transform="translate(3125.8, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D463"></use></g><g data-mml-node="TeXAtom" transform="translate(485, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-31"></use><use xlink:href="#MJX-1-TEX-N-31" transform="translate(500, 0)"></use></g></g></g></g></g></svg></mjx-container></p><p>Shortest path length: <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="11.485ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 5076.2 1000" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path><path id="MJX-1-TEX-N-28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path><path id="MJX-1-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-1-TEX-N-2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path><path id="MJX-1-TEX-N-29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path><path id="MJX-1-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-1-TEX-N-39" d="M352 287Q304 211 232 211Q154 211 104 270T44 396Q42 412 42 436V444Q42 537 111 606Q171 666 243 666Q245 666 249 666T257 665H261Q273 665 286 663T323 651T370 619T413 560Q456 472 456 334Q456 194 396 97Q361 41 312 10T208 -22Q147 -22 108 7T68 93T121 149Q143 149 158 135T173 96Q173 78 164 65T148 49T135 44L131 43Q131 41 138 37T164 27T206 22H212Q272 22 313 86Q352 142 352 280V287ZM244 248Q292 248 321 297T351 430Q351 508 343 542Q341 552 337 562T323 588T293 615T246 625Q208 625 181 598Q160 576 154 546T147 441Q147 358 152 329T172 282Q197 248 244 248Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D451"></use></g><g data-mml-node="mo" transform="translate(520, 0)"><use xlink:href="#MJX-1-TEX-N-28"></use></g><g data-mml-node="mn" transform="translate(909, 0)"><use xlink:href="#MJX-1-TEX-N-31"></use></g><g data-mml-node="mo" transform="translate(1409, 0)"><use xlink:href="#MJX-1-TEX-N-2C"></use></g><g data-mml-node="mn" transform="translate(1853.7, 0)"><use xlink:href="#MJX-1-TEX-N-31"></use><use xlink:href="#MJX-1-TEX-N-31" transform="translate(500, 0)"></use></g><g data-mml-node="mo" transform="translate(2853.7, 0)"><use xlink:href="#MJX-1-TEX-N-29"></use></g><g data-mml-node="mo" transform="translate(3520.4, 0)"><use xlink:href="#MJX-1-TEX-N-3D"></use></g><g data-mml-node="mn" transform="translate(4576.2, 0)"><use xlink:href="#MJX-1-TEX-N-39"></use></g></g></g></svg></mjx-container></p><p>Total number of path: 2</p><hr><p>For <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.186ex" xmlns="http://www.w3.org/2000/svg" width="6.458ex" height="1.756ex" role="img" focusable="false" viewBox="0 -694 2854.6 776" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path><path id="MJX-1-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-1-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D458"></use></g><g data-mml-node="mo" transform="translate(798.8, 0)"><use xlink:href="#MJX-1-TEX-N-3D"></use></g><g data-mml-node="mn" transform="translate(1854.6, 0)"><use xlink:href="#MJX-1-TEX-N-31"></use><use xlink:href="#MJX-1-TEX-N-31" transform="translate(500, 0)"></use></g></g></g></svg></mjx-container>:</p><pre class="line-numbers language-r" data-language="r"><code class="language-r">power<span class="token punctuation">(</span>B<span class="token punctuation">,</span> <span class="token number">11</span><span class="token punctuation">)</span>     <span class="token punctuation">[</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token punctuation">[</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token punctuation">[</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">]</span> <span class="token punctuation">[</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">]</span> <span class="token punctuation">[</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">]</span> <span class="token punctuation">[</span><span class="token punctuation">,</span><span class="token number">6</span><span class="token punctuation">]</span> <span class="token punctuation">[</span><span class="token punctuation">,</span><span class="token number">7</span><span class="token punctuation">]</span> <span class="token punctuation">[</span><span class="token punctuation">,</span><span class="token number">8</span><span class="token punctuation">]</span> <span class="token punctuation">[</span><span class="token punctuation">,</span><span class="token number">9</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token punctuation">]</span>    <span class="token number">4</span>   <span class="token number">28</span>  <span class="token number">164</span>    <span class="token number">0</span>  <span class="token number">690</span> <span class="token number">3353</span> <span class="token number">2284</span> <span class="token number">3353</span> <span class="token number">1619</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token punctuation">]</span>   <span class="token number">28</span>   <span class="token number">86</span>  <span class="token number">277</span>    <span class="token number">0</span>  <span class="token number">879</span> <span class="token number">3628</span> <span class="token number">2556</span> <span class="token number">3628</span> <span class="token number">1734</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token punctuation">]</span>  <span class="token number">164</span>  <span class="token number">277</span>  <span class="token number">360</span>    <span class="token number">0</span>  <span class="token number">574</span> <span class="token number">1212</span> <span class="token number">1011</span> <span class="token number">1212</span>  <span class="token number">550</span><span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token punctuation">]</span>    <span class="token number">0</span>    <span class="token number">0</span>    <span class="token number">0</span>    <span class="token number">0</span>    <span class="token number">0</span>    <span class="token number">0</span>    <span class="token number">0</span>    <span class="token number">0</span>    <span class="token number">0</span><span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token punctuation">]</span>  <span class="token number">690</span>  <span class="token number">879</span>  <span class="token number">574</span>    <span class="token number">0</span>  <span class="token number">492</span>  <span class="token number">357</span>  <span class="token number">442</span>  <span class="token number">357</span>  <span class="token number">140</span><span class="token punctuation">[</span><span class="token number">6</span><span class="token punctuation">,</span><span class="token punctuation">]</span> <span class="token number">3353</span> <span class="token number">3628</span> <span class="token number">1212</span>    <span class="token number">0</span>  <span class="token number">357</span>   <span class="token number">15</span>   <span class="token number">84</span>   <span class="token number">15</span>    <span class="token number">2</span><span class="token punctuation">[</span><span class="token number">7</span><span class="token punctuation">,</span><span class="token punctuation">]</span> <span class="token number">2284</span> <span class="token number">2556</span> <span class="token number">1011</span>    <span class="token number">0</span>  <span class="token number">442</span>   <span class="token number">84</span>  <span class="token number">195</span>   <span class="token number">84</span>   <span class="token number">24</span><span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">,</span><span class="token punctuation">]</span> <span class="token number">3353</span> <span class="token number">3628</span> <span class="token number">1212</span>    <span class="token number">0</span>  <span class="token number">357</span>   <span class="token number">15</span>   <span class="token number">84</span>   <span class="token number">15</span>    <span class="token number">2</span><span class="token punctuation">[</span><span class="token number">9</span><span class="token punctuation">,</span><span class="token punctuation">]</span> <span class="token number">1619</span> <span class="token number">1734</span>  <span class="token number">550</span>    <span class="token number">0</span>  <span class="token number">140</span>    <span class="token number">2</span>   <span class="token number">24</span>    <span class="token number">2</span>    <span class="token number">0</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>Vertex first visited: <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.471ex" xmlns="http://www.w3.org/2000/svg" width="9.882ex" height="1.79ex" role="img" focusable="false" viewBox="0 -583 4367.9 791" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path><path id="MJX-1-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-1-TEX-N-2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path><path id="MJX-1-TEX-N-39" d="M352 287Q304 211 232 211Q154 211 104 270T44 396Q42 412 42 436V444Q42 537 111 606Q171 666 243 666Q245 666 249 666T257 665H261Q273 665 286 663T323 651T370 619T413 560Q456 472 456 334Q456 194 396 97Q361 41 312 10T208 -22Q147 -22 108 7T68 93T121 149Q143 149 158 135T173 96Q173 78 164 65T148 49T135 44L131 43Q131 41 138 37T164 27T206 22H212Q272 22 313 86Q352 142 352 280V287ZM244 248Q292 248 321 297T351 430Q351 508 343 542Q341 552 337 562T323 588T293 615T246 625Q208 625 181 598Q160 576 154 546T147 441Q147 358 152 329T172 282Q197 248 244 248Z"></path><path id="MJX-1-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-1-TEX-N-30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D463"></use></g><g data-mml-node="TeXAtom" transform="translate(485, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-31"></use></g><g data-mml-node="mo" transform="translate(500, 0)"><use xlink:href="#MJX-1-TEX-N-2B"></use></g><g data-mml-node="mn" transform="translate(1278, 0)"><use xlink:href="#MJX-1-TEX-N-39"></use></g></g></g><g data-mml-node="mo" transform="translate(2070, 0)"><use xlink:href="#MJX-1-TEX-N-3D"></use></g><g data-mml-node="msub" transform="translate(3125.8, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D463"></use></g><g data-mml-node="TeXAtom" transform="translate(485, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-31"></use><use xlink:href="#MJX-1-TEX-N-30" transform="translate(500, 0)"></use></g></g></g></g></g></svg></mjx-container></p><p>Shortest path length: <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="12.616ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 5576.2 1000" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path><path id="MJX-1-TEX-N-28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path><path id="MJX-1-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-1-TEX-N-2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path><path id="MJX-1-TEX-N-30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path><path id="MJX-1-TEX-N-29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path><path id="MJX-1-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D451"></use></g><g data-mml-node="mo" transform="translate(520, 0)"><use xlink:href="#MJX-1-TEX-N-28"></use></g><g data-mml-node="mn" transform="translate(909, 0)"><use xlink:href="#MJX-1-TEX-N-31"></use></g><g data-mml-node="mo" transform="translate(1409, 0)"><use xlink:href="#MJX-1-TEX-N-2C"></use></g><g data-mml-node="mn" transform="translate(1853.7, 0)"><use xlink:href="#MJX-1-TEX-N-31"></use><use xlink:href="#MJX-1-TEX-N-30" transform="translate(500, 0)"></use></g><g data-mml-node="mo" transform="translate(2853.7, 0)"><use xlink:href="#MJX-1-TEX-N-29"></use></g><g data-mml-node="mo" transform="translate(3520.4, 0)"><use xlink:href="#MJX-1-TEX-N-3D"></use></g><g data-mml-node="mn" transform="translate(4576.2, 0)"><use xlink:href="#MJX-1-TEX-N-31"></use><use xlink:href="#MJX-1-TEX-N-31" transform="translate(500, 0)"></use></g></g></g></svg></mjx-container></p><p>Total number of path: 4</p><hr><p>Therefore all the missionaries and cannibals have crossed the river safely.</p><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>As a math major student, it may very well be true that we won’t use some of the more abstract mathematical concepts we learn in school unless we choose to work in specific project. We are taught lots of useless things in math major. However, the underlying skills we developed in mathametics class, such as thinking logically and solving problems, will last a lifetime and help us solve work-related and real-world problems.</p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ol><li><a href="https://ccjou.wordpress.com/2012/08/09/%E6%B8%A1%E6%B2%B3%E5%95%8F%E9%A1%8C/">https://ccjou.wordpress.com/2012/08/09/%E6%B8%A1%E6%B2%B3%E5%95%8F%E9%A1%8C/</a></li><li><a href="https://stackoverflow.com/questions/3274818/matrix-power-in-r">https://stackoverflow.com/questions/3274818/matrix-power-in-r</a></li><li><a href="http://page.mi.fu-berlin.de/rote/Papers/pdf/Crossing+the+bridge+at+night.pdf">http://page.mi.fu-berlin.de/rote/Papers/pdf/Crossing+the+bridge+at+night.pdf</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Math Programming </category>
          
      </categories>
      
      
        <tags>
            
            <tag> R </tag>
            
            <tag> Linear Algebra </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Quality of Search Engine - PageRank</title>
      <link href="/Hexo-Blog/2021/01/26/2021-01-26-quality-of-search-engine/"/>
      <url>/Hexo-Blog/2021/01/26/2021-01-26-quality-of-search-engine/</url>
      
        <content type="html"><![CDATA[<p>Web search engines such as Google.com distinguish themselves by the qualtiy of their returns to search queries. I will discuss a rough approximation of Google’s method for judging the qualtiy of web pages by using PageRank.</p><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>When a web search is initiated, there is rather a complex series of tasks that are carried out by the search engine. One obvious task is <strong>word-matching</strong>, to find pages that contain the query words, in the title or body of the page. Another key task is to <strong>rate the pages</strong> that are identified by the first task, to help the user wade through the possibly large set of choices. This is basically how information retrieval task works!</p><h1 id="PageRank"><a href="#PageRank" class="headerlink" title="PageRank"></a>PageRank</h1><p>When you google for a keyword “steak sauce”, it would possibly return several million pages, begining with the some recipes for steak, a reasonably outcome. How is this ranking determined? The answer to this question is that Google.com assigns a non-negative real number, called the <code>page rank</code>, to each web page that it indexes.</p><p>Consider a graph like the following: </p><img src="/Hexo-Blog/2021/01/26/2021-01-26-quality-of-search-engine/figure.png" class=""><p>Each of $n$ nodes represents a web page, and a directed edge from node <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="0.781ex" height="1.52ex" role="img" focusable="false" viewBox="0 -661 345 672" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D456"></use></g></g></g></svg></mjx-container> to <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.462ex" xmlns="http://www.w3.org/2000/svg" width="0.932ex" height="1.957ex" role="img" focusable="false" viewBox="0 -661 412 865" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D457"></use></g></g></g></svg></mjx-container> means that page <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="0.781ex" height="1.52ex" role="img" focusable="false" viewBox="0 -661 345 672" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D456"></use></g></g></g></svg></mjx-container> contains a web link to page <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.462ex" xmlns="http://www.w3.org/2000/svg" width="0.932ex" height="1.957ex" role="img" focusable="false" viewBox="0 -661 412 865" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D457"></use></g></g></g></svg></mjx-container>. Let <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0" xmlns="http://www.w3.org/2000/svg" width="1.697ex" height="1.62ex" role="img" focusable="false" viewBox="0 -716 750 716" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D434"></use></g></g></g></svg></mjx-container> denote the adjacency matrix, an <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="5.481ex" height="1.136ex" role="img" focusable="false" viewBox="0 -491 2422.4 502" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-1-TEX-N-D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D45B"></use></g><g data-mml-node="mo" transform="translate(822.2, 0)"><use xlink:href="#MJX-1-TEX-N-D7"></use></g><g data-mml-node="mi" transform="translate(1822.4, 0)"><use xlink:href="#MJX-1-TEX-I-1D45B"></use></g></g></g></svg></mjx-container> matrix whose <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.462ex" xmlns="http://www.w3.org/2000/svg" width="1.713ex" height="1.957ex" role="img" focusable="false" viewBox="0 -661 757 865" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path><path id="MJX-1-TEX-I-1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D456"></use></g><g data-mml-node="mi" transform="translate(345, 0)"><use xlink:href="#MJX-1-TEX-I-1D457"></use></g></g></g></svg></mjx-container>-th entry is 1 if there is a link from node <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="0.781ex" height="1.52ex" role="img" focusable="false" viewBox="0 -661 345 672" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D456"></use></g></g></g></svg></mjx-container> to node <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.462ex" xmlns="http://www.w3.org/2000/svg" width="0.932ex" height="1.957ex" role="img" focusable="false" viewBox="0 -661 412 865" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D457"></use></g></g></g></svg></mjx-container>, and 0 otherwise.</p><div style="display: flex;justify-content: center;">  <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -22.738ex" xmlns="http://www.w3.org/2000/svg" width="56.375ex" height="46.606ex" role="img" focusable="false" viewBox="0 -10550 24917.6 20600" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"></path><path id="MJX-1-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-1-TEX-S4-23A1" d="M319 -645V1154H666V1070H403V-645H319Z"></path><path id="MJX-1-TEX-S4-23A3" d="M319 -644V1155H403V-560H666V-644H319Z"></path><path id="MJX-1-TEX-S4-23A2" d="M319 0V602H403V0H319Z"></path><path id="MJX-1-TEX-N-30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path><path id="MJX-1-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-1-TEX-S4-23A4" d="M0 1070V1154H347V-645H263V1070H0Z"></path><path id="MJX-1-TEX-S4-23A6" d="M263 -560V1155H347V-644H0V-560H263Z"></path><path id="MJX-1-TEX-S4-23A5" d="M263 0V602H347V0H263Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D434"></use></g><g data-mml-node="mo" transform="translate(1027.8, 0)"><use xlink:href="#MJX-1-TEX-N-3D"></use></g><g data-mml-node="mrow" transform="translate(2083.6, 0)"><g data-mml-node="mo"><use xlink:href="#MJX-1-TEX-S4-23A1" transform="translate(0, 9396)"></use><use xlink:href="#MJX-1-TEX-S4-23A3" transform="translate(0, -9406)"></use><svg width="667" height="17202" y="-8351" x="0" viewBox="0 4300.5 667 17202"><use xlink:href="#MJX-1-TEX-S4-23A2" transform="scale(1, 42.862)"></use></svg></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(667, 0)"><g data-mml-node="mtable"><g data-mml-node="mtr" transform="translate(0, 9800)"><g data-mml-node="mtd"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(1500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-31"></use></g></g><g data-mml-node="mtd" transform="translate(3000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(4500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(6000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(7500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(9000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(10500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(12000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-31"></use></g></g><g data-mml-node="mtd" transform="translate(13500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(15000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(16500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(18000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(19500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(21000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g></g><g data-mml-node="mtr" transform="translate(0, 8400)"><g data-mml-node="mtd"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(1500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(3000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-31"></use></g></g><g data-mml-node="mtd" transform="translate(4500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(6000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-31"></use></g></g><g data-mml-node="mtd" transform="translate(7500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(9000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-31"></use></g></g><g data-mml-node="mtd" transform="translate(10500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(12000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(13500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(15000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(16500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(18000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(19500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(21000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g></g><g data-mml-node="mtr" transform="translate(0, 7000)"><g data-mml-node="mtd"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(1500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-31"></use></g></g><g data-mml-node="mtd" transform="translate(3000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(4500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(6000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(7500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-31"></use></g></g><g data-mml-node="mtd" transform="translate(9000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(10500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-31"></use></g></g><g data-mml-node="mtd" transform="translate(12000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(13500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(15000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(16500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(18000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(19500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(21000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g></g><g data-mml-node="mtr" transform="translate(0, 5600)"><g data-mml-node="mtd"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(1500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(3000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-31"></use></g></g><g data-mml-node="mtd" transform="translate(4500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(6000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(7500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(9000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(10500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(12000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(13500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(15000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(16500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-31"></use></g></g><g data-mml-node="mtd" transform="translate(18000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(19500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(21000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g></g><g data-mml-node="mtr" transform="translate(0, 4200)"><g data-mml-node="mtd"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-31"></use></g></g><g data-mml-node="mtd" transform="translate(1500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(3000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(4500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(6000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(7500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(9000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(10500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(12000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(13500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-31"></use></g></g><g data-mml-node="mtd" transform="translate(15000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(16500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(18000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(19500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(21000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g></g><g data-mml-node="mtr" transform="translate(0, 2800)"><g data-mml-node="mtd"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(1500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(3000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(4500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(6000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(7500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(9000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(10500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(12000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(13500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-31"></use></g></g><g data-mml-node="mtd" transform="translate(15000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-31"></use></g></g><g data-mml-node="mtd" transform="translate(16500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(18000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(19500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(21000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g></g><g data-mml-node="mtr" transform="translate(0, 1400)"><g data-mml-node="mtd"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(1500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(3000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(4500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(6000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(7500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(9000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(10500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(12000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(13500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-31"></use></g></g><g data-mml-node="mtd" transform="translate(15000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-31"></use></g></g><g data-mml-node="mtd" transform="translate(16500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(18000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(19500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(21000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g></g><g data-mml-node="mtr" transform="translate(0, 0)"><g data-mml-node="mtd"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(1500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(3000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(4500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-31"></use></g></g><g data-mml-node="mtd" transform="translate(6000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(7500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(9000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(10500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(12000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(13500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(15000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-31"></use></g></g><g data-mml-node="mtd" transform="translate(16500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(18000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(19500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(21000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g></g><g data-mml-node="mtr" transform="translate(0, -1400)"><g data-mml-node="mtd"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(1500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(3000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(4500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(6000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-31"></use></g></g><g data-mml-node="mtd" transform="translate(7500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(9000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(10500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(12000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(13500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-31"></use></g></g><g data-mml-node="mtd" transform="translate(15000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(16500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(18000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(19500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(21000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g></g><g data-mml-node="mtr" transform="translate(0, -2800)"><g data-mml-node="mtd"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(1500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(3000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(4500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(6000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(7500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(9000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(10500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(12000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(13500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(15000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(16500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(18000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-31"></use></g></g><g data-mml-node="mtd" transform="translate(19500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(21000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g></g><g data-mml-node="mtr" transform="translate(0, -4200)"><g data-mml-node="mtd"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(1500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(3000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(4500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(6000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(7500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(9000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(10500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(12000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(13500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(15000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(16500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(18000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(19500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(21000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-31"></use></g></g></g><g data-mml-node="mtr" transform="translate(0, -5600)"><g data-mml-node="mtd"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(1500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(3000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(4500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(6000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(7500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(9000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-31"></use></g></g><g data-mml-node="mtd" transform="translate(10500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-31"></use></g></g><g data-mml-node="mtd" transform="translate(12000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(13500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(15000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-31"></use></g></g><g data-mml-node="mtd" transform="translate(16500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(18000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(19500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(21000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g></g><g data-mml-node="mtr" transform="translate(0, -7000)"><g data-mml-node="mtd"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(1500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(3000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(4500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(6000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(7500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(9000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(10500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(12000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-31"></use></g></g><g data-mml-node="mtd" transform="translate(13500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(15000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(16500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(18000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(19500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-31"></use></g></g><g data-mml-node="mtd" transform="translate(21000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g></g><g data-mml-node="mtr" transform="translate(0, -8400)"><g data-mml-node="mtd"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(1500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(3000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(4500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(6000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(7500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(9000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(10500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(12000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(13500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-31"></use></g></g><g data-mml-node="mtd" transform="translate(15000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-31"></use></g></g><g data-mml-node="mtd" transform="translate(16500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(18000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-31"></use></g></g><g data-mml-node="mtd" transform="translate(19500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(21000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-31"></use></g></g></g><g data-mml-node="mtr" transform="translate(0, -9800)"><g data-mml-node="mtd"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(1500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(3000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(4500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(6000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(7500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(9000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(10500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(12000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(13500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(15000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(16500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-31"></use></g></g><g data-mml-node="mtd" transform="translate(18000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(19500, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-31"></use></g></g><g data-mml-node="mtd" transform="translate(21000, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g></g></g></g><g data-mml-node="mo" transform="translate(22167, 0)"><use xlink:href="#MJX-1-TEX-S4-23A4" transform="translate(0, 9396)"></use><use xlink:href="#MJX-1-TEX-S4-23A6" transform="translate(0, -9406)"></use><svg width="667" height="17202" y="-8351" x="0" viewBox="0 4300.5 667 17202"><use xlink:href="#MJX-1-TEX-S4-23A5" transform="scale(1, 42.862)"></use></svg></g></g></g></g></svg></mjx-container></div><p>The invention of Google imagined a surfer on a network of <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="1.357ex" height="1.025ex" role="img" focusable="false" viewBox="0 -442 600 453" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D45B"></use></g></g></g></svg></mjx-container> pages, who currently sits at page <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="0.781ex" height="1.52ex" role="img" focusable="false" viewBox="0 -661 345 672" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D456"></use></g></g></g></svg></mjx-container> with probability <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.439ex" xmlns="http://www.w3.org/2000/svg" width="1.803ex" height="1.439ex" role="img" focusable="false" viewBox="0 -442 797 636" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"></path><path id="MJX-1-TEX-I-1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D45D"></use></g><g data-mml-node="mi" transform="translate(503, -150) scale(0.707)"><use xlink:href="#MJX-1-TEX-I-1D456"></use></g></g></g></g></svg></mjx-container>. Next, the surfer either moves to a random page (with fixed probability <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.439ex" xmlns="http://www.w3.org/2000/svg" width="1.041ex" height="1.439ex" role="img" focusable="false" viewBox="0 -442 460 636" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D45E" d="M33 157Q33 258 109 349T280 441Q340 441 372 389Q373 390 377 395T388 406T404 418Q438 442 450 442Q454 442 457 439T460 434Q460 425 391 149Q320 -135 320 -139Q320 -147 365 -148H390Q396 -156 396 -157T393 -175Q389 -188 383 -194H370Q339 -192 262 -192Q234 -192 211 -192T174 -192T157 -193Q143 -193 143 -185Q143 -182 145 -170Q149 -154 152 -151T172 -148Q220 -148 230 -141Q238 -136 258 -53T279 32Q279 33 272 29Q224 -10 172 -10Q117 -10 75 30T33 157ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D45E"></use></g></g></g></svg></mjx-container>, often around 0.15) or with probability <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.439ex" xmlns="http://www.w3.org/2000/svg" width="4.938ex" height="1.946ex" role="img" focusable="false" viewBox="0 -666 2182.4 860" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-1-TEX-N-2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path><path id="MJX-1-TEX-I-1D45E" d="M33 157Q33 258 109 349T280 441Q340 441 372 389Q373 390 377 395T388 406T404 418Q438 442 450 442Q454 442 457 439T460 434Q460 425 391 149Q320 -135 320 -139Q320 -147 365 -148H390Q396 -156 396 -157T393 -175Q389 -188 383 -194H370Q339 -192 262 -192Q234 -192 211 -192T174 -192T157 -193Q143 -193 143 -185Q143 -182 145 -170Q149 -154 152 -151T172 -148Q220 -148 230 -141Q238 -136 258 -53T279 32Q279 33 272 29Q224 -10 172 -10Q117 -10 75 30T33 157ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-31"></use></g><g data-mml-node="mo" transform="translate(722.2, 0)"><use xlink:href="#MJX-1-TEX-N-2212"></use></g><g data-mml-node="mi" transform="translate(1722.4, 0)"><use xlink:href="#MJX-1-TEX-I-1D45E"></use></g></g></g></svg></mjx-container>, clicks randomly on a link from the current page <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="0.781ex" height="1.52ex" role="img" focusable="false" viewBox="0 -661 345 672" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D456"></use></g></g></g></svg></mjx-container>. The probability that the surfer moves from page <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="0.781ex" height="1.52ex" role="img" focusable="false" viewBox="0 -661 345 672" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D456"></use></g></g></g></svg></mjx-container> to page <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.462ex" xmlns="http://www.w3.org/2000/svg" width="0.932ex" height="1.957ex" role="img" focusable="false" viewBox="0 -661 412 865" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D457"></use></g></g></g></svg></mjx-container> after the click is </p><div style="display: flex;justify-content: center;">    <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -1.033ex" xmlns="http://www.w3.org/2000/svg" width="18.86ex" height="3.418ex" role="img" focusable="false" viewBox="0 -1054.4 8336.2 1510.9" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D45E" d="M33 157Q33 258 109 349T280 441Q340 441 372 389Q373 390 377 395T388 406T404 418Q438 442 450 442Q454 442 457 439T460 434Q460 425 391 149Q320 -135 320 -139Q320 -147 365 -148H390Q396 -156 396 -157T393 -175Q389 -188 383 -194H370Q339 -192 262 -192Q234 -192 211 -192T174 -192T157 -193Q143 -193 143 -185Q143 -182 145 -170Q149 -154 152 -151T172 -148Q220 -148 230 -141Q238 -136 258 -53T279 32Q279 33 272 29Q224 -10 172 -10Q117 -10 75 30T33 157ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path><path id="MJX-1-TEX-N-22C5" d="M78 250Q78 274 95 292T138 310Q162 310 180 294T199 251Q199 226 182 208T139 190T96 207T78 250Z"></path><path id="MJX-1-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-1-TEX-I-1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-1-TEX-N-2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path><path id="MJX-1-TEX-N-28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path><path id="MJX-1-TEX-N-2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path><path id="MJX-1-TEX-N-29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path><path id="MJX-1-TEX-I-1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"></path><path id="MJX-1-TEX-I-1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path><path id="MJX-1-TEX-I-1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D45E"></use></g><g data-mml-node="mo" transform="translate(682.2, 0)"><use xlink:href="#MJX-1-TEX-N-22C5"></use></g><g data-mml-node="mfrac" transform="translate(1182.4, 0)"><g data-mml-node="mn" transform="translate(255.4, 394) scale(0.707)"><use xlink:href="#MJX-1-TEX-N-31"></use></g><g data-mml-node="mi" transform="translate(220, -345) scale(0.707)"><use xlink:href="#MJX-1-TEX-I-1D45B"></use></g><rect width="624.3" height="60" x="120" y="220"></rect></g><g data-mml-node="mo" transform="translate(2268.9, 0)"><use xlink:href="#MJX-1-TEX-N-2B"></use></g><g data-mml-node="mo" transform="translate(3269.2, 0)"><use xlink:href="#MJX-1-TEX-N-28"></use></g><g data-mml-node="mn" transform="translate(3658.2, 0)"><use xlink:href="#MJX-1-TEX-N-31"></use></g><g data-mml-node="mo" transform="translate(4380.4, 0)"><use xlink:href="#MJX-1-TEX-N-2212"></use></g><g data-mml-node="mi" transform="translate(5380.6, 0)"><use xlink:href="#MJX-1-TEX-I-1D45E"></use></g><g data-mml-node="mo" transform="translate(5840.6, 0)"><use xlink:href="#MJX-1-TEX-N-29"></use></g><g data-mml-node="mo" transform="translate(6451.8, 0)"><use xlink:href="#MJX-1-TEX-N-22C5"></use></g><g data-mml-node="mfrac" transform="translate(6952, 0)"><g data-mml-node="msub" transform="translate(220, 548.1) scale(0.707)"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D434"></use></g><g data-mml-node="TeXAtom" transform="translate(750, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D456"></use></g><g data-mml-node="mi" transform="translate(345, 0)"><use xlink:href="#MJX-1-TEX-I-1D457"></use></g></g></g><g data-mml-node="msub" transform="translate(376, -345) scale(0.707)"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D45B"></use></g><g data-mml-node="mi" transform="translate(600, -150) scale(0.707)"><use xlink:href="#MJX-1-TEX-I-1D456"></use></g></g><rect width="1144.2" height="60" x="120" y="220"></rect></g></g></g></svg></mjx-container></div><p>where <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.666ex" xmlns="http://www.w3.org/2000/svg" width="3.021ex" height="2.286ex" role="img" focusable="false" viewBox="0 -716 1335.3 1010.2" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"></path><path id="MJX-1-TEX-I-1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path><path id="MJX-1-TEX-I-1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D434"></use></g><g data-mml-node="TeXAtom" transform="translate(750, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D456"></use></g><g data-mml-node="mi" transform="translate(345, 0)"><use xlink:href="#MJX-1-TEX-I-1D457"></use></g></g></g></g></g></svg></mjx-container> is the entry of the adjacency matrix <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0" xmlns="http://www.w3.org/2000/svg" width="1.697ex" height="1.62ex" role="img" focusable="false" viewBox="0 -716 750 716" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D434"></use></g></g></g></svg></mjx-container>, and <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex" xmlns="http://www.w3.org/2000/svg" width="2.023ex" height="1.357ex" role="img" focusable="false" viewBox="0 -442 894 599.8" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-1-TEX-I-1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D45B"></use></g><g data-mml-node="mi" transform="translate(600, -150) scale(0.707)"><use xlink:href="#MJX-1-TEX-I-1D456"></use></g></g></g></g></svg></mjx-container> is the sum of the <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="0.781ex" height="1.52ex" role="img" focusable="false" viewBox="0 -661 345 672" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D456"></use></g></g></g></svg></mjx-container>-th row of <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0" xmlns="http://www.w3.org/2000/svg" width="1.697ex" height="1.62ex" role="img" focusable="false" viewBox="0 -716 750 716" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D434"></use></g></g></g></svg></mjx-container>. Since the time is arbitrary, the probability of being at node <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.462ex" xmlns="http://www.w3.org/2000/svg" width="0.932ex" height="1.957ex" role="img" focusable="false" viewBox="0 -661 412 865" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D457"></use></g></g></g></svg></mjx-container> is the sum of this expression over all <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="0.781ex" height="1.52ex" role="img" focusable="false" viewBox="0 -661 345 672" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D456"></use></g></g></g></svg></mjx-container>, and it is independent of time; that is, </p><div style="display: flex;justify-content: center;">    <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -1.033ex" xmlns="http://www.w3.org/2000/svg" width="33.244ex" height="2.82ex" role="img" focusable="false" viewBox="0 -789.7 14693.7 1246.3" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"></path><path id="MJX-1-TEX-I-1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path><path id="MJX-1-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-1-TEX-SO-2211" d="M61 748Q64 750 489 750H913L954 640Q965 609 976 579T993 533T999 516H979L959 517Q936 579 886 621T777 682Q724 700 655 705T436 710H319Q183 710 183 709Q186 706 348 484T511 259Q517 250 513 244L490 216Q466 188 420 134T330 27L149 -187Q149 -188 362 -188Q388 -188 436 -188T506 -189Q679 -189 778 -162T936 -43Q946 -27 959 6H999L913 -249L489 -250Q65 -250 62 -248Q56 -246 56 -239Q56 -234 118 -161Q186 -81 245 -11L428 206Q428 207 242 462L57 717L56 728Q56 744 61 748Z"></path><path id="MJX-1-TEX-I-1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path><path id="MJX-1-TEX-N-28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path><path id="MJX-1-TEX-I-1D45E" d="M33 157Q33 258 109 349T280 441Q340 441 372 389Q373 390 377 395T388 406T404 418Q438 442 450 442Q454 442 457 439T460 434Q460 425 391 149Q320 -135 320 -139Q320 -147 365 -148H390Q396 -156 396 -157T393 -175Q389 -188 383 -194H370Q339 -192 262 -192Q234 -192 211 -192T174 -192T157 -193Q143 -193 143 -185Q143 -182 145 -170Q149 -154 152 -151T172 -148Q220 -148 230 -141Q238 -136 258 -53T279 32Q279 33 272 29Q224 -10 172 -10Q117 -10 75 30T33 157ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path><path id="MJX-1-TEX-N-22C5" d="M78 250Q78 274 95 292T138 310Q162 310 180 294T199 251Q199 226 182 208T139 190T96 207T78 250Z"></path><path id="MJX-1-TEX-I-1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-1-TEX-N-2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path><path id="MJX-1-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-1-TEX-N-2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path><path id="MJX-1-TEX-N-29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path><path id="MJX-1-TEX-I-1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D45D"></use></g><g data-mml-node="mi" transform="translate(503, -150) scale(0.707)"><use xlink:href="#MJX-1-TEX-I-1D457"></use></g></g><g data-mml-node="mo" transform="translate(1122.1, 0)"><use xlink:href="#MJX-1-TEX-N-3D"></use></g><g data-mml-node="munder" transform="translate(2177.9, 0)"><g data-mml-node="mo"><use xlink:href="#MJX-1-TEX-SO-2211"></use></g><g data-mml-node="TeXAtom" transform="translate(1056, -285.4) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D456"></use></g></g></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(3694.5, 0)"></g><g data-mml-node="mo" transform="translate(3694.5, 0)"><use xlink:href="#MJX-1-TEX-N-28"></use></g><g data-mml-node="mi" transform="translate(4083.5, 0)"><use xlink:href="#MJX-1-TEX-I-1D45E"></use></g><g data-mml-node="mo" transform="translate(4765.7, 0)"><use xlink:href="#MJX-1-TEX-N-22C5"></use></g><g data-mml-node="mfrac" transform="translate(5265.9, 0)"><g data-mml-node="msub" transform="translate(220, 477.2) scale(0.707)"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D45D"></use></g><g data-mml-node="mi" transform="translate(503, -150) scale(0.707)"><use xlink:href="#MJX-1-TEX-I-1D456"></use></g></g><g data-mml-node="mi" transform="translate(289.6, -345) scale(0.707)"><use xlink:href="#MJX-1-TEX-I-1D45B"></use></g><rect width="763.5" height="60" x="120" y="220"></rect></g><g data-mml-node="mo" transform="translate(6491.7, 0)"><use xlink:href="#MJX-1-TEX-N-2B"></use></g><g data-mml-node="mo" transform="translate(7491.9, 0)"><use xlink:href="#MJX-1-TEX-N-28"></use></g><g data-mml-node="mn" transform="translate(7880.9, 0)"><use xlink:href="#MJX-1-TEX-N-31"></use></g><g data-mml-node="mo" transform="translate(8603.1, 0)"><use xlink:href="#MJX-1-TEX-N-2212"></use></g><g data-mml-node="mi" transform="translate(9603.4, 0)"><use xlink:href="#MJX-1-TEX-I-1D45E"></use></g><g data-mml-node="mo" transform="translate(10063.4, 0)"><use xlink:href="#MJX-1-TEX-N-29"></use></g><g data-mml-node="mo" transform="translate(10674.6, 0)"><use xlink:href="#MJX-1-TEX-N-22C5"></use></g><g data-mml-node="mfrac" transform="translate(11174.8, 0)"><g data-mml-node="msub" transform="translate(254.3, 477.2) scale(0.707)"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D45D"></use></g><g data-mml-node="mi" transform="translate(503, -150) scale(0.707)"><use xlink:href="#MJX-1-TEX-I-1D456"></use></g></g><g data-mml-node="msub" transform="translate(220, -345) scale(0.707)"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D45B"></use></g><g data-mml-node="mi" transform="translate(600, -150) scale(0.707)"><use xlink:href="#MJX-1-TEX-I-1D456"></use></g></g><rect width="832.1" height="60" x="120" y="220"></rect></g><g data-mml-node="mo" transform="translate(12469.2, 0)"><use xlink:href="#MJX-1-TEX-N-22C5"></use></g><g data-mml-node="msub" transform="translate(12969.4, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D434"></use></g><g data-mml-node="TeXAtom" transform="translate(750, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D456"></use></g><g data-mml-node="mi" transform="translate(345, 0)"><use xlink:href="#MJX-1-TEX-I-1D457"></use></g></g></g><g data-mml-node="mo" transform="translate(14304.7, 0)"><use xlink:href="#MJX-1-TEX-N-29"></use></g></g></g></svg></mjx-container></div><p>which is equivalent in matrix terms to the eigenvalue equation</p><div style="display: flex;justify-content: center;">    <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.439ex" xmlns="http://www.w3.org/2000/svg" width="7.071ex" height="2.034ex" role="img" focusable="false" viewBox="0 -705 3125.6 899" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"></path><path id="MJX-1-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-1-TEX-I-1D43A" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q492 659 471 656T418 643T357 615T294 567T236 496T189 394T158 260Q156 242 156 221Q156 173 170 136T206 79T256 45T308 28T353 24Q407 24 452 47T514 106Q517 114 529 161T541 214Q541 222 528 224T468 227H431Q425 233 425 235T427 254Q431 267 437 273H454Q494 271 594 271Q634 271 659 271T695 272T707 272Q721 272 721 263Q721 261 719 249Q714 230 709 228Q706 227 694 227Q674 227 653 224Q646 221 643 215T629 164Q620 131 614 108Q589 6 586 3Q584 1 581 1Q571 1 553 21T530 52Q530 53 528 52T522 47Q448 -22 322 -22Q201 -22 126 55T50 252Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D45D"></use></g><g data-mml-node="mo" transform="translate(780.8, 0)"><use xlink:href="#MJX-1-TEX-N-3D"></use></g><g data-mml-node="mi" transform="translate(1836.6, 0)"><use xlink:href="#MJX-1-TEX-I-1D43A"></use></g><g data-mml-node="mi" transform="translate(2622.6, 0)"><use xlink:href="#MJX-1-TEX-I-1D45D"></use></g></g></g></svg></mjx-container></div><p>where</p><div style="display: flex;justify-content: center;">    <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -1.45ex" xmlns="http://www.w3.org/2000/svg" width="21.152ex" height="3.89ex" role="img" focusable="false" viewBox="0 -1078.4 9349.2 1719.5" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D43A" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q492 659 471 656T418 643T357 615T294 567T236 496T189 394T158 260Q156 242 156 221Q156 173 170 136T206 79T256 45T308 28T353 24Q407 24 452 47T514 106Q517 114 529 161T541 214Q541 222 528 224T468 227H431Q425 233 425 235T427 254Q431 267 437 273H454Q494 271 594 271Q634 271 659 271T695 272T707 272Q721 272 721 263Q721 261 719 249Q714 230 709 228Q706 227 694 227Q674 227 653 224Q646 221 643 215T629 164Q620 131 614 108Q589 6 586 3Q584 1 581 1Q571 1 553 21T530 52Q530 53 528 52T522 47Q448 -22 322 -22Q201 -22 126 55T50 252Z"></path><path id="MJX-1-TEX-N-28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path><path id="MJX-1-TEX-I-1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path><path id="MJX-1-TEX-N-2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path><path id="MJX-1-TEX-I-1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path><path id="MJX-1-TEX-N-29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path><path id="MJX-1-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-1-TEX-I-1D45E" d="M33 157Q33 258 109 349T280 441Q340 441 372 389Q373 390 377 395T388 406T404 418Q438 442 450 442Q454 442 457 439T460 434Q460 425 391 149Q320 -135 320 -139Q320 -147 365 -148H390Q396 -156 396 -157T393 -175Q389 -188 383 -194H370Q339 -192 262 -192Q234 -192 211 -192T174 -192T157 -193Q143 -193 143 -185Q143 -182 145 -170Q149 -154 152 -151T172 -148Q220 -148 230 -141Q238 -136 258 -53T279 32Q279 33 272 29Q224 -10 172 -10Q117 -10 75 30T33 157ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path><path id="MJX-1-TEX-I-1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-1-TEX-N-2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path><path id="MJX-1-TEX-I-1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"></path><path id="MJX-1-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-1-TEX-N-2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path><path id="MJX-1-TEX-SO-2211" d="M61 748Q64 750 489 750H913L954 640Q965 609 976 579T993 533T999 516H979L959 517Q936 579 886 621T777 682Q724 700 655 705T436 710H319Q183 710 183 709Q186 706 348 484T511 259Q517 250 513 244L490 216Q466 188 420 134T330 27L149 -187Q149 -188 362 -188Q388 -188 436 -188T506 -189Q679 -189 778 -162T936 -43Q946 -27 959 6H999L913 -249L489 -250Q65 -250 62 -248Q56 -246 56 -239Q56 -234 118 -161Q186 -81 245 -11L428 206Q428 207 242 462L57 717L56 728Q56 744 61 748Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D43A"></use></g><g data-mml-node="mo" transform="translate(786, 0)"><use xlink:href="#MJX-1-TEX-N-28"></use></g><g data-mml-node="mi" transform="translate(1175, 0)"><use xlink:href="#MJX-1-TEX-I-1D456"></use></g><g data-mml-node="mo" transform="translate(1520, 0)"><use xlink:href="#MJX-1-TEX-N-2C"></use></g><g data-mml-node="mi" transform="translate(1964.7, 0)"><use xlink:href="#MJX-1-TEX-I-1D457"></use></g><g data-mml-node="mo" transform="translate(2376.7, 0)"><use xlink:href="#MJX-1-TEX-N-29"></use></g><g data-mml-node="mo" transform="translate(3043.4, 0)"><use xlink:href="#MJX-1-TEX-N-3D"></use></g><g data-mml-node="mfrac" transform="translate(4099.2, 0)"><g data-mml-node="mi" transform="translate(269.5, 477.2) scale(0.707)"><use xlink:href="#MJX-1-TEX-I-1D45E"></use></g><g data-mml-node="mi" transform="translate(220, -345) scale(0.707)"><use xlink:href="#MJX-1-TEX-I-1D45B"></use></g><rect width="624.3" height="60" x="120" y="220"></rect></g><g data-mml-node="mo" transform="translate(5185.7, 0)"><use xlink:href="#MJX-1-TEX-N-2B"></use></g><g data-mml-node="mfrac" transform="translate(6185.9, 0)"><g data-mml-node="mrow" transform="translate(220, 548.1) scale(0.707)"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D434"></use></g><g data-mml-node="TeXAtom" transform="translate(750, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D457"></use></g><g data-mml-node="mi" transform="translate(412, 0)"><use xlink:href="#MJX-1-TEX-I-1D456"></use></g></g></g><g data-mml-node="mo" transform="translate(1335.3, 0)"><use xlink:href="#MJX-1-TEX-N-28"></use></g><g data-mml-node="mn" transform="translate(1724.3, 0)"><use xlink:href="#MJX-1-TEX-N-31"></use></g><g data-mml-node="mo" transform="translate(2224.3, 0)"><use xlink:href="#MJX-1-TEX-N-2212"></use></g><g data-mml-node="mi" transform="translate(3002.3, 0)"><use xlink:href="#MJX-1-TEX-I-1D45E"></use></g><g data-mml-node="mo" transform="translate(3462.3, 0)"><use xlink:href="#MJX-1-TEX-N-29"></use></g></g><g data-mml-node="mrow" transform="translate(253.8, -398.3) scale(0.707)"><g data-mml-node="munderover"><g data-mml-node="mo"><use xlink:href="#MJX-1-TEX-SO-2211"></use></g><g data-mml-node="TeXAtom" transform="translate(1056, 477.1) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D45B"></use></g></g><g data-mml-node="TeXAtom" transform="translate(1056, -285.4) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D456"></use></g><g data-mml-node="mo" transform="translate(345, 0)"><use xlink:href="#MJX-1-TEX-N-3D"></use></g><g data-mml-node="mn" transform="translate(1123, 0)"><use xlink:href="#MJX-1-TEX-N-31"></use></g></g></g><g data-mml-node="msub" transform="translate(2420.3, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D434"></use></g><g data-mml-node="TeXAtom" transform="translate(750, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D457"></use></g><g data-mml-node="mi" transform="translate(412, 0)"><use xlink:href="#MJX-1-TEX-I-1D456"></use></g></g></g></g><rect width="2923.3" height="60" x="120" y="220"></rect></g></g></g></svg></mjx-container></div><p>Let’s contruct the adjacency matrix using MATLAB!</p><pre class="line-numbers language-matlab" data-language="matlab"><code class="language-matlab"><span class="token comment">%% Construct adjacency matrix A, there are 34 arrows</span>clear all<span class="token punctuation">;</span> clcn <span class="token operator">=</span> <span class="token number">15</span><span class="token number">i</span> <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">5</span> <span class="token number">1</span> <span class="token number">3</span> <span class="token number">2</span> <span class="token number">4</span> <span class="token number">8</span> <span class="token number">2</span> <span class="token number">9</span> <span class="token number">3</span> <span class="token number">9</span> <span class="token number">2</span> <span class="token number">12</span> <span class="token number">3</span> <span class="token number">12</span> <span class="token number">1</span> <span class="token number">13</span> <span class="token number">5</span> <span class="token number">6</span> <span class="token number">7</span> <span class="token number">9</span> <span class="token number">14</span> <span class="token number">6</span> <span class="token number">7</span> <span class="token number">8</span> <span class="token number">12</span> <span class="token number">14</span> <span class="token number">4</span> <span class="token number">15</span> <span class="token number">10</span> <span class="token number">14</span> <span class="token number">13</span> <span class="token number">15</span> <span class="token number">11</span> <span class="token number">14</span><span class="token punctuation">]</span><span class="token punctuation">;</span><span class="token number">j</span> <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">1</span> <span class="token number">2</span> <span class="token number">2</span> <span class="token number">3</span> <span class="token number">3</span> <span class="token number">4</span> <span class="token number">5</span> <span class="token number">5</span> <span class="token number">6</span> <span class="token number">6</span> <span class="token number">7</span> <span class="token number">7</span> <span class="token number">8</span> <span class="token number">8</span> <span class="token number">9</span> <span class="token number">9</span> <span class="token number">10</span> <span class="token number">10</span> <span class="token number">10</span> <span class="token number">10</span> <span class="token number">10</span> <span class="token number">11</span> <span class="token number">11</span> <span class="token number">11</span> <span class="token number">11</span> <span class="token number">11</span> <span class="token number">12</span> <span class="token number">12</span> <span class="token number">13</span> <span class="token number">13</span> <span class="token number">14</span> <span class="token number">14</span> <span class="token number">15</span> <span class="token number">15</span><span class="token punctuation">]</span><span class="token punctuation">;</span>A <span class="token operator">=</span> <span class="token function">sparse</span><span class="token punctuation">(</span><span class="token number">i</span><span class="token punctuation">,</span> <span class="token number">j</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> n<span class="token punctuation">,</span> n<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token function">full</span><span class="token punctuation">(</span>A<span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>We also know that <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.439ex" xmlns="http://www.w3.org/2000/svg" width="1.041ex" height="1.439ex" role="img" focusable="false" viewBox="0 -442 460 636" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D45E" d="M33 157Q33 258 109 349T280 441Q340 441 372 389Q373 390 377 395T388 406T404 418Q438 442 450 442Q454 442 457 439T460 434Q460 425 391 149Q320 -135 320 -139Q320 -147 365 -148H390Q396 -156 396 -157T393 -175Q389 -188 383 -194H370Q339 -192 262 -192Q234 -192 211 -192T174 -192T157 -193Q143 -193 143 -185Q143 -182 145 -170Q149 -154 152 -151T172 -148Q220 -148 230 -141Q238 -136 258 -53T279 32Q279 33 272 29Q224 -10 172 -10Q117 -10 75 30T33 157ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D45E"></use></g></g></g></svg></mjx-container>, <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="1.357ex" height="1.025ex" role="img" focusable="false" viewBox="0 -442 600 453" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D45B"></use></g></g></g></svg></mjx-container>, <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.666ex" xmlns="http://www.w3.org/2000/svg" width="3.021ex" height="2.286ex" role="img" focusable="false" viewBox="0 -716 1335.3 1010.2" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"></path><path id="MJX-1-TEX-I-1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path><path id="MJX-1-TEX-I-1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D434"></use></g><g data-mml-node="TeXAtom" transform="translate(750, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D456"></use></g><g data-mml-node="mi" transform="translate(345, 0)"><use xlink:href="#MJX-1-TEX-I-1D457"></use></g></g></g></g></g></svg></mjx-container> and <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="6.698ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 2960.4 1000" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-N-28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path><path id="MJX-1-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-1-TEX-N-2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path><path id="MJX-1-TEX-I-1D45E" d="M33 157Q33 258 109 349T280 441Q340 441 372 389Q373 390 377 395T388 406T404 418Q438 442 450 442Q454 442 457 439T460 434Q460 425 391 149Q320 -135 320 -139Q320 -147 365 -148H390Q396 -156 396 -157T393 -175Q389 -188 383 -194H370Q339 -192 262 -192Q234 -192 211 -192T174 -192T157 -193Q143 -193 143 -185Q143 -182 145 -170Q149 -154 152 -151T172 -148Q220 -148 230 -141Q238 -136 258 -53T279 32Q279 33 272 29Q224 -10 172 -10Q117 -10 75 30T33 157ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path><path id="MJX-1-TEX-N-29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mo"><use xlink:href="#MJX-1-TEX-N-28"></use></g><g data-mml-node="mn" transform="translate(389, 0)"><use xlink:href="#MJX-1-TEX-N-31"></use></g><g data-mml-node="mo" transform="translate(1111.2, 0)"><use xlink:href="#MJX-1-TEX-N-2212"></use></g><g data-mml-node="mi" transform="translate(2111.4, 0)"><use xlink:href="#MJX-1-TEX-I-1D45E"></use></g><g data-mml-node="mo" transform="translate(2571.4, 0)"><use xlink:href="#MJX-1-TEX-N-29"></use></g></g></g></svg></mjx-container> are non-negative, so <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="10.405ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 4599.2 1000" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D43A" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q492 659 471 656T418 643T357 615T294 567T236 496T189 394T158 260Q156 242 156 221Q156 173 170 136T206 79T256 45T308 28T353 24Q407 24 452 47T514 106Q517 114 529 161T541 214Q541 222 528 224T468 227H431Q425 233 425 235T427 254Q431 267 437 273H454Q494 271 594 271Q634 271 659 271T695 272T707 272Q721 272 721 263Q721 261 719 249Q714 230 709 228Q706 227 694 227Q674 227 653 224Q646 221 643 215T629 164Q620 131 614 108Q589 6 586 3Q584 1 581 1Q571 1 553 21T530 52Q530 53 528 52T522 47Q448 -22 322 -22Q201 -22 126 55T50 252Z"></path><path id="MJX-1-TEX-N-28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path><path id="MJX-1-TEX-I-1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path><path id="MJX-1-TEX-N-2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path><path id="MJX-1-TEX-I-1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path><path id="MJX-1-TEX-N-29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path><path id="MJX-1-TEX-N-3E" d="M84 520Q84 528 88 533T96 539L99 540Q106 540 253 471T544 334L687 265Q694 260 694 250T687 235Q685 233 395 96L107 -40H101Q83 -38 83 -20Q83 -19 83 -17Q82 -10 98 -1Q117 9 248 71Q326 108 378 132L626 250L378 368Q90 504 86 509Q84 513 84 520Z"></path><path id="MJX-1-TEX-N-30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D43A"></use></g><g data-mml-node="mo" transform="translate(786, 0)"><use xlink:href="#MJX-1-TEX-N-28"></use></g><g data-mml-node="mi" transform="translate(1175, 0)"><use xlink:href="#MJX-1-TEX-I-1D456"></use></g><g data-mml-node="mo" transform="translate(1520, 0)"><use xlink:href="#MJX-1-TEX-N-2C"></use></g><g data-mml-node="mi" transform="translate(1964.7, 0)"><use xlink:href="#MJX-1-TEX-I-1D457"></use></g><g data-mml-node="mo" transform="translate(2376.7, 0)"><use xlink:href="#MJX-1-TEX-N-29"></use></g><g data-mml-node="mo" transform="translate(3043.4, 0)"><use xlink:href="#MJX-1-TEX-N-3E"></use></g><g data-mml-node="mn" transform="translate(4099.2, 0)"><use xlink:href="#MJX-1-TEX-N-30"></use></g></g></g></svg></mjx-container>. Since the total of transition probability from a state <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="0.781ex" height="1.52ex" role="img" focusable="false" viewBox="0 -661 345 672" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D456"></use></g></g></g></svg></mjx-container> to all other states must be1, that is, by the construction of the sum of all non-negative elements inside each matrix column is equal to unity, thus this Google matrix $G$ is a stochastic matrix.</p><blockquote><p>In mathematics, a stocastic matrix is a square matrix used to describe the transitions of a Markov chain. Each of the entries is a non-negative real number representing a probability. It is also called a probability matrix, transition matrix, or Markov matrix.</p></blockquote><p>Next step is to contruct the matrix <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex" xmlns="http://www.w3.org/2000/svg" width="1.778ex" height="1.645ex" role="img" focusable="false" viewBox="0 -705 786 727" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D43A" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q492 659 471 656T418 643T357 615T294 567T236 496T189 394T158 260Q156 242 156 221Q156 173 170 136T206 79T256 45T308 28T353 24Q407 24 452 47T514 106Q517 114 529 161T541 214Q541 222 528 224T468 227H431Q425 233 425 235T427 254Q431 267 437 273H454Q494 271 594 271Q634 271 659 271T695 272T707 272Q721 272 721 263Q721 261 719 249Q714 230 709 228Q706 227 694 227Q674 227 653 224Q646 221 643 215T629 164Q620 131 614 108Q589 6 586 3Q584 1 581 1Q571 1 553 21T530 52Q530 53 528 52T522 47Q448 -22 322 -22Q201 -22 126 55T50 252Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D43A"></use></g></g></g></svg></mjx-container> for the network, and verify the given dominant eigenvector <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.439ex" xmlns="http://www.w3.org/2000/svg" width="1.138ex" height="1.439ex" role="img" focusable="false" viewBox="0 -442 503 636" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D45D"></use></g></g></g></svg></mjx-container>.</p><pre class="line-numbers language-matlab" data-language="matlab"><code class="language-matlab"><span class="token comment">%% Construct google matrix G</span>G <span class="token operator">=</span> <span class="token function">zeros</span><span class="token punctuation">(</span>n<span class="token punctuation">,</span> n<span class="token punctuation">)</span>q <span class="token operator">=</span> <span class="token number">0.15</span><span class="token keyword">for</span> <span class="token number">i</span> <span class="token operator">=</span> <span class="token number">1</span><span class="token operator">:</span>n    <span class="token keyword">for</span> <span class="token number">j</span> <span class="token operator">=</span> <span class="token number">1</span><span class="token operator">:</span>n        <span class="token function">G</span><span class="token punctuation">(</span><span class="token number">i</span><span class="token punctuation">,</span> <span class="token number">j</span><span class="token punctuation">)</span> <span class="token operator">=</span> q<span class="token operator">/</span>n <span class="token operator">+</span> <span class="token function">A</span><span class="token punctuation">(</span><span class="token number">j</span><span class="token punctuation">,</span> <span class="token number">i</span><span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token punctuation">(</span><span class="token number">1</span><span class="token operator">-</span>q<span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token function">sum</span><span class="token punctuation">(</span><span class="token function">A</span><span class="token punctuation">(</span><span class="token number">j</span><span class="token punctuation">,</span> <span class="token operator">:</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">end</span><span class="token keyword">end</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>We also define a power method to computeeigen values and eigenvectors.</p><pre class="line-numbers language-matlab" data-language="matlab"><code class="language-matlab"><span class="token keyword">function</span> <span class="token punctuation">[</span>V<span class="token punctuation">,</span> D<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token function">power</span><span class="token punctuation">(</span>A<span class="token punctuation">,</span> iter<span class="token punctuation">)</span>    <span class="token comment">% Power Method</span>    <span class="token punctuation">[</span>m<span class="token punctuation">,</span> n<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token function">size</span><span class="token punctuation">(</span>A<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token comment">% Random number generator</span>    <span class="token function">rng</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>    <span class="token comment">% Initial vector</span>    x0 <span class="token operator">=</span> <span class="token function">randn</span><span class="token punctuation">(</span>n<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    x <span class="token operator">=</span> x0<span class="token punctuation">;</span>    <span class="token keyword">for</span> <span class="token number">j</span> <span class="token operator">=</span> <span class="token number">1</span><span class="token operator">:</span>iter        u <span class="token operator">=</span> x <span class="token operator">/</span> <span class="token function">norm</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">;</span>        x <span class="token operator">=</span> A <span class="token operator">*</span> u<span class="token punctuation">;</span>        <span class="token comment">% Rayleigh Quotient</span>        lam <span class="token operator">=</span> u<span class="token operator">'</span> <span class="token operator">*</span> x<span class="token punctuation">;</span>        <span class="token function">L</span><span class="token punctuation">(</span><span class="token number">j</span><span class="token punctuation">)</span> <span class="token operator">=</span> lam<span class="token punctuation">;</span>    <span class="token keyword">end</span>    <span class="token comment">% Eigenvalue D</span>    D <span class="token operator">=</span> <span class="token function">L</span><span class="token punctuation">(</span><span class="token keyword">end</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    u <span class="token operator">=</span> x <span class="token operator">/</span> <span class="token function">norm</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token comment">% Eigenvector V</span>    V <span class="token operator">=</span> u<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>We use the above function to find the dominant eigenvector <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.439ex" xmlns="http://www.w3.org/2000/svg" width="1.138ex" height="1.439ex" role="img" focusable="false" viewBox="0 -442 503 636" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D45D"></use></g></g></g></svg></mjx-container>.</p><pre class="line-numbers language-matlab" data-language="matlab"><code class="language-matlab">iter <span class="token operator">=</span> <span class="token number">1000</span><span class="token punctuation">;</span><span class="token punctuation">[</span>p<span class="token punctuation">,</span> D<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token function">power</span><span class="token punctuation">(</span>G<span class="token punctuation">,</span> iter<span class="token punctuation">)</span><span class="token punctuation">;</span>p <span class="token operator">=</span> p <span class="token operator">/</span> <span class="token function">sum</span><span class="token punctuation">(</span>p<span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>The entries of <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.439ex" xmlns="http://www.w3.org/2000/svg" width="1.138ex" height="1.439ex" role="img" focusable="false" viewBox="0 -442 503 636" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D45D"></use></g></g></g></svg></mjx-container> represents the importance level of each 15 websites.</p><pre class="line-numbers language-matlab" data-language="matlab"><code class="language-matlab"><span class="token comment">%---------- Results ----------%</span>p <span class="token operator">=</span>     <span class="token number">0.0268</span>    <span class="token number">0.0299</span>    <span class="token number">0.0299</span>    <span class="token number">0.0268</span>    <span class="token number">0.0396</span>    <span class="token number">0.0396</span>    <span class="token number">0.0396</span>    <span class="token number">0.0396</span>    <span class="token number">0.0746</span>    <span class="token number">0.1063</span>    <span class="token number">0.1063</span>    <span class="token number">0.0746</span>    <span class="token number">0.1251</span>    <span class="token number">0.1163</span>    <span class="token number">0.1251</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="Jump-Probability"><a href="#Jump-Probability" class="headerlink" title="Jump Probability"></a>Jump Probability</h2><p>What is the purpose of the jump probability? There is some probability <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.439ex" xmlns="http://www.w3.org/2000/svg" width="1.041ex" height="1.439ex" role="img" focusable="false" viewBox="0 -442 460 636" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D45E" d="M33 157Q33 258 109 349T280 441Q340 441 372 389Q373 390 377 395T388 406T404 418Q438 442 450 442Q454 442 457 439T460 434Q460 425 391 149Q320 -135 320 -139Q320 -147 365 -148H390Q396 -156 396 -157T393 -175Q389 -188 383 -194H370Q339 -192 262 -192Q234 -192 211 -192T174 -192T157 -193Q143 -193 143 -185Q143 -182 145 -170Q149 -154 152 -151T172 -148Q220 -148 230 -141Q238 -136 258 -53T279 32Q279 33 272 29Q224 -10 172 -10Q117 -10 75 30T33 157ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D45E"></use></g></g></g></svg></mjx-container> such that at every step the walk has an <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.439ex" xmlns="http://www.w3.org/2000/svg" width="1.041ex" height="1.439ex" role="img" focusable="false" viewBox="0 -442 460 636" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D45E" d="M33 157Q33 258 109 349T280 441Q340 441 372 389Q373 390 377 395T388 406T404 418Q438 442 450 442Q454 442 457 439T460 434Q460 425 391 149Q320 -135 320 -139Q320 -147 365 -148H390Q396 -156 396 -157T393 -175Q389 -188 383 -194H370Q339 -192 262 -192Q234 -192 211 -192T174 -192T157 -193Q143 -193 143 -185Q143 -182 145 -170Q149 -154 152 -151T172 -148Q220 -148 230 -141Q238 -136 258 -53T279 32Q279 33 272 29Q224 -10 172 -10Q117 -10 75 30T33 157ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D45E"></use></g></g></g></svg></mjx-container> probability of jumping to a uniformly chosen random page. They tell us that <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.439ex" xmlns="http://www.w3.org/2000/svg" width="1.041ex" height="1.439ex" role="img" focusable="false" viewBox="0 -442 460 636" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D45E" d="M33 157Q33 258 109 349T280 441Q340 441 372 389Q373 390 377 395T388 406T404 418Q438 442 450 442Q454 442 457 439T460 434Q460 425 391 149Q320 -135 320 -139Q320 -147 365 -148H390Q396 -156 396 -157T393 -175Q389 -188 383 -194H370Q339 -192 262 -192Q234 -192 211 -192T174 -192T157 -193Q143 -193 143 -185Q143 -182 145 -170Q149 -154 152 -151T172 -148Q220 -148 230 -141Q238 -136 258 -53T279 32Q279 33 272 29Q224 -10 172 -10Q117 -10 75 30T33 157ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D45E"></use></g></g></g></svg></mjx-container> is set to some moderately small constant like 0.15. This is equivalent to adding a low-weight edge between every pair of vertices.</p><p>You may wonder what if we change the jump probability <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.439ex" xmlns="http://www.w3.org/2000/svg" width="1.041ex" height="1.439ex" role="img" focusable="false" viewBox="0 -442 460 636" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D45E" d="M33 157Q33 258 109 349T280 441Q340 441 372 389Q373 390 377 395T388 406T404 418Q438 442 450 442Q454 442 457 439T460 434Q460 425 391 149Q320 -135 320 -139Q320 -147 365 -148H390Q396 -156 396 -157T393 -175Q389 -188 383 -194H370Q339 -192 262 -192Q234 -192 211 -192T174 -192T157 -193Q143 -193 143 -185Q143 -182 145 -170Q149 -154 152 -151T172 -148Q220 -148 230 -141Q238 -136 258 -53T279 32Q279 33 272 29Q224 -10 172 -10Q117 -10 75 30T33 157ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D45E"></use></g></g></g></svg></mjx-container>? I describe the resulting changes below.</p><table><thead><tr><th>page</th><th align="center">q = 0.15</th><th align="right">q = 0</th><th align="right">q = 0.5</th></tr></thead><tbody><tr><td>1</td><td align="center">0.0268</td><td align="right">0.0154</td><td align="right">0.0467</td></tr><tr><td>2</td><td align="center">0.0299</td><td align="right">0.0116</td><td align="right">0.0540</td></tr><tr><td>3</td><td align="center">0.0299</td><td align="right">0.0116</td><td align="right">0.0540</td></tr><tr><td>4</td><td align="center">0.0268</td><td align="right">0.0154</td><td align="right">0.0467</td></tr><tr><td>5</td><td align="center">0.0396</td><td align="right">0.0309</td><td align="right">0.0536</td></tr><tr><td>6</td><td align="center">0.0396</td><td align="right">0.0309</td><td align="right">0.0536</td></tr><tr><td>7</td><td align="center">0.0396</td><td align="right">0.0309</td><td align="right">0.0536</td></tr><tr><td>8</td><td align="center">0.0396</td><td align="right">0.0309</td><td align="right">0.0536</td></tr><tr><td>9</td><td align="center">0.0746</td><td align="right">0.0811</td><td align="right">0.0676</td></tr><tr><td>10</td><td align="center">0.1063</td><td align="right">0.1100</td><td align="right">0.0946</td></tr><tr><td>11</td><td align="center">0.1063</td><td align="right">0.1100</td><td align="right">0.0946</td></tr><tr><td>12</td><td align="center">0.0746</td><td align="right">0.0811</td><td align="right">0.0676</td></tr><tr><td>13</td><td align="center">0.1251</td><td align="right">0.1467</td><td align="right">0.0905</td></tr><tr><td>14</td><td align="center">0.1163</td><td align="right">0.1467</td><td align="right">0.0786</td></tr><tr><td>15</td><td align="center">0.1163</td><td align="right">0.1467</td><td align="right">0.0905</td></tr></tbody></table><img src="/Hexo-Blog/2021/01/26/2021-01-26-quality-of-search-engine/jump.png" class=""><p>How does PageRank work for disjoint connected components? PageRank “fixes” this with the “teleportation” parameter, famously set to 0.15, that weakly connects all nodes to all other nodes. This can be thought of as an implicit link to all <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0" xmlns="http://www.w3.org/2000/svg" width="2.009ex" height="1.545ex" role="img" focusable="false" viewBox="0 -683 888 683" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D441"></use></g></g></g></svg></mjx-container> other nodes in the graph with weight <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.781ex" xmlns="http://www.w3.org/2000/svg" width="3.84ex" height="2.737ex" role="img" focusable="false" viewBox="0 -864.9 1697.2 1209.9" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-N-30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path><path id="MJX-1-TEX-N-2E" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z"></path><path id="MJX-1-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-1-TEX-N-35" d="M164 157Q164 133 148 117T109 101H102Q148 22 224 22Q294 22 326 82Q345 115 345 210Q345 313 318 349Q292 382 260 382H254Q176 382 136 314Q132 307 129 306T114 304Q97 304 95 310Q93 314 93 485V614Q93 664 98 664Q100 666 102 666Q103 666 123 658T178 642T253 634Q324 634 389 662Q397 666 402 666Q410 666 410 648V635Q328 538 205 538Q174 538 149 544L139 546V374Q158 388 169 396T205 412T256 420Q337 420 393 355T449 201Q449 109 385 44T229 -22Q148 -22 99 32T50 154Q50 178 61 192T84 210T107 214Q132 214 148 197T164 157Z"></path><path id="MJX-1-TEX-I-1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mfrac"><g data-mml-node="mn" transform="translate(220, 394) scale(0.707)"><use xlink:href="#MJX-1-TEX-N-30"></use><use xlink:href="#MJX-1-TEX-N-2E" transform="translate(500, 0)"></use><use xlink:href="#MJX-1-TEX-N-31" transform="translate(778, 0)"></use><use xlink:href="#MJX-1-TEX-N-35" transform="translate(1278, 0)"></use></g><g data-mml-node="mi" transform="translate(534.7, -345) scale(0.707)"><use xlink:href="#MJX-1-TEX-I-1D441"></use></g><rect width="1457.2" height="60" x="120" y="220"></rect></g></g></g></svg></mjx-container>. Making this teleportation factor higher should allow a higher probability of escaping from a small connected component.</p><p>Without transportation, PageRank doesn’t provide a way to compute the centralities of nodes in different components. That is, computing the PageRank of a node tells you its relative importance “within that component”, but doen’t tell anything about how different components compare.</p><p>Suppose that Page 7 in the network wanted to improve its page rank, compared with its competitor Page 6, by persuading Pages 2 and 12 to more prominently display its links to Page 7. Model this by replacing <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.65ex" xmlns="http://www.w3.org/2000/svg" width="3.854ex" height="2.27ex" role="img" focusable="false" viewBox="0 -716 1703.7 1003.2" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"></path><path id="MJX-1-TEX-N-32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path><path id="MJX-1-TEX-N-2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path><path id="MJX-1-TEX-N-37" d="M55 458Q56 460 72 567L88 674Q88 676 108 676H128V672Q128 662 143 655T195 646T364 644H485V605L417 512Q408 500 387 472T360 435T339 403T319 367T305 330T292 284T284 230T278 162T275 80Q275 66 275 52T274 28V19Q270 2 255 -10T221 -22Q210 -22 200 -19T179 0T168 40Q168 198 265 368Q285 400 349 489L395 552H302Q128 552 119 546Q113 543 108 522T98 479L95 458V455H55V458Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D434"></use></g><g data-mml-node="TeXAtom" transform="translate(750, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-32"></use></g><g data-mml-node="mo" transform="translate(500, 0)"><use xlink:href="#MJX-1-TEX-N-2C"></use></g><g data-mml-node="mn" transform="translate(778, 0)"><use xlink:href="#MJX-1-TEX-N-37"></use></g></g></g></g></g></svg></mjx-container> and <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.65ex" xmlns="http://www.w3.org/2000/svg" width="4.654ex" height="2.27ex" role="img" focusable="false" viewBox="0 -716 2057.2 1003.2" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"></path><path id="MJX-1-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-1-TEX-N-32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path><path id="MJX-1-TEX-N-2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path><path id="MJX-1-TEX-N-37" d="M55 458Q56 460 72 567L88 674Q88 676 108 676H128V672Q128 662 143 655T195 646T364 644H485V605L417 512Q408 500 387 472T360 435T339 403T319 367T305 330T292 284T284 230T278 162T275 80Q275 66 275 52T274 28V19Q270 2 255 -10T221 -22Q210 -22 200 -19T179 0T168 40Q168 198 265 368Q285 400 349 489L395 552H302Q128 552 119 546Q113 543 108 522T98 479L95 458V455H55V458Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-I-1D434"></use></g><g data-mml-node="TeXAtom" transform="translate(750, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><use xlink:href="#MJX-1-TEX-N-31"></use><use xlink:href="#MJX-1-TEX-N-32" transform="translate(500, 0)"></use></g><g data-mml-node="mo" transform="translate(1000, 0)"><use xlink:href="#MJX-1-TEX-N-2C"></use></g><g data-mml-node="mn" transform="translate(1278, 0)"><use xlink:href="#MJX-1-TEX-N-37"></use></g></g></g></g></g></svg></mjx-container> by 2 in the adjacency matrix. Let’s see whether this strategy succeed or not.</p><pre class="line-numbers language-matlab" data-language="matlab"><code class="language-matlab"><span class="token comment">%% Construct adjacency matrix A</span>n <span class="token operator">=</span> <span class="token number">15</span><span class="token number">i</span> <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">5</span> <span class="token number">1</span> <span class="token number">3</span> <span class="token number">2</span> <span class="token number">4</span> <span class="token number">8</span> <span class="token number">2</span> <span class="token number">9</span> <span class="token number">3</span> <span class="token number">9</span> <span class="token number">2</span> <span class="token number">12</span> <span class="token number">3</span> <span class="token number">12</span> <span class="token number">1</span> <span class="token number">13</span> <span class="token number">5</span> <span class="token number">6</span> <span class="token number">7</span> <span class="token number">9</span> <span class="token number">14</span> <span class="token number">6</span> <span class="token number">7</span> <span class="token number">8</span> <span class="token number">12</span> <span class="token number">14</span> <span class="token number">4</span> <span class="token number">15</span> <span class="token number">10</span> <span class="token number">14</span> <span class="token number">13</span> <span class="token number">15</span> <span class="token number">11</span> <span class="token number">14</span><span class="token punctuation">]</span><span class="token punctuation">;</span><span class="token number">j</span> <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">1</span> <span class="token number">2</span> <span class="token number">2</span> <span class="token number">3</span> <span class="token number">3</span> <span class="token number">4</span> <span class="token number">5</span> <span class="token number">5</span> <span class="token number">6</span> <span class="token number">6</span> <span class="token number">7</span> <span class="token number">7</span> <span class="token number">8</span> <span class="token number">8</span> <span class="token number">9</span> <span class="token number">9</span> <span class="token number">10</span> <span class="token number">10</span> <span class="token number">10</span> <span class="token number">10</span> <span class="token number">10</span> <span class="token number">11</span> <span class="token number">11</span> <span class="token number">11</span> <span class="token number">11</span> <span class="token number">11</span> <span class="token number">12</span> <span class="token number">12</span> <span class="token number">13</span> <span class="token number">13</span> <span class="token number">14</span> <span class="token number">14</span> <span class="token number">15</span> <span class="token number">15</span><span class="token punctuation">]</span><span class="token punctuation">;</span>A <span class="token operator">=</span> <span class="token function">sparse</span><span class="token punctuation">(</span><span class="token number">i</span><span class="token punctuation">,</span> <span class="token number">j</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> n<span class="token punctuation">,</span> n<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token function">A</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">)</span> <span class="token operator">=</span> <span class="token number">2</span><span class="token punctuation">;</span><span class="token function">A</span><span class="token punctuation">(</span><span class="token number">12</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">)</span> <span class="token operator">=</span> <span class="token number">2</span><span class="token punctuation">;</span><span class="token function">full</span><span class="token punctuation">(</span>A<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment">%% Construct Google Matrix</span>G <span class="token operator">=</span> <span class="token function">zeros</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>q <span class="token operator">=</span> <span class="token number">0.15</span><span class="token keyword">for</span> <span class="token number">i</span> <span class="token operator">=</span> <span class="token number">1</span><span class="token operator">:</span>n    <span class="token keyword">for</span> <span class="token number">j</span> <span class="token operator">=</span> <span class="token number">1</span><span class="token operator">:</span>n        <span class="token function">G</span><span class="token punctuation">(</span><span class="token number">i</span><span class="token punctuation">,</span> <span class="token number">j</span><span class="token punctuation">)</span> <span class="token operator">=</span> q<span class="token operator">/</span>n <span class="token operator">+</span> <span class="token function">A</span><span class="token punctuation">(</span><span class="token number">j</span><span class="token punctuation">,</span> <span class="token number">i</span><span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token punctuation">(</span><span class="token number">1</span><span class="token operator">-</span>q<span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token function">sum</span><span class="token punctuation">(</span><span class="token function">A</span><span class="token punctuation">(</span><span class="token number">j</span><span class="token punctuation">,</span> <span class="token operator">:</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">end</span><span class="token keyword">end</span><span class="token comment">%% Solve eigenvalue problem for the ranking vector</span>iter <span class="token operator">=</span> <span class="token number">1000</span><span class="token punctuation">;</span><span class="token punctuation">[</span>p<span class="token punctuation">,</span> D<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token function">power</span><span class="token punctuation">(</span>G<span class="token punctuation">,</span> iter<span class="token punctuation">)</span><span class="token punctuation">;</span>p <span class="token operator">=</span> p <span class="token operator">/</span> <span class="token function">sum</span><span class="token punctuation">(</span>p<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token function">bar</span><span class="token punctuation">(</span>p<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token function">title</span><span class="token punctuation">(</span><span class="token string">'PageRank with q = 0.15'</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><img src="/Hexo-Blog/2021/01/26/2021-01-26-quality-of-search-engine/page7.png" class=""><p>As you can see, the rank of Page 7 successfully exceed Page 6.</p><hr><p>Let’s study the effect of removing Page 10 from the network (All links from Page 10 are deleted). Which page ranks increase, and which decrease?</p><img src="/Hexo-Blog/2021/01/26/2021-01-26-quality-of-search-engine/page10.png" class=""><table><thead><tr><th>page</th><th align="center">q = 0.15</th><th align="right">q = 0.15 (Remove Page 10)</th><th align="right">Increase or Decrease</th></tr></thead><tbody><tr><td>1</td><td align="center">0.0268</td><td align="right">0.0462</td><td align="right">Increase</td></tr><tr><td>2</td><td align="center">0.0299</td><td align="right">0.0393</td><td align="right">Increase</td></tr><tr><td>3</td><td align="center">0.0299</td><td align="right">0.0341</td><td align="right">Increase</td></tr><tr><td>4</td><td align="center">0.0268</td><td align="right">0.0305</td><td align="right">Increase</td></tr><tr><td>5</td><td align="center">0.0396</td><td align="right">0.0426</td><td align="right">Increase</td></tr><tr><td>6</td><td align="center">0.0396</td><td align="right">0.0412</td><td align="right">Increase</td></tr><tr><td>7</td><td align="center">0.0396</td><td align="right">0.0496</td><td align="right">Increase</td></tr><tr><td>8</td><td align="center">0.0396</td><td align="right">0.0481</td><td align="right">Increase</td></tr><tr><td>9</td><td align="center">0.0746</td><td align="right">0.0506</td><td align="right">Decrease</td></tr><tr><td>10</td><td align="center">0.1063</td><td align="right">0.0100</td><td align="right">Decrease</td></tr><tr><td>11</td><td align="center">0.1063</td><td align="right">0.1669</td><td align="right">Increase</td></tr><tr><td>12</td><td align="center">0.0746</td><td align="right">0.1005</td><td align="right">Increase</td></tr><tr><td>13</td><td align="center">0.1251</td><td align="right">0.0492</td><td align="right">Decrease</td></tr><tr><td>14</td><td align="center">0.1163</td><td align="right">0.1085</td><td align="right">Decrease</td></tr><tr><td>15</td><td align="center">0.1163</td><td align="right">0.1826</td><td align="right">Increase</td></tr></tbody></table><p>There are 73% of the pages will increase, and 27% od pages will decrease. The reason is that, before removing Page 10, it can be seen that there are many pages (33% of the pages) point to Page 10, it means that Page 10 is relatively important to other websites.</p><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>PageRank is a powerful algorithm and has a wide application, such as ranking the football teams, ranking tweets in Twitter, or ranking track athletes. The merit of PageRank comes from its power in evaluating <em>network measures</em> in a connected system. Clearly, the surprisingly wide variety of these existing applications of PageRank point to a rich future for the algorithm in research contexts of all types. It seems intuitive that any problem in any field where a network comes into play might benefit from using PageRank.</p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ol><li>[<a href="https://www.sciencedirect.com/science/article/abs/pii/S016975529800110X">link</a>] S. Brin and L. Page, The anatomy of a large-scale hypertextual web search engine</li><li>[<a href="https://projecteuclid.org/download/pdf_1/euclid.involve/1513733537">link</a>] Laurie Zack, Ron Lamb and Sarah Ball, An Application of Google’s PageRank to NFL Rankings</li><li>[<a href="https://web.stanford.edu/class/msande233/handouts/lecture8.pdf">link</a>] Ashish Goel, Applications of PageRank to Recommendation Systems</li><li>[<a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0178458">link</a>] Clive B. Beggs, A Novel Application of PageRank and User Preference Algorithms for Assessing the Relative Performance of Track Athletes in Competition</li><li>[<a href="https://www.hindawi.com/journals/ijta/2019/8612021/">link</a>] Shahram Payandeh and Eddie Chiu, Application of Modified PageRank Algorithm for Anomaly Detection in Movements of Older Adults</li></ol>]]></content>
      
      
      <categories>
          
          <category> Math Programming </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MATLAB </tag>
            
            <tag> PageRank </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>GitHub Pages from Zero to Hero</title>
      <link href="/Hexo-Blog/2021/01/25/2021-01-25-github-page-from-zero-to-hero/"/>
      <url>/Hexo-Blog/2021/01/25/2021-01-25-github-page-from-zero-to-hero/</url>
      
        <content type="html"><![CDATA[<p>This article aimes to build website for your project, hosted directly from your GitHub repository. GitHub offers a web hosting service whereby you can serve a static website from a GitHub repository. This platform, GitHub Pages, can be used with CloudFlare whilst using a custom domain name. In this tutorial, I will demonstrate how to use GitHub, Freenom, and Cloudflare together. By taking advantage of CloudFlare’s global network, you can utilise our CDN service to improve your site’s performance and security.</p><h1 id="Jekyll"><a href="#Jekyll" class="headerlink" title="Jekyll"></a>Jekyll</h1><p>First, select your favorite <a href="https://jekyllthemes.io/free">Jekyll</a> themes template. There are several free themes options, you can pick whatever you want. Some of them charges a bit, it really depends on you!</p><p>Second, fork the project you pick by clicking the <code>Fork</code> button at the top right corner of the page. Forking means that you now copied this entired project and all the files into your repository.</p><p>Third, name the preoject to <code>&lt;yourusername&gt;.github.io</code>. Click on <code>Settings</code> at the top and on that page you’ll have option to rename the project. After doing so, you will automatically set up your GitHub Pages.</p><h2 id="Configuration"><a href="#Configuration" class="headerlink" title="Configuration"></a>Configuration</h2><ol><li>Pull your repository to your local folder.</li></ol><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token function">git</span> clone git@github.com:<span class="token operator">&lt;</span>github_account_name<span class="token operator">></span>/<span class="token operator">&lt;</span>repo_name<span class="token operator">></span>.git<span class="token builtin class-name">cd</span> <span class="token operator">&lt;</span>repo_name<span class="token operator">></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><ol start="2"><li><p>Customize your website settings. Edit the <code>_config.yml</code> file to change any settings you want.</p></li><li><p>If you want to run your website locally, you should follow instructions from <a href="https://beautifuljekyll.com/getstarted/#install-steps-hard">here</a>. This is hard to set up initially, but once you complete all instuctions, it makes it super easy to keep your site up to date and you can debug your code locally by using <code>bundle exec jekyll serve</code> command.</p></li></ol><h2 id="Start-blogging"><a href="#Start-blogging" class="headerlink" title="Start blogging!"></a>Start blogging!</h2><p>To add pages to your site, you can either write a markdown file (<code>.md</code>) or you can write an HTML file (<code>.html</code>). Files you create inside the <code>_posts</code> directory will be treated as blog entries. Note the format of the blog post files - they must follow the naming convention of <code>YEAR-MONTH-DAY-title.md</code>. After you successfully add your own post, you can delete the existing files inside <code>_posts</code> to remove the sample posts.</p><h2 id="Write-LaTeX-in-your-Post"><a href="#Write-LaTeX-in-your-Post" class="headerlink" title="Write LaTeX in your Post"></a>Write LaTeX in your Post</h2><p>If you want to display math equations, you have to enable LaTeX support in markdown. Make sure you add this script to the <code>&lt;head&gt;</code> in <code>_includes</code> folder:</p><pre class="line-numbers language-javascript" data-language="javascript"><code class="language-javascript"><span class="token operator">&lt;</span>script type<span class="token operator">=</span><span class="token string">"text/x-mathjax-config"</span><span class="token operator">></span>MathJax<span class="token punctuation">.</span>Hub<span class="token punctuation">.</span><span class="token function">Config</span><span class="token punctuation">(</span><span class="token punctuation">&#123;</span>  tex2jax<span class="token operator">:</span> <span class="token punctuation">&#123;</span>    skipTags<span class="token operator">:</span> <span class="token punctuation">[</span><span class="token string">'script'</span><span class="token punctuation">,</span> <span class="token string">'noscript'</span><span class="token punctuation">,</span> <span class="token string">'style'</span><span class="token punctuation">,</span> <span class="token string">'textarea'</span><span class="token punctuation">,</span> <span class="token string">'pre'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>    inlineMath<span class="token operator">:</span> <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token string">'$'</span><span class="token punctuation">,</span><span class="token string">'$'</span><span class="token punctuation">]</span><span class="token punctuation">]</span>  <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token operator">&lt;</span><span class="token operator">/</span>script<span class="token operator">></span><span class="token operator">&lt;</span>script src<span class="token operator">=</span><span class="token string">"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"</span> type<span class="token operator">=</span><span class="token string">"text/javascript"</span><span class="token operator">></span><span class="token operator">&lt;</span><span class="token operator">/</span>script<span class="token operator">></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="Get-Free-Domain"><a href="#Get-Free-Domain" class="headerlink" title="Get Free Domain"></a>Get Free Domain</h1><h2 id="Freenom"><a href="#Freenom" class="headerlink" title="Freenom"></a>Freenom</h2><p>Freenom is the world’s first and only free domain provider. You can set up or update certain DNS records and your repository settings to point the default domain for your GitHub Pages site to a custom domain.</p><ol><li>Go to <a href="https://www.freenom.com/">https://www.freenom.com</a></li><li>Check <code>Availability</code>.<img src="/Hexo-Blog/2021/01/25/2021-01-25-github-page-from-zero-to-hero/availability.png" class=""></li><li>If the domain name is available click <code>Get it now!</code> and the <code>Checkout</code>.<img src="/Hexo-Blog/2021/01/25/2021-01-25-github-page-from-zero-to-hero/freedomains.png" class=""></li><li>Set the period to 12 months the click <code>Continue</code>.<img src="/Hexo-Blog/2021/01/25/2021-01-25-github-page-from-zero-to-hero/checkout.png" class=""></li><li>Check <code>I have read and agree to the Terms &amp; Conditions</code>. Then click <code>Complete Order</code>.<img src="/Hexo-Blog/2021/01/25/2021-01-25-github-page-from-zero-to-hero/complete.png" class=""></li></ol><h2 id="Cloudflare"><a href="#Cloudflare" class="headerlink" title="Cloudflare"></a>Cloudflare</h2><p>Cloudflare is one of the biggest networks operating on the Internet. People use Cloudflare services for the purposes of increasing the security and performance of their web sites and services.</p><ol><li>Go to <a href="https://www.cloudflare.com/">https://www.cloudflare.com</a> and create an account.</li><li>Click <code>Add site</code> in <code>Account Dashboard</code> and put your site domain.<img src="/Hexo-Blog/2021/01/25/2021-01-25-github-page-from-zero-to-hero/addsite.png" class=""></li><li>Select <code>Free</code> plan and click <code>Confirm plan</code>.</li><li>Wait till the Cloudflare checking the exsiting DNS record and click <code>Continue</code>.</li><li>Cloudflare will give you two nameservers that you need to set in Freenom.<img src="/Hexo-Blog/2021/01/25/2021-01-25-github-page-from-zero-to-hero/cloudflare.png" class=""></li><li>Go back to Freenom <code>Click Service &gt; My Domains</code>.<img src="/Hexo-Blog/2021/01/25/2021-01-25-github-page-from-zero-to-hero/domain.png" class=""></li><li>Click <code>Management Tools &gt; Nameservers</code>.<img src="/Hexo-Blog/2021/01/25/2021-01-25-github-page-from-zero-to-hero/nameserve.png" class=""></li><li>Select <code>Use custom nameservers</code> and add the cloudflare nameservers, which you get from step 5, to custom name servers.</li><li>Finish Setting up your Domain on Cloudflare and go to the Domain Dashboard. Open the <code>Cloudflare Settings</code> for your domain, and change the SSL Setting to <code>Flexible SSL</code>.<img src="/Hexo-Blog/2021/01/25/2021-01-25-github-page-from-zero-to-hero/ssl.png" class=""></li></ol><h2 id="GitHub-Pages"><a href="#GitHub-Pages" class="headerlink" title="GitHub Pages"></a>GitHub Pages</h2><p>Go to github and select the repository you want to add custom domain and select settings. Scroll to bottom you will see section <code>Custom Domain</code>. Add your domain and save, now all are set and it will take a while, so please be patient.</p><img src="/Hexo-Blog/2021/01/25/2021-01-25-github-page-from-zero-to-hero/success.png" class=""><h2 id="Final-Step"><a href="#Final-Step" class="headerlink" title="Final Step"></a>Final Step</h2><h3 id="Letting-Search-Engines-know"><a href="#Letting-Search-Engines-know" class="headerlink" title="Letting Search Engines know"></a>Letting Search Engines know</h3><p>In your <code>_config.yml</code>, add these:</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">url: https://www.yoursite.com   <span class="token comment"># with the https protocol</span>enforce_ssl: www.yoursite.com   <span class="token comment"># without any protocol</span><span class="token comment"># For example, my configuration is this:</span>url: https://nlper.mlenforce_ssl: nlper.ml<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>Make sure you have a canonical link in your <code>&lt;head&gt;</code>:</p><pre class="line-numbers language-javascript" data-language="javascript"><code class="language-javascript"><span class="token operator">&lt;</span>link rel<span class="token operator">=</span><span class="token string">"canonical"</span> href<span class="token operator">=</span><span class="token string">" &#123; &#123; site.url &#125; &#125;&#123; &#123; page.url &#125; &#125;"</span> <span class="token operator">/</span><span class="token operator">></span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="Redirect-users-to-https"><a href="#Redirect-users-to-https" class="headerlink" title="Redirect users to https"></a>Redirect users to https</h3><p>Add this script to the very top of your <code>&lt;head&gt;</code>:</p><pre class="line-numbers language-javascript" data-language="javascript"><code class="language-javascript"><span class="token operator">&lt;</span>script type<span class="token operator">=</span><span class="token string">"text/javascript"</span><span class="token operator">></span>    <span class="token keyword">var</span> host <span class="token operator">=</span> <span class="token string">"yoursite.com"</span><span class="token punctuation">;</span>    <span class="token keyword">if</span> <span class="token punctuation">(</span><span class="token punctuation">(</span>host <span class="token operator">==</span> window<span class="token punctuation">.</span>location<span class="token punctuation">.</span>host<span class="token punctuation">)</span> <span class="token operator">&amp;&amp;</span> <span class="token punctuation">(</span>window<span class="token punctuation">.</span>location<span class="token punctuation">.</span>protocol <span class="token operator">!=</span> <span class="token string">"https:"</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        window<span class="token punctuation">.</span>location<span class="token punctuation">.</span>protocol <span class="token operator">=</span> <span class="token string">"https"</span><span class="token punctuation">;</span><span class="token operator">&lt;</span><span class="token operator">/</span>script<span class="token operator">></span> <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>After Done You can enter your Domain name on browser and check. Hope this article help you build your website! Enjoy your Free Domain ❤</p>]]></content>
      
      
      <categories>
          
          <category> Installation </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GitHub </tag>
            
            <tag> Jekyll </tag>
            
            <tag> Freenom </tag>
            
            <tag> Cloudflare </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Simplest way to Build Web Crawler</title>
      <link href="/Hexo-Blog/2021/01/25/2021-01-25-simplest-way-to-build-web-crawler/"/>
      <url>/Hexo-Blog/2021/01/25/2021-01-25-simplest-way-to-build-web-crawler/</url>
      
        <content type="html"><![CDATA[<p>A web crawler, sometimes called a spiderbot or scraper, is an internet bot that systematically browses the net. We can get the information we need without copy-paste. The goal of this article is to let you know how I scrape web and store it into database or csv file.</p><img src="/Hexo-Blog/2021/01/25/2021-01-25-simplest-way-to-build-web-crawler/database.png" class=""><h1 id="Build-a-Web-Crawler-as-a-Beginner"><a href="#Build-a-Web-Crawler-as-a-Beginner" class="headerlink" title="Build a Web Crawler as a Beginner"></a>Build a Web Crawler as a Beginner</h1><p>Wrting scripts with computer languanges are predominantly used by programmers. However, beginners may not know how to set up environment for their first program, and also not able to debug in a text editor or IDE. I suggest you to read through this <a href="/Hexo-Blog/2020/12/31/2020-12-31-conda-environment-setup/" title="article">article</a>) first before kickstarting this tutorial.</p><p>Assume that you have already built your virtual environment using conda, open your notebook using <code>jupyter notebook</code> in command line.</p><h2 id="Crawler-Workflow"><a href="#Crawler-Workflow" class="headerlink" title="Crawler Workflow"></a>Crawler Workflow</h2><p>In general, web scraping using Python invloves three main steps:</p><ol><li>Send a request to the URL to the website.</li><li>Since the website are usually written in HTML, we need to parse the website to a tree structure.</li><li>Store our result in a dictionary or list for future use.</li></ol><p>Next, I will demonstrate the three steps above to scrape volleyball game stats from <a href="https://worldofvolley.com/">WorldofVolley</a> website. Let’s get started!</p><h2 id="Time-to-Crawl"><a href="#Time-to-Crawl" class="headerlink" title="Time to Crawl"></a>Time to Crawl</h2><p>For stats <a href="https://worldofvolley.com/statistics/game-statistics.html?orderBy=name">table</a> in the website, you will see every rows represents different match on different dates. You can’t help but ask: So how do you see the data that the browser secretly downloads? The answer is the <code>F12</code> shortcut in Google Chrome, or you can open the developer tools that come with Chrome through the right-click menu <code>inspect</code>.The developer tools will appear on the left side of the browser page or below (adjustable), it looks like this:</p><img src="/Hexo-Blog/2021/01/25/2021-01-25-simplest-way-to-build-web-crawler/worldofvolley.png" class=""><h3 id="Step-1"><a href="#Step-1" class="headerlink" title="Step 1"></a>Step 1</h3><p>Basically, we only need two libraries to scrape most of the websites: <code>Requests</code> and <code>BeautifulSoup4</code>. Install them first and load these two libraries.</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># Install it first</span>! pip install requests! pip install beautifulsoup4<span class="token comment"># Load packages</span><span class="token keyword">import</span> requests<span class="token keyword">from</span> bs4 <span class="token keyword">import</span> BeautifulSoup<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>We use <code>requests.get()</code> method since we are sending a GET request to the specific url, in this case, to <code>https://worldofvolley.com/statistics/game-statistics.html?orderBy=name</code>. </p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># Install it first</span>url <span class="token operator">=</span> <span class="token string">"https://worldofvolley.com/statistics/game-statistics.html?orderBy=name"</span>source <span class="token operator">=</span> requests<span class="token punctuation">.</span>get<span class="token punctuation">(</span>url<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h3 id="Step-2"><a href="#Step-2" class="headerlink" title="Step 2"></a>Step 2</h3><p><code>BeautifulSoup</code> is a Python library for pulling data out of HTML and XML files. HTML will look something like this:</p><pre class="line-numbers language-html" data-language="html"><code class="language-html"><span class="token doctype"><span class="token punctuation">&lt;!</span><span class="token name">doctype</span> <span class="token name">html</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>html</span> <span class="token attr-name">lang</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>en<span class="token punctuation">"</span></span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>head</span><span class="token punctuation">></span></span>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>meta</span> <span class="token attr-name">charset</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>utf-8<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>title</span><span class="token punctuation">></span></span>The HTML5 Herald<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>title</span><span class="token punctuation">></span></span>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>meta</span> <span class="token attr-name">name</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>description<span class="token punctuation">"</span></span> <span class="token attr-name">content</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>The HTML5 Herald<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>meta</span> <span class="token attr-name">name</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>author<span class="token punctuation">"</span></span> <span class="token attr-name">content</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>SitePoint<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>link</span> <span class="token attr-name">rel</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>stylesheet<span class="token punctuation">"</span></span> <span class="token attr-name">href</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>css/styles.css?v=1.0<span class="token punctuation">"</span></span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>head</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>body</span><span class="token punctuation">></span></span>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>script</span> <span class="token attr-name">src</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>js/scripts.js<span class="token punctuation">"</span></span><span class="token punctuation">></span></span><span class="token script"></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>script</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>body</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>html</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>Below is a simple illustration.</p><pre class="line-numbers language-html" data-language="html"><code class="language-html"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>tag</span> <span class="token attr-name">attribute</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>value<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>Element content<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>tag</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><code>BeautifulSoup</code> parses anything you give it, and does the tree traversal stuff for you. </p><pre class="line-numbers language-python" data-language="python"><code class="language-python">soup <span class="token operator">=</span> BeautifulSoup<span class="token punctuation">(</span>source<span class="token punctuation">.</span>text<span class="token punctuation">,</span> <span class="token string">"html.parser"</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>If you do <code>print(soup)</code> and <code>print(source)</code>, it looks the same, but the source is just plain the response data, and the soup is an object that we can actually interact with, by tag, now. Inside devtools subwindow, you can see that every rows of the data we need is inside <code>&lt;div id=&quot;stats&quot;&gt;...&lt;/div&gt;</code> element.</p><img src="/Hexo-Blog/2021/01/25/2021-01-25-simplest-way-to-build-web-crawler/soup.png" class=""><p>As you can see, there are exactly 10 rows of data in the table, also there are 10 <code>&lt;tr&gt;...&lt;/tr&gt;</code> elements in devtool subwindow.</p><img src="/Hexo-Blog/2021/01/25/2021-01-25-simplest-way-to-build-web-crawler/table.png" class=""><pre class="line-numbers language-python" data-language="python"><code class="language-python">stats <span class="token operator">=</span> soup<span class="token punctuation">.</span>find<span class="token punctuation">(</span><span class="token string">"div"</span><span class="token punctuation">,</span> attrs<span class="token operator">=</span><span class="token punctuation">&#123;</span><span class="token string">"id"</span><span class="token punctuation">:</span> <span class="token string">"stats"</span><span class="token punctuation">&#125;</span><span class="token punctuation">)</span>table <span class="token operator">=</span> stats<span class="token punctuation">.</span>find<span class="token punctuation">(</span><span class="token string">"tbody"</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>Right now, do <code>print(table)</code>, Voila!</p><h3 id="Step-3"><a href="#Step-3" class="headerlink" title="Step 3"></a>Step 3</h3><p>In step 3, we need to iterate through every rows of them.</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">result <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token keyword">for</span> row <span class="token keyword">in</span> table<span class="token punctuation">.</span>find_all<span class="token punctuation">(</span><span class="token string">"tr"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    tds <span class="token operator">=</span> row<span class="token punctuation">(</span><span class="token string">"td"</span><span class="token punctuation">)</span>    date <span class="token operator">=</span> tds<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>find<span class="token punctuation">(</span><span class="token string">"span"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>text <span class="token operator">+</span> tds<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>find<span class="token punctuation">(</span><span class="token string">"strong"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>text    contries <span class="token operator">=</span> <span class="token punctuation">[</span>x<span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">for</span> x <span class="token keyword">in</span> tds<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>find_all<span class="token punctuation">(</span>text<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span> <span class="token keyword">if</span> x <span class="token operator">!=</span> <span class="token string">"\n"</span><span class="token punctuation">]</span>    scores <span class="token operator">=</span> <span class="token punctuation">[</span>x<span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">for</span> x <span class="token keyword">in</span> tds<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">.</span>find_all<span class="token punctuation">(</span>text<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span> <span class="token keyword">if</span> x <span class="token operator">!=</span> <span class="token string">"\n"</span><span class="token punctuation">]</span>    set_1 <span class="token operator">=</span> <span class="token punctuation">[</span>x<span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">for</span> x <span class="token keyword">in</span> tds<span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">.</span>find_all<span class="token punctuation">(</span>text<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span> <span class="token keyword">if</span> x <span class="token operator">!=</span> <span class="token string">"\n"</span><span class="token punctuation">]</span>    set_2 <span class="token operator">=</span> <span class="token punctuation">[</span>x<span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">for</span> x <span class="token keyword">in</span> tds<span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">.</span>find_all<span class="token punctuation">(</span>text<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span> <span class="token keyword">if</span> x <span class="token operator">!=</span> <span class="token string">"\n"</span><span class="token punctuation">]</span>    set_3 <span class="token operator">=</span> <span class="token punctuation">[</span>x<span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">for</span> x <span class="token keyword">in</span> tds<span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">.</span>find_all<span class="token punctuation">(</span>text<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span> <span class="token keyword">if</span> x <span class="token operator">!=</span> <span class="token string">"\n"</span><span class="token punctuation">]</span>    set_4 <span class="token operator">=</span> <span class="token punctuation">[</span>x<span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">for</span> x <span class="token keyword">in</span> tds<span class="token punctuation">[</span><span class="token number">6</span><span class="token punctuation">]</span><span class="token punctuation">.</span>find_all<span class="token punctuation">(</span>text<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span> <span class="token keyword">if</span> x <span class="token operator">!=</span> <span class="token string">"\n"</span><span class="token punctuation">]</span>    set_5 <span class="token operator">=</span> <span class="token punctuation">[</span>x<span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">for</span> x <span class="token keyword">in</span> tds<span class="token punctuation">[</span><span class="token number">7</span><span class="token punctuation">]</span><span class="token punctuation">.</span>find_all<span class="token punctuation">(</span>text<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span> <span class="token keyword">if</span> x <span class="token operator">!=</span> <span class="token string">"\n"</span><span class="token punctuation">]</span>    result<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token punctuation">[</span>date<span class="token punctuation">,</span> contries<span class="token punctuation">,</span> scores<span class="token punctuation">,</span> set_1<span class="token punctuation">,</span> set_2<span class="token punctuation">,</span> set_3<span class="token punctuation">,</span> set_4<span class="token punctuation">,</span> set_5<span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>Finding every texts inside tag <code>&lt;td&gt;</code> is a fairly common task. If we only want one element, we could use <code>find()</code> function to get what we want. In the case above, such as TEAMS row, there are two countries in the cell, what if we wanted to find them all? We could simply use <code>find_all(text=True)</code> function, this will get all the texts under each <code>tds</code>.</p><p>Finally, store the <code>result</code> inside a pandas DataFrame object.</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> pandas <span class="token keyword">as</span> pdcolumns <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">"date"</span><span class="token punctuation">,</span> <span class="token string">"contries"</span><span class="token punctuation">,</span> <span class="token string">"scores"</span><span class="token punctuation">,</span> <span class="token string">"set_1"</span><span class="token punctuation">,</span> <span class="token string">"set_2"</span><span class="token punctuation">,</span> <span class="token string">"set_3"</span><span class="token punctuation">,</span> <span class="token string">"set_4"</span><span class="token punctuation">,</span> <span class="token string">"set_5"</span><span class="token punctuation">]</span>data <span class="token operator">=</span> pd<span class="token punctuation">.</span>DataFrame<span class="token punctuation">(</span>result<span class="token punctuation">,</span> columns<span class="token operator">=</span>columns<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><img src="/Hexo-Blog/2021/01/25/2021-01-25-simplest-way-to-build-web-crawler/pd.png" class=""><p>Save it to csv file: </p><pre class="line-numbers language-python" data-language="python"><code class="language-python">data<span class="token punctuation">.</span>to_csv<span class="token punctuation">(</span><span class="token string">"volley.csv"</span><span class="token punctuation">,</span> index<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>Or save it to database (I suppose you have a <code>&lt;database_name&gt;.db</code> file in your working directory).</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">save_to_database</span><span class="token punctuation">(</span>data<span class="token punctuation">,</span> db_name<span class="token operator">=</span><span class="token string">"volley.db"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    database_path <span class="token operator">=</span> <span class="token string-interpolation"><span class="token string">f'sqlite:///data/</span><span class="token interpolation"><span class="token punctuation">&#123;</span>db_name<span class="token punctuation">&#125;</span></span><span class="token string">'</span></span>    engine <span class="token operator">=</span> create_engine<span class="token punctuation">(</span>database_path<span class="token punctuation">,</span> echo<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>    data<span class="token punctuation">.</span>to_sql<span class="token punctuation">(</span><span class="token string">"volley"</span><span class="token punctuation">,</span> con<span class="token operator">=</span>engine<span class="token punctuation">,</span> if_exists<span class="token operator">=</span><span class="token string">'replace'</span><span class="token punctuation">)</span>    engine<span class="token punctuation">.</span>dispose<span class="token punctuation">(</span><span class="token punctuation">)</span>save_to_database<span class="token punctuation">(</span>data<span class="token punctuation">,</span> db_name<span class="token operator">=</span><span class="token string">"volley.db"</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="Some-Advance-Tips"><a href="#Some-Advance-Tips" class="headerlink" title="Some Advance Tips"></a>Some Advance Tips</h2><h3 id="Fake-a-Browser-Visit"><a href="#Fake-a-Browser-Visit" class="headerlink" title="Fake a Browser Visit"></a>Fake a Browser Visit</h3><p>Some of the developers of the website had made some blocks for people who wants to visit their webiste. One workaround is to provdie a <code>user-agent</code> header inside <code>requests()</code>.</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">headers <span class="token operator">=</span> <span class="token punctuation">&#123;</span><span class="token string">'User-Agent'</span><span class="token punctuation">:</span> <span class="token string">'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'</span><span class="token punctuation">&#125;</span>source <span class="token operator">=</span> requests<span class="token punctuation">.</span>get<span class="token punctuation">(</span>url<span class="token punctuation">,</span> headers<span class="token operator">=</span>headers<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h3 id="Delays"><a href="#Delays" class="headerlink" title="Delays"></a>Delays</h3><p>It’s always good to put some delay between requests. In our case, we don’t need it if we only send our requests once. What if we need to scrape every pages on the website, then we probably need this. </p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> time <span class="token keyword">import</span> timetime<span class="token punctuation">.</span>sleep<span class="token punctuation">(</span>delay<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h1 id="Pull-it-All-Together"><a href="#Pull-it-All-Together" class="headerlink" title="Pull it All Together"></a>Pull it All Together</h1><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">PyCrawler</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""    Crawler for https://worldofvolley.com        Parameters    ----------    pages: int        Total pages you want to crawl.        Returns    -------    data: pd.DataFrame        A pd.DataFrame contains "date", "contries", "scores", "set_1", "set_2", "set_3", "set_4", "set_5".    """</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> pages<span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>pages <span class="token operator">=</span> pages        self<span class="token punctuation">.</span>headers <span class="token operator">=</span> <span class="token punctuation">&#123;</span><span class="token string">'User-Agent'</span><span class="token punctuation">:</span> 'Mozilla<span class="token operator">/</span><span class="token number">5.0</span> <span class="token punctuation">(</span>Macintosh<span class="token punctuation">;</span> Intel Mac OS X 10_10_1<span class="token punctuation">)</span> \                         AppleWebKit<span class="token operator">/</span><span class="token number">537.36</span> <span class="token punctuation">(</span>KHTML<span class="token punctuation">,</span> like Gecko<span class="token punctuation">)</span> Chrome<span class="token operator">/</span><span class="token number">39.0</span><span class="token number">.2171</span><span class="token number">.95</span> Safari<span class="token operator">/</span><span class="token number">537.36</span>'<span class="token punctuation">&#125;</span>            <span class="token keyword">def</span> <span class="token function">run</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        dfs <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>        <span class="token keyword">for</span> page <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>pages<span class="token punctuation">)</span><span class="token punctuation">:</span>            url <span class="token operator">=</span> <span class="token string-interpolation"><span class="token string">f"https://worldofvolley.com/statistics/game-statistics.html?orderBy=name&amp;position=</span><span class="token interpolation"><span class="token punctuation">&#123;</span>page<span class="token punctuation">&#125;</span></span><span class="token string">"</span></span>            source <span class="token operator">=</span> requests<span class="token punctuation">.</span>get<span class="token punctuation">(</span>url<span class="token punctuation">,</span> headers<span class="token operator">=</span>self<span class="token punctuation">.</span>headers<span class="token punctuation">)</span>            soup <span class="token operator">=</span> BeautifulSoup<span class="token punctuation">(</span>source<span class="token punctuation">.</span>text<span class="token punctuation">,</span> <span class="token string">"html.parser"</span><span class="token punctuation">)</span>            stats <span class="token operator">=</span> soup<span class="token punctuation">.</span>find<span class="token punctuation">(</span><span class="token string">"div"</span><span class="token punctuation">,</span> attrs<span class="token operator">=</span><span class="token punctuation">&#123;</span><span class="token string">"id"</span><span class="token punctuation">:</span> <span class="token string">"stats"</span><span class="token punctuation">&#125;</span><span class="token punctuation">)</span>            table <span class="token operator">=</span> stats<span class="token punctuation">.</span>find<span class="token punctuation">(</span><span class="token string">"tbody"</span><span class="token punctuation">)</span>            result <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>            <span class="token keyword">for</span> row <span class="token keyword">in</span> table<span class="token punctuation">.</span>find_all<span class="token punctuation">(</span><span class="token string">"tr"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>                tds <span class="token operator">=</span> row<span class="token punctuation">(</span><span class="token string">"td"</span><span class="token punctuation">)</span>                date <span class="token operator">=</span> tds<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>find<span class="token punctuation">(</span><span class="token string">"span"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>text <span class="token operator">+</span> tds<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>find<span class="token punctuation">(</span><span class="token string">"strong"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>text                contries <span class="token operator">=</span> <span class="token punctuation">[</span>x<span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">for</span> x <span class="token keyword">in</span> tds<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>find_all<span class="token punctuation">(</span>text<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span> <span class="token keyword">if</span> x <span class="token operator">!=</span> <span class="token string">"\n"</span><span class="token punctuation">]</span>                scores <span class="token operator">=</span> <span class="token punctuation">[</span>x<span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">for</span> x <span class="token keyword">in</span> tds<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">.</span>find_all<span class="token punctuation">(</span>text<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span> <span class="token keyword">if</span> x <span class="token operator">!=</span> <span class="token string">"\n"</span><span class="token punctuation">]</span>                set_1 <span class="token operator">=</span> <span class="token punctuation">[</span>x<span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">for</span> x <span class="token keyword">in</span> tds<span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">.</span>find_all<span class="token punctuation">(</span>text<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span> <span class="token keyword">if</span> x <span class="token operator">!=</span> <span class="token string">"\n"</span><span class="token punctuation">]</span>                set_2 <span class="token operator">=</span> <span class="token punctuation">[</span>x<span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">for</span> x <span class="token keyword">in</span> tds<span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">.</span>find_all<span class="token punctuation">(</span>text<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span> <span class="token keyword">if</span> x <span class="token operator">!=</span> <span class="token string">"\n"</span><span class="token punctuation">]</span>                set_3 <span class="token operator">=</span> <span class="token punctuation">[</span>x<span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">for</span> x <span class="token keyword">in</span> tds<span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">.</span>find_all<span class="token punctuation">(</span>text<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span> <span class="token keyword">if</span> x <span class="token operator">!=</span> <span class="token string">"\n"</span><span class="token punctuation">]</span>                set_4 <span class="token operator">=</span> <span class="token punctuation">[</span>x<span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">for</span> x <span class="token keyword">in</span> tds<span class="token punctuation">[</span><span class="token number">6</span><span class="token punctuation">]</span><span class="token punctuation">.</span>find_all<span class="token punctuation">(</span>text<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span> <span class="token keyword">if</span> x <span class="token operator">!=</span> <span class="token string">"\n"</span><span class="token punctuation">]</span>                set_5 <span class="token operator">=</span> <span class="token punctuation">[</span>x<span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">for</span> x <span class="token keyword">in</span> tds<span class="token punctuation">[</span><span class="token number">7</span><span class="token punctuation">]</span><span class="token punctuation">.</span>find_all<span class="token punctuation">(</span>text<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span> <span class="token keyword">if</span> x <span class="token operator">!=</span> <span class="token string">"\n"</span><span class="token punctuation">]</span>                result<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token punctuation">[</span>date<span class="token punctuation">,</span> contries<span class="token punctuation">,</span> scores<span class="token punctuation">,</span> set_1<span class="token punctuation">,</span> set_2<span class="token punctuation">,</span> set_3<span class="token punctuation">,</span> set_4<span class="token punctuation">,</span> set_5<span class="token punctuation">]</span><span class="token punctuation">)</span>            columns <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">"date"</span><span class="token punctuation">,</span> <span class="token string">"contries"</span><span class="token punctuation">,</span> <span class="token string">"scores"</span><span class="token punctuation">,</span> <span class="token string">"set_1"</span><span class="token punctuation">,</span> <span class="token string">"set_2"</span><span class="token punctuation">,</span> <span class="token string">"set_3"</span><span class="token punctuation">,</span> <span class="token string">"set_4"</span><span class="token punctuation">,</span> <span class="token string">"set_5"</span><span class="token punctuation">]</span>            dfs<span class="token punctuation">.</span>append<span class="token punctuation">(</span>pd<span class="token punctuation">.</span>DataFrame<span class="token punctuation">(</span>result<span class="token punctuation">,</span> columns<span class="token operator">=</span>columns<span class="token punctuation">)</span><span class="token punctuation">)</span>        data <span class="token operator">=</span> pd<span class="token punctuation">.</span>concat<span class="token punctuation">(</span>dfs<span class="token punctuation">)</span>        <span class="token keyword">return</span> datacrawler <span class="token operator">=</span> PyCrawler<span class="token punctuation">(</span>pages<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">)</span>data <span class="token operator">=</span> crawler<span class="token punctuation">.</span>run<span class="token punctuation">(</span><span class="token punctuation">)</span>data<span class="token punctuation">.</span>to_csv<span class="token punctuation">(</span><span class="token string">"volley.csv"</span><span class="token punctuation">,</span> index<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>If all goes well then that’s it! I hope it illustrated the basic concepts at work in building a web crawler. Perhaps now is a good time to step back and review your code!</p>]]></content>
      
      
      <categories>
          
          <category> Data Science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> Crawler </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Weighted Word Embedding</title>
      <link href="/Hexo-Blog/2021/01/25/2021-01-25-weighted-word-embedding/"/>
      <url>/Hexo-Blog/2021/01/25/2021-01-25-weighted-word-embedding/</url>
      
        <content type="html"><![CDATA[<p>We just had a long Christmas vacation. It’s time to get back to study mode! Today I’m going to summarise some important point about weighted word embedding for some specific NLP tasks. Frankly speaking, this is the topic I wish to write about a few months ago, however, I was so busy during my MSc.</p><h1 id="Load-the-Dataset"><a href="#Load-the-Dataset" class="headerlink" title="Load the Dataset"></a>Load the Dataset</h1><p>First things first, download IMDb dataset from HuggingFace’s <a href="https://github.com/huggingface/datasets">Datasets</a> library: </p><pre class="line-numbers language-python" data-language="python"><code class="language-python">dataset <span class="token operator">=</span> load_dataset<span class="token punctuation">(</span><span class="token string">"imdb"</span><span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">dataset_to_dataframe</span><span class="token punctuation">(</span>dataset<span class="token punctuation">)</span><span class="token punctuation">:</span>    X_train<span class="token punctuation">,</span> y_train <span class="token operator">=</span> dataset<span class="token punctuation">[</span><span class="token string">"train"</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">"text"</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dataset<span class="token punctuation">[</span><span class="token string">"train"</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">"label"</span><span class="token punctuation">]</span>    X_test<span class="token punctuation">,</span> y_test <span class="token operator">=</span> dataset<span class="token punctuation">[</span><span class="token string">"test"</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">"text"</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dataset<span class="token punctuation">[</span><span class="token string">"test"</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">"label"</span><span class="token punctuation">]</span>        df_train <span class="token operator">=</span> pd<span class="token punctuation">.</span>DataFrame<span class="token punctuation">(</span><span class="token punctuation">&#123;</span>        <span class="token string">"text"</span><span class="token punctuation">:</span> X_train<span class="token punctuation">,</span>         <span class="token string">"label"</span><span class="token punctuation">:</span> y_train<span class="token punctuation">,</span>         <span class="token string">"tag"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"train"</span><span class="token punctuation">]</span><span class="token operator">*</span><span class="token builtin">len</span><span class="token punctuation">(</span>dataset<span class="token punctuation">[</span><span class="token string">"train"</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    <span class="token punctuation">&#125;</span><span class="token punctuation">)</span>        df_test <span class="token operator">=</span> pd<span class="token punctuation">.</span>DataFrame<span class="token punctuation">(</span><span class="token punctuation">&#123;</span>        <span class="token string">"text"</span><span class="token punctuation">:</span> X_test<span class="token punctuation">,</span>         <span class="token string">"label"</span><span class="token punctuation">:</span> y_test<span class="token punctuation">,</span>         <span class="token string">"tag"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"test"</span><span class="token punctuation">]</span><span class="token operator">*</span><span class="token builtin">len</span><span class="token punctuation">(</span>dataset<span class="token punctuation">[</span><span class="token string">"test"</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    <span class="token punctuation">&#125;</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> df_train<span class="token punctuation">,</span> df_test<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>I tranformed the format into <code>pd.DataFrame</code> format.</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">dataset <span class="token operator">=</span> load_dataset<span class="token punctuation">(</span><span class="token string">"imdb"</span><span class="token punctuation">)</span>df_train<span class="token punctuation">,</span> df_test <span class="token operator">=</span> dataset_to_dataframe<span class="token punctuation">(</span>dataset<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Shape of training data: &#123;&#125;"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>df_train<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Shape of tesing data: &#123;&#125;"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>df_test<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>There are 25000 samples for each training data and testing data.</p><h1 id="Tokenisation"><a href="#Tokenisation" class="headerlink" title="Tokenisation"></a>Tokenisation</h1><p>To tackle text related problem in NLP and ML area, tokenisation is one of the common pre-processing. There are various types of text processing techniques, such as lowercasing, stemming words, lemmetising words, and so on. In this article, I only went through handling work with tokenisation and lowercase.</p><p>There are many tools that support segmentation tool by all means, as I have listed below.</p><ol><li>English Segmentation Tools</li></ol><ul><li><a href="https://github.com/nltk/nltk">NLTK</a></li><li><a href="https://spacy.io/api/tokenizer">spaCy</a></li><li><a href="https://github.com/google/sentencepiece">SentencePiece</a></li><li><a href="https://github.com/stanfordnlp/CoreNLP">Stanford CoreNLP</a></li></ul><ol start="2"><li>Chinese Segmentation Tools</li></ol><ul><li><a href="https://github.com/fxsjy/jieba">Jieba</a></li><li><a href="https://github.com/isnowfy/snownlp">SnowNLP</a></li><li><a href="https://www.ltp-cloud.com/">LTP</a></li><li><a href="https://github.com/hankcs/HanLP">HanNLP</a></li><li><a href="https://github.com/lancopku/pkuseg-python">PKUSEG</a></li></ul><ol start="3"><li>Japanese Segmentation Tools</li></ol><ul><li><a href="https://github.com/ikegami-yukino/mecab/releases">MeCab</a></li><li><a href="https://www.dampfkraft.com/nlp/how-to-tokenize-japanese.html">Fugashi</a></li><li><a href="https://mocobeta.github.io/janome/en/">Janome</a></li></ul><p> I tokenised the sentence in the simplest way using regex.</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">tokenization</span><span class="token punctuation">(</span>text<span class="token punctuation">)</span><span class="token punctuation">:</span>    text <span class="token operator">=</span> re<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">'\W+'</span><span class="token punctuation">,</span> text<span class="token punctuation">)</span>    <span class="token keyword">return</span> textdf_train<span class="token punctuation">[</span><span class="token string">"text_tokenised"</span><span class="token punctuation">]</span> <span class="token operator">=</span> df_train<span class="token punctuation">[</span><span class="token string">"text"</span><span class="token punctuation">]</span><span class="token punctuation">.</span><span class="token builtin">apply</span><span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> tokenization<span class="token punctuation">(</span>x<span class="token punctuation">.</span>lower<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>df_test<span class="token punctuation">[</span><span class="token string">"text_tokenised"</span><span class="token punctuation">]</span> <span class="token operator">=</span> df_test<span class="token punctuation">[</span><span class="token string">"text"</span><span class="token punctuation">]</span><span class="token punctuation">.</span><span class="token builtin">apply</span><span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> tokenization<span class="token punctuation">(</span>x<span class="token punctuation">.</span>lower<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="Word-Embedding-Model"><a href="#Word-Embedding-Model" class="headerlink" title="Word Embedding Model"></a>Word Embedding Model</h1><h2 id="Word2Vec"><a href="#Word2Vec" class="headerlink" title="Word2Vec"></a>Word2Vec</h2><p>Proposed by Tomas Mikolov et al. in their <a href="https://arxiv.org/pdf/1309.4168v1.pdf">paper</a>, there are two architectures (CBOW and Skip-gram) of word2vec to learn the underlying word representations for each word by using neural networks. </p><p>In the CBOW model, the distributed representations of context (or surrounding words) are combined to predict the word in the middle. </p><img src="https://miro.medium.com/max/1050/1*zNtM3sUehDXg4Fpbt60U-w.jpeg" alt="Drawing" style="width: 500px;"/><p>While in the Skip-gram model, the distributed representation of the input word is used to predict the context.</p><img src="https://miro.medium.com/max/1050/1*evJZHepBAUET1wdk65MB0A.png" alt="Drawing" style="width: 500px;"/><h2 id="GloVe"><a href="#GloVe" class="headerlink" title="GloVe"></a>GloVe</h2><p>GloVe is an unsupervised learning algorithm, proposed by Jeffrey Pennington et al., for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space. The <a href="https://nlp.stanford.edu/pubs/glove.pdf">paper</a> shows that the model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context<br>windows in a large corpus.</p><h2 id="FastText"><a href="#FastText" class="headerlink" title="FastText"></a>FastText</h2><p>FastText is capable of building word vectors for words that do not appear in the training set. For such words, the authors simply average the vector representation of its n-grams. This shows that they build robust word representations where prefixes and suffixes can be ignored if the grammatical form is not found in the dictionary.</p><p>There are two application of FastText.</p><ol><li>Using for learning word representations: P. Bojanowski, et al., <a href="https://arxiv.org/pdf/1607.04606.pdf">Enriching Word Vectors with Subword Information</a></li><li>Using for text classification: A. Joulin, et al., <a href="https://arxiv.org/pdf/1607.01759.pdf">Bag of Tricks for Efficient Text Classification</a></li></ol><h1 id="Load-Pre-trained-Model-from-Gensim"><a href="#Load-Pre-trained-Model-from-Gensim" class="headerlink" title="Load Pre-trained Model from Gensim"></a>Load Pre-trained Model from Gensim</h1><ul><li>Word2Vec pre-trained vectors <a href="https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz">download</a><ul><li>Unzip the file: <pre class="line-numbers language-none"><code class="language-none">!gunzip .&#x2F;GoogleNews-vectors-negative300.bin.gz<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li></ul></li><li>GloVe pre-trained vectors <a href="https://nlp.stanford.edu/projects/glove/">download</a><ul><li>Convert file into a gensim word2vec format: <pre class="line-numbers language-none"><code class="language-none">glove2word2vec(glove_input_file&#x3D;r&quot;glove.840B.300d.txt&quot;, word2vec_output_file&#x3D;r&quot;gensim_glove_vectors.txt&quot;)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li></ul></li><li>FastText pre-trained vectors <a href="https://fasttext.cc/docs/en/crawl-vectors.html">download</a><ul><li>Convert file into a binary format: <pre class="line-numbers language-none"><code class="language-none">embedding_dict &#x3D; KeyedVectors.load_word2vec_format(r&quot;cc.en.300.vec&quot;, binary&#x3D;False)embedding_dict.save_word2vec_format(MODEL_PATH, binary&#x3D;True)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre></li></ul></li></ul><p>While loading pre-trained model, you can grab a cup of tea or coffee, cause it will cost some time.</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">w2v_model <span class="token operator">=</span> KeyedVectors<span class="token punctuation">.</span>load_word2vec_format<span class="token punctuation">(</span><span class="token string">"GoogleNews-vectors-negative300.bin"</span><span class="token punctuation">,</span> binary<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>glove_model <span class="token operator">=</span> KeyedVectors<span class="token punctuation">.</span>load_word2vec_format<span class="token punctuation">(</span><span class="token string">"gensim_glove_vectors.txt"</span><span class="token punctuation">,</span> binary<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>ft_model <span class="token operator">=</span> KeyedVectors<span class="token punctuation">.</span>load_word2vec_format<span class="token punctuation">(</span><span class="token string">"cc.en.300.bin"</span><span class="token punctuation">,</span> binary<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h1 id="Weighted-Method"><a href="#Weighted-Method" class="headerlink" title="Weighted Method"></a>Weighted Method</h1><p>The aim of this section is to construct sentence embeddings, obtained from word embeddings. If the sentence <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.023ex" xmlns="http://www.w3.org/2000/svg" width="1.061ex" height="1.023ex" role="img" focusable="false" viewBox="0 -442 469 452" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-7-TEX-I-1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-7-TEX-I-1D460"></use></g></g></g></svg></mjx-container> consists of words <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.339ex" xmlns="http://www.w3.org/2000/svg" width="2.533ex" height="1.342ex" role="img" focusable="false" viewBox="0 -443 1119.6 593" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-6-TEX-I-1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path><path id="MJX-6-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-6-TEX-I-1D464"></use></g><g data-mml-node="TeXAtom" transform="translate(716, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><use xlink:href="#MJX-6-TEX-N-31"></use></g></g></g></g></g></svg></mjx-container>, …, <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex" xmlns="http://www.w3.org/2000/svg" width="2.693ex" height="1.359ex" role="img" focusable="false" viewBox="0 -443 1190.3 600.8" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-6-TEX-I-1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path><path id="MJX-6-TEX-I-1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-6-TEX-I-1D464"></use></g><g data-mml-node="TeXAtom" transform="translate(716, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-6-TEX-I-1D45B"></use></g></g></g></g></g></svg></mjx-container>, let’s define an embedding vector <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="13.715ex" height="2.604ex" role="img" focusable="false" viewBox="0 -900.8 6061.9 1150.8" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-6-TEX-I-1D438" d="M492 213Q472 213 472 226Q472 230 477 250T482 285Q482 316 461 323T364 330H312Q311 328 277 192T243 52Q243 48 254 48T334 46Q428 46 458 48T518 61Q567 77 599 117T670 248Q680 270 683 272Q690 274 698 274Q718 274 718 261Q613 7 608 2Q605 0 322 0H133Q31 0 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q146 66 215 342T285 622Q285 629 281 629Q273 632 228 634H197Q191 640 191 642T193 659Q197 676 203 680H757Q764 676 764 669Q764 664 751 557T737 447Q735 440 717 440H705Q698 445 698 453L701 476Q704 500 704 528Q704 558 697 578T678 609T643 625T596 632T532 634H485Q397 633 392 631Q388 629 386 622Q385 619 355 499T324 377Q347 376 372 376H398Q464 376 489 391T534 472Q538 488 540 490T557 493Q562 493 565 493T570 492T572 491T574 487T577 483L544 351Q511 218 508 216Q505 213 492 213Z"></path><path id="MJX-6-TEX-I-1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-6-TEX-I-1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"></path><path id="MJX-6-TEX-I-1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path><path id="MJX-6-TEX-N-28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path><path id="MJX-6-TEX-N-29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path><path id="MJX-6-TEX-N-2208" d="M84 250Q84 372 166 450T360 539Q361 539 377 539T419 540T469 540H568Q583 532 583 520Q583 511 570 501L466 500Q355 499 329 494Q280 482 242 458T183 409T147 354T129 306T124 272V270H568Q583 262 583 250T568 230H124V228Q124 207 134 177T167 112T231 48T328 7Q355 1 466 0H570Q583 -10 583 -20Q583 -32 568 -40H471Q464 -40 446 -40T417 -41Q262 -41 172 45Q84 127 84 250Z"></path><path id="MJX-6-TEX-D-211D" d="M17 665Q17 672 28 683H221Q415 681 439 677Q461 673 481 667T516 654T544 639T566 623T584 607T597 592T607 578T614 565T618 554L621 548Q626 530 626 497Q626 447 613 419Q578 348 473 326L455 321Q462 310 473 292T517 226T578 141T637 72T686 35Q705 30 705 16Q705 7 693 -1H510Q503 6 404 159L306 310H268V183Q270 67 271 59Q274 42 291 38Q295 37 319 35Q344 35 353 28Q362 17 353 3L346 -1H28Q16 5 16 16Q16 35 55 35Q96 38 101 52Q106 60 106 341T101 632Q95 645 55 648Q17 648 17 665ZM241 35Q238 42 237 45T235 78T233 163T233 337V621L237 635L244 648H133Q136 641 137 638T139 603T141 517T141 341Q141 131 140 89T134 37Q133 36 133 35H241ZM457 496Q457 540 449 570T425 615T400 634T377 643Q374 643 339 648Q300 648 281 635Q271 628 270 610T268 481V346H284Q327 346 375 352Q421 364 439 392T457 496ZM492 537T492 496T488 427T478 389T469 371T464 361Q464 360 465 360Q469 360 497 370Q593 400 593 495Q593 592 477 630L457 637L461 626Q474 611 488 561Q492 537 492 496ZM464 243Q411 317 410 317Q404 317 401 315Q384 315 370 312H346L526 35H619L606 50Q553 109 464 243Z"></path><path id="MJX-6-TEX-I-1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-6-TEX-I-1D438"></use></g><g data-mml-node="mi" transform="translate(764, 0)"><use xlink:href="#MJX-6-TEX-I-1D45A"></use></g><g data-mml-node="msub" transform="translate(1642, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-6-TEX-I-1D44F"></use></g><g data-mml-node="TeXAtom" transform="translate(429, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-6-TEX-I-1D460"></use></g></g></g><g data-mml-node="mo" transform="translate(2452.6, 0)"><use xlink:href="#MJX-6-TEX-N-28"></use></g><g data-mml-node="mi" transform="translate(2841.6, 0)"><use xlink:href="#MJX-6-TEX-I-1D460"></use></g><g data-mml-node="mo" transform="translate(3310.6, 0)"><use xlink:href="#MJX-6-TEX-N-29"></use></g><g data-mml-node="mo" transform="translate(3977.4, 0)"><use xlink:href="#MJX-6-TEX-N-2208"></use></g><g data-mml-node="msup" transform="translate(4922.2, 0)"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-6-TEX-D-211D"></use></g></g><g data-mml-node="mi" transform="translate(722, 410.1) scale(0.707)"><use xlink:href="#MJX-6-TEX-I-1D451"></use></g></g></g></g></svg></mjx-container> for some <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.09ex" xmlns="http://www.w3.org/2000/svg" width="5.325ex" height="1.661ex" role="img" focusable="false" viewBox="0 -694 2353.6 734" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-6-TEX-I-1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path><path id="MJX-6-TEX-N-3E" d="M84 520Q84 528 88 533T96 539L99 540Q106 540 253 471T544 334L687 265Q694 260 694 250T687 235Q685 233 395 96L107 -40H101Q83 -38 83 -20Q83 -19 83 -17Q82 -10 98 -1Q117 9 248 71Q326 108 378 132L626 250L378 368Q90 504 86 509Q84 513 84 520Z"></path><path id="MJX-6-TEX-N-30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-6-TEX-I-1D451"></use></g><g data-mml-node="mo" transform="translate(797.8, 0)"><use xlink:href="#MJX-6-TEX-N-3E"></use></g><g data-mml-node="mn" transform="translate(1853.6, 0)"><use xlink:href="#MJX-6-TEX-N-30"></use></g></g></g></svg></mjx-container>. We can compute sentence embedding from the embeddings of words <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex" xmlns="http://www.w3.org/2000/svg" width="2.285ex" height="1.359ex" role="img" focusable="false" viewBox="0 -443 1010 600.8" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-6-TEX-I-1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path><path id="MJX-6-TEX-I-1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-6-TEX-I-1D464"></use></g><g data-mml-node="TeXAtom" transform="translate(716, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-6-TEX-I-1D456"></use></g></g></g></g></g></svg></mjx-container>, let’s call them <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="9.989ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 4415.2 1000" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-6-TEX-I-1D438" d="M492 213Q472 213 472 226Q472 230 477 250T482 285Q482 316 461 323T364 330H312Q311 328 277 192T243 52Q243 48 254 48T334 46Q428 46 458 48T518 61Q567 77 599 117T670 248Q680 270 683 272Q690 274 698 274Q718 274 718 261Q613 7 608 2Q605 0 322 0H133Q31 0 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q146 66 215 342T285 622Q285 629 281 629Q273 632 228 634H197Q191 640 191 642T193 659Q197 676 203 680H757Q764 676 764 669Q764 664 751 557T737 447Q735 440 717 440H705Q698 445 698 453L701 476Q704 500 704 528Q704 558 697 578T678 609T643 625T596 632T532 634H485Q397 633 392 631Q388 629 386 622Q385 619 355 499T324 377Q347 376 372 376H398Q464 376 489 391T534 472Q538 488 540 490T557 493Q562 493 565 493T570 492T572 491T574 487T577 483L544 351Q511 218 508 216Q505 213 492 213Z"></path><path id="MJX-6-TEX-I-1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-6-TEX-I-1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"></path><path id="MJX-6-TEX-I-1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path><path id="MJX-6-TEX-N-28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path><path id="MJX-6-TEX-I-1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path><path id="MJX-6-TEX-N-29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-6-TEX-I-1D438"></use></g><g data-mml-node="mi" transform="translate(764, 0)"><use xlink:href="#MJX-6-TEX-I-1D45A"></use></g><g data-mml-node="msub" transform="translate(1642, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-6-TEX-I-1D44F"></use></g><g data-mml-node="TeXAtom" transform="translate(429, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-6-TEX-I-1D464"></use></g></g></g><g data-mml-node="mo" transform="translate(2627.3, 0)"><use xlink:href="#MJX-6-TEX-N-28"></use></g><g data-mml-node="msub" transform="translate(3016.3, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-6-TEX-I-1D464"></use></g><g data-mml-node="TeXAtom" transform="translate(716, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-6-TEX-I-1D456"></use></g></g></g><g data-mml-node="mo" transform="translate(4026.2, 0)"><use xlink:href="#MJX-6-TEX-N-29"></use></g></g></g></svg></mjx-container>, so that <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="8.37ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 3699.6 1000" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-6-TEX-I-1D438" d="M492 213Q472 213 472 226Q472 230 477 250T482 285Q482 316 461 323T364 330H312Q311 328 277 192T243 52Q243 48 254 48T334 46Q428 46 458 48T518 61Q567 77 599 117T670 248Q680 270 683 272Q690 274 698 274Q718 274 718 261Q613 7 608 2Q605 0 322 0H133Q31 0 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q146 66 215 342T285 622Q285 629 281 629Q273 632 228 634H197Q191 640 191 642T193 659Q197 676 203 680H757Q764 676 764 669Q764 664 751 557T737 447Q735 440 717 440H705Q698 445 698 453L701 476Q704 500 704 528Q704 558 697 578T678 609T643 625T596 632T532 634H485Q397 633 392 631Q388 629 386 622Q385 619 355 499T324 377Q347 376 372 376H398Q464 376 489 391T534 472Q538 488 540 490T557 493Q562 493 565 493T570 492T572 491T574 487T577 483L544 351Q511 218 508 216Q505 213 492 213Z"></path><path id="MJX-6-TEX-I-1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-6-TEX-I-1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"></path><path id="MJX-6-TEX-I-1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path><path id="MJX-6-TEX-N-28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path><path id="MJX-6-TEX-N-29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-6-TEX-I-1D438"></use></g><g data-mml-node="mi" transform="translate(764, 0)"><use xlink:href="#MJX-6-TEX-I-1D45A"></use></g><g data-mml-node="msub" transform="translate(1642, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-6-TEX-I-1D44F"></use></g><g data-mml-node="TeXAtom" transform="translate(429, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-6-TEX-I-1D460"></use></g></g></g><g data-mml-node="mo" transform="translate(2452.6, 0)"><use xlink:href="#MJX-6-TEX-N-28"></use></g><g data-mml-node="mi" transform="translate(2841.6, 0)"><use xlink:href="#MJX-6-TEX-I-1D460"></use></g><g data-mml-node="mo" transform="translate(3310.6, 0)"><use xlink:href="#MJX-6-TEX-N-29"></use></g></g></g></svg></mjx-container> is a linear combination of <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="9.989ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 4415.2 1000" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-6-TEX-I-1D438" d="M492 213Q472 213 472 226Q472 230 477 250T482 285Q482 316 461 323T364 330H312Q311 328 277 192T243 52Q243 48 254 48T334 46Q428 46 458 48T518 61Q567 77 599 117T670 248Q680 270 683 272Q690 274 698 274Q718 274 718 261Q613 7 608 2Q605 0 322 0H133Q31 0 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q146 66 215 342T285 622Q285 629 281 629Q273 632 228 634H197Q191 640 191 642T193 659Q197 676 203 680H757Q764 676 764 669Q764 664 751 557T737 447Q735 440 717 440H705Q698 445 698 453L701 476Q704 500 704 528Q704 558 697 578T678 609T643 625T596 632T532 634H485Q397 633 392 631Q388 629 386 622Q385 619 355 499T324 377Q347 376 372 376H398Q464 376 489 391T534 472Q538 488 540 490T557 493Q562 493 565 493T570 492T572 491T574 487T577 483L544 351Q511 218 508 216Q505 213 492 213Z"></path><path id="MJX-6-TEX-I-1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-6-TEX-I-1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"></path><path id="MJX-6-TEX-I-1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path><path id="MJX-6-TEX-N-28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path><path id="MJX-6-TEX-I-1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path><path id="MJX-6-TEX-N-29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-6-TEX-I-1D438"></use></g><g data-mml-node="mi" transform="translate(764, 0)"><use xlink:href="#MJX-6-TEX-I-1D45A"></use></g><g data-mml-node="msub" transform="translate(1642, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-6-TEX-I-1D44F"></use></g><g data-mml-node="TeXAtom" transform="translate(429, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-6-TEX-I-1D464"></use></g></g></g><g data-mml-node="mo" transform="translate(2627.3, 0)"><use xlink:href="#MJX-6-TEX-N-28"></use></g><g data-mml-node="msub" transform="translate(3016.3, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-6-TEX-I-1D464"></use></g><g data-mml-node="TeXAtom" transform="translate(716, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-6-TEX-I-1D456"></use></g></g></g><g data-mml-node="mo" transform="translate(4026.2, 0)"><use xlink:href="#MJX-6-TEX-N-29"></use></g></g></g></svg></mjx-container> and has the same dimensionality <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.023ex" xmlns="http://www.w3.org/2000/svg" width="1.176ex" height="1.593ex" role="img" focusable="false" viewBox="0 -694 520 704" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-6-TEX-I-1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-6-TEX-I-1D451"></use></g></g></g></svg></mjx-container>: </p><div>    <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.898ex" xmlns="http://www.w3.org/2000/svg" width="30.968ex" height="2.595ex" role="img" focusable="false" viewBox="0 -750 13687.9 1146.9" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-6-TEX-I-1D438" d="M492 213Q472 213 472 226Q472 230 477 250T482 285Q482 316 461 323T364 330H312Q311 328 277 192T243 52Q243 48 254 48T334 46Q428 46 458 48T518 61Q567 77 599 117T670 248Q680 270 683 272Q690 274 698 274Q718 274 718 261Q613 7 608 2Q605 0 322 0H133Q31 0 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q146 66 215 342T285 622Q285 629 281 629Q273 632 228 634H197Q191 640 191 642T193 659Q197 676 203 680H757Q764 676 764 669Q764 664 751 557T737 447Q735 440 717 440H705Q698 445 698 453L701 476Q704 500 704 528Q704 558 697 578T678 609T643 625T596 632T532 634H485Q397 633 392 631Q388 629 386 622Q385 619 355 499T324 377Q347 376 372 376H398Q464 376 489 391T534 472Q538 488 540 490T557 493Q562 493 565 493T570 492T572 491T574 487T577 483L544 351Q511 218 508 216Q505 213 492 213Z"></path><path id="MJX-6-TEX-I-1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-6-TEX-I-1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"></path><path id="MJX-6-TEX-I-1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path><path id="MJX-6-TEX-N-28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path><path id="MJX-6-TEX-N-29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path><path id="MJX-6-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-6-TEX-SO-2211" d="M61 748Q64 750 489 750H913L954 640Q965 609 976 579T993 533T999 516H979L959 517Q936 579 886 621T777 682Q724 700 655 705T436 710H319Q183 710 183 709Q186 706 348 484T511 259Q517 250 513 244L490 216Q466 188 420 134T330 27L149 -187Q149 -188 362 -188Q388 -188 436 -188T506 -189Q679 -189 778 -162T936 -43Q946 -27 959 6H999L913 -249L489 -250Q65 -250 62 -248Q56 -246 56 -239Q56 -234 118 -161Q186 -81 245 -11L428 206Q428 207 242 462L57 717L56 728Q56 744 61 748Z"></path><path id="MJX-6-TEX-I-1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path><path id="MJX-6-TEX-I-1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path><path id="MJX-6-TEX-N-2208" d="M84 250Q84 372 166 450T360 539Q361 539 377 539T419 540T469 540H568Q583 532 583 520Q583 511 570 501L466 500Q355 499 329 494Q280 482 242 458T183 409T147 354T129 306T124 272V270H568Q583 262 583 250T568 230H124V228Q124 207 134 177T167 112T231 48T328 7Q355 1 466 0H570Q583 -10 583 -20Q583 -32 568 -40H471Q464 -40 446 -40T417 -41Q262 -41 172 45Q84 127 84 250Z"></path><path id="MJX-6-TEX-I-1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z"></path><path id="MJX-6-TEX-N-22C5" d="M78 250Q78 274 95 292T138 310Q162 310 180 294T199 251Q199 226 182 208T139 190T96 207T78 250Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-6-TEX-I-1D438"></use></g><g data-mml-node="mi" transform="translate(764, 0)"><use xlink:href="#MJX-6-TEX-I-1D45A"></use></g><g data-mml-node="msub" transform="translate(1642, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-6-TEX-I-1D44F"></use></g><g data-mml-node="TeXAtom" transform="translate(429, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-6-TEX-I-1D460"></use></g></g></g><g data-mml-node="mo" transform="translate(2452.6, 0)"><use xlink:href="#MJX-6-TEX-N-28"></use></g><g data-mml-node="mi" transform="translate(2841.6, 0)"><use xlink:href="#MJX-6-TEX-I-1D460"></use></g><g data-mml-node="mo" transform="translate(3310.6, 0)"><use xlink:href="#MJX-6-TEX-N-29"></use></g><g data-mml-node="mo" transform="translate(3977.4, 0)"><use xlink:href="#MJX-6-TEX-N-3D"></use></g><g data-mml-node="munder" transform="translate(5033.2, 0)"><g data-mml-node="mo"><use xlink:href="#MJX-6-TEX-SO-2211"></use></g><g data-mml-node="TeXAtom" transform="translate(1056, -285.4) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-6-TEX-I-1D464"></use></g><g data-mml-node="TeXAtom" transform="translate(716, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-6-TEX-I-1D456"></use></g></g></g><g data-mml-node="mo" transform="translate(1010, 0)"><use xlink:href="#MJX-6-TEX-N-2208"></use></g><g data-mml-node="mi" transform="translate(1677, 0)"><use xlink:href="#MJX-6-TEX-I-1D460"></use></g></g></g><g data-mml-node="msub" transform="translate(7823.3, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-6-TEX-I-1D450"></use></g><g data-mml-node="mi" transform="translate(433, -150) scale(0.707)"><use xlink:href="#MJX-6-TEX-I-1D456"></use></g></g><g data-mml-node="mo" transform="translate(8772.4, 0)"><use xlink:href="#MJX-6-TEX-N-22C5"></use></g><g data-mml-node="mi" transform="translate(9272.7, 0)"><use xlink:href="#MJX-6-TEX-I-1D438"></use></g><g data-mml-node="mi" transform="translate(10036.7, 0)"><use xlink:href="#MJX-6-TEX-I-1D45A"></use></g><g data-mml-node="msub" transform="translate(10914.7, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-6-TEX-I-1D44F"></use></g><g data-mml-node="TeXAtom" transform="translate(429, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-6-TEX-I-1D464"></use></g></g></g><g data-mml-node="mo" transform="translate(11900, 0)"><use xlink:href="#MJX-6-TEX-N-28"></use></g><g data-mml-node="msub" transform="translate(12289, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-6-TEX-I-1D464"></use></g><g data-mml-node="mi" transform="translate(716, -150) scale(0.707)"><use xlink:href="#MJX-6-TEX-I-1D456"></use></g></g><g data-mml-node="mo" transform="translate(13298.9, 0)"><use xlink:href="#MJX-6-TEX-N-29"></use></g></g></g></svg></mjx-container></div><p>where <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex" xmlns="http://www.w3.org/2000/svg" width="6.044ex" height="1.902ex" role="img" focusable="false" viewBox="0 -683 2671.5 840.8" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-6-TEX-I-1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z"></path><path id="MJX-6-TEX-I-1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path><path id="MJX-6-TEX-N-2208" d="M84 250Q84 372 166 450T360 539Q361 539 377 539T419 540T469 540H568Q583 532 583 520Q583 511 570 501L466 500Q355 499 329 494Q280 482 242 458T183 409T147 354T129 306T124 272V270H568Q583 262 583 250T568 230H124V228Q124 207 134 177T167 112T231 48T328 7Q355 1 466 0H570Q583 -10 583 -20Q583 -32 568 -40H471Q464 -40 446 -40T417 -41Q262 -41 172 45Q84 127 84 250Z"></path><path id="MJX-6-TEX-D-211D" d="M17 665Q17 672 28 683H221Q415 681 439 677Q461 673 481 667T516 654T544 639T566 623T584 607T597 592T607 578T614 565T618 554L621 548Q626 530 626 497Q626 447 613 419Q578 348 473 326L455 321Q462 310 473 292T517 226T578 141T637 72T686 35Q705 30 705 16Q705 7 693 -1H510Q503 6 404 159L306 310H268V183Q270 67 271 59Q274 42 291 38Q295 37 319 35Q344 35 353 28Q362 17 353 3L346 -1H28Q16 5 16 16Q16 35 55 35Q96 38 101 52Q106 60 106 341T101 632Q95 645 55 648Q17 648 17 665ZM241 35Q238 42 237 45T235 78T233 163T233 337V621L237 635L244 648H133Q136 641 137 638T139 603T141 517T141 341Q141 131 140 89T134 37Q133 36 133 35H241ZM457 496Q457 540 449 570T425 615T400 634T377 643Q374 643 339 648Q300 648 281 635Q271 628 270 610T268 481V346H284Q327 346 375 352Q421 364 439 392T457 496ZM492 537T492 496T488 427T478 389T469 371T464 361Q464 360 465 360Q469 360 497 370Q593 400 593 495Q593 592 477 630L457 637L461 626Q474 611 488 561Q492 537 492 496ZM464 243Q411 317 410 317Q404 317 401 315Q384 315 370 312H346L526 35H619L606 50Q553 109 464 243Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-6-TEX-I-1D450"></use></g><g data-mml-node="mi" transform="translate(433, -150) scale(0.707)"><use xlink:href="#MJX-6-TEX-I-1D456"></use></g></g><g data-mml-node="mo" transform="translate(1004.7, 0)"><use xlink:href="#MJX-6-TEX-N-2208"></use></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(1949.5, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-6-TEX-D-211D"></use></g></g></g></g></svg></mjx-container> are the coefficients (scalars).</p><h2 id="BoW"><a href="#BoW" class="headerlink" title="BoW"></a>BoW</h2><p>Averaging the component word vectors in every documents.</p><div style="display: flex;justify-content: center;">    <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -1.238ex" xmlns="http://www.w3.org/2000/svg" width="17.462ex" height="3.195ex" role="img" focusable="false" viewBox="0 -864.9 7718 1412" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-6-TEX-I-1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path><path id="MJX-6-TEX-I-1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path><path id="MJX-6-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-6-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-6-TEX-N-2016" d="M133 736Q138 750 153 750Q164 750 170 739Q172 735 172 250T170 -239Q164 -250 152 -250Q144 -250 138 -244L137 -243Q133 -241 133 -179T132 250Q132 731 133 736ZM329 739Q334 750 346 750Q353 750 361 744L362 743Q366 741 366 679T367 250T367 -178T362 -243L361 -244Q355 -250 347 -250Q335 -250 329 -239Q327 -235 327 250T329 739Z"></path><path id="MJX-6-TEX-I-1D446" d="M308 24Q367 24 416 76T466 197Q466 260 414 284Q308 311 278 321T236 341Q176 383 176 462Q176 523 208 573T273 648Q302 673 343 688T407 704H418H425Q521 704 564 640Q565 640 577 653T603 682T623 704Q624 704 627 704T632 705Q645 705 645 698T617 577T585 459T569 456Q549 456 549 465Q549 471 550 475Q550 478 551 494T553 520Q553 554 544 579T526 616T501 641Q465 662 419 662Q362 662 313 616T263 510Q263 480 278 458T319 427Q323 425 389 408T456 390Q490 379 522 342T554 242Q554 216 546 186Q541 164 528 137T492 78T426 18T332 -20Q320 -22 298 -22Q199 -22 144 33L134 44L106 13Q83 -14 78 -18T65 -22Q52 -22 52 -14Q52 -11 110 221Q112 227 130 227H143Q149 221 149 216Q149 214 148 207T144 186T142 153Q144 114 160 87T203 47T255 29T308 24Z"></path><path id="MJX-6-TEX-SO-2211" d="M61 748Q64 750 489 750H913L954 640Q965 609 976 579T993 533T999 516H979L959 517Q936 579 886 621T777 682Q724 700 655 705T436 710H319Q183 710 183 709Q186 706 348 484T511 259Q517 250 513 244L490 216Q466 188 420 134T330 27L149 -187Q149 -188 362 -188Q388 -188 436 -188T506 -189Q679 -189 778 -162T936 -43Q946 -27 959 6H999L913 -249L489 -250Q65 -250 62 -248Q56 -246 56 -239Q56 -234 118 -161Q186 -81 245 -11L428 206Q428 207 242 462L57 717L56 728Q56 744 61 748Z"></path><path id="MJX-6-TEX-I-1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path><path id="MJX-6-TEX-N-2208" d="M84 250Q84 372 166 450T360 539Q361 539 377 539T419 540T469 540H568Q583 532 583 520Q583 511 570 501L466 500Q355 499 329 494Q280 482 242 458T183 409T147 354T129 306T124 272V270H568Q583 262 583 250T568 230H124V228Q124 207 134 177T167 112T231 48T328 7Q355 1 466 0H570Q583 -10 583 -20Q583 -32 568 -40H471Q464 -40 446 -40T417 -41Q262 -41 172 45Q84 127 84 250Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-6-TEX-I-1D463"></use></g><g data-mml-node="TeXAtom" transform="translate(485, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-6-TEX-I-1D460"></use></g></g></g><g data-mml-node="mo" transform="translate(1144.4, 0)"><use xlink:href="#MJX-6-TEX-N-3D"></use></g><g data-mml-node="mfrac" transform="translate(2200.2, 0)"><g data-mml-node="mn" transform="translate(624.8, 394) scale(0.707)"><use xlink:href="#MJX-6-TEX-N-31"></use></g><g data-mml-node="mrow" transform="translate(220, -370.3) scale(0.707)"><g data-mml-node="mo"><use xlink:href="#MJX-6-TEX-N-2016"></use></g><g data-mml-node="mi" transform="translate(500, 0)"><use xlink:href="#MJX-6-TEX-I-1D446"></use></g><g data-mml-node="mo" transform="translate(1145, 0)"><use xlink:href="#MJX-6-TEX-N-2016"></use></g></g><rect width="1363.2" height="60" x="120" y="220"></rect></g><g data-mml-node="munder" transform="translate(3970, 0)"><g data-mml-node="mo"><use xlink:href="#MJX-6-TEX-SO-2211"></use></g><g data-mml-node="TeXAtom" transform="translate(1056, -285.4) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-6-TEX-I-1D464"></use></g><g data-mml-node="mo" transform="translate(716, 0)"><use xlink:href="#MJX-6-TEX-N-2208"></use></g><g data-mml-node="mi" transform="translate(1383, 0)"><use xlink:href="#MJX-6-TEX-I-1D446"></use></g></g></g><g data-mml-node="msub" transform="translate(6676.7, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-6-TEX-I-1D463"></use></g><g data-mml-node="TeXAtom" transform="translate(485, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-6-TEX-I-1D464"></use></g></g></g></g></g></svg></mjx-container></div><h2 id="TFIDF"><a href="#TFIDF" class="headerlink" title="TFIDF"></a>TFIDF</h2><p>Term frequency–inverse document frequency (TF-IDF) is a popular method to capture the significance of a token to a particular input with respect to all the inputs.</p><div style="display: flex;justify-content: center;">    <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -1.108ex" xmlns="http://www.w3.org/2000/svg" width="17.667ex" height="3.093ex" role="img" focusable="false" viewBox="0 -877 7808.7 1366.9" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-6-TEX-I-1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path><path id="MJX-6-TEX-I-1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path><path id="MJX-6-TEX-N-2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path><path id="MJX-6-TEX-I-1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path><path id="MJX-6-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-6-TEX-I-1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path><path id="MJX-6-TEX-I-1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"></path><path id="MJX-6-TEX-N-22C5" d="M78 250Q78 274 95 292T138 310Q162 310 180 294T199 251Q199 226 182 208T139 190T96 207T78 250Z"></path><path id="MJX-6-TEX-I-1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path><path id="MJX-6-TEX-I-1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path><path id="MJX-6-TEX-I-1D454" d="M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z"></path><path id="MJX-6-TEX-I-1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"></path><path id="MJX-6-TEX-I-1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-6-TEX-I-1D464"></use></g><g data-mml-node="TeXAtom" transform="translate(716, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-6-TEX-I-1D456"></use></g><g data-mml-node="mo" transform="translate(345, 0)"><use xlink:href="#MJX-6-TEX-N-2C"></use></g><g data-mml-node="mi" transform="translate(623, 0)"><use xlink:href="#MJX-6-TEX-I-1D457"></use></g></g></g><g data-mml-node="mo" transform="translate(1775.6, 0)"><use xlink:href="#MJX-6-TEX-N-3D"></use></g><g data-mml-node="mi" transform="translate(2831.4, 0)"><use xlink:href="#MJX-6-TEX-I-1D461"></use></g><g data-mml-node="msub" transform="translate(3192.4, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-6-TEX-I-1D453"></use></g><g data-mml-node="TeXAtom" transform="translate(490, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-6-TEX-I-1D456"></use></g><g data-mml-node="mo" transform="translate(345, 0)"><use xlink:href="#MJX-6-TEX-N-2C"></use></g><g data-mml-node="mi" transform="translate(623, 0)"><use xlink:href="#MJX-6-TEX-I-1D457"></use></g></g></g><g data-mml-node="mo" transform="translate(4686.5, 0)"><use xlink:href="#MJX-6-TEX-N-22C5"></use></g><g data-mml-node="mi" transform="translate(5186.7, 0)"><use xlink:href="#MJX-6-TEX-I-1D459"></use></g><g data-mml-node="mi" transform="translate(5484.7, 0)"><use xlink:href="#MJX-6-TEX-I-1D45C"></use></g><g data-mml-node="mi" transform="translate(5969.7, 0)"><use xlink:href="#MJX-6-TEX-I-1D454"></use></g><g data-mml-node="mfrac" transform="translate(6446.7, 0)"><g data-mml-node="mi" transform="translate(367.1, 394) scale(0.707)"><use xlink:href="#MJX-6-TEX-I-1D441"></use></g><g data-mml-node="mrow" transform="translate(220, -345) scale(0.707)"><g data-mml-node="mi"><use xlink:href="#MJX-6-TEX-I-1D451"></use></g><g data-mml-node="msub" transform="translate(520, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-6-TEX-I-1D453"></use></g><g data-mml-node="mi" transform="translate(490, -150) scale(0.707)"><use xlink:href="#MJX-6-TEX-I-1D456"></use></g></g></g><rect width="1122" height="60" x="120" y="220"></rect></g></g></g></svg></mjx-container></div><ul><li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.666ex" xmlns="http://www.w3.org/2000/svg" width="3.389ex" height="1.668ex" role="img" focusable="false" viewBox="0 -443 1497.9 737.2" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-6-TEX-I-1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path><path id="MJX-6-TEX-I-1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path><path id="MJX-6-TEX-N-2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path><path id="MJX-6-TEX-I-1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-6-TEX-I-1D464"></use></g><g data-mml-node="TeXAtom" transform="translate(716, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-6-TEX-I-1D456"></use></g><g data-mml-node="mo" transform="translate(345, 0)"><use xlink:href="#MJX-6-TEX-N-2C"></use></g><g data-mml-node="mi" transform="translate(623, 0)"><use xlink:href="#MJX-6-TEX-I-1D457"></use></g></g></g></g></g></svg></mjx-container> is the tf-idf weight for term i in document j</li><li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.666ex" xmlns="http://www.w3.org/2000/svg" width="3.694ex" height="2.261ex" role="img" focusable="false" viewBox="0 -705 1632.9 999.2" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-6-TEX-I-1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path><path id="MJX-6-TEX-I-1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"></path><path id="MJX-6-TEX-I-1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path><path id="MJX-6-TEX-N-2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path><path id="MJX-6-TEX-I-1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-6-TEX-I-1D461"></use></g><g data-mml-node="msub" transform="translate(361, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-6-TEX-I-1D453"></use></g><g data-mml-node="TeXAtom" transform="translate(490, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-6-TEX-I-1D456"></use></g><g data-mml-node="mo" transform="translate(345, 0)"><use xlink:href="#MJX-6-TEX-N-2C"></use></g><g data-mml-node="mi" transform="translate(623, 0)"><use xlink:href="#MJX-6-TEX-I-1D457"></use></g></g></g></g></g></svg></mjx-container> is number of time term i appear in document j</li><li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0" xmlns="http://www.w3.org/2000/svg" width="2.009ex" height="1.545ex" role="img" focusable="false" viewBox="0 -683 888 683" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-6-TEX-I-1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-6-TEX-I-1D441"></use></g></g></g></svg></mjx-container> is the total number of documents</li><li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.464ex" xmlns="http://www.w3.org/2000/svg" width="2.95ex" height="2.059ex" role="img" focusable="false" viewBox="0 -705 1304 910" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-6-TEX-I-1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path><path id="MJX-6-TEX-I-1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"></path><path id="MJX-6-TEX-I-1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-6-TEX-I-1D451"></use></g><g data-mml-node="msub" transform="translate(520, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-6-TEX-I-1D453"></use></g><g data-mml-node="mi" transform="translate(490, -150) scale(0.707)"><use xlink:href="#MJX-6-TEX-I-1D456"></use></g></g></g></g></svg></mjx-container> is the number of documents with token i</li></ul><h2 id="SIF"><a href="#SIF" class="headerlink" title="SIF"></a>SIF</h2><p>Instead of just averaging the component word vectors as suggested by this equation for BoW, SIF generate the sentence vector <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.355ex" xmlns="http://www.w3.org/2000/svg" width="1.961ex" height="1.358ex" role="img" focusable="false" viewBox="0 -443 866.6 600.1" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-6-TEX-I-1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path><path id="MJX-6-TEX-I-1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-6-TEX-I-1D463"></use></g><g data-mml-node="mi" transform="translate(485, -150) scale(0.707)"><use xlink:href="#MJX-6-TEX-I-1D460"></use></g></g></g></g></svg></mjx-container> by multiplying each component vector <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex" xmlns="http://www.w3.org/2000/svg" width="2.356ex" height="1.359ex" role="img" focusable="false" viewBox="0 -443 1041.3 600.8" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-6-TEX-I-1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path><path id="MJX-6-TEX-I-1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-6-TEX-I-1D463"></use></g><g data-mml-node="mi" transform="translate(485, -150) scale(0.707)"><use xlink:href="#MJX-6-TEX-I-1D464"></use></g></g></g></g></svg></mjx-container> by the inverse of its probability of occurrence. Here α is a smoothing constant, its default value as suggested in the paper is 0.001. We then sum these normalized smoothed word vectors and divide by the number of words.</p><div style="display: flex;justify-content: center;">    <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -1.238ex" xmlns="http://www.w3.org/2000/svg" width="22.42ex" height="3.195ex" role="img" focusable="false" viewBox="0 -864.9 9909.7 1412" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-6-TEX-I-1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path><path id="MJX-6-TEX-I-1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path><path id="MJX-6-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-6-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-6-TEX-N-2016" d="M133 736Q138 750 153 750Q164 750 170 739Q172 735 172 250T170 -239Q164 -250 152 -250Q144 -250 138 -244L137 -243Q133 -241 133 -179T132 250Q132 731 133 736ZM329 739Q334 750 346 750Q353 750 361 744L362 743Q366 741 366 679T367 250T367 -178T362 -243L361 -244Q355 -250 347 -250Q335 -250 329 -239Q327 -235 327 250T329 739Z"></path><path id="MJX-6-TEX-I-1D446" d="M308 24Q367 24 416 76T466 197Q466 260 414 284Q308 311 278 321T236 341Q176 383 176 462Q176 523 208 573T273 648Q302 673 343 688T407 704H418H425Q521 704 564 640Q565 640 577 653T603 682T623 704Q624 704 627 704T632 705Q645 705 645 698T617 577T585 459T569 456Q549 456 549 465Q549 471 550 475Q550 478 551 494T553 520Q553 554 544 579T526 616T501 641Q465 662 419 662Q362 662 313 616T263 510Q263 480 278 458T319 427Q323 425 389 408T456 390Q490 379 522 342T554 242Q554 216 546 186Q541 164 528 137T492 78T426 18T332 -20Q320 -22 298 -22Q199 -22 144 33L134 44L106 13Q83 -14 78 -18T65 -22Q52 -22 52 -14Q52 -11 110 221Q112 227 130 227H143Q149 221 149 216Q149 214 148 207T144 186T142 153Q144 114 160 87T203 47T255 29T308 24Z"></path><path id="MJX-6-TEX-SO-2211" d="M61 748Q64 750 489 750H913L954 640Q965 609 976 579T993 533T999 516H979L959 517Q936 579 886 621T777 682Q724 700 655 705T436 710H319Q183 710 183 709Q186 706 348 484T511 259Q517 250 513 244L490 216Q466 188 420 134T330 27L149 -187Q149 -188 362 -188Q388 -188 436 -188T506 -189Q679 -189 778 -162T936 -43Q946 -27 959 6H999L913 -249L489 -250Q65 -250 62 -248Q56 -246 56 -239Q56 -234 118 -161Q186 -81 245 -11L428 206Q428 207 242 462L57 717L56 728Q56 744 61 748Z"></path><path id="MJX-6-TEX-I-1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path><path id="MJX-6-TEX-N-2208" d="M84 250Q84 372 166 450T360 539Q361 539 377 539T419 540T469 540H568Q583 532 583 520Q583 511 570 501L466 500Q355 499 329 494Q280 482 242 458T183 409T147 354T129 306T124 272V270H568Q583 262 583 250T568 230H124V228Q124 207 134 177T167 112T231 48T328 7Q355 1 466 0H570Q583 -10 583 -20Q583 -32 568 -40H471Q464 -40 446 -40T417 -41Q262 -41 172 45Q84 127 84 250Z"></path><path id="MJX-6-TEX-I-1D6FC" d="M34 156Q34 270 120 356T309 442Q379 442 421 402T478 304Q484 275 485 237V208Q534 282 560 374Q564 388 566 390T582 393Q603 393 603 385Q603 376 594 346T558 261T497 161L486 147L487 123Q489 67 495 47T514 26Q528 28 540 37T557 60Q559 67 562 68T577 70Q597 70 597 62Q597 56 591 43Q579 19 556 5T512 -10H505Q438 -10 414 62L411 69L400 61Q390 53 370 41T325 18T267 -2T203 -11Q124 -11 79 39T34 156ZM208 26Q257 26 306 47T379 90L403 112Q401 255 396 290Q382 405 304 405Q235 405 183 332Q156 292 139 224T121 120Q121 71 146 49T208 26Z"></path><path id="MJX-6-TEX-N-2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path><path id="MJX-6-TEX-I-1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-6-TEX-I-1D463"></use></g><g data-mml-node="TeXAtom" transform="translate(485, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-6-TEX-I-1D460"></use></g></g></g><g data-mml-node="mo" transform="translate(1144.4, 0)"><use xlink:href="#MJX-6-TEX-N-3D"></use></g><g data-mml-node="mfrac" transform="translate(2200.2, 0)"><g data-mml-node="mn" transform="translate(624.8, 394) scale(0.707)"><use xlink:href="#MJX-6-TEX-N-31"></use></g><g data-mml-node="mrow" transform="translate(220, -370.3) scale(0.707)"><g data-mml-node="mo"><use xlink:href="#MJX-6-TEX-N-2016"></use></g><g data-mml-node="mi" transform="translate(500, 0)"><use xlink:href="#MJX-6-TEX-I-1D446"></use></g><g data-mml-node="mo" transform="translate(1145, 0)"><use xlink:href="#MJX-6-TEX-N-2016"></use></g></g><rect width="1363.2" height="60" x="120" y="220"></rect></g><g data-mml-node="munder" transform="translate(3970, 0)"><g data-mml-node="mo"><use xlink:href="#MJX-6-TEX-SO-2211"></use></g><g data-mml-node="TeXAtom" transform="translate(1056, -285.4) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-6-TEX-I-1D464"></use></g><g data-mml-node="mo" transform="translate(716, 0)"><use xlink:href="#MJX-6-TEX-N-2208"></use></g><g data-mml-node="mi" transform="translate(1383, 0)"><use xlink:href="#MJX-6-TEX-I-1D446"></use></g></g></g><g data-mml-node="mfrac" transform="translate(6676.7, 0)"><g data-mml-node="mi" transform="translate(869.6, 394) scale(0.707)"><use xlink:href="#MJX-6-TEX-I-1D6FC"></use></g><g data-mml-node="mrow" transform="translate(220, -345) scale(0.707)"><g data-mml-node="mi"><use xlink:href="#MJX-6-TEX-I-1D6FC"></use></g><g data-mml-node="mo" transform="translate(640, 0)"><use xlink:href="#MJX-6-TEX-N-2B"></use></g><g data-mml-node="msub" transform="translate(1418, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-6-TEX-I-1D45D"></use></g><g data-mml-node="mi" transform="translate(503, -150) scale(0.707)"><use xlink:href="#MJX-6-TEX-I-1D464"></use></g></g></g><rect width="1951.7" height="60" x="120" y="220"></rect></g><g data-mml-node="msub" transform="translate(8868.4, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-6-TEX-I-1D463"></use></g><g data-mml-node="TeXAtom" transform="translate(485, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-6-TEX-I-1D464"></use></g></g></g></g></g></svg></mjx-container></div><h1 id="Talk-is-Cheap-Show-me-the-Code"><a href="#Talk-is-Cheap-Show-me-the-Code" class="headerlink" title="Talk is Cheap, Show me the Code"></a>Talk is Cheap, Show me the Code</h1><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">EmbeddingVectorizer</span><span class="token punctuation">(</span>BaseEstimator<span class="token punctuation">,</span> TransformerMixin<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">pass</span>    <span class="token keyword">def</span> <span class="token function">fit</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> X<span class="token punctuation">,</span> y<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> self    <span class="token keyword">def</span> <span class="token function">transform</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> X    <span class="token keyword">def</span> <span class="token function">fit_transform</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> X<span class="token punctuation">,</span> y<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>X<span class="token punctuation">,</span> y<span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">progressbar</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> iteration<span class="token punctuation">,</span> prefix<span class="token operator">=</span><span class="token string">""</span><span class="token punctuation">,</span> size<span class="token operator">=</span><span class="token number">50</span><span class="token punctuation">,</span> <span class="token builtin">file</span><span class="token operator">=</span>sys<span class="token punctuation">.</span>stdout<span class="token punctuation">)</span><span class="token punctuation">:</span>        count <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>iteration<span class="token punctuation">)</span>        <span class="token keyword">def</span> <span class="token function">show</span><span class="token punctuation">(</span>t<span class="token punctuation">)</span><span class="token punctuation">:</span>            x <span class="token operator">=</span> <span class="token builtin">int</span><span class="token punctuation">(</span>size<span class="token operator">*</span>t<span class="token operator">/</span>count<span class="token punctuation">)</span>            <span class="token comment"># file.write("%s[%s%s] %i/%i\r" % (prefix, "#"*x, "."*(size-x), int(100*t/count), 100))</span>            <span class="token builtin">file</span><span class="token punctuation">.</span>write<span class="token punctuation">(</span><span class="token string">"&#123;&#125;[&#123;&#125;&#123;&#125;] &#123;&#125;%\r"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>prefix<span class="token punctuation">,</span> <span class="token string">"█"</span><span class="token operator">*</span>x<span class="token punctuation">,</span> <span class="token string">"."</span><span class="token operator">*</span><span class="token punctuation">(</span>size<span class="token operator">-</span>x<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token builtin">int</span><span class="token punctuation">(</span><span class="token number">100</span><span class="token operator">*</span>t<span class="token operator">/</span>count<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>            <span class="token builtin">file</span><span class="token punctuation">.</span>flush<span class="token punctuation">(</span><span class="token punctuation">)</span>        show<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>        <span class="token keyword">for</span> i<span class="token punctuation">,</span> item <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>iteration<span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token keyword">yield</span> item            show<span class="token punctuation">(</span>i<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span>        <span class="token builtin">file</span><span class="token punctuation">.</span>write<span class="token punctuation">(</span><span class="token string">"\n"</span><span class="token punctuation">)</span>        <span class="token builtin">file</span><span class="token punctuation">.</span>flush<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">class</span> <span class="token class-name">MeanEmbeddingVectorizer</span><span class="token punctuation">(</span>EmbeddingVectorizer<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""    Parameters    ----------    word2vec: gensim.models.KeyedVectors()        Word2Vec: https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz        GloVe: https://nlp.stanford.edu/projects/glove/        FastText: https://fasttext.cc/docs/en/crawl-vectors.html    Examples    --------    >>> from gensim.scripts import glove2word2vec    >>> from gensim.models import KeyedVectors    >>> w2v_model = KeyedVectors.load_word2vec_format("GoogleNews-vectors-negative300.bin", binary=True)    >>> glove2word2vec(glove_input_file=r"glove.840B.300d.txt", word2vec_output_file=r"gensim_glove_vectors.txt")    >>> glove_model = KeyedVectors.load_word2vec_format("gensim_glove_vectors.txt", binary=False)    >>> embedding_dict = KeyedVectors.load_word2vec_format(r"cc.en.300.vec", binary=False)    >>> embedding_dict.save_word2vec_format(r"cc.en.300.bin", binary=True)    >>> ft_model = KeyedVectors.load_word2vec_format("cc.en.300.bin", binary=True)    >>> vectoriser = MeanEmbeddingVectorizer(word2vec=w2v_model)    >>> feature = vectoriser.fit_transform(df["text"], None)    """</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> word2vec<span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>word2vec <span class="token operator">=</span> word2vec        self<span class="token punctuation">.</span>dim <span class="token operator">=</span> word2vec<span class="token punctuation">.</span>vector_size    <span class="token keyword">def</span> <span class="token function">fit</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> X<span class="token punctuation">,</span> y<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> self    <span class="token keyword">def</span> <span class="token function">transform</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> X<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span>            np<span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">[</span>self<span class="token punctuation">.</span>word2vec<span class="token punctuation">[</span>w<span class="token punctuation">]</span> <span class="token keyword">for</span> w <span class="token keyword">in</span> words <span class="token keyword">if</span> w <span class="token keyword">in</span> self<span class="token punctuation">.</span>word2vec<span class="token punctuation">]</span>                    <span class="token keyword">or</span> <span class="token punctuation">[</span>np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>self<span class="token punctuation">.</span>dim<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>            <span class="token keyword">for</span> words <span class="token keyword">in</span> self<span class="token punctuation">.</span>progressbar<span class="token punctuation">(</span>X<span class="token punctuation">,</span> prefix<span class="token operator">=</span><span class="token string">"Mean"</span><span class="token punctuation">)</span>        <span class="token punctuation">]</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">fit_transform</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> X<span class="token punctuation">,</span> y<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>X<span class="token punctuation">,</span> y<span class="token punctuation">)</span>        <span class="token keyword">return</span> self<span class="token punctuation">.</span>transform<span class="token punctuation">(</span>X<span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">get_params</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> deep<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> <span class="token punctuation">&#123;</span><span class="token string">"word2vec"</span><span class="token punctuation">:</span> self<span class="token punctuation">.</span>word2vec<span class="token punctuation">&#125;</span>    <span class="token keyword">def</span> <span class="token function">set_params</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> <span class="token operator">**</span>parameters<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">for</span> parameter<span class="token punctuation">,</span> value <span class="token keyword">in</span> parameters<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token builtin">setattr</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> parameter<span class="token punctuation">,</span> value<span class="token punctuation">)</span>        <span class="token keyword">return</span> self<span class="token keyword">class</span> <span class="token class-name">TfidfEmbeddingVectorizer</span><span class="token punctuation">(</span>EmbeddingVectorizer<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""    Parameters    ----------    word2vec: gensim.models.KeyedVectors()        Word2Vec: https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz        GloVe: https://nlp.stanford.edu/projects/glove/        FastText: https://fasttext.cc/docs/en/crawl-vectors.html    use_idf: boolean        IDF stands for "Inverse Document Frequency", it is a measure of how much information        the word provide.    Examples    --------    >>> from gensim.scripts import glove2word2vec    >>> from gensim.models import KeyedVectors    >>> w2v_model = KeyedVectors.load_word2vec_format("GoogleNews-vectors-negative300.bin", binary=True)    >>> glove2word2vec(glove_input_file=r"glove.840B.300d.txt", word2vec_output_file=r"gensim_glove_vectors.txt")    >>> glove_model = KeyedVectors.load_word2vec_format("gensim_glove_vectors.txt", binary=False)    >>> embedding_dict = KeyedVectors.load_word2vec_format(r"cc.en.300.vec", binary=False)    >>> embedding_dict.save_word2vec_format(r"cc.en.300.bin", binary=True)    >>> ft_model = KeyedVectors.load_word2vec_format("cc.en.300.bin", binary=True)    >>> vectoriser = TfidfEmbeddingVectorizer(word2vec=w2v_model, use_idf=True)    >>> feature = vectoriser.fit_transform(df["text"], None)    """</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> word2vec<span class="token punctuation">,</span> use_idf<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>word2vec <span class="token operator">=</span> word2vec        self<span class="token punctuation">.</span>word2weight <span class="token operator">=</span> <span class="token boolean">None</span>        self<span class="token punctuation">.</span>use_idf <span class="token operator">=</span> use_idf        self<span class="token punctuation">.</span>dim <span class="token operator">=</span> word2vec<span class="token punctuation">.</span>vector_size    <span class="token keyword">def</span> <span class="token function">word2tf</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> term_list<span class="token punctuation">)</span><span class="token punctuation">:</span>        term_freq <span class="token operator">=</span> Counter<span class="token punctuation">(</span>term_list<span class="token punctuation">)</span>        total_len <span class="token operator">=</span> <span class="token builtin">sum</span><span class="token punctuation">(</span>term_freq<span class="token punctuation">.</span>values<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        term_freq <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">(</span>term<span class="token punctuation">,</span> term_freq<span class="token punctuation">[</span>term<span class="token punctuation">]</span><span class="token operator">/</span>total_len<span class="token punctuation">)</span> <span class="token keyword">for</span> term<span class="token punctuation">,</span> count <span class="token keyword">in</span> term_freq<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">]</span>        <span class="token keyword">return</span> <span class="token builtin">dict</span><span class="token punctuation">(</span>term_freq<span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">fit</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> X<span class="token punctuation">,</span> y<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        tfidf <span class="token operator">=</span> TfidfVectorizer<span class="token punctuation">(</span>analyzer<span class="token operator">=</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> x<span class="token punctuation">)</span>        tfidf<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>X<span class="token punctuation">)</span>        max_idf <span class="token operator">=</span> <span class="token builtin">max</span><span class="token punctuation">(</span>tfidf<span class="token punctuation">.</span>idf_<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>word2weight <span class="token operator">=</span> defaultdict<span class="token punctuation">(</span>            <span class="token keyword">lambda</span><span class="token punctuation">:</span> max_idf<span class="token punctuation">,</span>            <span class="token punctuation">[</span><span class="token punctuation">(</span>w<span class="token punctuation">,</span> tfidf<span class="token punctuation">.</span>idf_<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token keyword">for</span> w<span class="token punctuation">,</span> i <span class="token keyword">in</span> tfidf<span class="token punctuation">.</span>vocabulary_<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> self    <span class="token keyword">def</span> <span class="token function">transform</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> X<span class="token punctuation">)</span><span class="token punctuation">:</span>        transformed_X <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>        <span class="token keyword">for</span> doc <span class="token keyword">in</span> self<span class="token punctuation">.</span>progressbar<span class="token punctuation">(</span>X<span class="token punctuation">,</span> prefix<span class="token operator">=</span><span class="token string">"TF-IDF"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            weighted_array <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>            <span class="token keyword">for</span> term <span class="token keyword">in</span> doc<span class="token punctuation">:</span>                <span class="token keyword">if</span> term <span class="token keyword">in</span> self<span class="token punctuation">.</span>word2vec<span class="token punctuation">:</span>                    <span class="token keyword">if</span> self<span class="token punctuation">.</span>use_idf<span class="token punctuation">:</span>                        weighted_term <span class="token operator">=</span> self<span class="token punctuation">.</span>word2vec<span class="token punctuation">[</span>term<span class="token punctuation">]</span> <span class="token operator">*</span> self<span class="token punctuation">.</span>word2tf<span class="token punctuation">(</span>doc<span class="token punctuation">)</span><span class="token punctuation">[</span>term<span class="token punctuation">]</span> <span class="token operator">*</span> self<span class="token punctuation">.</span>word2weight<span class="token punctuation">[</span>term<span class="token punctuation">]</span>                    <span class="token keyword">else</span><span class="token punctuation">:</span>                        weighted_term <span class="token operator">=</span> self<span class="token punctuation">.</span>word2vec<span class="token punctuation">[</span>term<span class="token punctuation">]</span> <span class="token operator">*</span> self<span class="token punctuation">.</span>word2tf<span class="token punctuation">(</span>doc<span class="token punctuation">)</span><span class="token punctuation">[</span>term<span class="token punctuation">]</span>                    weighted_array<span class="token punctuation">.</span>append<span class="token punctuation">(</span>weighted_term<span class="token punctuation">)</span>            weighted_array <span class="token operator">=</span> np<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>weighted_array <span class="token keyword">or</span> <span class="token punctuation">[</span>np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>self<span class="token punctuation">.</span>dim<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>            transformed_X<span class="token punctuation">.</span>append<span class="token punctuation">(</span>weighted_array<span class="token punctuation">)</span>        <span class="token keyword">return</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>transformed_X<span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">fit_transform</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> X<span class="token punctuation">,</span> y<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>X<span class="token punctuation">,</span> y<span class="token punctuation">)</span>        <span class="token keyword">return</span> self<span class="token punctuation">.</span>transform<span class="token punctuation">(</span>X<span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">get_params</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> deep<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> <span class="token punctuation">&#123;</span><span class="token string">"word2vec"</span><span class="token punctuation">:</span> self<span class="token punctuation">.</span>word2vec<span class="token punctuation">,</span> <span class="token string">"use_idf"</span><span class="token punctuation">:</span> self<span class="token punctuation">.</span>use_idf<span class="token punctuation">&#125;</span>    <span class="token keyword">def</span> <span class="token function">set_params</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> <span class="token operator">**</span>parameters<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">for</span> parameter<span class="token punctuation">,</span> value <span class="token keyword">in</span> parameters<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token builtin">setattr</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> parameter<span class="token punctuation">,</span> value<span class="token punctuation">)</span>        <span class="token keyword">return</span> self    <span class="token keyword">class</span> <span class="token class-name">SifEmbeddingVectorizer</span><span class="token punctuation">(</span>EmbeddingVectorizer<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""    Parameters    ----------    word2vec: gensim.models.KeyedVectors()        Word2Vec: https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz        GloVe: https://nlp.stanford.edu/projects/glove/        FastText: https://fasttext.cc/docs/en/crawl-vectors.html    smoothing_constant: float (default: 1e-3)        Default value of smoothing constant suggested in the paper is 0.001.        The range of a suggested in the paper: [1e−4, 1e−3]    Examples    --------    >>> from gensim.scripts import glove2word2vec    >>> from gensim.models import KeyedVectors    >>> w2v_model = KeyedVectors.load_word2vec_format("GoogleNews-vectors-negative300.bin", binary=True)    >>> glove2word2vec(glove_input_file=r"glove.840B.300d.txt", word2vec_output_file=r"gensim_glove_vectors.txt")    >>> glove_model = KeyedVectors.load_word2vec_format("gensim_glove_vectors.txt", binary=False)    >>> embedding_dict = KeyedVectors.load_word2vec_format(r"cc.en.300.vec", binary=False)    >>> embedding_dict.save_word2vec_format(r"cc.en.300.bin", binary=True)    >>> ft_model = KeyedVectors.load_word2vec_format("cc.en.300.bin", binary=True)    >>> vectoriser = SifEmbeddingVectorizer(word2vec=w2v_model)    >>> feature = vectoriser.fit_transform(df["text"], None)    """</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> word2vec<span class="token punctuation">,</span> smoothing_constant<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>word2vec <span class="token operator">=</span> word2vec        self<span class="token punctuation">.</span>word2weight <span class="token operator">=</span> <span class="token boolean">None</span>        self<span class="token punctuation">.</span>dim <span class="token operator">=</span> word2vec<span class="token punctuation">.</span>vector_size        self<span class="token punctuation">.</span>smoothing_constant <span class="token operator">=</span> smoothing_constant        self<span class="token punctuation">.</span>term_freq <span class="token operator">=</span> <span class="token boolean">None</span>    <span class="token keyword">def</span> <span class="token function">fit</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> X<span class="token punctuation">,</span> y<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        X_list <span class="token operator">=</span> <span class="token punctuation">[</span>item <span class="token keyword">for</span> sublist <span class="token keyword">in</span> X <span class="token keyword">for</span> item <span class="token keyword">in</span> sublist<span class="token punctuation">]</span>        term_freq <span class="token operator">=</span> Counter<span class="token punctuation">(</span>X_list<span class="token punctuation">)</span>        total_len <span class="token operator">=</span> <span class="token builtin">sum</span><span class="token punctuation">(</span>term_freq<span class="token punctuation">.</span>values<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        term_freq <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">(</span>term<span class="token punctuation">,</span> term_freq<span class="token punctuation">[</span>term<span class="token punctuation">]</span><span class="token operator">/</span>total_len<span class="token punctuation">)</span> <span class="token keyword">for</span> term<span class="token punctuation">,</span> count <span class="token keyword">in</span> term_freq<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">]</span>        self<span class="token punctuation">.</span>term_freq <span class="token operator">=</span> <span class="token builtin">dict</span><span class="token punctuation">(</span>term_freq<span class="token punctuation">)</span>        <span class="token keyword">return</span> self    <span class="token keyword">def</span> <span class="token function">transform</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> X<span class="token punctuation">)</span><span class="token punctuation">:</span>        transformed_X <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>        <span class="token keyword">for</span> doc <span class="token keyword">in</span> self<span class="token punctuation">.</span>progressbar<span class="token punctuation">(</span>X<span class="token punctuation">,</span> prefix<span class="token operator">=</span><span class="token string">"SIF"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            weighted_array <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>            <span class="token keyword">for</span> term <span class="token keyword">in</span> doc<span class="token punctuation">:</span>                <span class="token keyword">if</span> term <span class="token keyword">in</span> self<span class="token punctuation">.</span>word2vec<span class="token punctuation">:</span>                    <span class="token comment"># Compute smooth inverse frequency (SIF)</span>                    weight <span class="token operator">=</span> self<span class="token punctuation">.</span>smoothing_constant <span class="token operator">/</span> <span class="token punctuation">(</span>self<span class="token punctuation">.</span>smoothing_constant <span class="token operator">+</span> self<span class="token punctuation">.</span>term_freq<span class="token punctuation">.</span>get<span class="token punctuation">(</span>term<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span>                    weighted_term <span class="token operator">=</span> self<span class="token punctuation">.</span>word2vec<span class="token punctuation">[</span>term<span class="token punctuation">]</span> <span class="token operator">*</span> weight                    weighted_array<span class="token punctuation">.</span>append<span class="token punctuation">(</span>weighted_term<span class="token punctuation">)</span>            weighted_array <span class="token operator">=</span> np<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>weighted_array <span class="token keyword">or</span> <span class="token punctuation">[</span>np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>self<span class="token punctuation">.</span>dim<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>            transformed_X<span class="token punctuation">.</span>append<span class="token punctuation">(</span>weighted_array<span class="token punctuation">)</span>        transformed_X <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>transformed_X<span class="token punctuation">)</span>        <span class="token comment"># Common component removal: remove the projections of the average vectors on their first singular vector</span>        svd <span class="token operator">=</span> TruncatedSVD<span class="token punctuation">(</span>n_components<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> n_iter<span class="token operator">=</span><span class="token number">20</span><span class="token punctuation">,</span> random_state<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>        svd<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>transformed_X<span class="token punctuation">)</span>        pc <span class="token operator">=</span> svd<span class="token punctuation">.</span>components_        transformed_X <span class="token operator">=</span> transformed_X <span class="token operator">-</span> transformed_X<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>pc<span class="token punctuation">.</span>T<span class="token punctuation">)</span><span class="token punctuation">.</span>dot<span class="token punctuation">(</span>pc<span class="token punctuation">)</span>        <span class="token keyword">return</span> transformed_X    <span class="token keyword">def</span> <span class="token function">fit_transform</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> X<span class="token punctuation">,</span> y<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>X<span class="token punctuation">,</span> y<span class="token punctuation">)</span>        <span class="token keyword">return</span> self<span class="token punctuation">.</span>transform<span class="token punctuation">(</span>X<span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">get_params</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> deep<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> <span class="token punctuation">&#123;</span><span class="token string">"word2vec"</span><span class="token punctuation">:</span> self<span class="token punctuation">.</span>word2vec<span class="token punctuation">,</span> <span class="token string">"smoothing_constant"</span><span class="token punctuation">:</span> self<span class="token punctuation">.</span>smoothing_constant<span class="token punctuation">&#125;</span>    <span class="token keyword">def</span> <span class="token function">set_params</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> <span class="token operator">**</span>parameters<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">for</span> parameter<span class="token punctuation">,</span> value <span class="token keyword">in</span> parameters<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token builtin">setattr</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> parameter<span class="token punctuation">,</span> value<span class="token punctuation">)</span>        <span class="token keyword">return</span> self<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>After constructing all needed class for weighted word embedding, next step is to see which combination of the embedding method performs the best.</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># Word2Vec</span>vectoriser_w2v_mean <span class="token operator">=</span> MeanEmbeddingVectorizer<span class="token punctuation">(</span>word2vec<span class="token operator">=</span>w2v_model<span class="token punctuation">)</span>feature_train_w2v_mean <span class="token operator">=</span> vectoriser_w2v_mean<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>df_train<span class="token punctuation">[</span><span class="token string">"text_tokenised"</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">)</span>vectoriser_w2v_tfidf <span class="token operator">=</span> TfidfEmbeddingVectorizer<span class="token punctuation">(</span>word2vec<span class="token operator">=</span>w2v_model<span class="token punctuation">)</span>feature_train_w2v_tfidf <span class="token operator">=</span> vectoriser_w2v_tfidf<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>df_train<span class="token punctuation">[</span><span class="token string">"text_tokenised"</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">)</span>vectoriser_w2v_sif <span class="token operator">=</span> SifEmbeddingVectorizer<span class="token punctuation">(</span>word2vec<span class="token operator">=</span>w2v_model<span class="token punctuation">)</span>feature_train_w2v_sif <span class="token operator">=</span> vectoriser_w2v_sif<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>df_train<span class="token punctuation">[</span><span class="token string">"text_tokenised"</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># GloVe</span>vectoriser_glove_mean <span class="token operator">=</span> MeanEmbeddingVectorizer<span class="token punctuation">(</span>word2vec<span class="token operator">=</span>glove_model<span class="token punctuation">)</span>feature_train_glove_mean <span class="token operator">=</span> vectoriser_glove_mean<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>df_train<span class="token punctuation">[</span><span class="token string">"text_tokenised"</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">)</span>vectoriser_glove_tfidf <span class="token operator">=</span> TfidfEmbeddingVectorizer<span class="token punctuation">(</span>word2vec<span class="token operator">=</span>glove_model<span class="token punctuation">)</span>feature_train_glove_tfidf <span class="token operator">=</span> vectoriser_glove_tfidf<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>df_train<span class="token punctuation">[</span><span class="token string">"text_tokenised"</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">)</span>vectoriser_glove_sif <span class="token operator">=</span> SifEmbeddingVectorizer<span class="token punctuation">(</span>word2vec<span class="token operator">=</span>glove_model<span class="token punctuation">)</span>feature_train_glove_sif <span class="token operator">=</span> vectoriser_glove_sif<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>df_train<span class="token punctuation">[</span><span class="token string">"text_tokenised"</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># FastText</span>vectoriser_ft_mean <span class="token operator">=</span> MeanEmbeddingVectorizer<span class="token punctuation">(</span>word2vec<span class="token operator">=</span>ft_model<span class="token punctuation">)</span>feature_train_ft_mean <span class="token operator">=</span> vectoriser_ft_mean<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>df_train<span class="token punctuation">[</span><span class="token string">"text_tokenised"</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">)</span>vectoriser_ft_tfidf <span class="token operator">=</span> TfidfEmbeddingVectorizer<span class="token punctuation">(</span>word2vec<span class="token operator">=</span>ft_model<span class="token punctuation">)</span>feature_train_ft_tfidf <span class="token operator">=</span> vectoriser_ft_tfidf<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>df_train<span class="token punctuation">[</span><span class="token string">"text_tokenised"</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">)</span>vectoriser_ft_sif <span class="token operator">=</span> SifEmbeddingVectorizer<span class="token punctuation">(</span>word2vec<span class="token operator">=</span>ft_model<span class="token punctuation">)</span>feature_train_ft_sif <span class="token operator">=</span> vectoriser_ft_sif<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>df_train<span class="token punctuation">[</span><span class="token string">"text_tokenised"</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>I stored these into dictionaries.</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">vectorisers <span class="token operator">=</span> <span class="token punctuation">&#123;</span>    <span class="token string">"word2vec"</span><span class="token punctuation">:</span> <span class="token punctuation">&#123;</span>        <span class="token string">"mean"</span><span class="token punctuation">:</span> vectoriser_w2v_mean<span class="token punctuation">,</span>         <span class="token string">"tfidf"</span><span class="token punctuation">:</span> vectoriser_w2v_tfidf<span class="token punctuation">,</span>         <span class="token string">"sif"</span><span class="token punctuation">:</span> vectoriser_w2v_sif    <span class="token punctuation">&#125;</span><span class="token punctuation">,</span>     <span class="token string">"glove"</span><span class="token punctuation">:</span> <span class="token punctuation">&#123;</span>        <span class="token string">"mean"</span><span class="token punctuation">:</span> vectoriser_glove_mean<span class="token punctuation">,</span>         <span class="token string">"tfidf"</span><span class="token punctuation">:</span> vectoriser_glove_tfidf<span class="token punctuation">,</span>         <span class="token string">"sif"</span><span class="token punctuation">:</span> vectoriser_glove_sif    <span class="token punctuation">&#125;</span><span class="token punctuation">,</span>     <span class="token string">"fasttext"</span><span class="token punctuation">:</span> <span class="token punctuation">&#123;</span>        <span class="token string">"mean"</span><span class="token punctuation">:</span> vectoriser_ft_mean<span class="token punctuation">,</span>         <span class="token string">"tfidf"</span><span class="token punctuation">:</span> vectoriser_ft_tfidf<span class="token punctuation">,</span>         <span class="token string">"sif"</span><span class="token punctuation">:</span> vectoriser_ft_sif    <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span>features <span class="token operator">=</span> <span class="token punctuation">&#123;</span>    <span class="token string">"word2vec"</span><span class="token punctuation">:</span> <span class="token punctuation">&#123;</span>        <span class="token string">"mean"</span><span class="token punctuation">:</span> feature_train_w2v_mean<span class="token punctuation">,</span>         <span class="token string">"tfidf"</span><span class="token punctuation">:</span> feature_train_w2v_tfidf<span class="token punctuation">,</span>         <span class="token string">"sif"</span><span class="token punctuation">:</span> feature_train_w2v_sif    <span class="token punctuation">&#125;</span><span class="token punctuation">,</span>     <span class="token string">"glove"</span><span class="token punctuation">:</span> <span class="token punctuation">&#123;</span>        <span class="token string">"mean"</span><span class="token punctuation">:</span> feature_train_glove_mean<span class="token punctuation">,</span>         <span class="token string">"tfidf"</span><span class="token punctuation">:</span> feature_train_glove_tfidf<span class="token punctuation">,</span>         <span class="token string">"sif"</span><span class="token punctuation">:</span> feature_train_glove_sif    <span class="token punctuation">&#125;</span><span class="token punctuation">,</span>     <span class="token string">"fasttext"</span><span class="token punctuation">:</span> <span class="token punctuation">&#123;</span>        <span class="token string">"mean"</span><span class="token punctuation">:</span> feature_train_ft_mean<span class="token punctuation">,</span>         <span class="token string">"tfidf"</span><span class="token punctuation">:</span> feature_train_ft_tfidf<span class="token punctuation">,</span>         <span class="token string">"sif"</span><span class="token punctuation">:</span> feature_train_ft_sif    <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="Build-Models"><a href="#Build-Models" class="headerlink" title="Build Models"></a>Build Models</h1><p>Finally, we could test it with machine learning model! </p><p>Define a cross validation function <code>cross_val()</code> to prevent models from overfitting.</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">cross_val</span><span class="token punctuation">(</span>Xtrain<span class="token punctuation">,</span> ytrain<span class="token punctuation">,</span> clf<span class="token punctuation">,</span> cv<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">,</span> verbose<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    best_clf <span class="token operator">=</span> <span class="token boolean">None</span>    best_score <span class="token operator">=</span> <span class="token number">0.0</span>    num_folds <span class="token operator">=</span> <span class="token number">0</span>    cv_scores <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    kfold <span class="token operator">=</span> KFold<span class="token punctuation">(</span>n_splits<span class="token operator">=</span>cv<span class="token punctuation">)</span>    <span class="token keyword">for</span> train<span class="token punctuation">,</span> val <span class="token keyword">in</span> kfold<span class="token punctuation">.</span>split<span class="token punctuation">(</span>Xtrain<span class="token punctuation">)</span><span class="token punctuation">:</span>        Xctrain<span class="token punctuation">,</span> Xctest<span class="token punctuation">,</span> yctrain<span class="token punctuation">,</span> yctest <span class="token operator">=</span> Xtrain<span class="token punctuation">[</span>train<span class="token punctuation">]</span><span class="token punctuation">,</span> Xtrain<span class="token punctuation">[</span>val<span class="token punctuation">]</span><span class="token punctuation">,</span> ytrain<span class="token punctuation">[</span>train<span class="token punctuation">]</span><span class="token punctuation">,</span> ytrain<span class="token punctuation">[</span>val<span class="token punctuation">]</span>        clf<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>Xctrain<span class="token punctuation">,</span> yctrain<span class="token punctuation">)</span>        score <span class="token operator">=</span> clf<span class="token punctuation">.</span>score<span class="token punctuation">(</span>Xctest<span class="token punctuation">,</span> yctest<span class="token punctuation">)</span>        <span class="token keyword">if</span> score <span class="token operator">></span> best_score<span class="token punctuation">:</span>            best_score <span class="token operator">=</span> score            best_clf <span class="token operator">=</span> clf        <span class="token keyword">if</span> verbose<span class="token punctuation">:</span>            <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Fold &#123;:d&#125;: score: &#123;:.4f&#125;"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>num_folds<span class="token punctuation">,</span> score<span class="token punctuation">)</span><span class="token punctuation">)</span>        cv_scores<span class="token punctuation">.</span>append<span class="token punctuation">(</span>score<span class="token punctuation">)</span>        num_folds <span class="token operator">+=</span> <span class="token number">1</span>    <span class="token keyword">return</span> best_clf<span class="token punctuation">,</span> cv_scores<span class="token keyword">def</span> <span class="token function">test_eval</span><span class="token punctuation">(</span>Xtest<span class="token punctuation">,</span> ytest<span class="token punctuation">,</span> clf<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Test set results"</span><span class="token punctuation">)</span>    ytest_ <span class="token operator">=</span> clf<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>Xtest<span class="token punctuation">)</span>    accuracy <span class="token operator">=</span> accuracy_score<span class="token punctuation">(</span>ytest<span class="token punctuation">,</span> ytest_<span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Accuracy: &#123;:.4f&#125;"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>accuracy<span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>Let the training begin!</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">results <span class="token operator">=</span> defaultdict<span class="token punctuation">(</span><span class="token builtin">dict</span><span class="token punctuation">)</span><span class="token keyword">for</span> embedding <span class="token keyword">in</span> <span class="token punctuation">[</span><span class="token string">"word2vec"</span><span class="token punctuation">,</span> <span class="token string">"glove"</span><span class="token punctuation">,</span> <span class="token string">"fasttext"</span><span class="token punctuation">]</span><span class="token punctuation">:</span>    <span class="token keyword">for</span> weighting <span class="token keyword">in</span> <span class="token punctuation">[</span><span class="token string">"mean"</span><span class="token punctuation">,</span> <span class="token string">"tfidf"</span><span class="token punctuation">,</span> <span class="token string">"sif"</span><span class="token punctuation">]</span><span class="token punctuation">:</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Training with </span><span class="token interpolation"><span class="token punctuation">&#123;</span>embedding<span class="token punctuation">&#125;</span></span><span class="token string">-</span><span class="token interpolation"><span class="token punctuation">&#123;</span>weighting<span class="token punctuation">&#125;</span></span><span class="token string"> model..."</span></span><span class="token punctuation">)</span>        clf <span class="token operator">=</span> xgboost<span class="token punctuation">.</span>XGBClassifier<span class="token punctuation">(</span>n_estimators<span class="token operator">=</span><span class="token number">300</span><span class="token punctuation">,</span> max_depth<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">)</span>        clf<span class="token punctuation">,</span> cv_scores <span class="token operator">=</span> cross_val<span class="token punctuation">(</span>features<span class="token punctuation">[</span>embedding<span class="token punctuation">]</span><span class="token punctuation">[</span>weighting<span class="token punctuation">]</span><span class="token punctuation">,</span> df_train<span class="token punctuation">.</span>label<span class="token punctuation">.</span>values<span class="token punctuation">,</span> clf<span class="token punctuation">,</span> verbose<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>        results<span class="token punctuation">[</span><span class="token string-interpolation"><span class="token string">f"</span><span class="token interpolation"><span class="token punctuation">&#123;</span>embedding<span class="token punctuation">&#125;</span></span><span class="token string">-</span><span class="token interpolation"><span class="token punctuation">&#123;</span>weighting<span class="token punctuation">&#125;</span></span><span class="token string">"</span></span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">"classifer"</span><span class="token punctuation">]</span> <span class="token operator">=</span> clf        results<span class="token punctuation">[</span><span class="token string-interpolation"><span class="token string">f"</span><span class="token interpolation"><span class="token punctuation">&#123;</span>embedding<span class="token punctuation">&#125;</span></span><span class="token string">-</span><span class="token interpolation"><span class="token punctuation">&#123;</span>weighting<span class="token punctuation">&#125;</span></span><span class="token string">"</span></span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">"cv_result"</span><span class="token punctuation">]</span> <span class="token operator">=</span> cv_scores        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Done with training model!"</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>You can also play around with putting these customised functions in Scikit-Learn pipeline. We have seen that some estimators can transform data and that some estimators can predict variables. We can also create combined estimators like the following:</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">pca <span class="token operator">=</span> PCA<span class="token punctuation">(</span><span class="token punctuation">)</span>logistic <span class="token operator">=</span> LogisticRegression<span class="token punctuation">(</span>max_iter<span class="token operator">=</span><span class="token number">10000</span><span class="token punctuation">,</span> tol<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">)</span>pipe <span class="token operator">=</span> Pipeline<span class="token punctuation">(</span>steps<span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token string">"word2vec vectorizer (tfidf)"</span><span class="token punctuation">,</span> TfidfEmbeddingVectorizer<span class="token punctuation">(</span>w2v_model<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                        <span class="token punctuation">(</span><span class="token string">'pca'</span><span class="token punctuation">,</span> pca<span class="token punctuation">)</span><span class="token punctuation">,</span>                        <span class="token punctuation">(</span><span class="token string">'logistic'</span><span class="token punctuation">,</span> logistic<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span> verbose<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token comment"># Parameters of pipelines can be set using ‘__’ separated parameter names:</span>param_grid <span class="token operator">=</span> <span class="token punctuation">&#123;</span>    <span class="token string">'pca__n_components'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">]</span><span class="token punctuation">&#125;</span>search <span class="token operator">=</span> GridSearchCV<span class="token punctuation">(</span>pipe<span class="token punctuation">,</span> param_grid<span class="token punctuation">,</span> n_jobs<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">,</span> cv<span class="token operator">=</span>StratifiedKFold<span class="token punctuation">(</span>n_splits<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> random_state<span class="token operator">=</span><span class="token number">42</span><span class="token punctuation">)</span><span class="token punctuation">)</span>search<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>X_train<span class="token punctuation">,</span> y_train<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>Plot the PCA specturm.</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">X_embed <span class="token operator">=</span> TfidfEmbeddingVectorizer<span class="token punctuation">(</span>w2v_model<span class="token punctuation">)</span><span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>X_train<span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">)</span>pca<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>X_embed<span class="token punctuation">)</span>fig<span class="token punctuation">,</span> <span class="token punctuation">(</span>ax0<span class="token punctuation">,</span> ax1<span class="token punctuation">)</span> <span class="token operator">=</span> plt<span class="token punctuation">.</span>subplots<span class="token punctuation">(</span>nrows<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> sharex<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> figsize<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">15</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">)</span><span class="token punctuation">)</span>ax0<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>np<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> pca<span class="token punctuation">.</span>n_components_ <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>         pca<span class="token punctuation">.</span>explained_variance_ratio_<span class="token punctuation">,</span> <span class="token string">'+'</span><span class="token punctuation">,</span> linewidth<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>ax0<span class="token punctuation">.</span>set_ylabel<span class="token punctuation">(</span><span class="token string">'PCA explained variance ratio'</span><span class="token punctuation">)</span>ax0<span class="token punctuation">.</span>axvline<span class="token punctuation">(</span>search<span class="token punctuation">.</span>best_estimator_<span class="token punctuation">.</span>named_steps<span class="token punctuation">[</span><span class="token string">'pca'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>n_components<span class="token punctuation">,</span>            linestyle<span class="token operator">=</span><span class="token string">':'</span><span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">'N Components Chosen'</span><span class="token punctuation">)</span>ax0<span class="token punctuation">.</span>legend<span class="token punctuation">(</span>prop<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span>size<span class="token operator">=</span><span class="token number">12</span><span class="token punctuation">)</span><span class="token punctuation">,</span> loc<span class="token operator">=</span><span class="token string">"upper right"</span><span class="token punctuation">)</span>ax0<span class="token punctuation">.</span>grid<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment"># For each number of components, find the best classifier results</span>results <span class="token operator">=</span> pd<span class="token punctuation">.</span>DataFrame<span class="token punctuation">(</span>search<span class="token punctuation">.</span>cv_results_<span class="token punctuation">)</span>components_col <span class="token operator">=</span> <span class="token string">'param_pca__n_components'</span>best_clfs <span class="token operator">=</span> results<span class="token punctuation">.</span>groupby<span class="token punctuation">(</span>components_col<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">apply</span><span class="token punctuation">(</span>    <span class="token keyword">lambda</span> g<span class="token punctuation">:</span> g<span class="token punctuation">.</span>nlargest<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token string">'mean_test_score'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>best_clfs<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>x<span class="token operator">=</span>components_col<span class="token punctuation">,</span> y<span class="token operator">=</span><span class="token string">'mean_test_score'</span><span class="token punctuation">,</span> yerr<span class="token operator">=</span><span class="token string">'std_test_score'</span><span class="token punctuation">,</span>               legend<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> ax<span class="token operator">=</span>ax1<span class="token punctuation">)</span>ax1<span class="token punctuation">.</span>set_ylabel<span class="token punctuation">(</span><span class="token string">'Classification accuracy (val)'</span><span class="token punctuation">)</span>ax1<span class="token punctuation">.</span>set_xlabel<span class="token punctuation">(</span><span class="token string">'N Components'</span><span class="token punctuation">)</span>ax1<span class="token punctuation">.</span>grid<span class="token punctuation">(</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>xlim<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">130</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>tight_layout<span class="token punctuation">(</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><img src="/Hexo-Blog/2021/01/25/2021-01-25-weighted-word-embedding/pca.png" class=""><h1 id="Perfomance"><a href="#Perfomance" class="headerlink" title="Perfomance"></a>Perfomance</h1><p>Some algorithms favor simple averaging, some algorithms perform better with TF-IDF weighting. Let’s see what’s the best in this text classification task.</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt<span class="token keyword">import</span> seaborn <span class="token keyword">as</span> snsplt<span class="token punctuation">.</span>figure<span class="token punctuation">(</span>figsize<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">15</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">)</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>boxplot<span class="token punctuation">(</span><span class="token punctuation">[</span>c<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token keyword">for</span> c <span class="token keyword">in</span> cv_list<span class="token punctuation">]</span><span class="token punctuation">,</span> labels<span class="token operator">=</span><span class="token punctuation">[</span>c<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token keyword">for</span> c <span class="token keyword">in</span> cv_list<span class="token punctuation">]</span><span class="token punctuation">,</span> showmeans<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><img src="/Hexo-Blog/2021/01/25/2021-01-25-weighted-word-embedding/performance.png" class=""><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>Distributed representation of word is an interesting field that is actively studied. Furthermore, it can be applied to many downstream tasks and real-life application. I hope this article will help you understand the basic concept of word embedding. I have been posting information and tutorials on my <a href="https://github.com/penguinwang96825">GitHub</a>, if you are interested in these fields, PLEASE FOLLOW ME!!!</p><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ol><li><a href="https://www.kaggle.com/reiinakano/basic-nlp-bag-of-words-tf-idf-word2vec-lstm">https://www.kaggle.com/reiinakano/basic-nlp-bag-of-words-tf-idf-word2vec-lstm</a></li><li><a href="http://nadbordrozd.github.io/blog/2016/05/20/text-classification-with-word2vec/">http://nadbordrozd.github.io/blog/2016/05/20/text-classification-with-word2vec/</a></li><li><a href="https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/">https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/</a></li><li><a href="https://jonathan-hui.medium.com/nlp-word-embedding-glove-5e7f523999f6">https://jonathan-hui.medium.com/nlp-word-embedding-glove-5e7f523999f6</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> NLP </tag>
            
            <tag> Embedding </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Train Word2Vec Model on WSL</title>
      <link href="/Hexo-Blog/2021/01/22/2021-01-22-train-word2vec-on-wsl/"/>
      <url>/Hexo-Blog/2021/01/22/2021-01-22-train-word2vec-on-wsl/</url>
      
        <content type="html"><![CDATA[<p>Bloomberg LP recently published a new paper, claimed that popular implementations of word2vec with negative sampling such as <a href="https://github.com/tmikolov/word2vec/">word2vec</a> and <a href="https://github.com/RaRe-Technologies/gensim/">gensim</a> do not implement the CBOW update correctly, thus potentially leading to misconceptions about the performance of CBOW embeddings when trained correctly. Therefore, they release <a href="https://github.com/bloomberg/koan">kōan</a> so that others can efficiently train CBOW embeddings using the corrected weight update.</p><p>In this article, I’m going to build my own pre-trained word embedding on WSL, which stands for Windows Subsystem for Linux, and it is a compatibility layer for running Linux binary executables (in ELF format) natively on Windows 10.. The reason why I train the model on Linux instead of Windows is that it’s not user-freiendly to run C++ and some other packages on Windows.</p><h1 id="Windows-Subsystem-for-Linux"><a href="#Windows-Subsystem-for-Linux" class="headerlink" title="Windows-Subsystem-for-Linux"></a>Windows-Subsystem-for-Linux</h1><h2 id="Pre-steps"><a href="#Pre-steps" class="headerlink" title="Pre-steps"></a>Pre-steps</h2><ol><li>Download PowerShell from <a href="https://github.com/PowerShell/PowerShell/releases">here</a>.</li><li>Install Ubuntu from microsoft shop.<img src="/Hexo-Blog/2021/01/22/2021-01-22-train-word2vec-on-wsl/ubuntu.jpg" class=""></li><li>Open PowerShell and type command below.<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">Enable-WindowsOptionalFeature -Online -FeatureName Microsoft-Windows-Subsystem-Linux<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>Once you done the 3 steps above, you can visit your linux homepage through <code>C:\Users\&lt;username&gt;\AppData\Local\Packages\CanonicalGroupLimited.Ubuntu18.04onWindows_79rhkp1fndgsc\LocalState\rootfs\home\</code></li></ol><h2 id="Connect-with-GitHub"><a href="#Connect-with-GitHub" class="headerlink" title="Connect with GitHub"></a>Connect with GitHub</h2><h3 id="Generate-a-New-SSH-Key"><a href="#Generate-a-New-SSH-Key" class="headerlink" title="Generate a New SSH Key"></a>Generate a New SSH Key</h3><p>What is <code>ssh-keygen</code>? <code>ssh-keygen</code> is a tool for creating new authentication key pairs for SSH. Such key pairs are used for automating logins, single sign-on, and for authenticating hosts. The SSH protocol uses public key cryptography for authenticating hosts and users. The authentication keys, called SSH keys, are created using the keygen program.</p><ol><li>The client can generate a public-private key pair as follows: <code>ssh-keygen</code>. After this, you will see something like this.<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">Generating public/private rsa key pair.Enter <span class="token function">file</span> <span class="token keyword">in</span> <span class="token function">which</span> to save the key <span class="token punctuation">(</span>/home/ylo/.ssh/id_rsa<span class="token punctuation">)</span>: Enter passphrase <span class="token punctuation">(</span>empty <span class="token keyword">for</span> no passphrase<span class="token punctuation">)</span>: Enter same passphrase again: Your identification has been saved <span class="token keyword">in</span> /home/ylo/.ssh/id_rsa.Your public key has been saved <span class="token keyword">in</span> /home/ylo/.ssh/id_rsa.pub.The key fingerprint is:SHA256:Up6KjbnEV4Hgfo75YM393QdQsK3Z0aTNBz0DoirrW+c ylo@klarThe key's randomart image is:+---<span class="token punctuation">[</span>RSA <span class="token number">2048</span><span class="token punctuation">]</span>----+<span class="token operator">|</span>    <span class="token builtin class-name">.</span>      <span class="token punctuation">..</span>oo<span class="token punctuation">..</span><span class="token operator">|</span><span class="token operator">|</span>   <span class="token builtin class-name">.</span> <span class="token builtin class-name">.</span> <span class="token builtin class-name">.</span>  <span class="token builtin class-name">.</span> .o.X.<span class="token operator">|</span><span class="token operator">|</span>    <span class="token builtin class-name">.</span> <span class="token builtin class-name">.</span> o.  <span class="token punctuation">..</span>+ B<span class="token operator">|</span><span class="token operator">|</span>   <span class="token builtin class-name">.</span>   o.o  .+ <span class="token punctuation">..</span><span class="token operator">|</span><span class="token operator">|</span>    <span class="token punctuation">..</span>o.S   o<span class="token punctuation">..</span>  <span class="token operator">|</span><span class="token operator">|</span>   <span class="token builtin class-name">.</span> %o<span class="token operator">=</span>      <span class="token builtin class-name">.</span>  <span class="token operator">|</span><span class="token operator">|</span>    @.B<span class="token punctuation">..</span>.     <span class="token builtin class-name">.</span> <span class="token operator">|</span><span class="token operator">|</span>   o.<span class="token operator">=</span>. o. <span class="token builtin class-name">.</span> <span class="token builtin class-name">.</span>  <span class="token builtin class-name">.</span><span class="token operator">|</span><span class="token operator">|</span>    .oo  E. <span class="token builtin class-name">.</span> <span class="token punctuation">..</span> <span class="token operator">|</span>+----<span class="token punctuation">[</span>SHA256<span class="token punctuation">]</span>-----+<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li>Now, you can find your public key as follows: <code>cat ~/.ssh/id_rsa.pub</code></li><li>Open your <a href="https://github.com/">GitHub</a> and go to the <code>Settings</code> section.<img src="/Hexo-Blog/2021/01/22/2021-01-22-train-word2vec-on-wsl/settings.png" class=""></li><li>Go to <code>SSH and GPG keys</code> and click the <code>New SSH key</code> button.</li><li>Copy your public key to <code>Key</code> text field and press <code>Add SSH key</code>.<img src="/Hexo-Blog/2021/01/22/2021-01-22-train-word2vec-on-wsl/ssh.png" class=""></li></ol><h2 id="Ananconda-for-Python-Optional"><a href="#Ananconda-for-Python-Optional" class="headerlink" title="Ananconda for Python (Optional)"></a>Ananconda for Python (Optional)</h2><p>I wrote a <a href="/Hexo-Blog/2020/12/31/2020-12-31-conda-environment-setup/" title="blog">blog</a> discussing about why we should use anaconda for python. Please take a look first.</p><h3 id="Install-Packages"><a href="#Install-Packages" class="headerlink" title="Install Packages"></a>Install Packages</h3><pre class="line-numbers language-console" data-language="console"><code class="language-console">sudo apt-get updatesudo apt-get install python-pip<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h3 id="Install-Anaconda"><a href="#Install-Anaconda" class="headerlink" title="Install Anaconda"></a>Install Anaconda</h3><ol><li>Download anaconda3.<pre class="line-numbers language-console" data-language="console"><code class="language-console">curl -O https:&#x2F;&#x2F;repo.anaconda.com&#x2F;archive&#x2F;Anaconda3-2020.02-Linux-x86_64.shbash Anaconda3-2020.02-Linux-x86_64.shexport PATH&#x3D;~&#x2F;anaconda3&#x2F;bin:$PATHconda --version<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre></li><li>Update and build virtual environment.<pre class="line-numbers language-console" data-language="console"><code class="language-console">conda config --set auto_activate_base falseconda update condaconda update anacondaconda create --name nlp python&#x3D;3.7source activate nlpconda install ipykernel -ypython -m ipykernel install --user --name nlp --display-name &quot;nlp&quot;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ol><p><strong>CommandNotFoundError: Your shell has not been properly configured to use ‘conda activate’.</strong></p><p>If can not activate conda environment, I come up with a workaround below.</p><pre class="line-numbers language-console" data-language="console"><code class="language-console">source activateconda deactivateconda activate nlp<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h2 id="Install-Python-Packages"><a href="#Install-Python-Packages" class="headerlink" title="Install Python Packages"></a>Install Python Packages</h2><pre class="line-numbers language-console" data-language="console"><code class="language-console">conda install pytorch torchvision cudatoolkit&#x3D;10.1 -c pytorch -y<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h1 id="Train-Word2Vec-using-Koan"><a href="#Train-Word2Vec-using-Koan" class="headerlink" title="Train Word2Vec using Kōan"></a>Train Word2Vec using Kōan</h1><p>It is a common belief in the NLP community that continuous bag-of-words (CBOW) word embeddings tend to underperform skip-gram (SG) embeddings. Ozan Irsoy and Adrian Benton from Bloomberg LP show that their correct <a href="https://arxiv.org/pdf/2012.15332.pdf">implementation</a> of CBOW yields word embeddings that are fully competitive with SG on various intrinsic and extrinsic tasks while being more than three times as fast to train.</p><h2 id="Building"><a href="#Building" class="headerlink" title="Building"></a>Building</h2><p>To train word embeddings on Wikitext-2, first clone and build koan:</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token function">git</span> clone --recursive git@github.com:bloomberg/koan.git<span class="token builtin class-name">cd</span> koan<span class="token function">mkdir</span> build<span class="token builtin class-name">cd</span> buildcmake <span class="token punctuation">..</span> <span class="token operator">&amp;&amp;</span> cmake --build ./<span class="token builtin class-name">cd</span> <span class="token punctuation">..</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>Run tests with (assuming you are still under build):</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">./test_gradcheck./test_utils<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>Download and unzip the Wikitext-2 corpus:</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token function">curl</span> https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-v1.zip --output wikitext-2-v1.zip<span class="token function">unzip</span> wikitext-2-v1.zip<span class="token function">head</span> -n <span class="token number">5</span> ./wikitext-2/wiki.train.tokens<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h2 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h2><p>Learn CBOW embeddings on the training fold with:</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">./build/koan -V <span class="token number">2000000</span> <span class="token punctuation">\</span>             --epochs <span class="token number">10</span> <span class="token punctuation">\</span>             --dim <span class="token number">300</span> <span class="token punctuation">\</span>             --negatives <span class="token number">5</span> <span class="token punctuation">\</span>             --context-size <span class="token number">5</span> <span class="token punctuation">\</span>             -l <span class="token number">0.075</span> <span class="token punctuation">\</span>             --threads <span class="token number">16</span> <span class="token punctuation">\</span>             --cbow <span class="token boolean">true</span> <span class="token punctuation">\</span>             --min-count <span class="token number">2</span> <span class="token punctuation">\</span>             --file ./wikitext-2/wiki.train.tokens<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>or skipgram embeddings by running with <code>--cbow false</code>. <code>./build/koan</code> –help for a full list of command-line arguments and descriptions. Learned embeddings will be saved to <code>embeddings_$&#123;CURRENT_TIMESTAMP&#125;.txt</code> in the present working directory.</p><p>After you get your final pre-trained word embedding vectors, you can copy paste to your windows folder. Next, convert it into a word2vec format in order to put it into a gensim model.</p><p>Gensim can load two binary formats, word2vec and fastText, and a generic plain text format which can be created by most word embedding tools. The generic plain text format should look like this (in this example 20000 is the size of the vocabulary and 300 is the length of vector).</p><pre class="line-numbers language-console" data-language="console"><code class="language-console">20000 100the 0.476841 -0.620207 -0.002157 0.359706 -0.591816 [295 more numbers...]and 0.223408  0.231993 -0.231131 -0.900311 -0.225111 [295 more numbers..][19998 more lines...]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>Finally, you can do interesting stuff in other NLP tasks, such as this <a href="/Hexo-Blog/2021/01/25/2021-01-25-weighted-word-embedding/" title="article">article</a> I wrote before.</p><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>It is really helpful to make some pre-computed word embedding vectors from scratch, rather than having pre-trained vectors from other websites! Stay Hungry! Stay Foolish! See you next time!</p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> WSL </tag>
            
            <tag> Ubuntu </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Set Up Anaconda for Python</title>
      <link href="/Hexo-Blog/2020/12/31/2020-12-31-conda-environment-setup/"/>
      <url>/Hexo-Blog/2020/12/31/2020-12-31-conda-environment-setup/</url>
      
        <content type="html"><![CDATA[<p>Recently, python is getting more popular, because it can complete a project in a short time. However, setting up virtual environment is crucial for programming several projects. In this article, I will introduce how I setting up a anaconda environment for python.</p><h2 id="Create-Conda-Environment"><a href="#Create-Conda-Environment" class="headerlink" title="Create Conda Environment"></a>Create Conda Environment</h2><p>When you start learning Python, it is a good starting point to install the newest Python version with the latest versions of the packages you need or want to play around with. Then, most likely, you immerse yourself in this world, and download Python applications from GitHub, Kaggle or other sources. These applications may need other versions of Python/packages than the ones you have been currently using.</p><ol><li><p>Install <a href="https://www.python.org/downloads/release/python-373/">python</a> version 3.7.3</p></li><li><p>Install <a href="https://www.anaconda.com/distribution/#download-section">Anaconda</a> 3 for win10</p></li><li><p>Create a virtual environment and change the PYTHONPATH of an ipython kernel:</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">conda update conda -yconda create --name my_env <span class="token assign-left variable">python</span><span class="token operator">=</span><span class="token number">3.7</span>.3conda activate MY_ENVconda <span class="token function">install</span> ipykernel -ypython -m ipykernel <span class="token function">install</span> --user --name MY_ENV --display-name <span class="token string">"MY_ENV"</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p>GPU support software requirements:</p></li></ol><ul><li>NVIDIA® GPU <a href="https://www.nvidia.com/Download/index.aspx?lang=en-us">drivers</a></li><li><a href="https://developer.nvidia.com/cuda-toolkit-archive">CUDA®</a> Toolkit</li><li><a href="https://developer.nvidia.com/cudnn">cuDNN</a> SDK</li><li>(Optional) <a href="https://developer.nvidia.com/tensorrt">TensorRT</a> 5.0</li></ul><ol start="5"><li>Windows setup</li></ol><ul><li>Add the CUDA, CUPTI, and cuDNN installation directories to the %PATH% environmental variable. For example, if the CUDA Toolkit is installed to C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0 and cuDNN to C:\tools\cuda, update your %PATH% to match:<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ <span class="token builtin class-name">export</span> <span class="token assign-left variable"><span class="token environment constant">PATH</span></span><span class="token operator">=</span>C:<span class="token punctuation">\</span>Program Files<span class="token punctuation">\</span>NVIDIA GPU Computing Toolkit<span class="token punctuation">\</span>CUDA<span class="token punctuation">\</span>v10.0<span class="token punctuation">\</span>bin<span class="token punctuation">;</span>%<span class="token environment constant">PATH</span>%$ <span class="token builtin class-name">export</span> <span class="token assign-left variable"><span class="token environment constant">PATH</span></span><span class="token operator">=</span>C:<span class="token punctuation">\</span>Program Files<span class="token punctuation">\</span>NVIDIA GPU Computing Toolkit<span class="token punctuation">\</span>CUDA<span class="token punctuation">\</span>v10.0<span class="token punctuation">\</span>extras<span class="token punctuation">\</span>CUPTI<span class="token punctuation">\</span>libx64<span class="token punctuation">;</span>%<span class="token environment constant">PATH</span>%$ <span class="token builtin class-name">export</span> <span class="token assign-left variable"><span class="token environment constant">PATH</span></span><span class="token operator">=</span>C:<span class="token punctuation">\</span>Program Files<span class="token punctuation">\</span>NVIDIA GPU Computing Toolkit<span class="token punctuation">\</span>CUDA<span class="token punctuation">\</span>v10.0<span class="token punctuation">\</span>include<span class="token punctuation">;</span>%<span class="token environment constant">PATH</span>%$ <span class="token builtin class-name">export</span> <span class="token assign-left variable"><span class="token environment constant">PATH</span></span><span class="token operator">=</span>C:<span class="token punctuation">\</span>tools<span class="token punctuation">\</span>cuda<span class="token punctuation">\</span>bin<span class="token punctuation">;</span>%<span class="token environment constant">PATH</span>%<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre></li><li>Add the absolute path to the TensorRTlib directory to the environment variable LD_LIBRARY_PATH</li></ul><p>Enjoy!</p>]]></content>
      
      
      <categories>
          
          <category> Installation </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> Conda </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Integrate Atom with Cmder</title>
      <link href="/Hexo-Blog/2020/12/15/2020-12-15-integrate-atom-with-cmder/"/>
      <url>/Hexo-Blog/2020/12/15/2020-12-15-integrate-atom-with-cmder/</url>
      
        <content type="html"><![CDATA[<p>In this blog, I’ll be sharing with you my programming setup for Atom and Cmder, and talking about the most useful tips in it.</p><h1 id="Atom"><a href="#Atom" class="headerlink" title="Atom"></a>Atom</h1><p>Atom is a hackable text editor for the 21st century, built on Electron, and based on everything we love about our favorite editors. Compares to VS, the performance differences between tjem come down to a few factors, but one major aspect is the approach with which each app is developed. Visual Studio Code has a tightly controlled core set of functionality, with plugins adding surface-level features. Atom, on the other hand, uses a plugin-based approach to nearly everything. This approach has benefits, but also drawbacks. Atom is slightly slower out of the box, and this only gets worse when adding certain plugins. VS Code has the clear advantage when it comes to performance, but neither editor is slow on a modern machine. This changes when you’re editing huge files. Visual Studio Code fares better than Atom, but either is noticeably slow when compared to an editor like Vim or even Sublime Text.</p><p>However, I prefer Atom personally. So I will show how to setup Atom in the following blog. Please download <a href="https://github.com/atom/atom/releases/tag/v1.50.0">Atom</a> first from its GitHub website.</p><h2 id="Install-Packages-inside-Atom"><a href="#Install-Packages-inside-Atom" class="headerlink" title="Install Packages inside Atom"></a>Install Packages inside Atom</h2><p>These are the packages I recommended for everyone. They are very powerful and handful for your speed of programming and they helps improve your quality of coding.</p><ol><li>platformio-ide-terminal (Before installing please pre-install visual studio)</li><li>atom-beautify</li><li>tabs-to-spaces</li><li>git-plus</li><li>color-picker</li><li>autocomplete</li></ol><h2 id="Config-Setting"><a href="#Config-Setting" class="headerlink" title="Config Setting"></a>Config Setting</h2><p>In <code>file</code> &gt; <code>Config</code>, please add the followings,</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token string">"*"</span><span class="token builtin class-name">:</span>  core:    telemetryConsent: <span class="token string">"no"</span>  editor:    fontSize: <span class="token number">22</span>    invisibles: <span class="token punctuation">&#123;</span><span class="token punctuation">&#125;</span>    showInvisibles: <span class="token boolean">true</span>  <span class="token string">"exception-reporting"</span><span class="token builtin class-name">:</span>    userId: <span class="token string">"6baad082-b933-4f93-acf5-fea77bb41230"</span>  <span class="token string">"line-ending-selector"</span><span class="token builtin class-name">:</span>    defaultLineEnding: <span class="token string">"LF"</span>  <span class="token string">"platformio-ide-terminal"</span><span class="token builtin class-name">:</span>    core:      autoRunCommand: <span class="token string">"D:<span class="token entity" title="\\">\\</span>cmder<span class="token entity" title="\\">\\</span>atom.bat &amp;&amp; conda activate nlp &amp;&amp; clear"</span>      shell: <span class="token string">"C:<span class="token entity" title="\\">\\</span>Windows<span class="token entity" title="\\">\\</span>System32<span class="token entity" title="\\">\\</span>cmd.exe"</span>  <span class="token string">"tabs-to-spaces"</span><span class="token builtin class-name">:</span>    onSave: <span class="token string">"untabify"</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="Cmder"><a href="#Cmder" class="headerlink" title="Cmder"></a>Cmder</h1><p>Cmder is a software package created out of pure frustration over the absence of nice console emulators on Windows. It is based on amazing software, and spiced up with the Monokai color scheme and a custom prompt layout, looking sexy from the start.</p><h2 id="Integrate-with-Atom"><a href="#Integrate-with-Atom" class="headerlink" title="Integrate with Atom"></a>Integrate with Atom</h2><ol><li>Install platformio-ide-terminal.</li><li>Create atom.bat in cmder root folder.</li></ol><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">@echo off SET <span class="token assign-left variable">CMDER_ROOT</span><span class="token operator">=</span>C:<span class="token punctuation">\</span>Path<span class="token punctuation">\</span>To<span class="token punctuation">\</span>cmder%CMDER_ROOT%<span class="token punctuation">\</span>vendor<span class="token punctuation">\</span>init.bat<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><ol start="3"><li>Settings in platformio-ide-terminal.</li></ol><ul><li>Auto Run Command: C:\Path\To\cmder\atom.bat &amp;&amp; clear</li><li>Shell Override: C:\Windows\System32\cmd.exe</li></ul><ol start="4"><li>(Optional) If you want to set up conda environment, please go to this <a href="https://github.com/penguinwang96825/Set-Up-Conda-Environment">tutorial</a> I wrote before. After you build conda environment, set command in Auto Run Command to <code>C:\Path\To\cmder\atom.bat &amp;&amp; conda activate env_name &amp;&amp; clear</code>.</li></ol><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>That’s it for todays sharing, if there are more software applications I found useful, I’ll post them over here to let you guys know! Cheers, stay tuned!</p>]]></content>
      
      
      <categories>
          
          <category> Installation </category>
          
      </categories>
      
      
        <tags>
            
            <tag> IDE </tag>
            
            <tag> Atom </tag>
            
            <tag> GitHub </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>TensorFlow 2.0 Installation</title>
      <link href="/Hexo-Blog/2020/11/28/2020-11-28-installation-for-tf2/"/>
      <url>/Hexo-Blog/2020/11/28/2020-11-28-installation-for-tf2/</url>
      
        <content type="html"><![CDATA[<p>TensorFlow makes people love and hate. It is an end-to-end open source platform for Machine Learning and Deep Learning. However, I always have trouble with installing TensorFlow a bunch of times. Thus I decide to share my experience in order to help others to solve this same problem.</p><h2 id="Set-up-Conda-Environment"><a href="#Set-up-Conda-Environment" class="headerlink" title="Set up Conda Environment"></a>Set up Conda Environment</h2><p>Conda is a package and environment management tool that allows you to install Python packages on your computer as well as create and manage multiple Python environments, each containing different packages.</p><ol><li>Install Python from its official <a href="https://www.python.org/downloads/release/python-373/">website</a>.</li><li>Install <a href="https://www.anaconda.com/distribution/#download-section">Anaconda</a> 3 for win10.</li><li>Open command line. (<code>Windows Key</code> + <code>R</code> and type in CMD)</li><li>Create a virtual environment and change the <code>ENV_NAME</code> of an ipython kernel.</li></ol><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">conda update condaconda create --name ENV_NAME <span class="token assign-left variable">python</span><span class="token operator">=</span><span class="token number">3.7</span>.3conda activate ENV_NAME conda <span class="token function">install</span> ipykernel -ypython -m ipykernel <span class="token function">install</span> --user --name ENV_NAME --display-name <span class="token string">"ENV_NAME"</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="CUDA-and-cuDNN"><a href="#CUDA-and-cuDNN" class="headerlink" title="CUDA and cuDNN"></a>CUDA and cuDNN</h2><p>If you intend to utilise GPU to speed up your computation, it is neccessary to install CUDA and cuDNN. This <a href="https://www.tensorflow.org/install/source#gpu">link</a> is the overview of the compatible versions for Tensorflow. Please download the list below first.</p><ul><li>NVIDIA® GPU drivers: <a href="https://www.nvidia.com/Download/index.aspx?lang=en-us">link</a></li><li>CUDA® Toolkit: <a href="https://developer.nvidia.com/cuda-toolkit-archive">link</a></li><li>cuDNN SDK (Unzip to C:\tools\cuda): <a href="https://developer.nvidia.com/cudnn">link</a></li><li>(Optional) TensorRT 5.0: <a href="https://developer.nvidia.com/tensorrt">link</a></li></ul><p>After installing particular CUDA/cuDNN combination, we have to set environment variable.</p><p><img src="https://miro.medium.com/max/618/1*NIVaXFnphn-_xCJr4-snJA.png" alt="System Properties"></p><p>On the Environment Variables dialog, you’ll see two sets of variables: one for user variables and the other for system variables. Just choose the upper one to edit. Next, on the Edit environment variable dialog, you’ll see a list of all the paths that are currently in the PATH variable. Add the following to PATH variable.</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">C:<span class="token punctuation">\</span>Program Files<span class="token punctuation">\</span>NVIDIA GPU Computing Toolkit<span class="token punctuation">\</span>CUDA<span class="token punctuation">\</span>v10.0<span class="token punctuation">\</span>binC:<span class="token punctuation">\</span>Program Files<span class="token punctuation">\</span>NVIDIA GPU Computing Toolkit<span class="token punctuation">\</span>CUDA<span class="token punctuation">\</span>v10.0<span class="token punctuation">\</span>extras<span class="token punctuation">\</span>CUPTI<span class="token punctuation">\</span>libx64C:<span class="token punctuation">\</span>Program Files<span class="token punctuation">\</span>NVIDIA GPU Computing Toolkit<span class="token punctuation">\</span>CUDA<span class="token punctuation">\</span>v10.0<span class="token punctuation">\</span>includeC:<span class="token punctuation">\</span>tools<span class="token punctuation">\</span>cuda<span class="token punctuation">\</span>bin<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>There is a shorcut to add PATH variable more efficiently. That is, by adding it via command line. Use <code>Windows Key</code> + <code>R</code> to quickly launch Apps as administrator, and type in CMD. After opening command line, input the following code.</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">SET <span class="token assign-left variable"><span class="token environment constant">PATH</span></span><span class="token operator">=</span>C:<span class="token punctuation">\</span>Program Files<span class="token punctuation">\</span>NVIDIA GPU Computing Toolkit<span class="token punctuation">\</span>CUDA<span class="token punctuation">\</span>v10.1<span class="token punctuation">\</span>bin<span class="token punctuation">;</span>%<span class="token environment constant">PATH</span>%SET <span class="token assign-left variable"><span class="token environment constant">PATH</span></span><span class="token operator">=</span>C:<span class="token punctuation">\</span>Program Files<span class="token punctuation">\</span>NVIDIA GPU Computing Toolkit<span class="token punctuation">\</span>CUDA<span class="token punctuation">\</span>v10.1<span class="token punctuation">\</span>extras<span class="token punctuation">\</span>CUPTI<span class="token punctuation">\</span>lib64<span class="token punctuation">;</span>%<span class="token environment constant">PATH</span>%SET <span class="token assign-left variable"><span class="token environment constant">PATH</span></span><span class="token operator">=</span>C:<span class="token punctuation">\</span>Program Files<span class="token punctuation">\</span>NVIDIA GPU Computing Toolkit<span class="token punctuation">\</span>CUDA<span class="token punctuation">\</span>v10.1<span class="token punctuation">\</span>include<span class="token punctuation">;</span>%<span class="token environment constant">PATH</span>%SET <span class="token assign-left variable"><span class="token environment constant">PATH</span></span><span class="token operator">=</span>C:<span class="token punctuation">\</span>tools<span class="token punctuation">\</span>cuda<span class="token punctuation">\</span>bin<span class="token punctuation">;</span>%<span class="token environment constant">PATH</span>%<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><h2 id="Final-Step"><a href="#Final-Step" class="headerlink" title="Final Step"></a>Final Step</h2><p>This is the final step. In command line just type in <code>pip install tensorflow</code> and it is done. Congratulations!</p><p>Hope everyone enjoy Tensorflow!</p>]]></content>
      
      
      <categories>
          
          <category> Installation </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> Anaconda </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>EDA for Predicting Insurance Claim</title>
      <link href="/Hexo-Blog/2020/11/27/2020-11-27-exploratory-data-analysis-for-predicting-insurance-claim/"/>
      <url>/Hexo-Blog/2020/11/27/2020-11-27-exploratory-data-analysis-for-predicting-insurance-claim/</url>
      
        <content type="html"><![CDATA[<p>Exploratory Data Analysis (EDA) is understanding the data sets by summarizing their main characteristics often plotting them visually. This step is very important especially when we arrive at modeling the data in order to apply Machine learning. In this article, I’ll show you how I did for this!</p><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Imaging you are hired as a Senior Data Analyst at Intelligent Insurances Co. The company wants to develop a predictive model that uses vehicle characteristics to accurately predict insurance claim payments. Such a model will allow the company to assess the potential risk that a vehicle represents.</p><p>The company puts you in charge of coming up with a solution for this problem and provides you with a historic dataset of previous insurance claims. The claimed amount can be zero or greater than zero and it is given in US dollars.</p><p>In this article, I will design my model before conducting some EDA. Let’s get started!</p><h1 id="Load-Data-and-Libraries"><a href="#Load-Data-and-Libraries" class="headerlink" title="Load Data and Libraries"></a>Load Data and Libraries</h1><h2 id="Load-Libraries"><a href="#Load-Libraries" class="headerlink" title="Load Libraries"></a>Load Libraries</h2><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> sys<span class="token keyword">import</span> zipfile<span class="token keyword">import</span> warnings<span class="token keyword">import</span> concurrent<span class="token punctuation">.</span>futures<span class="token keyword">import</span> pandas <span class="token keyword">as</span> pd<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt<span class="token keyword">import</span> seaborn <span class="token keyword">as</span> sns<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>base <span class="token keyword">import</span> BaseEstimator<span class="token punctuation">,</span> TransformerMixin<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>model_selection <span class="token keyword">import</span> KFold<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>pipeline <span class="token keyword">import</span> Pipeline<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>preprocessing <span class="token keyword">import</span> OneHotEncoder<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>preprocessing <span class="token keyword">import</span> StandardScaler<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>preprocessing <span class="token keyword">import</span> MinMaxScaler<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>compose <span class="token keyword">import</span> ColumnTransformer<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>model_selection <span class="token keyword">import</span> StratifiedKFold<span class="token keyword">from</span> itertools <span class="token keyword">import</span> productwarnings<span class="token punctuation">.</span>filterwarnings<span class="token punctuation">(</span><span class="token string">"ignore"</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="Archive-Data"><a href="#Archive-Data" class="headerlink" title="Archive Data"></a>Archive Data</h2><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">with</span> zipfile<span class="token punctuation">.</span>ZipFile<span class="token punctuation">(</span><span class="token string">"./data/data.zip"</span><span class="token punctuation">,</span> <span class="token string">'r'</span><span class="token punctuation">)</span> <span class="token keyword">as</span> extractor<span class="token punctuation">:</span>     <span class="token comment"># Print all the contents of the zip file </span>    extractor<span class="token punctuation">.</span>printdir<span class="token punctuation">(</span><span class="token punctuation">)</span>     <span class="token comment"># Extract all the files </span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'Extracting all the files now...'</span><span class="token punctuation">)</span>     extractor<span class="token punctuation">.</span>extractall<span class="token punctuation">(</span>path<span class="token operator">=</span><span class="token string">"./data/"</span><span class="token punctuation">)</span>     <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'Done!'</span><span class="token punctuation">)</span> <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><pre class="line-numbers language-console" data-language="console"><code class="language-console">File Name                                             Modified             Sizetest.csv                                       2020-11-12 09:27:10         1989__MACOSX&#x2F;._test.csv                            2020-11-12 09:27:10         1224train.csv                                      2020-10-15 21:32:38      6110914__MACOSX&#x2F;._train.csv                           2020-10-15 21:32:38         1224data_dictionary.html                           2020-10-15 21:27:32        24739__MACOSX&#x2F;._data_dictionary.html                2020-10-15 21:27:32         1280Extracting all the files now...Done!<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>Let’s take a quick view.</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">data <span class="token operator">=</span> pd<span class="token punctuation">.</span>read_csv<span class="token punctuation">(</span><span class="token string">"./data/train.csv"</span><span class="token punctuation">)</span>data<span class="token punctuation">.</span>info<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><hr><pre class="line-numbers language-console" data-language="console"><code class="language-console">&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;RangeIndex: 30000 entries, 0 to 29999Data columns (total 35 columns): #   Column          Non-Null Count  Dtype  ---  ------          --------------  -----   0   Row_ID          30000 non-null  int64   1   Household_ID    30000 non-null  int64   2   Vehicle         30000 non-null  int64   3   Calendar_Year   30000 non-null  int64   4   Model_Year      30000 non-null  int64   5   Blind_Make      30000 non-null  object  6   Blind_Model     30000 non-null  object  7   Blind_Submodel  30000 non-null  object  8   Cat1            30000 non-null  object  9   Cat2            30000 non-null  object  10  Cat3            30000 non-null  object  11  Cat4            30000 non-null  object  12  Cat5            30000 non-null  object  13  Cat6            30000 non-null  object  14  Cat7            30000 non-null  object  15  Cat8            30000 non-null  object  16  Cat9            30000 non-null  object  17  Cat10           30000 non-null  object  18  Cat11           30000 non-null  object  19  Cat12           29948 non-null  object  20  OrdCat          30000 non-null  object  21  Var1            30000 non-null  float64 22  Var2            30000 non-null  float64 23  Var3            30000 non-null  float64 24  Var4            30000 non-null  float64 25  Var5            30000 non-null  float64 26  Var6            30000 non-null  float64 27  Var7            30000 non-null  float64 28  Var8            30000 non-null  float64 29  NVCat           30000 non-null  object  30  NVVar1          30000 non-null  float64 31  NVVar2          30000 non-null  float64 32  NVVar3          30000 non-null  float64 33  NVVar4          30000 non-null  float64 34  Claim_Amount    30000 non-null  float64dtypes: float64(13), int64(5), object(17)memory usage: 8.0+ MB<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>Check the types in each columns.</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">type_of_col</span><span class="token punctuation">(</span>data<span class="token punctuation">,</span> label_col<span class="token operator">=</span><span class="token string">"Claim_Amount"</span><span class="token punctuation">,</span> show<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    df <span class="token operator">=</span> data<span class="token punctuation">.</span>copy<span class="token punctuation">(</span><span class="token punctuation">)</span>    df <span class="token operator">=</span> df<span class="token punctuation">.</span>drop<span class="token punctuation">(</span>label_col<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>    int_features <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    float_features <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    object_features <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    <span class="token keyword">for</span> dtype<span class="token punctuation">,</span> feature <span class="token keyword">in</span> <span class="token builtin">zip</span><span class="token punctuation">(</span>df<span class="token punctuation">.</span>dtypes<span class="token punctuation">,</span> df<span class="token punctuation">.</span>columns<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">if</span> dtype <span class="token operator">==</span> <span class="token string">'float64'</span><span class="token punctuation">:</span>            float_features<span class="token punctuation">.</span>append<span class="token punctuation">(</span>feature<span class="token punctuation">)</span>        <span class="token keyword">elif</span> dtype <span class="token operator">==</span> <span class="token string">'int64'</span><span class="token punctuation">:</span>            int_features<span class="token punctuation">.</span>append<span class="token punctuation">(</span>feature<span class="token punctuation">)</span>        <span class="token keyword">else</span><span class="token punctuation">:</span>            object_features<span class="token punctuation">.</span>append<span class="token punctuation">(</span>feature<span class="token punctuation">)</span>    <span class="token keyword">if</span> show<span class="token punctuation">:</span>         <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'</span><span class="token interpolation"><span class="token punctuation">&#123;</span><span class="token builtin">len</span><span class="token punctuation">(</span>int_features<span class="token punctuation">)</span><span class="token punctuation">&#125;</span></span><span class="token string"> Integer Features : </span><span class="token interpolation"><span class="token punctuation">&#123;</span>int_features<span class="token punctuation">&#125;</span></span><span class="token string">\n'</span></span><span class="token punctuation">)</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'</span><span class="token interpolation"><span class="token punctuation">&#123;</span><span class="token builtin">len</span><span class="token punctuation">(</span>float_features<span class="token punctuation">)</span><span class="token punctuation">&#125;</span></span><span class="token string"> Float Features : </span><span class="token interpolation"><span class="token punctuation">&#123;</span>float_features<span class="token punctuation">&#125;</span></span><span class="token string">\n'</span></span><span class="token punctuation">)</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'</span><span class="token interpolation"><span class="token punctuation">&#123;</span><span class="token builtin">len</span><span class="token punctuation">(</span>object_features<span class="token punctuation">)</span><span class="token punctuation">&#125;</span></span><span class="token string"> Object Features : </span><span class="token interpolation"><span class="token punctuation">&#123;</span>object_features<span class="token punctuation">&#125;</span></span><span class="token string">'</span></span><span class="token punctuation">)</span>    <span class="token keyword">return</span> int_features<span class="token punctuation">,</span> float_features<span class="token punctuation">,</span> object_featuresint_features<span class="token punctuation">,</span> float_features<span class="token punctuation">,</span> object_features <span class="token operator">=</span> type_of_col<span class="token punctuation">(</span>data<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="Numerical-Types"><a href="#Numerical-Types" class="headerlink" title="Numerical Types"></a>Numerical Types</h2><p>I’ll show you the distribution data if data types are <code>float64</code> and <code>int64</code>.</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">df_num <span class="token operator">=</span> data<span class="token punctuation">.</span>select_dtypes<span class="token punctuation">(</span>include<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'float64'</span><span class="token punctuation">,</span> <span class="token string">'int64'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>df_num<span class="token punctuation">.</span>hist<span class="token punctuation">(</span>figsize<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">20</span><span class="token punctuation">)</span><span class="token punctuation">,</span> bins<span class="token operator">=</span><span class="token number">50</span><span class="token punctuation">,</span> xlabelsize<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">,</span> ylabelsize<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><img src="/Hexo-Blog/2020/11/27/2020-11-27-exploratory-data-analysis-for-predicting-insurance-claim/hist.png" class=""><p>Visualising pairwise relationships in this dataset.</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>df_num<span class="token punctuation">.</span>columns<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    sns<span class="token punctuation">.</span>pairplot<span class="token punctuation">(</span>data<span class="token operator">=</span>df_num<span class="token punctuation">,</span> x_vars<span class="token operator">=</span>df_num<span class="token punctuation">.</span>columns<span class="token punctuation">[</span>i<span class="token punctuation">:</span>i<span class="token operator">+</span><span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">,</span> y_vars<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'Claim_Amount'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><img src="/Hexo-Blog/2020/11/27/2020-11-27-exploratory-data-analysis-for-predicting-insurance-claim/pairplot.png" class=""><p>Review heatmaps among numerical types of data.</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">corr <span class="token operator">=</span> df_num<span class="token punctuation">.</span>drop<span class="token punctuation">(</span><span class="token string">'Claim_Amount'</span><span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>corr<span class="token punctuation">(</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>figure<span class="token punctuation">(</span>figsize<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">12</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span>sns<span class="token punctuation">.</span>heatmap<span class="token punctuation">(</span>    corr<span class="token punctuation">[</span><span class="token punctuation">(</span>corr <span class="token operator">>=</span> <span class="token number">0.5</span><span class="token punctuation">)</span> <span class="token operator">|</span> <span class="token punctuation">(</span>corr <span class="token operator">&lt;=</span> <span class="token operator">-</span><span class="token number">0.4</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span>     cmap<span class="token operator">=</span><span class="token string">'viridis'</span><span class="token punctuation">,</span> vmax<span class="token operator">=</span><span class="token number">1.0</span><span class="token punctuation">,</span> vmin<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1.0</span><span class="token punctuation">,</span> linewidths<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">,</span>    annot<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> annot_kws<span class="token operator">=</span><span class="token punctuation">&#123;</span><span class="token string">"size"</span><span class="token punctuation">:</span> <span class="token number">8</span><span class="token punctuation">&#125;</span><span class="token punctuation">,</span> square<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><img src="/Hexo-Blog/2020/11/27/2020-11-27-exploratory-data-analysis-for-predicting-insurance-claim/heatmaps.png" class=""><h2 id="Categorical-Types"><a href="#Categorical-Types" class="headerlink" title="Categorical Types"></a>Categorical Types</h2><p>While visualising categorical types of data, I saw there are some “?” in some features.</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">df_not_num <span class="token operator">=</span> data<span class="token punctuation">.</span>select_dtypes<span class="token punctuation">(</span>include<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'O'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>fig<span class="token punctuation">,</span> axes <span class="token operator">=</span> plt<span class="token punctuation">.</span>subplots<span class="token punctuation">(</span><span class="token builtin">round</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>df_not_num<span class="token punctuation">.</span>columns<span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> figsize<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">12</span><span class="token punctuation">,</span> <span class="token number">20</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">for</span> i<span class="token punctuation">,</span> ax <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>fig<span class="token punctuation">.</span>axes<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">if</span> i <span class="token operator">&lt;</span> <span class="token builtin">len</span><span class="token punctuation">(</span>df_not_num<span class="token punctuation">.</span>columns<span class="token punctuation">)</span><span class="token punctuation">:</span>        ax<span class="token punctuation">.</span>set_xticklabels<span class="token punctuation">(</span>ax<span class="token punctuation">.</span>xaxis<span class="token punctuation">.</span>get_majorticklabels<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> rotation<span class="token operator">=</span><span class="token number">45</span><span class="token punctuation">)</span>        sns<span class="token punctuation">.</span>countplot<span class="token punctuation">(</span>x<span class="token operator">=</span>df_not_num<span class="token punctuation">.</span>columns<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span> alpha<span class="token operator">=</span><span class="token number">0.7</span><span class="token punctuation">,</span> data<span class="token operator">=</span>df_not_num<span class="token punctuation">,</span> ax<span class="token operator">=</span>ax<span class="token punctuation">)</span>fig<span class="token punctuation">.</span>tight_layout<span class="token punctuation">(</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><img src="/Hexo-Blog/2020/11/27/2020-11-27-exploratory-data-analysis-for-predicting-insurance-claim/cat.png" class=""><h1 id="Feature-Selection"><a href="#Feature-Selection" class="headerlink" title="Feature Selection"></a>Feature Selection</h1><p>Feature Selection is the process where you automatically or manually select those features which contribute most to your prediction variable or output in which you are interested in.</p><table><thead><tr><th>Feature \ Label</th><th>Continuous</th><th>Categorical</th></tr></thead><tbody><tr><td>Continuous</td><td>Pearson’s Correlation</td><td>LDA</td></tr><tr><td>Categorical</td><td>ANOVA</td><td>Chi-Square</td></tr></tbody></table><h3 id="Featrue-Selection-using-Chi-Square-Test"><a href="#Featrue-Selection-using-Chi-Square-Test" class="headerlink" title="Featrue Selection using Chi-Square Test"></a>Featrue Selection using Chi-Square Test</h3><p>The Chi-Square test of independence is a statistical test to determine if there is a significant relationship between 2 categorical variables. In simple words, the Chi-Square statistic will test whether there is a significant difference in the observed vs the expected frequencies of both variables. </p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> scipy<span class="token punctuation">.</span>stats <span class="token keyword">as</span> stats<span class="token keyword">from</span> scipy<span class="token punctuation">.</span>stats <span class="token keyword">import</span> chi2_contingency<span class="token keyword">class</span> <span class="token class-name">ChiSquare</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">""""    H0: No association between two variables.    H1: There is evidence to suggest there is an association between two variables.    """</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> data<span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>data <span class="token operator">=</span> data        self<span class="token punctuation">.</span>p_value <span class="token operator">=</span> <span class="token boolean">None</span>        self<span class="token punctuation">.</span>chi2 <span class="token operator">=</span> <span class="token boolean">None</span>        self<span class="token punctuation">.</span>dof <span class="token operator">=</span> <span class="token boolean">None</span>        self<span class="token punctuation">.</span>data_observed <span class="token operator">=</span> <span class="token boolean">None</span>        self<span class="token punctuation">.</span>data_expected <span class="token operator">=</span> <span class="token boolean">None</span>        self<span class="token punctuation">.</span>important_features <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>        self<span class="token punctuation">.</span>unimportant_features <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>            <span class="token keyword">def</span> <span class="token function">print_result</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> col<span class="token punctuation">,</span> alpha<span class="token operator">=</span><span class="token number">0.05</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">if</span> self<span class="token punctuation">.</span>p_value <span class="token operator">&lt;</span> alpha<span class="token punctuation">:</span>            <span class="token comment"># Reject null hypothesis H0</span>            <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"</span><span class="token interpolation"><span class="token punctuation">&#123;</span>col<span class="token punctuation">&#125;</span></span><span class="token string"> is an IMPORTANT feature."</span></span><span class="token punctuation">)</span>        <span class="token keyword">else</span><span class="token punctuation">:</span>            <span class="token comment"># Accept null hypothesis H0</span>            <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"</span><span class="token interpolation"><span class="token punctuation">&#123;</span>col<span class="token punctuation">&#125;</span></span><span class="token string"> is NOT an IMPORTANT feature."</span></span><span class="token punctuation">)</span>        <span class="token keyword">def</span> <span class="token function">get_result</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> col<span class="token punctuation">,</span> alpha<span class="token operator">=</span><span class="token number">0.05</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">if</span> self<span class="token punctuation">.</span>p_value <span class="token operator">&lt;</span> alpha<span class="token punctuation">:</span>            <span class="token comment"># Reject null hypothesis H0</span>            self<span class="token punctuation">.</span>important_features<span class="token punctuation">.</span>append<span class="token punctuation">(</span>col<span class="token punctuation">)</span>        <span class="token keyword">else</span><span class="token punctuation">:</span>            <span class="token comment"># Accept null hypothesis H0</span>            self<span class="token punctuation">.</span>unimportant_features<span class="token punctuation">.</span>append<span class="token punctuation">(</span>col<span class="token punctuation">)</span>        <span class="token keyword">def</span> <span class="token function">get_important_features</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> self<span class="token punctuation">.</span>important_features        <span class="token keyword">def</span> <span class="token function">get_unimportant_features</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> self<span class="token punctuation">.</span>unimportant_features            <span class="token keyword">def</span> <span class="token function">test</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> col_features<span class="token punctuation">,</span> col_y<span class="token punctuation">,</span> alpha<span class="token operator">=</span><span class="token number">0.05</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">for</span> col_x <span class="token keyword">in</span> col_features<span class="token punctuation">:</span>            X <span class="token operator">=</span> self<span class="token punctuation">.</span>data<span class="token punctuation">[</span>col_x<span class="token punctuation">]</span><span class="token punctuation">.</span>astype<span class="token punctuation">(</span><span class="token builtin">str</span><span class="token punctuation">)</span>            y <span class="token operator">=</span> self<span class="token punctuation">.</span>data<span class="token punctuation">[</span>col_y<span class="token punctuation">]</span><span class="token punctuation">.</span><span class="token builtin">apply</span><span class="token punctuation">(</span><span class="token keyword">lambda</span> label<span class="token punctuation">:</span> <span class="token number">0</span> <span class="token keyword">if</span> label<span class="token operator">==</span><span class="token number">0.0</span> <span class="token keyword">else</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>astype<span class="token punctuation">(</span><span class="token builtin">str</span><span class="token punctuation">)</span>            self<span class="token punctuation">.</span>data_observed <span class="token operator">=</span> pd<span class="token punctuation">.</span>crosstab<span class="token punctuation">(</span>y<span class="token punctuation">,</span> X<span class="token punctuation">)</span>            chi2<span class="token punctuation">,</span> p_value<span class="token punctuation">,</span> dof<span class="token punctuation">,</span> expected <span class="token operator">=</span> chi2_contingency<span class="token punctuation">(</span>self<span class="token punctuation">.</span>data_observed<span class="token punctuation">.</span>values<span class="token punctuation">)</span>            self<span class="token punctuation">.</span>chi2 <span class="token operator">=</span> chi2            self<span class="token punctuation">.</span>p_value <span class="token operator">=</span> p_value            self<span class="token punctuation">.</span>dof <span class="token operator">=</span> dof            self<span class="token punctuation">.</span>data_expected <span class="token operator">=</span> pd<span class="token punctuation">.</span>DataFrame<span class="token punctuation">(</span>                expected<span class="token punctuation">,</span>                 columns<span class="token operator">=</span>self<span class="token punctuation">.</span>data_observed<span class="token punctuation">.</span>columns<span class="token punctuation">,</span>                 index<span class="token operator">=</span>self<span class="token punctuation">.</span>data_observed<span class="token punctuation">.</span>index<span class="token punctuation">)</span>            self<span class="token punctuation">.</span>get_result<span class="token punctuation">(</span>col_x<span class="token punctuation">,</span> alpha<span class="token punctuation">)</span>            self<span class="token punctuation">.</span>print_result<span class="token punctuation">(</span>col_x<span class="token punctuation">,</span> alpha<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>Chi-Square Test for Categorical Features.</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">chi_test <span class="token operator">=</span> ChiSquare<span class="token punctuation">(</span>data<span class="token punctuation">)</span>test_cols <span class="token operator">=</span> df_not_num<span class="token punctuation">.</span>columns<span class="token punctuation">.</span>tolist<span class="token punctuation">(</span><span class="token punctuation">)</span>chi_test<span class="token punctuation">.</span>test<span class="token punctuation">(</span>test_cols<span class="token punctuation">,</span> <span class="token string">"Claim_Amount"</span><span class="token punctuation">)</span>important_cat_features <span class="token operator">=</span> chi_test<span class="token punctuation">.</span>get_important_features<span class="token punctuation">(</span><span class="token punctuation">)</span>unimportant_cat_features <span class="token operator">=</span> chi_test<span class="token punctuation">.</span>get_unimportant_features<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><pre class="line-numbers language-console" data-language="console"><code class="language-console">Blind_Make is an IMPORTANT feature.Blind_Model is an IMPORTANT feature.Blind_Submodel is NOT an IMPORTANT feature.Cat1 is an IMPORTANT feature.Cat2 is an IMPORTANT feature.Cat3 is an IMPORTANT feature.Cat4 is an IMPORTANT feature.Cat5 is an IMPORTANT feature.Cat6 is an IMPORTANT feature.Cat7 is an IMPORTANT feature.Cat8 is NOT an IMPORTANT feature.Cat9 is an IMPORTANT feature.Cat10 is NOT an IMPORTANT feature.Cat11 is NOT an IMPORTANT feature.Cat12 is NOT an IMPORTANT feature.OrdCat is an IMPORTANT feature.NVCat is an IMPORTANT feature.<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>You can find out that ‘Blind_Make’, ‘Blind_Model’, ‘Cat1’, ‘Cat2’, ‘Cat3’, ‘Cat4’, ‘Cat5’, ‘Cat6’, ‘Cat7’, ‘Cat9’, ‘OrdCat’, ‘NVCat’ are important features, and ‘Blind_Submodel’, ‘Cat8’, ‘Cat10’, ‘Cat11’, ‘Cat12’ are unimportant features by Chi-Square Test.</p><h3 id="Feature-Selection-using-XGBoost"><a href="#Feature-Selection-using-XGBoost" class="headerlink" title="Feature Selection using XGBoost"></a>Feature Selection using XGBoost</h3><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> xgboost <span class="token keyword">as</span> xgb<span class="token keyword">from</span> xgboost <span class="token keyword">import</span> XGBClassifier<span class="token keyword">from</span> xgboost <span class="token keyword">import</span> plot_importance<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>metrics <span class="token keyword">import</span> f1_score<span class="token keyword">def</span> <span class="token function">model</span><span class="token punctuation">(</span>X_train<span class="token punctuation">,</span> y_train<span class="token punctuation">,</span> n_splits<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    scores<span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">]</span>    params <span class="token operator">=</span> <span class="token punctuation">&#123;</span>        <span class="token string">'colsample_bytree'</span><span class="token punctuation">:</span> <span class="token number">0.8</span><span class="token punctuation">,</span>                         <span class="token string">'learning_rate'</span><span class="token punctuation">:</span> <span class="token number">0.08</span><span class="token punctuation">,</span>        <span class="token string">'max_depth'</span><span class="token punctuation">:</span> <span class="token number">10</span><span class="token punctuation">,</span>        <span class="token string">'subsample'</span><span class="token punctuation">:</span> <span class="token number">1</span><span class="token punctuation">,</span>        <span class="token string">'objective'</span><span class="token punctuation">:</span> <span class="token string">'multi:softprob'</span><span class="token punctuation">,</span>        <span class="token string">'num_class'</span><span class="token punctuation">:</span> <span class="token number">2</span><span class="token punctuation">,</span>        <span class="token string">'eval_metric'</span><span class="token punctuation">:</span> <span class="token string">'mlogloss'</span><span class="token punctuation">,</span>        <span class="token string">'min_child_weight'</span><span class="token punctuation">:</span> <span class="token number">3</span><span class="token punctuation">,</span>        <span class="token string">'gamma'</span><span class="token punctuation">:</span> <span class="token number">0.25</span><span class="token punctuation">,</span>    <span class="token punctuation">&#125;</span>        kf <span class="token operator">=</span> StratifiedKFold<span class="token punctuation">(</span>n_splits<span class="token operator">=</span>n_splits<span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> random_state<span class="token operator">=</span><span class="token number">42</span><span class="token punctuation">)</span>    <span class="token keyword">for</span> train_index<span class="token punctuation">,</span> val_index <span class="token keyword">in</span> kf<span class="token punctuation">.</span>split<span class="token punctuation">(</span>X_train<span class="token punctuation">,</span> y_train<span class="token punctuation">)</span><span class="token punctuation">:</span>        train_X <span class="token operator">=</span> X_train<span class="token punctuation">.</span>iloc<span class="token punctuation">[</span>train_index<span class="token punctuation">]</span>        val_X <span class="token operator">=</span> X_train<span class="token punctuation">.</span>iloc<span class="token punctuation">[</span>val_index<span class="token punctuation">]</span>        train_y <span class="token operator">=</span> y_train<span class="token punctuation">[</span>train_index<span class="token punctuation">]</span>        val_y <span class="token operator">=</span> y_train<span class="token punctuation">[</span>val_index<span class="token punctuation">]</span>        xgb_train <span class="token operator">=</span> xgb<span class="token punctuation">.</span>DMatrix<span class="token punctuation">(</span>train_X<span class="token punctuation">,</span> train_y<span class="token punctuation">)</span>        xgb_eval <span class="token operator">=</span> xgb<span class="token punctuation">.</span>DMatrix<span class="token punctuation">(</span>val_X<span class="token punctuation">,</span> val_y<span class="token punctuation">)</span>        xgb_model <span class="token operator">=</span> xgb<span class="token punctuation">.</span>train<span class="token punctuation">(</span>            params<span class="token punctuation">,</span>            xgb_train<span class="token punctuation">,</span>            num_boost_round<span class="token operator">=</span><span class="token number">1000</span><span class="token punctuation">,</span>            evals<span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">(</span>xgb_train<span class="token punctuation">,</span> <span class="token string">'train'</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>xgb_eval<span class="token punctuation">,</span> <span class="token string">'val'</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span>            verbose_eval<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>            early_stopping_rounds<span class="token operator">=</span><span class="token number">20</span>            <span class="token punctuation">)</span>        val_X <span class="token operator">=</span> xgb<span class="token punctuation">.</span>DMatrix<span class="token punctuation">(</span>val_X<span class="token punctuation">)</span>        pred_val <span class="token operator">=</span> <span class="token punctuation">[</span>np<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>x<span class="token punctuation">)</span> <span class="token keyword">for</span> x <span class="token keyword">in</span> xgb_model<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>val_X<span class="token punctuation">)</span><span class="token punctuation">]</span>        score <span class="token operator">=</span> f1_score<span class="token punctuation">(</span>pred_val<span class="token punctuation">,</span> val_y<span class="token punctuation">)</span>        scores<span class="token punctuation">.</span>append<span class="token punctuation">(</span>score<span class="token punctuation">)</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'F1 score: '</span><span class="token punctuation">,</span> score<span class="token punctuation">)</span>    <span class="token keyword">return</span> xgb_modelnum_feature <span class="token operator">=</span> <span class="token punctuation">[</span>        <span class="token string">'Vehicle'</span><span class="token punctuation">,</span> <span class="token string">'Calendar_Year'</span><span class="token punctuation">,</span> <span class="token string">'Model_Year'</span><span class="token punctuation">,</span> <span class="token string">'Var1'</span><span class="token punctuation">,</span> <span class="token string">'Var2'</span><span class="token punctuation">,</span> <span class="token string">'Var3'</span><span class="token punctuation">,</span>         <span class="token string">'Var4'</span><span class="token punctuation">,</span> <span class="token string">'Var5'</span><span class="token punctuation">,</span> <span class="token string">'Var6'</span><span class="token punctuation">,</span> <span class="token string">'Var7'</span><span class="token punctuation">,</span> <span class="token string">'Var8'</span><span class="token punctuation">,</span> <span class="token string">'NVVar1'</span><span class="token punctuation">,</span> <span class="token string">'NVVar2'</span><span class="token punctuation">,</span> <span class="token string">'NVVar3'</span><span class="token punctuation">,</span> <span class="token string">'NVVar4'</span><span class="token punctuation">]</span>xgb_model <span class="token operator">=</span> model<span class="token punctuation">(</span>data<span class="token punctuation">[</span>num_feature<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>data<span class="token punctuation">.</span>Claim_Amount<span class="token operator">!=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>astype<span class="token punctuation">(</span><span class="token builtin">int</span><span class="token punctuation">)</span><span class="token punctuation">,</span> n_splits<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">)</span>fig<span class="token punctuation">,</span> ax <span class="token operator">=</span> plt<span class="token punctuation">.</span>subplots<span class="token punctuation">(</span>figsize<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span>xgb<span class="token punctuation">.</span>plot_importance<span class="token punctuation">(</span>xgb_model<span class="token punctuation">,</span> max_num_features<span class="token operator">=</span><span class="token number">50</span><span class="token punctuation">,</span> height<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">,</span> ax<span class="token operator">=</span>ax<span class="token punctuation">,</span> importance_type<span class="token operator">=</span><span class="token string">'gain'</span><span class="token punctuation">,</span>show_values<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><img src="/Hexo-Blog/2020/11/27/2020-11-27-exploratory-data-analysis-for-predicting-insurance-claim/xgb.png" class=""><h1 id="Missing-Data"><a href="#Missing-Data" class="headerlink" title="Missing Data"></a>Missing Data</h1><p>In this dataset, missing data is represented by a ‘?’ or a missing value. Therefore, it should start with tackling missing data. First, fill all <code>None</code> and missing data with <code>np.nan</code>. Second, replace ‘?’ by <code>np.nan</code>. Finally, plot a heatmap to get a view over the dataset.</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">plot_missing_value_heatmap</span><span class="token punctuation">(</span>data<span class="token punctuation">)</span><span class="token punctuation">:</span>    plt<span class="token punctuation">.</span>figure<span class="token punctuation">(</span>figsize<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">15</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    sns<span class="token punctuation">.</span>heatmap<span class="token punctuation">(</span>data<span class="token punctuation">.</span>isnull<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> cbar<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> cmap<span class="token operator">=</span>sns<span class="token punctuation">.</span>color_palette<span class="token punctuation">(</span><span class="token string">"cubehelix"</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    plt<span class="token punctuation">.</span>title<span class="token punctuation">(</span><span class="token string">"Missing Values Heatmap"</span><span class="token punctuation">,</span> fontdict<span class="token operator">=</span><span class="token punctuation">&#123;</span><span class="token string">'family'</span><span class="token punctuation">:</span> <span class="token string">'serif'</span><span class="token punctuation">,</span> <span class="token string">'weight'</span><span class="token punctuation">:</span> <span class="token string">'normal'</span><span class="token punctuation">,</span> <span class="token string">'size'</span><span class="token punctuation">:</span> <span class="token number">16</span><span class="token punctuation">,</span><span class="token punctuation">&#125;</span><span class="token punctuation">)</span>    plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>data_pre <span class="token operator">=</span> data<span class="token punctuation">.</span>copy<span class="token punctuation">(</span><span class="token punctuation">)</span>data_pre<span class="token punctuation">.</span>fillna<span class="token punctuation">(</span>value<span class="token operator">=</span>np<span class="token punctuation">.</span>nan<span class="token punctuation">,</span> inplace<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token keyword">for</span> col <span class="token keyword">in</span> object_features<span class="token punctuation">:</span>    data_pre<span class="token punctuation">.</span>loc<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> col<span class="token punctuation">]</span> <span class="token operator">=</span> data_pre<span class="token punctuation">.</span>loc<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> col<span class="token punctuation">]</span><span class="token punctuation">.</span>replace<span class="token punctuation">(</span>to_replace<span class="token operator">=</span><span class="token string">'?'</span><span class="token punctuation">,</span>value<span class="token operator">=</span>np<span class="token punctuation">.</span>nan<span class="token punctuation">)</span>    data_pre<span class="token punctuation">.</span>loc<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> col<span class="token punctuation">]</span> <span class="token operator">=</span> data_pre<span class="token punctuation">.</span>loc<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> col<span class="token punctuation">]</span><span class="token punctuation">.</span>replace<span class="token punctuation">(</span>to_replace<span class="token operator">=</span><span class="token string">'""'</span><span class="token punctuation">,</span>value<span class="token operator">=</span>np<span class="token punctuation">.</span>nan<span class="token punctuation">)</span>plot_missing_value_heatmap<span class="token punctuation">(</span>data_pre<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><img src="/Hexo-Blog/2020/11/27/2020-11-27-exploratory-data-analysis-for-predicting-insurance-claim/missing.png" class=""><p>There are too many missing values in columns below:</p><ul><li>Cat2</li><li>Cat4</li><li>Cat5</li><li>Cat7</li></ul><p>So I just drop those columns.</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># Split data into features and label</span>data_pre_feature1 <span class="token operator">=</span> data_pre<span class="token punctuation">.</span>drop<span class="token punctuation">(</span><span class="token string">'Claim_Amount'</span><span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>data_pre_label <span class="token operator">=</span> data_pre<span class="token punctuation">[</span><span class="token string">'Claim_Amount'</span><span class="token punctuation">]</span><span class="token comment"># Convert int dtype into float dtype</span><span class="token keyword">for</span> col <span class="token keyword">in</span> int_features<span class="token punctuation">:</span>    data_pre_feature1<span class="token punctuation">[</span>col<span class="token punctuation">]</span> <span class="token operator">=</span> data_pre_feature1<span class="token punctuation">[</span>col<span class="token punctuation">]</span><span class="token punctuation">.</span>astype<span class="token punctuation">(</span><span class="token string">'float64'</span><span class="token punctuation">)</span>data_pre_feature1 <span class="token operator">=</span> data_pre_feature1<span class="token punctuation">.</span>drop<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token string">"Cat2"</span><span class="token punctuation">,</span> <span class="token string">"Cat4"</span><span class="token punctuation">,</span> <span class="token string">"Cat5"</span><span class="token punctuation">,</span> <span class="token string">"Cat7"</span><span class="token punctuation">]</span><span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>plot_missing_value_heatmap<span class="token punctuation">(</span>data_pre_feature1<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><img src="/Hexo-Blog/2020/11/27/2020-11-27-exploratory-data-analysis-for-predicting-insurance-claim/missing2.png" class=""><p>It’s quite good after removing these four columns! Therefore, for those features (“Cat2”, “Cat4”, “Cat5”, “Cat7”) which have more than 90% of missing value, I chose dropping them instead of filling . Rest of the featuers I chose using forward filling method from pandas library. </p><h1 id="Features-Transformation"><a href="#Features-Transformation" class="headerlink" title="Features Transformation"></a>Features Transformation</h1><p>I utilised different transformation methods for different data types. </p><ol><li>‘Blind_Make’, ‘Blind_Model’, ‘Blind_Submodel’: <code>MeanEncoder()</code> for high-cardinality categorical data.</li><li>‘Row_ID’, ‘Household_ID’, ‘Vehicle’, ‘Calendar_Year’, ‘Model_Year’, ‘Var1’, ‘Var2’, ‘Var3’, ‘Var4’, ‘Var5’, ‘Var6’, ‘Var7’, ‘Var8’, ‘NVVar1’, ‘NVVar2’, ‘NVVar3’, ‘NVVar4’: <code>MinMaxScaler()</code> for numerical dat.</li><li>‘Cat1’, ‘Cat3’, ‘Cat6’, ‘Cat8’, ‘Cat9’, ‘Cat10’, ‘Cat11’, ‘Cat12’, ‘OrdCat’, ‘NVCat’: <code>OneHotEncoder()</code> for categorical data (but in smaller dimension).</li></ol><pre class="line-numbers language-python" data-language="python"><code class="language-python">data_pre_feature2 <span class="token operator">=</span> data_pre_feature1<span class="token punctuation">.</span>fillna<span class="token punctuation">(</span>method<span class="token operator">=</span><span class="token string">'ffill'</span><span class="token punctuation">)</span>num_feature <span class="token operator">=</span> <span class="token punctuation">[</span>    <span class="token string">'Row_ID'</span><span class="token punctuation">,</span> <span class="token string">'Household_ID'</span><span class="token punctuation">,</span> <span class="token string">'Vehicle'</span><span class="token punctuation">,</span> <span class="token string">'Calendar_Year'</span><span class="token punctuation">,</span> <span class="token string">'Model_Year'</span><span class="token punctuation">,</span> <span class="token string">'Var1'</span><span class="token punctuation">,</span> <span class="token string">'Var2'</span><span class="token punctuation">,</span> <span class="token string">'Var3'</span><span class="token punctuation">,</span>     <span class="token string">'Var4'</span><span class="token punctuation">,</span> <span class="token string">'Var5'</span><span class="token punctuation">,</span> <span class="token string">'Var6'</span><span class="token punctuation">,</span> <span class="token string">'Var7'</span><span class="token punctuation">,</span> <span class="token string">'Var8'</span><span class="token punctuation">,</span> <span class="token string">'NVVar1'</span><span class="token punctuation">,</span> <span class="token string">'NVVar2'</span><span class="token punctuation">,</span> <span class="token string">'NVVar3'</span><span class="token punctuation">,</span> <span class="token string">'NVVar4'</span><span class="token punctuation">]</span>cat_feature <span class="token operator">=</span> <span class="token punctuation">[</span>    <span class="token string">'Cat1'</span><span class="token punctuation">,</span> <span class="token string">'Cat3'</span><span class="token punctuation">,</span> <span class="token string">'Cat6'</span><span class="token punctuation">,</span> <span class="token string">'Cat8'</span><span class="token punctuation">,</span> <span class="token string">'Cat9'</span><span class="token punctuation">,</span> <span class="token string">'Cat10'</span><span class="token punctuation">,</span> <span class="token string">'Cat11'</span><span class="token punctuation">,</span> <span class="token string">'Cat12'</span><span class="token punctuation">,</span> <span class="token string">'OrdCat'</span><span class="token punctuation">,</span> <span class="token string">'NVCat'</span><span class="token punctuation">]</span>full_transform <span class="token operator">=</span> ColumnTransformer<span class="token punctuation">(</span><span class="token punctuation">[</span>    <span class="token punctuation">(</span><span class="token string">"num"</span><span class="token punctuation">,</span> StandardScaler<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> num_feature<span class="token punctuation">)</span><span class="token punctuation">,</span>    <span class="token punctuation">(</span><span class="token string">"cat"</span><span class="token punctuation">,</span> OneHotEncoder<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> cat_feature<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>data_pre_feature3 <span class="token operator">=</span> full_transform<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>data_pre_feature2<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="Mean-Encoder"><a href="#Mean-Encoder" class="headerlink" title="Mean Encoder"></a>Mean Encoder</h2><p>Mean Encoding is a simple preprocessing scheme for high-cardinality categorical data that allows this class of attributes to be used in predictive models such as neural networks, linear and logistic regression. The proposed method is based on a well-established statistical method (empirical Bayes) that is straightforward to implement as an in-database procedure. Furthermore, for categorical attributes with an inherent hierarchical structure, like ZIP codes, the preprocessing scheme can directly leverage the hierarchy by blending statistics at the various levels of aggregation.</p><p>I made this <code>MeanEncoder()</code> into sklearn-compatible class object.</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">MeanEncoder</span><span class="token punctuation">(</span>TransformerMixin<span class="token punctuation">,</span> BaseEstimator<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""    http://helios.mm.di.uoa.gr/~rouvas/ssi/sigkdd/sigkdd.vol3.1/barreca.pdf    """</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> cat_features<span class="token punctuation">,</span> cv<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">,</span> target_type<span class="token operator">=</span><span class="token string">'classification'</span><span class="token punctuation">,</span> weight_func<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> k<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> f<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>cat_features <span class="token operator">=</span> cat_features        self<span class="token punctuation">.</span>cv <span class="token operator">=</span> cv        self<span class="token punctuation">.</span>k <span class="token operator">=</span> k        self<span class="token punctuation">.</span>f <span class="token operator">=</span> f        self<span class="token punctuation">.</span>learned_stats <span class="token operator">=</span> <span class="token punctuation">&#123;</span><span class="token punctuation">&#125;</span>        <span class="token keyword">if</span> target_type <span class="token operator">==</span> <span class="token string">'classification'</span><span class="token punctuation">:</span>            self<span class="token punctuation">.</span>target_type <span class="token operator">=</span> target_type            self<span class="token punctuation">.</span>target_values <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>        <span class="token keyword">elif</span> target_type <span class="token operator">==</span> <span class="token string">'regression'</span><span class="token punctuation">:</span>            self<span class="token punctuation">.</span>target_type <span class="token operator">=</span> <span class="token string">'regression'</span>            self<span class="token punctuation">.</span>target_values <span class="token operator">=</span> <span class="token boolean">None</span>        <span class="token keyword">else</span><span class="token punctuation">:</span>            <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Label type could only be 'classification' or 'regression'."</span><span class="token punctuation">)</span>        <span class="token comment"># Calculate smoothing factor: 1 / (1 + np.exp(- (counts - min_samples_leaf) / smoothing_slope))</span>        <span class="token keyword">if</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>weight_func<span class="token punctuation">,</span> <span class="token builtin">dict</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            self<span class="token punctuation">.</span>weight_func <span class="token operator">=</span> <span class="token builtin">eval</span><span class="token punctuation">(</span>                <span class="token string">'lambda x: 1 / (1 + np.exp(-(x-k)/f))'</span><span class="token punctuation">,</span> <span class="token builtin">dict</span><span class="token punctuation">(</span>weight_func<span class="token punctuation">,</span> np<span class="token operator">=</span>np<span class="token punctuation">,</span> k<span class="token operator">=</span>k<span class="token punctuation">,</span> f<span class="token operator">=</span>f<span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token keyword">elif</span> <span class="token builtin">callable</span><span class="token punctuation">(</span>weight_func<span class="token punctuation">)</span><span class="token punctuation">:</span>            self<span class="token punctuation">.</span>weight_func <span class="token operator">=</span> weight_func        <span class="token keyword">else</span><span class="token punctuation">:</span>            self<span class="token punctuation">.</span>weight_func <span class="token operator">=</span> <span class="token keyword">lambda</span> x<span class="token punctuation">:</span> <span class="token number">1</span> <span class="token operator">/</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">+</span> np<span class="token punctuation">.</span>exp<span class="token punctuation">(</span><span class="token operator">-</span><span class="token punctuation">(</span>x<span class="token operator">-</span>k<span class="token punctuation">)</span><span class="token operator">/</span>f<span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token comment"># For training dataset</span>    <span class="token keyword">def</span> <span class="token function">fit_transform</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> X<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">:</span>        X_new <span class="token operator">=</span> X<span class="token punctuation">.</span>copy<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">if</span> self<span class="token punctuation">.</span>target_type <span class="token operator">==</span> <span class="token string">'classification'</span><span class="token punctuation">:</span>            skf <span class="token operator">=</span> StratifiedKFold<span class="token punctuation">(</span>self<span class="token punctuation">.</span>cv<span class="token punctuation">)</span>        <span class="token keyword">else</span><span class="token punctuation">:</span>            skf <span class="token operator">=</span> KFold<span class="token punctuation">(</span>self<span class="token punctuation">.</span>cv<span class="token punctuation">)</span>        <span class="token comment"># Categorical label</span>        <span class="token keyword">if</span> self<span class="token punctuation">.</span>target_type <span class="token operator">==</span> <span class="token string">'classification'</span><span class="token punctuation">:</span>            self<span class="token punctuation">.</span>target_values <span class="token operator">=</span> <span class="token builtin">sorted</span><span class="token punctuation">(</span><span class="token builtin">set</span><span class="token punctuation">(</span>y<span class="token punctuation">)</span><span class="token punctuation">)</span>            self<span class="token punctuation">.</span>learned_stats <span class="token operator">=</span> <span class="token punctuation">&#123;</span><span class="token string">'&#123;&#125;_pred_&#123;&#125;'</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>variable<span class="token punctuation">,</span> target<span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token punctuation">]</span> <span class="token keyword">for</span> variable<span class="token punctuation">,</span> target <span class="token keyword">in</span>                                  product<span class="token punctuation">(</span>self<span class="token punctuation">.</span>cat_features<span class="token punctuation">,</span> self<span class="token punctuation">.</span>target_values<span class="token punctuation">)</span><span class="token punctuation">&#125;</span>            <span class="token keyword">for</span> variable<span class="token punctuation">,</span> target <span class="token keyword">in</span> product<span class="token punctuation">(</span>self<span class="token punctuation">.</span>cat_features<span class="token punctuation">,</span> self<span class="token punctuation">.</span>target_values<span class="token punctuation">)</span><span class="token punctuation">:</span>                nf_name <span class="token operator">=</span> <span class="token string">'&#123;&#125;_pred_&#123;&#125;'</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>variable<span class="token punctuation">,</span> target<span class="token punctuation">)</span>                X_new<span class="token punctuation">.</span>loc<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> nf_name<span class="token punctuation">]</span> <span class="token operator">=</span> np<span class="token punctuation">.</span>nan                <span class="token keyword">for</span> large_ind<span class="token punctuation">,</span> small_ind <span class="token keyword">in</span> skf<span class="token punctuation">.</span>split<span class="token punctuation">(</span>y<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">:</span>                    nf_large<span class="token punctuation">,</span> nf_small<span class="token punctuation">,</span> prior<span class="token punctuation">,</span> col_avg_y <span class="token operator">=</span> MeanEncoder<span class="token punctuation">.</span>mean_encode_blended<span class="token punctuation">(</span>                        X_new<span class="token punctuation">.</span>iloc<span class="token punctuation">[</span>large_ind<span class="token punctuation">]</span><span class="token punctuation">,</span>                         y<span class="token punctuation">.</span>iloc<span class="token punctuation">[</span>large_ind<span class="token punctuation">]</span><span class="token punctuation">,</span>                         X_new<span class="token punctuation">.</span>iloc<span class="token punctuation">[</span>small_ind<span class="token punctuation">]</span><span class="token punctuation">,</span>                         variable<span class="token punctuation">,</span>                         target<span class="token punctuation">,</span>                         self<span class="token punctuation">.</span>weight_func<span class="token punctuation">)</span>                    X_new<span class="token punctuation">.</span>iloc<span class="token punctuation">[</span>small_ind<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">=</span> nf_small                    self<span class="token punctuation">.</span>learned_stats<span class="token punctuation">[</span>nf_name<span class="token punctuation">]</span><span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token punctuation">(</span>prior<span class="token punctuation">,</span> col_avg_y<span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token comment"># Continuous label</span>        <span class="token keyword">else</span><span class="token punctuation">:</span>            self<span class="token punctuation">.</span>learned_stats <span class="token operator">=</span> <span class="token punctuation">&#123;</span><span class="token string">'&#123;&#125;_pred'</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>variable<span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token punctuation">]</span> <span class="token keyword">for</span> variable <span class="token keyword">in</span> self<span class="token punctuation">.</span>cat_features<span class="token punctuation">&#125;</span>            <span class="token keyword">for</span> variable <span class="token keyword">in</span> self<span class="token punctuation">.</span>cat_features<span class="token punctuation">:</span>                nf_name <span class="token operator">=</span> <span class="token string">'&#123;&#125;_pred'</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>variable<span class="token punctuation">)</span>                X_new<span class="token punctuation">.</span>loc<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> nf_name<span class="token punctuation">]</span> <span class="token operator">=</span> np<span class="token punctuation">.</span>nan                <span class="token keyword">for</span> large_ind<span class="token punctuation">,</span> small_ind <span class="token keyword">in</span> skf<span class="token punctuation">.</span>split<span class="token punctuation">(</span>y<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">:</span>                    nf_large<span class="token punctuation">,</span> nf_small<span class="token punctuation">,</span> prior<span class="token punctuation">,</span> col_avg_y <span class="token operator">=</span> MeanEncoder<span class="token punctuation">.</span>mean_encode_blended<span class="token punctuation">(</span>                        X_new<span class="token punctuation">.</span>iloc<span class="token punctuation">[</span>large_ind<span class="token punctuation">]</span><span class="token punctuation">,</span>                         y<span class="token punctuation">.</span>iloc<span class="token punctuation">[</span>large_ind<span class="token punctuation">]</span><span class="token punctuation">,</span>                         X_new<span class="token punctuation">.</span>iloc<span class="token punctuation">[</span>small_ind<span class="token punctuation">]</span><span class="token punctuation">,</span>                         variable<span class="token punctuation">,</span>                         <span class="token boolean">None</span><span class="token punctuation">,</span>                         self<span class="token punctuation">.</span>weight_func<span class="token punctuation">)</span>                    X_new<span class="token punctuation">.</span>iloc<span class="token punctuation">[</span>small_ind<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">=</span> nf_small                    self<span class="token punctuation">.</span>learned_stats<span class="token punctuation">[</span>nf_name<span class="token punctuation">]</span><span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token punctuation">(</span>prior<span class="token punctuation">,</span> col_avg_y<span class="token punctuation">)</span><span class="token punctuation">)</span>        X_new <span class="token operator">=</span> X_new<span class="token punctuation">.</span>drop<span class="token punctuation">(</span>self<span class="token punctuation">.</span>cat_features<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>        X_new<span class="token punctuation">.</span>columns <span class="token operator">=</span> self<span class="token punctuation">.</span>cat_features        <span class="token keyword">return</span> X_new    <span class="token comment"># For testing dataset</span>    <span class="token keyword">def</span> <span class="token function">transform</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> X<span class="token punctuation">)</span><span class="token punctuation">:</span>        X_new <span class="token operator">=</span> X<span class="token punctuation">.</span>copy<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment"># Categorical label</span>        <span class="token keyword">if</span> self<span class="token punctuation">.</span>target_type <span class="token operator">==</span> <span class="token string">'classification'</span><span class="token punctuation">:</span>            <span class="token keyword">for</span> variable<span class="token punctuation">,</span> target <span class="token keyword">in</span> product<span class="token punctuation">(</span>self<span class="token punctuation">.</span>cat_features<span class="token punctuation">,</span> self<span class="token punctuation">.</span>target_values<span class="token punctuation">)</span><span class="token punctuation">:</span>                nf_name <span class="token operator">=</span> <span class="token string">'&#123;&#125;_pred_&#123;&#125;'</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>variable<span class="token punctuation">,</span> target<span class="token punctuation">)</span>                X_new<span class="token punctuation">[</span>nf_name<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0</span>                <span class="token keyword">for</span> prior<span class="token punctuation">,</span> col_avg_y <span class="token keyword">in</span> self<span class="token punctuation">.</span>learned_stats<span class="token punctuation">[</span>nf_name<span class="token punctuation">]</span><span class="token punctuation">:</span>                    X_new<span class="token punctuation">[</span>nf_name<span class="token punctuation">]</span> <span class="token operator">+=</span> X_new<span class="token punctuation">[</span><span class="token punctuation">[</span>variable<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>                        col_avg_y<span class="token punctuation">,</span> on<span class="token operator">=</span>variable<span class="token punctuation">)</span><span class="token punctuation">.</span>fillna<span class="token punctuation">(</span>prior<span class="token punctuation">,</span> inplace<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">[</span>                        nf_name<span class="token punctuation">]</span>                X_new<span class="token punctuation">[</span>nf_name<span class="token punctuation">]</span> <span class="token operator">/=</span> self<span class="token punctuation">.</span>cv        <span class="token comment"># Continuous label</span>        <span class="token keyword">else</span><span class="token punctuation">:</span>            <span class="token keyword">for</span> variable <span class="token keyword">in</span> self<span class="token punctuation">.</span>cat_features<span class="token punctuation">:</span>                nf_name <span class="token operator">=</span> <span class="token string">'&#123;&#125;_pred'</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>variable<span class="token punctuation">)</span>                X_new<span class="token punctuation">[</span>nf_name<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0</span>                <span class="token keyword">for</span> prior<span class="token punctuation">,</span> col_avg_y <span class="token keyword">in</span> self<span class="token punctuation">.</span>learned_stats<span class="token punctuation">[</span>nf_name<span class="token punctuation">]</span><span class="token punctuation">:</span>                    X_new<span class="token punctuation">[</span>nf_name<span class="token punctuation">]</span> <span class="token operator">+=</span> X_new<span class="token punctuation">[</span><span class="token punctuation">[</span>variable<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>                        col_avg_y<span class="token punctuation">,</span> on<span class="token operator">=</span>variable<span class="token punctuation">)</span><span class="token punctuation">.</span>fillna<span class="token punctuation">(</span>prior<span class="token punctuation">,</span> inplace<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">[</span>                        nf_name<span class="token punctuation">]</span>                X_new<span class="token punctuation">[</span>nf_name<span class="token punctuation">]</span> <span class="token operator">/=</span> self<span class="token punctuation">.</span>cv        X_new <span class="token operator">=</span> X_new<span class="token punctuation">.</span>drop<span class="token punctuation">(</span>self<span class="token punctuation">.</span>cat_features<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>        X_new<span class="token punctuation">.</span>columns <span class="token operator">=</span> self<span class="token punctuation">.</span>cat_features        <span class="token keyword">return</span> X_new    <span class="token comment"># Prior probability and posterior probability</span>    <span class="token decorator annotation punctuation">@staticmethod</span>    <span class="token keyword">def</span> <span class="token function">mean_encode_blended</span><span class="token punctuation">(</span>X_train<span class="token punctuation">,</span> y_train<span class="token punctuation">,</span> X_test<span class="token punctuation">,</span> variable<span class="token punctuation">,</span> target<span class="token punctuation">,</span> weight_func<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">"""        S_i represents an estimate of the probability of Y=1 given X=X_i        """</span>        X_train <span class="token operator">=</span> X_train<span class="token punctuation">[</span><span class="token punctuation">[</span>variable<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">.</span>copy<span class="token punctuation">(</span><span class="token punctuation">)</span>        X_test <span class="token operator">=</span> X_test<span class="token punctuation">[</span><span class="token punctuation">[</span>variable<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">.</span>copy<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">if</span> target <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>            nf_name <span class="token operator">=</span> <span class="token string">'&#123;&#125;_pred_&#123;&#125;'</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>variable<span class="token punctuation">,</span> target<span class="token punctuation">)</span>            X_train<span class="token punctuation">[</span><span class="token string">'pred_temp'</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">(</span>y_train <span class="token operator">==</span> target<span class="token punctuation">)</span><span class="token punctuation">.</span>astype<span class="token punctuation">(</span><span class="token builtin">int</span><span class="token punctuation">)</span>        <span class="token keyword">else</span><span class="token punctuation">:</span>            nf_name <span class="token operator">=</span> <span class="token string">'&#123;&#125;_pred'</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>variable<span class="token punctuation">)</span>            X_train<span class="token punctuation">[</span><span class="token string">'pred_temp'</span><span class="token punctuation">]</span> <span class="token operator">=</span> y_train        <span class="token comment"># prior = n_Y / n_TR</span>        prior <span class="token operator">=</span> X_train<span class="token punctuation">[</span><span class="token string">'pred_temp'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment"># S_i['mean'] = n_iY/n_i and S_i['beta'] = lambda(n_i)</span>        S_i <span class="token operator">=</span> X_train<span class="token punctuation">.</span>groupby<span class="token punctuation">(</span>by<span class="token operator">=</span>variable<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token string">'pred_temp'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>agg<span class="token punctuation">(</span>mean<span class="token operator">=</span><span class="token string">"mean"</span><span class="token punctuation">,</span> beta<span class="token operator">=</span><span class="token string">"size"</span><span class="token punctuation">)</span>        S_i<span class="token punctuation">[</span><span class="token string">'beta'</span><span class="token punctuation">]</span> <span class="token operator">=</span> weight_func<span class="token punctuation">(</span>S_i<span class="token punctuation">[</span><span class="token string">'beta'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>        <span class="token comment"># Empirical Bayes Estimation: S_i = lambda(n_i)*n_iY/n_i + (1-lambda(n_i))*n_Y/n_TR</span>        S_i<span class="token punctuation">[</span>nf_name<span class="token punctuation">]</span> <span class="token operator">=</span> S_i<span class="token punctuation">[</span><span class="token string">'beta'</span><span class="token punctuation">]</span> <span class="token operator">*</span> S_i<span class="token punctuation">[</span><span class="token string">'mean'</span><span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> S_i<span class="token punctuation">[</span><span class="token string">'beta'</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">*</span> prior        S_i<span class="token punctuation">.</span>drop<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token string">'beta'</span><span class="token punctuation">,</span> <span class="token string">'mean'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> inplace<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>        nf_train <span class="token operator">=</span> X_train<span class="token punctuation">.</span>join<span class="token punctuation">(</span>S_i<span class="token punctuation">,</span> on<span class="token operator">=</span>variable<span class="token punctuation">)</span><span class="token punctuation">[</span>nf_name<span class="token punctuation">]</span><span class="token punctuation">.</span>values        nf_test <span class="token operator">=</span> X_test<span class="token punctuation">.</span>join<span class="token punctuation">(</span>S_i<span class="token punctuation">,</span> on<span class="token operator">=</span>variable<span class="token punctuation">)</span><span class="token punctuation">.</span>fillna<span class="token punctuation">(</span>prior<span class="token punctuation">,</span> inplace<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">[</span>nf_name<span class="token punctuation">]</span><span class="token punctuation">.</span>values        <span class="token keyword">return</span> nf_train<span class="token punctuation">,</span> nf_test<span class="token punctuation">,</span> prior<span class="token punctuation">,</span> S_i    <span class="token keyword">def</span> <span class="token function">get_params</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> deep<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> <span class="token punctuation">&#123;</span>            <span class="token string">"cat_features"</span><span class="token punctuation">:</span> self<span class="token punctuation">.</span>cat_features<span class="token punctuation">,</span>             <span class="token string">"target_type"</span><span class="token punctuation">:</span> self<span class="token punctuation">.</span>target_type<span class="token punctuation">,</span>             <span class="token string">"cv"</span><span class="token punctuation">:</span> self<span class="token punctuation">.</span>cv<span class="token punctuation">,</span>             <span class="token string">"k"</span><span class="token punctuation">:</span> self<span class="token punctuation">.</span>k<span class="token punctuation">,</span>             <span class="token string">"f"</span><span class="token punctuation">:</span> self<span class="token punctuation">.</span>f<span class="token punctuation">&#125;</span>    <span class="token keyword">def</span> <span class="token function">set_params</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> <span class="token operator">**</span>parameters<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">for</span> parameter<span class="token punctuation">,</span> value <span class="token keyword">in</span> parameters<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token builtin">setattr</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> parameter<span class="token punctuation">,</span> value<span class="token punctuation">)</span>        <span class="token keyword">return</span> self<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>Perform <code>MeanEncoder()</code> on ‘Blind_Make’, ‘Blind_Model’, ‘Blind_Submodel’</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># High-cardinality categorical data</span>mean_encoder_feature <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'Blind_Make'</span><span class="token punctuation">,</span> <span class="token string">'Blind_Model'</span><span class="token punctuation">,</span> <span class="token string">'Blind_Submodel'</span><span class="token punctuation">]</span>me <span class="token operator">=</span> MeanEncoder<span class="token punctuation">(</span>cat_features<span class="token operator">=</span>mean_encoder_feature<span class="token punctuation">,</span> cv<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">,</span> target_type<span class="token operator">=</span><span class="token string">'regression'</span><span class="token punctuation">)</span>data_pre_feature4 <span class="token operator">=</span> me<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>data_pre<span class="token punctuation">[</span>mean_encoder_feature<span class="token punctuation">]</span><span class="token punctuation">,</span> data_pre<span class="token punctuation">[</span><span class="token string">"Claim_Amount"</span><span class="token punctuation">]</span><span class="token punctuation">)</span>data_pre_feature4<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><img src="/Hexo-Blog/2020/11/27/2020-11-27-exploratory-data-analysis-for-predicting-insurance-claim/me.png" class=""><h1 id="Imbalanced-Data"><a href="#Imbalanced-Data" class="headerlink" title="Imbalanced Data"></a>Imbalanced Data</h1><p>The data is highly imbalanced: more records contain zero claims than not. When designing your predictive model, you need to account for this.</p><p>There are a couple of ways to deal with imbalanced data.</p><ol><li>Resampling</li><li>Over-sampling: SMOTE</li><li>Under-sampling: Clustering, Tomek links</li></ol><p>I built up-sampling and down-sampling functions to see whether they can improve the model.</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>utils <span class="token keyword">import</span> resample<span class="token keyword">from</span> imblearn<span class="token punctuation">.</span>pipeline <span class="token keyword">import</span> Pipeline<span class="token keyword">from</span> imblearn<span class="token punctuation">.</span>over_sampling <span class="token keyword">import</span> SMOTE<span class="token keyword">from</span> imblearn<span class="token punctuation">.</span>under_sampling <span class="token keyword">import</span> RandomUnderSamplerzero_label_num <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>data_pre_label<span class="token punctuation">[</span>data_pre_label<span class="token operator">==</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>non_zero_label_num <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>data_pre_label<span class="token punctuation">[</span>data_pre_label<span class="token operator">></span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">upsampling</span><span class="token punctuation">(</span>features<span class="token punctuation">,</span> labels<span class="token punctuation">)</span><span class="token punctuation">:</span>    df <span class="token operator">=</span> pd<span class="token punctuation">.</span>concat<span class="token punctuation">(</span><span class="token punctuation">[</span>features<span class="token punctuation">,</span> labels<span class="token punctuation">]</span><span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>    <span class="token comment"># Separate majority and minority classes</span>    df_majority <span class="token operator">=</span> df<span class="token punctuation">[</span>df<span class="token punctuation">.</span>Claim_Amount<span class="token operator">==</span><span class="token number">0</span><span class="token punctuation">]</span>    df_minority <span class="token operator">=</span> df<span class="token punctuation">[</span>df<span class="token punctuation">.</span>Claim_Amount<span class="token operator">!=</span><span class="token number">0</span><span class="token punctuation">]</span>    <span class="token comment"># Upsample minority class</span>    df_minority_upsampled <span class="token operator">=</span> resample<span class="token punctuation">(</span>df_minority<span class="token punctuation">,</span> replace<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> n_samples<span class="token operator">=</span>df_majority<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> random_state<span class="token operator">=</span><span class="token number">914</span><span class="token punctuation">)</span>    <span class="token comment"># Combine majority class with upsampled minority class</span>    df_upsampled <span class="token operator">=</span> pd<span class="token punctuation">.</span>concat<span class="token punctuation">(</span><span class="token punctuation">[</span>df_majority<span class="token punctuation">,</span> df_minority_upsampled<span class="token punctuation">]</span><span class="token punctuation">)</span>    <span class="token comment"># Display new class counts</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>        f"Non zero<span class="token punctuation">:</span> <span class="token punctuation">&#123;</span><span class="token builtin">len</span><span class="token punctuation">(</span>df_upsampled<span class="token punctuation">[</span>df_upsampled<span class="token punctuation">.</span>Claim_Amount<span class="token operator">></span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">&#125;</span> \        Zero<span class="token punctuation">:</span> <span class="token punctuation">&#123;</span><span class="token builtin">len</span><span class="token punctuation">(</span>df_upsampled<span class="token punctuation">[</span>df_upsampled<span class="token punctuation">.</span>Claim_Amount<span class="token operator">==</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">&#125;</span>"<span class="token punctuation">)</span>    <span class="token keyword">return</span> df_upsampled<span class="token punctuation">.</span>drop<span class="token punctuation">(</span><span class="token string">'Claim_Amount'</span><span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> df_upsampled<span class="token punctuation">.</span>Claim_Amount<span class="token keyword">def</span> <span class="token function">downsampling</span><span class="token punctuation">(</span>features<span class="token punctuation">,</span> labels<span class="token punctuation">)</span><span class="token punctuation">:</span>    df <span class="token operator">=</span> pd<span class="token punctuation">.</span>concat<span class="token punctuation">(</span><span class="token punctuation">[</span>features<span class="token punctuation">,</span> labels<span class="token punctuation">]</span><span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>    <span class="token comment"># Separate majority and minority classes</span>    df_majority <span class="token operator">=</span> df<span class="token punctuation">[</span>df<span class="token punctuation">.</span>Claim_Amount<span class="token operator">==</span><span class="token number">0</span><span class="token punctuation">]</span>    df_minority <span class="token operator">=</span> df<span class="token punctuation">[</span>df<span class="token punctuation">.</span>Claim_Amount<span class="token operator">!=</span><span class="token number">0</span><span class="token punctuation">]</span>    <span class="token comment"># Downsample majority class</span>    df_majority_downsampled <span class="token operator">=</span> resample<span class="token punctuation">(</span>df_majority<span class="token punctuation">,</span> replace<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> n_samples<span class="token operator">=</span>df_minority<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> random_state<span class="token operator">=</span><span class="token number">411</span><span class="token punctuation">)</span>    <span class="token comment"># Combine minority class with downsampled majority class</span>    df_downsampled <span class="token operator">=</span> pd<span class="token punctuation">.</span>concat<span class="token punctuation">(</span><span class="token punctuation">[</span>df_majority_downsampled<span class="token punctuation">,</span> df_minority<span class="token punctuation">]</span><span class="token punctuation">)</span>    <span class="token comment"># Display new class counts</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>        f"Non zero<span class="token punctuation">:</span> <span class="token punctuation">&#123;</span><span class="token builtin">len</span><span class="token punctuation">(</span>df_downsampled<span class="token punctuation">[</span>df_downsampled<span class="token punctuation">.</span>Claim_Amount<span class="token operator">></span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">&#125;</span> \        Zero<span class="token punctuation">:</span> <span class="token punctuation">&#123;</span><span class="token builtin">len</span><span class="token punctuation">(</span>df_downsampled<span class="token punctuation">[</span>df_downsampled<span class="token punctuation">.</span>Claim_Amount<span class="token operator">==</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">&#125;</span>"<span class="token punctuation">)</span>    <span class="token keyword">return</span> df_downsampled<span class="token punctuation">.</span>drop<span class="token punctuation">(</span><span class="token string">'Claim_Amount'</span><span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> df_downsampled<span class="token punctuation">.</span>Claim_Amount<span class="token keyword">def</span> <span class="token function">smote_sampling</span><span class="token punctuation">(</span>X<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">:</span>    smote <span class="token operator">=</span> SMOTE<span class="token punctuation">(</span>sampling_strategy<span class="token operator">=</span><span class="token string">"minority"</span><span class="token punctuation">)</span>    X<span class="token punctuation">,</span> y <span class="token operator">=</span> smote<span class="token punctuation">.</span>fit_resample<span class="token punctuation">(</span>X<span class="token punctuation">,</span> y<span class="token punctuation">)</span>    <span class="token keyword">return</span> X<span class="token punctuation">,</span> y<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>Put it all together!</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">preprocessing_baseline</span><span class="token punctuation">(</span>dataframe<span class="token punctuation">,</span> use_upsampling<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> use_downsampling<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment"># Split into training and validation</span>    data <span class="token operator">=</span> dataframe<span class="token punctuation">.</span>copy<span class="token punctuation">(</span><span class="token punctuation">)</span>    X_train<span class="token punctuation">,</span> X_valid<span class="token punctuation">,</span> y_train<span class="token punctuation">,</span> y_valid <span class="token operator">=</span> train_test_split<span class="token punctuation">(</span>        data<span class="token punctuation">.</span>drop<span class="token punctuation">(</span><span class="token string">"Claim_Amount"</span><span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>         data<span class="token punctuation">.</span>Claim_Amount<span class="token punctuation">,</span>         test_size<span class="token operator">=</span><span class="token number">0.15</span><span class="token punctuation">,</span>         random_state<span class="token operator">=</span><span class="token number">42</span><span class="token punctuation">,</span>         stratify<span class="token operator">=</span><span class="token punctuation">(</span>data<span class="token punctuation">.</span>Claim_Amount<span class="token operator">!=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>astype<span class="token punctuation">(</span><span class="token builtin">int</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">if</span> use_upsampling <span class="token operator">==</span> <span class="token boolean">True</span><span class="token punctuation">:</span> X_train<span class="token punctuation">,</span> y_train <span class="token operator">=</span> upsampling<span class="token punctuation">(</span>X_train<span class="token punctuation">,</span> y_train<span class="token punctuation">)</span>    <span class="token keyword">if</span> use_downsampling <span class="token operator">==</span> <span class="token boolean">True</span><span class="token punctuation">:</span> X_train<span class="token punctuation">,</span> y_train <span class="token operator">=</span> downsampling<span class="token punctuation">(</span>X_train<span class="token punctuation">,</span> y_train<span class="token punctuation">)</span>    <span class="token comment"># Define different datatype</span>    int_features<span class="token punctuation">,</span> float_features<span class="token punctuation">,</span> object_features <span class="token operator">=</span> type_of_col<span class="token punctuation">(</span>data<span class="token punctuation">,</span> label_col<span class="token operator">=</span><span class="token string">'Claim_Amount'</span><span class="token punctuation">,</span> show<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>    <span class="token comment"># ========================================</span>    <span class="token comment">#               Training</span>    <span class="token comment"># ========================================</span>    X_train<span class="token punctuation">.</span>fillna<span class="token punctuation">(</span>value<span class="token operator">=</span>np<span class="token punctuation">.</span>nan<span class="token punctuation">,</span> inplace<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>    <span class="token comment"># Replace ? with np.nan</span>    <span class="token keyword">for</span> col <span class="token keyword">in</span> object_features<span class="token punctuation">:</span>        X_train<span class="token punctuation">.</span>loc<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> col<span class="token punctuation">]</span> <span class="token operator">=</span> X_train<span class="token punctuation">.</span>loc<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> col<span class="token punctuation">]</span><span class="token punctuation">.</span>replace<span class="token punctuation">(</span>to_replace<span class="token operator">=</span><span class="token string">'?'</span><span class="token punctuation">,</span>value<span class="token operator">=</span>np<span class="token punctuation">.</span>nan<span class="token punctuation">)</span>        X_train<span class="token punctuation">.</span>loc<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> col<span class="token punctuation">]</span> <span class="token operator">=</span> X_train<span class="token punctuation">.</span>loc<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> col<span class="token punctuation">]</span><span class="token punctuation">.</span>replace<span class="token punctuation">(</span>to_replace<span class="token operator">=</span><span class="token string">'""'</span><span class="token punctuation">,</span>value<span class="token operator">=</span>np<span class="token punctuation">.</span>nan<span class="token punctuation">)</span>    <span class="token comment"># Convert int to float</span>    <span class="token keyword">for</span> col <span class="token keyword">in</span> int_features<span class="token punctuation">:</span>        X_train<span class="token punctuation">[</span>col<span class="token punctuation">]</span> <span class="token operator">=</span> X_train<span class="token punctuation">[</span>col<span class="token punctuation">]</span><span class="token punctuation">.</span>astype<span class="token punctuation">(</span><span class="token string">'float64'</span><span class="token punctuation">)</span>    X_train <span class="token operator">=</span> X_train<span class="token punctuation">.</span>fillna<span class="token punctuation">(</span>method<span class="token operator">=</span><span class="token string">'ffill'</span><span class="token punctuation">)</span>    full_transform <span class="token operator">=</span> ColumnTransformer<span class="token punctuation">(</span><span class="token punctuation">[</span>        <span class="token punctuation">(</span><span class="token string">"num"</span><span class="token punctuation">,</span> StandardScaler<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> int_features<span class="token operator">+</span>float_features<span class="token punctuation">)</span><span class="token punctuation">,</span>        <span class="token punctuation">(</span><span class="token string">"cat"</span><span class="token punctuation">,</span> OneHotEncoder<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> object_features<span class="token punctuation">)</span>    <span class="token punctuation">]</span><span class="token punctuation">)</span>    X_train <span class="token operator">=</span> full_transform<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>X_train<span class="token punctuation">)</span>    <span class="token comment"># ========================================</span>    <span class="token comment">#               Validation</span>    <span class="token comment"># ========================================</span>    X_valid<span class="token punctuation">.</span>fillna<span class="token punctuation">(</span>value<span class="token operator">=</span>np<span class="token punctuation">.</span>nan<span class="token punctuation">,</span> inplace<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>    <span class="token comment"># Replace ? with np.nan</span>    <span class="token keyword">for</span> col <span class="token keyword">in</span> object_features<span class="token punctuation">:</span>        X_valid<span class="token punctuation">.</span>loc<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> col<span class="token punctuation">]</span> <span class="token operator">=</span> X_valid<span class="token punctuation">.</span>loc<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> col<span class="token punctuation">]</span><span class="token punctuation">.</span>replace<span class="token punctuation">(</span>to_replace<span class="token operator">=</span><span class="token string">'?'</span><span class="token punctuation">,</span>value<span class="token operator">=</span>np<span class="token punctuation">.</span>nan<span class="token punctuation">)</span>    <span class="token comment"># Convert int to float</span>    <span class="token keyword">for</span> col <span class="token keyword">in</span> int_features<span class="token punctuation">:</span>        X_valid<span class="token punctuation">[</span>col<span class="token punctuation">]</span> <span class="token operator">=</span> X_valid<span class="token punctuation">[</span>col<span class="token punctuation">]</span><span class="token punctuation">.</span>astype<span class="token punctuation">(</span><span class="token string">'float64'</span><span class="token punctuation">)</span>    X_valid <span class="token operator">=</span> X_valid<span class="token punctuation">.</span>fillna<span class="token punctuation">(</span>method<span class="token operator">=</span><span class="token string">'ffill'</span><span class="token punctuation">)</span>    X_valid <span class="token operator">=</span> full_transform<span class="token punctuation">.</span>transform<span class="token punctuation">(</span>X_valid<span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Size of training feature: </span><span class="token interpolation"><span class="token punctuation">&#123;</span>X_train<span class="token punctuation">.</span>shape<span class="token punctuation">&#125;</span></span><span class="token string">\nSize of validation feature: </span><span class="token interpolation"><span class="token punctuation">&#123;</span>X_valid<span class="token punctuation">.</span>shape<span class="token punctuation">&#125;</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Size of training label: </span><span class="token interpolation"><span class="token punctuation">&#123;</span>y_train<span class="token punctuation">.</span>shape<span class="token punctuation">&#125;</span></span><span class="token string">\nSize of validation label: </span><span class="token interpolation"><span class="token punctuation">&#123;</span>y_valid<span class="token punctuation">.</span>shape<span class="token punctuation">&#125;</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>    <span class="token keyword">return</span> X_train<span class="token punctuation">,</span> X_valid<span class="token punctuation">,</span> y_train<span class="token punctuation">,</span> y_valid<span class="token punctuation">,</span> full_transform    <span class="token keyword">def</span> <span class="token function">preprocessing_v2</span><span class="token punctuation">(</span>dataframe<span class="token punctuation">,</span> use_upsampling<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> use_downsampling<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> use_blended<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment"># Split into training and validation</span>    data <span class="token operator">=</span> dataframe<span class="token punctuation">.</span>copy<span class="token punctuation">(</span><span class="token punctuation">)</span>    X_train<span class="token punctuation">,</span> X_valid<span class="token punctuation">,</span> y_train<span class="token punctuation">,</span> y_valid <span class="token operator">=</span> train_test_split<span class="token punctuation">(</span>        data<span class="token punctuation">.</span>drop<span class="token punctuation">(</span><span class="token string">"Claim_Amount"</span><span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>         data<span class="token punctuation">.</span>Claim_Amount<span class="token punctuation">,</span>         test_size<span class="token operator">=</span><span class="token number">0.15</span><span class="token punctuation">,</span>         random_state<span class="token operator">=</span><span class="token number">42</span><span class="token punctuation">,</span>         stratify<span class="token operator">=</span><span class="token punctuation">(</span>data<span class="token punctuation">.</span>Claim_Amount<span class="token operator">!=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>astype<span class="token punctuation">(</span><span class="token builtin">int</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">if</span> use_upsampling <span class="token operator">==</span> <span class="token boolean">True</span><span class="token punctuation">:</span> X_train<span class="token punctuation">,</span> y_train <span class="token operator">=</span> upsampling<span class="token punctuation">(</span>X_train<span class="token punctuation">,</span> y_train<span class="token punctuation">)</span>    <span class="token keyword">if</span> use_downsampling <span class="token operator">==</span> <span class="token boolean">True</span><span class="token punctuation">:</span> X_train<span class="token punctuation">,</span> y_train <span class="token operator">=</span> downsampling<span class="token punctuation">(</span>X_train<span class="token punctuation">,</span> y_train<span class="token punctuation">)</span>    <span class="token comment"># Define different datatype</span>    mean_encoder_feature <span class="token operator">=</span> <span class="token punctuation">[</span>        <span class="token string">'Blind_Make'</span><span class="token punctuation">,</span> <span class="token string">'Blind_Model'</span><span class="token punctuation">,</span> <span class="token string">'Blind_Submodel'</span><span class="token punctuation">]</span>    num_feature <span class="token operator">=</span> <span class="token punctuation">[</span>        <span class="token string">'Vehicle'</span><span class="token punctuation">,</span> <span class="token string">'Calendar_Year'</span><span class="token punctuation">,</span> <span class="token string">'Model_Year'</span><span class="token punctuation">,</span> <span class="token string">'Var1'</span><span class="token punctuation">,</span> <span class="token string">'Var2'</span><span class="token punctuation">,</span> <span class="token string">'Var3'</span><span class="token punctuation">,</span>         <span class="token string">'Var4'</span><span class="token punctuation">,</span> <span class="token string">'Var5'</span><span class="token punctuation">,</span> <span class="token string">'Var6'</span><span class="token punctuation">,</span> <span class="token string">'Var7'</span><span class="token punctuation">,</span> <span class="token string">'Var8'</span><span class="token punctuation">,</span> <span class="token string">'NVVar1'</span><span class="token punctuation">,</span> <span class="token string">'NVVar2'</span><span class="token punctuation">,</span> <span class="token string">'NVVar3'</span><span class="token punctuation">,</span> <span class="token string">'NVVar4'</span><span class="token punctuation">]</span>    onehot_feature <span class="token operator">=</span> <span class="token punctuation">[</span>        <span class="token string">'Cat1'</span><span class="token punctuation">,</span> <span class="token string">'Cat3'</span><span class="token punctuation">,</span> <span class="token string">'Cat6'</span><span class="token punctuation">,</span> <span class="token string">'Cat8'</span><span class="token punctuation">,</span> <span class="token string">'Cat9'</span><span class="token punctuation">,</span> <span class="token string">'Cat10'</span><span class="token punctuation">,</span> <span class="token string">'Cat11'</span><span class="token punctuation">,</span> <span class="token string">'Cat12'</span><span class="token punctuation">,</span> <span class="token string">'OrdCat'</span><span class="token punctuation">,</span> <span class="token string">'NVCat'</span><span class="token punctuation">]</span>    <span class="token comment"># ========================================</span>    <span class="token comment">#               Training</span>    <span class="token comment"># ========================================</span>    X_train <span class="token operator">=</span> X_train<span class="token punctuation">.</span>drop<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token string">'Row_ID'</span><span class="token punctuation">,</span> <span class="token string">'Household_ID'</span><span class="token punctuation">,</span> <span class="token string">"Cat2"</span><span class="token punctuation">,</span> <span class="token string">"Cat4"</span><span class="token punctuation">,</span> <span class="token string">"Cat5"</span><span class="token punctuation">,</span> <span class="token string">"Cat7"</span><span class="token punctuation">]</span><span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>    X_train<span class="token punctuation">.</span>fillna<span class="token punctuation">(</span>value<span class="token operator">=</span>np<span class="token punctuation">.</span>nan<span class="token punctuation">,</span> inplace<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>    <span class="token comment"># Replace ? and "" with np.nan</span>    <span class="token keyword">for</span> col <span class="token keyword">in</span> onehot_feature<span class="token punctuation">:</span>        X_train<span class="token punctuation">.</span>loc<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> col<span class="token punctuation">]</span> <span class="token operator">=</span> X_train<span class="token punctuation">.</span>loc<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> col<span class="token punctuation">]</span><span class="token punctuation">.</span>replace<span class="token punctuation">(</span>to_replace<span class="token operator">=</span><span class="token string">'?'</span><span class="token punctuation">,</span>value<span class="token operator">=</span>np<span class="token punctuation">.</span>nan<span class="token punctuation">)</span>        X_train<span class="token punctuation">.</span>loc<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> col<span class="token punctuation">]</span> <span class="token operator">=</span> X_train<span class="token punctuation">.</span>loc<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> col<span class="token punctuation">]</span><span class="token punctuation">.</span>replace<span class="token punctuation">(</span>to_replace<span class="token operator">=</span><span class="token string">'""'</span><span class="token punctuation">,</span>value<span class="token operator">=</span>np<span class="token punctuation">.</span>nan<span class="token punctuation">)</span>    <span class="token comment"># Convert int to float</span>    <span class="token keyword">for</span> col <span class="token keyword">in</span> num_feature<span class="token punctuation">:</span>        X_train<span class="token punctuation">[</span>col<span class="token punctuation">]</span> <span class="token operator">=</span> X_train<span class="token punctuation">[</span>col<span class="token punctuation">]</span><span class="token punctuation">.</span>astype<span class="token punctuation">(</span><span class="token string">'float64'</span><span class="token punctuation">)</span>    X_train <span class="token operator">=</span> X_train<span class="token punctuation">.</span>fillna<span class="token punctuation">(</span>method<span class="token operator">=</span><span class="token string">'ffill'</span><span class="token punctuation">)</span>    full_transform <span class="token operator">=</span> ColumnTransformer<span class="token punctuation">(</span><span class="token punctuation">[</span>        <span class="token punctuation">(</span><span class="token string">"num"</span><span class="token punctuation">,</span> MinMaxScaler<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> num_feature<span class="token punctuation">)</span><span class="token punctuation">,</span>        <span class="token punctuation">(</span><span class="token string">"cat"</span><span class="token punctuation">,</span> OneHotEncoder<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> onehot_feature<span class="token punctuation">)</span><span class="token punctuation">,</span>         <span class="token punctuation">(</span><span class="token string">"mean"</span><span class="token punctuation">,</span> MeanEncoder<span class="token punctuation">(</span>cat_features<span class="token operator">=</span>mean_encoder_feature<span class="token punctuation">,</span> target_type<span class="token operator">=</span><span class="token string">"regression"</span><span class="token punctuation">)</span><span class="token punctuation">,</span> mean_encoder_feature<span class="token punctuation">)</span>    <span class="token punctuation">]</span><span class="token punctuation">)</span>    X_train_final <span class="token operator">=</span> full_transform<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>X_train<span class="token punctuation">,</span> y_train<span class="token punctuation">)</span>    <span class="token comment"># ========================================</span>    <span class="token comment">#               Validation</span>    <span class="token comment"># ========================================</span>    X_valid <span class="token operator">=</span> X_valid<span class="token punctuation">.</span>drop<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token string">'Row_ID'</span><span class="token punctuation">,</span> <span class="token string">'Household_ID'</span><span class="token punctuation">,</span> <span class="token string">"Cat2"</span><span class="token punctuation">,</span> <span class="token string">"Cat4"</span><span class="token punctuation">,</span> <span class="token string">"Cat5"</span><span class="token punctuation">,</span> <span class="token string">"Cat7"</span><span class="token punctuation">]</span><span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>    X_valid<span class="token punctuation">.</span>fillna<span class="token punctuation">(</span>value<span class="token operator">=</span>np<span class="token punctuation">.</span>nan<span class="token punctuation">,</span> inplace<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>    <span class="token comment"># Replace ? and "" with np.nan</span>    <span class="token keyword">for</span> col <span class="token keyword">in</span> onehot_feature<span class="token punctuation">:</span>        X_valid<span class="token punctuation">.</span>loc<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> col<span class="token punctuation">]</span> <span class="token operator">=</span> X_valid<span class="token punctuation">.</span>loc<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> col<span class="token punctuation">]</span><span class="token punctuation">.</span>replace<span class="token punctuation">(</span>to_replace<span class="token operator">=</span><span class="token string">'?'</span><span class="token punctuation">,</span>value<span class="token operator">=</span>np<span class="token punctuation">.</span>nan<span class="token punctuation">)</span>        X_valid<span class="token punctuation">.</span>loc<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> col<span class="token punctuation">]</span> <span class="token operator">=</span> X_valid<span class="token punctuation">.</span>loc<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> col<span class="token punctuation">]</span><span class="token punctuation">.</span>replace<span class="token punctuation">(</span>to_replace<span class="token operator">=</span><span class="token string">'""'</span><span class="token punctuation">,</span>value<span class="token operator">=</span>np<span class="token punctuation">.</span>nan<span class="token punctuation">)</span>    <span class="token comment"># Convert int to float</span>    <span class="token keyword">for</span> col <span class="token keyword">in</span> num_feature<span class="token punctuation">:</span>        X_valid<span class="token punctuation">[</span>col<span class="token punctuation">]</span> <span class="token operator">=</span> X_valid<span class="token punctuation">[</span>col<span class="token punctuation">]</span><span class="token punctuation">.</span>astype<span class="token punctuation">(</span><span class="token string">'float64'</span><span class="token punctuation">)</span>    X_valid <span class="token operator">=</span> X_valid<span class="token punctuation">.</span>fillna<span class="token punctuation">(</span>method<span class="token operator">=</span><span class="token string">'ffill'</span><span class="token punctuation">)</span>    X_valid_final <span class="token operator">=</span> full_transform<span class="token punctuation">.</span>transform<span class="token punctuation">(</span>X_valid<span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Size of training data: </span><span class="token interpolation"><span class="token punctuation">&#123;</span>X_train_final<span class="token punctuation">.</span>shape<span class="token punctuation">&#125;</span></span><span class="token string">\nSize of validation data: </span><span class="token interpolation"><span class="token punctuation">&#123;</span>X_valid_final<span class="token punctuation">.</span>shape<span class="token punctuation">&#125;</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Size of training label: </span><span class="token interpolation"><span class="token punctuation">&#123;</span>y_train<span class="token punctuation">.</span>shape<span class="token punctuation">&#125;</span></span><span class="token string">\nSize of validation label: </span><span class="token interpolation"><span class="token punctuation">&#123;</span>y_valid<span class="token punctuation">.</span>shape<span class="token punctuation">&#125;</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>    <span class="token keyword">return</span> X_train_final<span class="token punctuation">,</span> X_valid_final<span class="token punctuation">,</span> y_train<span class="token punctuation">,</span> y_valid<span class="token punctuation">,</span> full_transform<span class="token keyword">def</span> <span class="token function">preprocessing_v3</span><span class="token punctuation">(</span>dataframe<span class="token punctuation">,</span> use_upsampling<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> use_downsampling<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> use_blended<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment"># Define different datatype</span>    label <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">"Claim_Amount"</span><span class="token punctuation">]</span>    mean_encoder_feature <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'Blind_Make'</span><span class="token punctuation">,</span> <span class="token string">'Blind_Model'</span><span class="token punctuation">]</span>    num_feature <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'Var1'</span><span class="token punctuation">,</span> <span class="token string">'Var6'</span><span class="token punctuation">,</span> <span class="token string">'NVVar1'</span><span class="token punctuation">,</span> <span class="token string">'NVVar2'</span><span class="token punctuation">,</span> <span class="token string">'NVVar3'</span><span class="token punctuation">,</span> <span class="token string">'NVVar4'</span><span class="token punctuation">]</span>    onehot_feature <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'Cat1'</span><span class="token punctuation">,</span> <span class="token string">'Cat3'</span><span class="token punctuation">,</span> <span class="token string">'Cat6'</span><span class="token punctuation">,</span> <span class="token string">'Cat9'</span><span class="token punctuation">,</span> <span class="token string">'OrdCat'</span><span class="token punctuation">,</span> <span class="token string">'NVCat'</span><span class="token punctuation">]</span>    <span class="token comment"># Split into training and validation</span>    data <span class="token operator">=</span> dataframe<span class="token punctuation">.</span>copy<span class="token punctuation">(</span><span class="token punctuation">)</span>    data <span class="token operator">=</span> data<span class="token punctuation">[</span>mean_encoder_feature<span class="token operator">+</span>num_feature<span class="token operator">+</span>onehot_feature<span class="token operator">+</span>label<span class="token punctuation">]</span>    X_train<span class="token punctuation">,</span> X_valid<span class="token punctuation">,</span> y_train<span class="token punctuation">,</span> y_valid <span class="token operator">=</span> train_test_split<span class="token punctuation">(</span>        data<span class="token punctuation">.</span>drop<span class="token punctuation">(</span><span class="token string">"Claim_Amount"</span><span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>         data<span class="token punctuation">.</span>Claim_Amount<span class="token punctuation">,</span>         test_size<span class="token operator">=</span><span class="token number">0.15</span><span class="token punctuation">,</span>         random_state<span class="token operator">=</span><span class="token number">42</span><span class="token punctuation">,</span>         stratify<span class="token operator">=</span><span class="token punctuation">(</span>data<span class="token punctuation">.</span>Claim_Amount<span class="token operator">!=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>astype<span class="token punctuation">(</span><span class="token builtin">int</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">if</span> use_upsampling <span class="token operator">==</span> <span class="token boolean">True</span><span class="token punctuation">:</span> X_train<span class="token punctuation">,</span> y_train <span class="token operator">=</span> upsampling<span class="token punctuation">(</span>X_train<span class="token punctuation">,</span> y_train<span class="token punctuation">)</span>    <span class="token keyword">if</span> use_downsampling <span class="token operator">==</span> <span class="token boolean">True</span><span class="token punctuation">:</span> X_train<span class="token punctuation">,</span> y_train <span class="token operator">=</span> downsampling<span class="token punctuation">(</span>X_train<span class="token punctuation">,</span> y_train<span class="token punctuation">)</span>    <span class="token comment"># ========================================</span>    <span class="token comment">#               Training</span>    <span class="token comment"># ========================================</span>    X_train<span class="token punctuation">.</span>fillna<span class="token punctuation">(</span>value<span class="token operator">=</span>np<span class="token punctuation">.</span>nan<span class="token punctuation">,</span> inplace<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>    <span class="token comment"># Replace ? and "" with np.nan</span>    <span class="token keyword">for</span> col <span class="token keyword">in</span> onehot_feature<span class="token punctuation">:</span>        X_train<span class="token punctuation">.</span>loc<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> col<span class="token punctuation">]</span> <span class="token operator">=</span> X_train<span class="token punctuation">.</span>loc<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> col<span class="token punctuation">]</span><span class="token punctuation">.</span>replace<span class="token punctuation">(</span>to_replace<span class="token operator">=</span><span class="token string">'?'</span><span class="token punctuation">,</span>value<span class="token operator">=</span>np<span class="token punctuation">.</span>nan<span class="token punctuation">)</span>        X_train<span class="token punctuation">.</span>loc<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> col<span class="token punctuation">]</span> <span class="token operator">=</span> X_train<span class="token punctuation">.</span>loc<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> col<span class="token punctuation">]</span><span class="token punctuation">.</span>replace<span class="token punctuation">(</span>to_replace<span class="token operator">=</span><span class="token string">'""'</span><span class="token punctuation">,</span>value<span class="token operator">=</span>np<span class="token punctuation">.</span>nan<span class="token punctuation">)</span>    <span class="token comment"># Convert int to float</span>    <span class="token keyword">for</span> col <span class="token keyword">in</span> num_feature<span class="token punctuation">:</span>        X_train<span class="token punctuation">[</span>col<span class="token punctuation">]</span> <span class="token operator">=</span> X_train<span class="token punctuation">[</span>col<span class="token punctuation">]</span><span class="token punctuation">.</span>astype<span class="token punctuation">(</span><span class="token string">'float64'</span><span class="token punctuation">)</span>    X_train <span class="token operator">=</span> X_train<span class="token punctuation">.</span>fillna<span class="token punctuation">(</span>method<span class="token operator">=</span><span class="token string">'ffill'</span><span class="token punctuation">)</span>    full_transform <span class="token operator">=</span> ColumnTransformer<span class="token punctuation">(</span><span class="token punctuation">[</span>        <span class="token punctuation">(</span><span class="token string">"num"</span><span class="token punctuation">,</span> MinMaxScaler<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> num_feature<span class="token punctuation">)</span><span class="token punctuation">,</span>        <span class="token punctuation">(</span><span class="token string">"cat"</span><span class="token punctuation">,</span> OneHotEncoder<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> onehot_feature<span class="token punctuation">)</span><span class="token punctuation">,</span>         <span class="token punctuation">(</span><span class="token string">"mean"</span><span class="token punctuation">,</span> MeanEncoder<span class="token punctuation">(</span>cat_features<span class="token operator">=</span>mean_encoder_feature<span class="token punctuation">,</span> target_type<span class="token operator">=</span><span class="token string">"regression"</span><span class="token punctuation">)</span><span class="token punctuation">,</span> mean_encoder_feature<span class="token punctuation">)</span>    <span class="token punctuation">]</span><span class="token punctuation">)</span>    X_train_final <span class="token operator">=</span> full_transform<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>X_train<span class="token punctuation">,</span> y_train<span class="token punctuation">)</span>    <span class="token comment"># ========================================</span>    <span class="token comment">#               Validation</span>    <span class="token comment"># ========================================</span>    X_valid<span class="token punctuation">.</span>fillna<span class="token punctuation">(</span>value<span class="token operator">=</span>np<span class="token punctuation">.</span>nan<span class="token punctuation">,</span> inplace<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>    <span class="token comment"># Replace ? and "" with np.nan</span>    <span class="token keyword">for</span> col <span class="token keyword">in</span> onehot_feature<span class="token punctuation">:</span>        X_valid<span class="token punctuation">.</span>loc<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> col<span class="token punctuation">]</span> <span class="token operator">=</span> X_valid<span class="token punctuation">.</span>loc<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> col<span class="token punctuation">]</span><span class="token punctuation">.</span>replace<span class="token punctuation">(</span>to_replace<span class="token operator">=</span><span class="token string">'?'</span><span class="token punctuation">,</span>value<span class="token operator">=</span>np<span class="token punctuation">.</span>nan<span class="token punctuation">)</span>        X_valid<span class="token punctuation">.</span>loc<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> col<span class="token punctuation">]</span> <span class="token operator">=</span> X_valid<span class="token punctuation">.</span>loc<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> col<span class="token punctuation">]</span><span class="token punctuation">.</span>replace<span class="token punctuation">(</span>to_replace<span class="token operator">=</span><span class="token string">'""'</span><span class="token punctuation">,</span>value<span class="token operator">=</span>np<span class="token punctuation">.</span>nan<span class="token punctuation">)</span>    <span class="token comment"># Convert int to float</span>    <span class="token keyword">for</span> col <span class="token keyword">in</span> num_feature<span class="token punctuation">:</span>        X_valid<span class="token punctuation">[</span>col<span class="token punctuation">]</span> <span class="token operator">=</span> X_valid<span class="token punctuation">[</span>col<span class="token punctuation">]</span><span class="token punctuation">.</span>astype<span class="token punctuation">(</span><span class="token string">'float64'</span><span class="token punctuation">)</span>    X_valid <span class="token operator">=</span> X_valid<span class="token punctuation">.</span>fillna<span class="token punctuation">(</span>method<span class="token operator">=</span><span class="token string">'ffill'</span><span class="token punctuation">)</span>    X_valid_final <span class="token operator">=</span> full_transform<span class="token punctuation">.</span>transform<span class="token punctuation">(</span>X_valid<span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Size of training data: </span><span class="token interpolation"><span class="token punctuation">&#123;</span>X_train_final<span class="token punctuation">.</span>shape<span class="token punctuation">&#125;</span></span><span class="token string">\nSize of validation data: </span><span class="token interpolation"><span class="token punctuation">&#123;</span>X_valid_final<span class="token punctuation">.</span>shape<span class="token punctuation">&#125;</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Size of training label: </span><span class="token interpolation"><span class="token punctuation">&#123;</span>y_train<span class="token punctuation">.</span>shape<span class="token punctuation">&#125;</span></span><span class="token string">\nSize of validation label: </span><span class="token interpolation"><span class="token punctuation">&#123;</span>y_valid<span class="token punctuation">.</span>shape<span class="token punctuation">&#125;</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>    <span class="token keyword">return</span> X_train_final<span class="token punctuation">,</span> X_valid_final<span class="token punctuation">,</span> y_train<span class="token punctuation">,</span> y_valid<span class="token punctuation">,</span> full_transform<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="Modelling"><a href="#Modelling" class="headerlink" title="Modelling"></a>Modelling</h1><h2 id="Tandem-Model"><a href="#Tandem-Model" class="headerlink" title="Tandem Model"></a>Tandem Model</h2><p>Tandem is a two-stage regression method that can be used when various input data types are correlated, for example gene expression and methylation in drug response prediction. In the first stage it uses the upstream features (such as methylation) to predict the response variable (such as drug response), and in the second stage it uses the downstream features (such as gene expression) to predict the residuals of the first stage.</p><img src="/Hexo-Blog/2020/11/27/2020-11-27-exploratory-data-analysis-for-predicting-insurance-claim/tandem.png" class=""><h2 id="Pipeline"><a href="#Pipeline" class="headerlink" title="Pipeline"></a>Pipeline</h2><img src="/Hexo-Blog/2020/11/27/2020-11-27-exploratory-data-analysis-for-predicting-insurance-claim/pipeline.png" class=""><h2 id="Performance-using-Single-Model"><a href="#Performance-using-Single-Model" class="headerlink" title="Performance using Single Model"></a>Performance using Single Model</h2><p>You can see the problem as a regression problem where the variable to predict is continuous (the claimed amount in USD). The performance of the regression model will depend on the quality of the training data. I’ll compare the performance of the following models:</p><ol><li>Linear regression</li><li>Ridge regression</li><li>Random forests for regression</li><li>Gradient tree boosting for regression</li></ol><p>For each model, I’ll use grid search with at least three options for each parameter and report the performance measure over a validation set.</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>metrics <span class="token keyword">import</span> mean_squared_error<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>decomposition <span class="token keyword">import</span> TruncatedSVD<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>linear_model <span class="token keyword">import</span> LinearRegression<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>linear_model <span class="token keyword">import</span> Ridge<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>ensemble <span class="token keyword">import</span> RandomForestRegressor<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>ensemble <span class="token keyword">import</span> GradientBoostingRegressor<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>model_selection <span class="token keyword">import</span> train_test_split<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>model_selection <span class="token keyword">import</span> GridSearchCV<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>model_selection <span class="token keyword">import</span> learning_curve<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>Let’s split our data into training and validation dataset using <code>preprocessing_v2()</code> we built above.</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">X_train<span class="token punctuation">,</span> X_valid<span class="token punctuation">,</span> y_train<span class="token punctuation">,</span> y_valid<span class="token punctuation">,</span> _ <span class="token operator">=</span> preprocessing_v2<span class="token punctuation">(</span>data<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="Main-Code"><a href="#Main-Code" class="headerlink" title="Main Code"></a>Main Code</h3><h4 id="Linear-Regression"><a href="#Linear-Regression" class="headerlink" title="Linear Regression"></a>Linear Regression</h4><p>RMSE: 274.5014</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token triple-quoted-string string">"""Best params: Origin: &#123;'copy_X': True, 'fit_intercept': False, 'normalize': True&#125;Upsampling: &#123;'copy_X': True, 'fit_intercept': True, 'normalize': False&#125;Downsampling: &#123;'copy_X': True, 'fit_intercept': False, 'normalize': True&#125;"""</span>USE_GRID <span class="token operator">=</span> <span class="token boolean">False</span><span class="token keyword">if</span> USE_GRID<span class="token punctuation">:</span>    param_grid <span class="token operator">=</span> <span class="token punctuation">&#123;</span>        <span class="token string">'normalize'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token boolean">True</span><span class="token punctuation">,</span> <span class="token boolean">False</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         <span class="token string">'fit_intercept'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token boolean">True</span><span class="token punctuation">,</span> <span class="token boolean">False</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         <span class="token string">'copy_X'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token boolean">True</span><span class="token punctuation">,</span> <span class="token boolean">False</span><span class="token punctuation">]</span>    <span class="token punctuation">&#125;</span>    grid <span class="token operator">=</span> GridSearchCV<span class="token punctuation">(</span>LinearRegression<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> cv<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> param_grid<span class="token operator">=</span>param_grid<span class="token punctuation">,</span> n_jobs<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">)</span>    grid<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>X_train<span class="token punctuation">,</span> y_train<span class="token punctuation">)</span>    lr <span class="token operator">=</span> grid<span class="token punctuation">.</span>best_estimator_    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Best params: </span><span class="token interpolation"><span class="token punctuation">&#123;</span>grid<span class="token punctuation">.</span>best_params_<span class="token punctuation">&#125;</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>    y_pred <span class="token operator">=</span> lr<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>X_valid<span class="token punctuation">)</span>    rmse <span class="token operator">=</span> np<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>mean_squared_error<span class="token punctuation">(</span>y_valid<span class="token punctuation">,</span> y_pred<span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"RMSE: </span><span class="token interpolation"><span class="token punctuation">&#123;</span><span class="token builtin">round</span><span class="token punctuation">(</span>rmse<span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">&#125;</span></span><span class="token string">"</span></span><span class="token punctuation">)</span><span class="token keyword">else</span><span class="token punctuation">:</span>     best_params <span class="token operator">=</span> <span class="token punctuation">&#123;</span>        <span class="token string">'copy_X'</span><span class="token punctuation">:</span> <span class="token boolean">True</span><span class="token punctuation">,</span> <span class="token string">'fit_intercept'</span><span class="token punctuation">:</span> <span class="token boolean">False</span><span class="token punctuation">,</span> <span class="token string">'normalize'</span><span class="token punctuation">:</span> <span class="token boolean">True</span>    <span class="token punctuation">&#125;</span>    lr <span class="token operator">=</span> LinearRegression<span class="token punctuation">(</span><span class="token operator">**</span>best_params<span class="token punctuation">)</span>    lr<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>X_train<span class="token punctuation">,</span> y_train<span class="token punctuation">)</span>    y_pred <span class="token operator">=</span> lr<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>X_valid<span class="token punctuation">)</span>    rmse <span class="token operator">=</span> np<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>mean_squared_error<span class="token punctuation">(</span>y_valid<span class="token punctuation">,</span> y_pred<span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"RMSE: </span><span class="token interpolation"><span class="token punctuation">&#123;</span><span class="token builtin">round</span><span class="token punctuation">(</span>rmse<span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">&#125;</span></span><span class="token string">"</span></span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="Ridge-Regression"><a href="#Ridge-Regression" class="headerlink" title="Ridge Regression"></a>Ridge Regression</h4><p>RMSE: 274.3249</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token triple-quoted-string string">"""Best params: Origin: &#123;'alpha': 1.0, 'fit_intercept': True, 'normalize': True&#125;Upsampling: &#123;'alpha': 1.0, 'fit_intercept': True, 'normalize': False&#125;Downsampling: &#123;'alpha': 1.0, 'fit_intercept': True, 'normalize': False&#125;"""</span>USE_GRID <span class="token operator">=</span> <span class="token boolean">False</span><span class="token keyword">if</span> USE_GRID<span class="token punctuation">:</span>    param_grid <span class="token operator">=</span> <span class="token punctuation">&#123;</span>        <span class="token string">'alpha'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token builtin">float</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span> <span class="token keyword">for</span> x <span class="token keyword">in</span> np<span class="token punctuation">.</span>linspace<span class="token punctuation">(</span><span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token number">1.0</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         <span class="token string">'fit_intercept'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token boolean">True</span><span class="token punctuation">,</span> <span class="token boolean">False</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         <span class="token string">'normalize'</span><span class="token punctuation">:</span><span class="token punctuation">[</span><span class="token boolean">True</span><span class="token punctuation">,</span> <span class="token boolean">False</span><span class="token punctuation">]</span>    <span class="token punctuation">&#125;</span>    grid <span class="token operator">=</span> GridSearchCV<span class="token punctuation">(</span>Ridge<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> cv<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> param_grid<span class="token operator">=</span>param_grid<span class="token punctuation">,</span> n_jobs<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">)</span>    grid<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>X_train<span class="token punctuation">,</span> y_train<span class="token punctuation">)</span>    rr <span class="token operator">=</span> grid<span class="token punctuation">.</span>best_estimator_    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Best params: </span><span class="token interpolation"><span class="token punctuation">&#123;</span>grid<span class="token punctuation">.</span>best_params_<span class="token punctuation">&#125;</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>    y_pred <span class="token operator">=</span> rr<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>X_valid<span class="token punctuation">)</span>    rmse <span class="token operator">=</span> np<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>mean_squared_error<span class="token punctuation">(</span>y_valid<span class="token punctuation">,</span> y_pred<span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"RMSE: </span><span class="token interpolation"><span class="token punctuation">&#123;</span><span class="token builtin">round</span><span class="token punctuation">(</span>rmse<span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">&#125;</span></span><span class="token string">"</span></span><span class="token punctuation">)</span><span class="token keyword">else</span><span class="token punctuation">:</span>     best_params <span class="token operator">=</span> <span class="token punctuation">&#123;</span>        <span class="token string">'alpha'</span><span class="token punctuation">:</span> <span class="token number">1.0</span><span class="token punctuation">,</span> <span class="token string">'fit_intercept'</span><span class="token punctuation">:</span> <span class="token boolean">True</span><span class="token punctuation">,</span> <span class="token string">'normalize'</span><span class="token punctuation">:</span> <span class="token boolean">True</span>    <span class="token punctuation">&#125;</span>    rr <span class="token operator">=</span> Ridge<span class="token punctuation">(</span><span class="token operator">**</span>best_params<span class="token punctuation">)</span>    rr<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>X_train<span class="token punctuation">,</span> y_train<span class="token punctuation">)</span>    y_pred <span class="token operator">=</span> rr<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>X_valid<span class="token punctuation">)</span>    rmse <span class="token operator">=</span> np<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>mean_squared_error<span class="token punctuation">(</span>y_valid<span class="token punctuation">,</span> y_pred<span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"RMSE: </span><span class="token interpolation"><span class="token punctuation">&#123;</span><span class="token builtin">round</span><span class="token punctuation">(</span>rmse<span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">&#125;</span></span><span class="token string">"</span></span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">pretty_print_coefficients</span><span class="token punctuation">(</span>coefficients<span class="token punctuation">,</span> names<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> sort<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">if</span> names <span class="token operator">==</span> <span class="token boolean">None</span><span class="token punctuation">:</span>        names <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">"X&#123;&#125;"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span> <span class="token keyword">for</span> x <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>coefficients<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">]</span>    lst <span class="token operator">=</span> <span class="token builtin">zip</span><span class="token punctuation">(</span>coefficients<span class="token punctuation">,</span> names<span class="token punctuation">)</span>    <span class="token keyword">if</span> sort<span class="token punctuation">:</span>        lst <span class="token operator">=</span> <span class="token builtin">sorted</span><span class="token punctuation">(</span>lst<span class="token punctuation">,</span> key<span class="token operator">=</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> <span class="token operator">-</span>np<span class="token punctuation">.</span><span class="token builtin">abs</span><span class="token punctuation">(</span>x<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> <span class="token string">" + "</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span><span class="token string">"%s * %s"</span> <span class="token operator">%</span> <span class="token punctuation">(</span><span class="token builtin">round</span><span class="token punctuation">(</span>coef<span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> name<span class="token punctuation">)</span> <span class="token keyword">for</span> coef<span class="token punctuation">,</span> name <span class="token keyword">in</span> lst<span class="token punctuation">)</span>pretty_print_coefficients<span class="token punctuation">(</span>rr<span class="token punctuation">.</span>coef_<span class="token punctuation">,</span> names<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> sort<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><pre class="line-numbers language-console" data-language="console"><code class="language-console">&#39;14.713 * X0 + -7.194 * X1 + 0.668 * X2 + 6.268 * X3 + 3.181 * X4 + 0.285 * X5 + 6.009 * X6 + 4.608 * X7 + -3.255 * X8 + 1.433 * X9 + -18.1 * X10 + -5.12 * X11 + 5.512 * X12 + -3.907 * X13 + -2.035 * X14 + -1.468 * X15 + 1.483 * X16 + -5.298 * X17 + 0.849 * X18 + -6.152 * X19 + -0.607 * X20 + 1.392 * X21 + 13.419 * X22 + -1.513 * X23 + -3.255 * X24 + 4.119 * X25 + -2.235 * X26 + -6.145 * X27 + -2.228 * X28 + -0.052 * X29 + -2.067 * X30 + -0.584 * X31 + 1.877 * X32 + 0.801 * X33 + -5.243 * X34 + -0.085 * X35 + 1.982 * X36 + -0.864 * X37 + -4.318 * X38 + -0.041 * X39 + 0.041 * X40 + 0.981 * X41 + -0.338 * X42 + -3.315 * X43 + 0.591 * X44 + 0.117 * X45 + 0.963 * X46 + 0.254 * X47 + -3.111 * X48 + -1.329 * X49 + 42.711 * X50 + -0.511 * X51 + 1.54 * X52 + 0.599 * X53 + -3.295 * X54 + -1.784 * X55 + 58.729 * X56 + -0.35 * X57 + -7.959 * X58 + 0.052 * X59 + 0.335 * X60 + -1.077 * X61 + -21.379 * X62 + 6.138 * X63 + -0.224 * X64 + 11.816 * X65 + -1.387 * X66 + 3.474 * X67 + -3.939 * X68 + -10.15 * X69 + -7.045 * X70 + -15.198 * X71 + -1.191 * X72 + -0.275 * X73 + -2.104 * X74 + 0.844 * X75 + 1.142 * X76 + -0.481 * X77 + 0.041 * X78 + -0.005 * X79 + -0.005 * X80&#39;<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h4 id="Random-Forest"><a href="#Random-Forest" class="headerlink" title="Random Forest"></a>Random Forest</h4><p>RMSE: 274.4148</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token triple-quoted-string string">"""Best params: Origin: &#123;'max_depth': 3, 'min_samples_split': 10, 'n_estimators': 100&#125;Upsampling: &#123;'max_depth': 5, 'min_samples_split': 5, 'n_estimators': 100&#125;Downsampling: &#123;'max_depth': 5, 'min_samples_split': 5, 'n_estimators': 200&#125;"""</span>USE_GRID <span class="token operator">=</span> <span class="token boolean">False</span><span class="token keyword">if</span> USE_GRID<span class="token punctuation">:</span>    param_grid <span class="token operator">=</span> <span class="token punctuation">&#123;</span>        <span class="token string">'n_estimators'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token builtin">int</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span> <span class="token keyword">for</span> x <span class="token keyword">in</span> np<span class="token punctuation">.</span>linspace<span class="token punctuation">(</span><span class="token number">100</span><span class="token punctuation">,</span> <span class="token number">300</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         <span class="token string">'max_depth'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token builtin">int</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span> <span class="token keyword">for</span> x <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         <span class="token string">'min_samples_split'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token comment">#     'min_samples_leaf': [1, 2, 4], </span><span class="token comment">#     'bootstrap': [True, False]</span>    <span class="token punctuation">&#125;</span>    grid <span class="token operator">=</span> GridSearchCV<span class="token punctuation">(</span>RandomForestRegressor<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> cv<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> param_grid<span class="token operator">=</span>param_grid<span class="token punctuation">,</span> n_jobs<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">)</span>    grid<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>X_train<span class="token punctuation">,</span> y_train<span class="token punctuation">)</span>    rfr <span class="token operator">=</span> grid<span class="token punctuation">.</span>best_estimator_    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Best params: </span><span class="token interpolation"><span class="token punctuation">&#123;</span>grid<span class="token punctuation">.</span>best_params_<span class="token punctuation">&#125;</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>    y_pred <span class="token operator">=</span> rfr<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>X_valid<span class="token punctuation">)</span>    rmse <span class="token operator">=</span> np<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>mean_squared_error<span class="token punctuation">(</span>y_valid<span class="token punctuation">,</span> y_pred<span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"RMSE: </span><span class="token interpolation"><span class="token punctuation">&#123;</span><span class="token builtin">round</span><span class="token punctuation">(</span>rmse<span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">&#125;</span></span><span class="token string">"</span></span><span class="token punctuation">)</span><span class="token keyword">else</span><span class="token punctuation">:</span>     best_params <span class="token operator">=</span> <span class="token punctuation">&#123;</span>        <span class="token string">'max_depth'</span><span class="token punctuation">:</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token string">'min_samples_split'</span><span class="token punctuation">:</span> <span class="token number">10</span><span class="token punctuation">,</span> <span class="token string">'n_estimators'</span><span class="token punctuation">:</span> <span class="token number">100</span>    <span class="token punctuation">&#125;</span>    rfr <span class="token operator">=</span> RandomForestRegressor<span class="token punctuation">(</span><span class="token operator">**</span>best_params<span class="token punctuation">)</span>    rfr<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>X_train<span class="token punctuation">,</span> y_train<span class="token punctuation">)</span>    y_pred <span class="token operator">=</span> rfr<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>X_valid<span class="token punctuation">)</span>    rmse <span class="token operator">=</span> np<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>mean_squared_error<span class="token punctuation">(</span>y_valid<span class="token punctuation">,</span> y_pred<span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"RMSE: </span><span class="token interpolation"><span class="token punctuation">&#123;</span><span class="token builtin">round</span><span class="token punctuation">(</span>rmse<span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">&#125;</span></span><span class="token string">"</span></span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="Gradient-Tree-Boosting"><a href="#Gradient-Tree-Boosting" class="headerlink" title="Gradient Tree Boosting"></a>Gradient Tree Boosting</h4><p>RMSE: 274.5765</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token triple-quoted-string string">"""Best params: Origin: &#123;'max_depth': 3, 'min_samples_split': 0.5, 'n_estimators': 200&#125;Upsampling: &#123;'max_depth': 5, 'min_samples_split': 0.1, 'n_estimators': 500&#125;Downsampling: &#123;'max_depth': 3, 'min_samples_split': 0.5, 'n_estimators': 200&#125;"""</span>USE_GRID <span class="token operator">=</span> <span class="token boolean">False</span><span class="token keyword">if</span> USE_GRID<span class="token punctuation">:</span>    param_grid <span class="token operator">=</span> <span class="token punctuation">&#123;</span>        <span class="token string">'n_estimators'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token builtin">int</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span> <span class="token keyword">for</span> x <span class="token keyword">in</span> np<span class="token punctuation">.</span>linspace<span class="token punctuation">(</span><span class="token number">200</span><span class="token punctuation">,</span> <span class="token number">400</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         <span class="token string">'max_depth'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token builtin">int</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span> <span class="token keyword">for</span> x <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         <span class="token string">"min_samples_split"</span><span class="token punctuation">:</span> np<span class="token punctuation">.</span>linspace<span class="token punctuation">(</span><span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token number">12</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token comment">#     'learning_rate': [0.1, 0.01, 0.001], </span><span class="token comment">#     'min_samples_leaf': np.linspace(0.1, 0.5, 12), </span><span class="token comment">#     "subsample": [0.5, 0.618, 0.8, 0.85, 0.9, 0.95, 1.0], </span><span class="token comment">#     'max_features': ["log2","sqrt"]</span>    <span class="token punctuation">&#125;</span>    grid <span class="token operator">=</span> GridSearchCV<span class="token punctuation">(</span>GradientBoostingRegressor<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> cv<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> param_grid<span class="token operator">=</span>param_grid<span class="token punctuation">,</span> n_jobs<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">)</span>    grid<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>X_train<span class="token punctuation">,</span> y_train<span class="token punctuation">)</span>    gbr <span class="token operator">=</span> grid<span class="token punctuation">.</span>best_estimator_    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Best params: </span><span class="token interpolation"><span class="token punctuation">&#123;</span>grid<span class="token punctuation">.</span>best_params_<span class="token punctuation">&#125;</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>    y_pred <span class="token operator">=</span> gbr<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>X_valid<span class="token punctuation">)</span>    rmse <span class="token operator">=</span> np<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>mean_squared_error<span class="token punctuation">(</span>y_valid<span class="token punctuation">,</span> y_pred<span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"RMSE: </span><span class="token interpolation"><span class="token punctuation">&#123;</span><span class="token builtin">round</span><span class="token punctuation">(</span>rmse<span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">&#125;</span></span><span class="token string">"</span></span><span class="token punctuation">)</span><span class="token keyword">else</span><span class="token punctuation">:</span>     best_params <span class="token operator">=</span> <span class="token punctuation">&#123;</span>        <span class="token string">'max_depth'</span><span class="token punctuation">:</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token string">'min_samples_split'</span><span class="token punctuation">:</span> <span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token string">'n_estimators'</span><span class="token punctuation">:</span> <span class="token number">200</span>    <span class="token punctuation">&#125;</span>    gbr <span class="token operator">=</span> GradientBoostingRegressor<span class="token punctuation">(</span><span class="token operator">**</span>best_params<span class="token punctuation">)</span>    gbr<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>X_train<span class="token punctuation">,</span> y_train<span class="token punctuation">)</span>    y_pred <span class="token operator">=</span> gbr<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>X_valid<span class="token punctuation">)</span>    rmse <span class="token operator">=</span> np<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>mean_squared_error<span class="token punctuation">(</span>y_valid<span class="token punctuation">,</span> y_pred<span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"RMSE: </span><span class="token interpolation"><span class="token punctuation">&#123;</span><span class="token builtin">round</span><span class="token punctuation">(</span>rmse<span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">&#125;</span></span><span class="token string">"</span></span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="Performance"><a href="#Performance" class="headerlink" title="Performance"></a>Performance</h3><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> IPython<span class="token punctuation">.</span>display <span class="token keyword">import</span> HTML<span class="token punctuation">,</span> displayreport <span class="token operator">=</span> <span class="token punctuation">[</span>    <span class="token punctuation">[</span><span class="token string">""</span><span class="token punctuation">,</span> <span class="token string">"RMSE (original)"</span><span class="token punctuation">,</span> <span class="token string">"RMSE (up-sampling)"</span><span class="token punctuation">,</span> <span class="token string">"RMSE (down-sampling)"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>    <span class="token punctuation">[</span><span class="token string">"Linear Regression"</span><span class="token punctuation">,</span> <span class="token number">204.9581</span><span class="token punctuation">,</span> <span class="token number">214.4630</span><span class="token punctuation">,</span> <span class="token number">214.3920</span><span class="token punctuation">]</span><span class="token punctuation">,</span>    <span class="token punctuation">[</span><span class="token string">"Ridge Regression"</span><span class="token punctuation">,</span> <span class="token number">204.6595</span><span class="token punctuation">,</span> <span class="token number">214.4686</span><span class="token punctuation">,</span> <span class="token number">214.3732</span><span class="token punctuation">]</span><span class="token punctuation">,</span>     <span class="token punctuation">[</span><span class="token string">"Random Forest"</span><span class="token punctuation">,</span> <span class="token number">204.6774</span><span class="token punctuation">,</span> <span class="token number">221.9525</span><span class="token punctuation">,</span> <span class="token number">233.0449</span><span class="token punctuation">]</span><span class="token punctuation">,</span>     <span class="token punctuation">[</span><span class="token string">"Gradient Tree Boosting"</span><span class="token punctuation">,</span> <span class="token number">205.4188</span><span class="token punctuation">,</span> <span class="token number">230.6640</span><span class="token punctuation">,</span> <span class="token number">219.3338</span><span class="token punctuation">]</span>    <span class="token punctuation">]</span>display<span class="token punctuation">(</span>HTML<span class="token punctuation">(</span>   <span class="token string">'&lt;table>&lt;tr>&#123;&#125;&lt;/tr>&lt;/table>'</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>       <span class="token string">'&lt;/tr>&lt;tr>'</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>           <span class="token string">'&lt;td>&#123;&#125;&lt;/td>'</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span><span class="token string">'&lt;/td>&lt;td>'</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span><span class="token builtin">str</span><span class="token punctuation">(</span>_<span class="token punctuation">)</span> <span class="token keyword">for</span> _ <span class="token keyword">in</span> row<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token keyword">for</span> row <span class="token keyword">in</span> report<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><img src="/Hexo-Blog/2020/11/27/2020-11-27-exploratory-data-analysis-for-predicting-insurance-claim/performance.png" class=""><h2 id="Performance-using-a-combination-of-two-models"><a href="#Performance-using-a-combination-of-two-models" class="headerlink" title="Performance using a combination of two models"></a>Performance using a combination of two models</h2><p>In this section, I will build a prediction model based on two separate models in tandem (one after the other). The first model will be a binary classifier that will tell whether the claim was zero or different from zero. I will compare the following classifiers: random forests for classification and gradient boosting for classification.</p><p>As usual, load in required libraries.</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>ensemble <span class="token keyword">import</span> RandomForestClassifier<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>ensemble <span class="token keyword">import</span> GradientBoostingClassifier<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>metrics <span class="token keyword">import</span> confusion_matrix<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>metrics <span class="token keyword">import</span> classification_report<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>And split it into training and validation dataset using <code>preprocessing_v2()</code>.</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">X_train<span class="token punctuation">,</span> X_valid<span class="token punctuation">,</span> y_train<span class="token punctuation">,</span> y_valid<span class="token punctuation">,</span> full_transform <span class="token operator">=</span> preprocessing_v2<span class="token punctuation">(</span>data<span class="token punctuation">)</span>y_train_binary <span class="token operator">=</span> <span class="token punctuation">(</span>y_train <span class="token operator">!=</span> <span class="token number">0.0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>astype<span class="token punctuation">(</span><span class="token builtin">int</span><span class="token punctuation">)</span>y_valid_binary <span class="token operator">=</span> <span class="token punctuation">(</span>y_valid <span class="token operator">!=</span> <span class="token number">0.0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>astype<span class="token punctuation">(</span><span class="token builtin">int</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h3 id="Second-Model"><a href="#Second-Model" class="headerlink" title="Second Model"></a>Second Model</h3><h4 id="Random-Forest-1"><a href="#Random-Forest-1" class="headerlink" title="Random Forest"></a>Random Forest</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token triple-quoted-string string">"""Best params: Origin: &#123;'max_depth': 3, 'min_samples_split': 2, 'n_estimators': 100&#125;Upsampling: &#123;'max_depth': 9, 'min_samples_split': 7, 'n_estimators': 300&#125;Downsampling: &#123;'max_depth': 9, 'min_samples_split': 7, 'n_estimators': 300&#125;"""</span>USE_GRID <span class="token operator">=</span> <span class="token boolean">False</span><span class="token keyword">if</span> USE_GRID<span class="token punctuation">:</span>    param_grid <span class="token operator">=</span> <span class="token punctuation">&#123;</span>        <span class="token string">'n_estimators'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token builtin">int</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span> <span class="token keyword">for</span> x <span class="token keyword">in</span> np<span class="token punctuation">.</span>linspace<span class="token punctuation">(</span><span class="token number">300</span><span class="token punctuation">,</span> <span class="token number">500</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         <span class="token string">'max_depth'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token builtin">int</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span> <span class="token keyword">for</span> x <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         <span class="token string">'min_samples_split'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">]</span>    <span class="token punctuation">&#125;</span>    grid <span class="token operator">=</span> GridSearchCV<span class="token punctuation">(</span>RandomForestClassifier<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> cv<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> param_grid<span class="token operator">=</span>param_grid<span class="token punctuation">,</span> n_jobs<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">,</span> scoring<span class="token operator">=</span><span class="token string">"f1"</span><span class="token punctuation">)</span>    grid<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>X_train<span class="token punctuation">,</span> y_train_binary<span class="token punctuation">)</span>    rfc <span class="token operator">=</span> grid<span class="token punctuation">.</span>best_estimator_    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Best params: </span><span class="token interpolation"><span class="token punctuation">&#123;</span>grid<span class="token punctuation">.</span>best_params_<span class="token punctuation">&#125;</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>    y_pred <span class="token operator">=</span> rfc<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>X_valid<span class="token punctuation">)</span><span class="token keyword">else</span><span class="token punctuation">:</span>     best_params <span class="token operator">=</span> <span class="token punctuation">&#123;</span>        <span class="token string">'max_depth'</span><span class="token punctuation">:</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token string">'min_samples_split'</span><span class="token punctuation">:</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token string">'n_estimators'</span><span class="token punctuation">:</span> <span class="token number">100</span>    <span class="token punctuation">&#125;</span>    rfc <span class="token operator">=</span> RandomForestClassifier<span class="token punctuation">(</span><span class="token operator">**</span>best_params<span class="token punctuation">)</span>    rfc<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>X_train<span class="token punctuation">,</span> y_train_binary<span class="token punctuation">)</span>    y_pred <span class="token operator">=</span> rfc<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>X_valid<span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>classification_report<span class="token punctuation">(</span>y_valid_binary<span class="token punctuation">,</span> y_pred<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>confusion_matrix<span class="token punctuation">(</span>y_valid_binary<span class="token punctuation">,</span> y_pred<span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="Gradient-Boosting"><a href="#Gradient-Boosting" class="headerlink" title="Gradient Boosting"></a>Gradient Boosting</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token triple-quoted-string string">"""Best params: Origin: &#123;'max_depth': 3, 'min_samples_split': 0.30000000000000004, 'n_estimators': 300&#125;Upsampling: &#123;'max_depth': 5, 'min_samples_split': 0.1, 'n_estimators': 400&#125;Downsampling: &#123;'max_depth': 5, 'min_samples_split': 0.1, 'n_estimators': 300&#125;"""</span>USE_GRID <span class="token operator">=</span> <span class="token boolean">False</span><span class="token keyword">if</span> USE_GRID<span class="token punctuation">:</span>    param_grid <span class="token operator">=</span> <span class="token punctuation">&#123;</span>        <span class="token string">'n_estimators'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token builtin">int</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span> <span class="token keyword">for</span> x <span class="token keyword">in</span> np<span class="token punctuation">.</span>linspace<span class="token punctuation">(</span><span class="token number">200</span><span class="token punctuation">,</span> <span class="token number">400</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         <span class="token string">'max_depth'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token builtin">int</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span> <span class="token keyword">for</span> x <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         <span class="token string">"min_samples_split"</span><span class="token punctuation">:</span> np<span class="token punctuation">.</span>linspace<span class="token punctuation">(</span><span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token comment">#     'learning_rate': [0.1, 0.01, 0.001], </span><span class="token comment">#     'min_samples_leaf': np.linspace(0.1, 0.5, 12), </span><span class="token comment">#     "subsample": [0.5, 0.618, 0.8, 0.85, 0.9, 0.95, 1.0], </span><span class="token comment">#     'max_features': ["log2","sqrt"]</span>    <span class="token punctuation">&#125;</span>    grid <span class="token operator">=</span> GridSearchCV<span class="token punctuation">(</span>GradientBoostingClassifier<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> cv<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> param_grid<span class="token operator">=</span>param_grid<span class="token punctuation">,</span> n_jobs<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">,</span> scoring<span class="token operator">=</span><span class="token string">"f1"</span><span class="token punctuation">)</span>    grid<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>X_train<span class="token punctuation">,</span> y_train_binary<span class="token punctuation">)</span>    gbc <span class="token operator">=</span> grid<span class="token punctuation">.</span>best_estimator_    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Best params: </span><span class="token interpolation"><span class="token punctuation">&#123;</span>grid<span class="token punctuation">.</span>best_params_<span class="token punctuation">&#125;</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>    y_pred <span class="token operator">=</span> gbc<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>X_valid<span class="token punctuation">)</span><span class="token keyword">else</span><span class="token punctuation">:</span>     best_params <span class="token operator">=</span> <span class="token punctuation">&#123;</span>        <span class="token string">'max_depth'</span><span class="token punctuation">:</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token string">'min_samples_split'</span><span class="token punctuation">:</span> <span class="token number">0.30000000000000004</span><span class="token punctuation">,</span> <span class="token string">'n_estimators'</span><span class="token punctuation">:</span> <span class="token number">300</span>    <span class="token punctuation">&#125;</span>    gbc <span class="token operator">=</span> GradientBoostingClassifier<span class="token punctuation">(</span><span class="token operator">**</span>best_params<span class="token punctuation">)</span>    gbc<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>X_train<span class="token punctuation">,</span> y_train_binary<span class="token punctuation">)</span>    y_pred <span class="token operator">=</span> gbc<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>X_valid<span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>classification_report<span class="token punctuation">(</span>y_valid_binary<span class="token punctuation">,</span> y_pred<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>confusion_matrix<span class="token punctuation">(</span>y_valid_binary<span class="token punctuation">,</span> y_pred<span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>Put these two models as second model, and combine with primary model. For the second model, if the claim was different from zero, train a regression model to predict the actual value of the claim.</p><p>This time, I’ll put all code together inside a code block.</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token triple-quoted-string string">"""Best params: Origin: &#123;'copy_X': True, 'fit_intercept': False, 'normalize': True&#125;Upsampling: &#123;'copy_X': True, 'fit_intercept': True, 'normalize': False&#125;Downsampling: &#123;'copy_X': True, 'fit_intercept': True, 'normalize': True&#125;"""</span>USE_GRID <span class="token operator">=</span> <span class="token boolean">False</span><span class="token keyword">if</span> USE_GRID<span class="token punctuation">:</span>    param_grid <span class="token operator">=</span> <span class="token punctuation">&#123;</span>        <span class="token string">'normalize'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token boolean">True</span><span class="token punctuation">,</span> <span class="token boolean">False</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         <span class="token string">'fit_intercept'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token boolean">True</span><span class="token punctuation">,</span> <span class="token boolean">False</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         <span class="token string">'copy_X'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token boolean">True</span><span class="token punctuation">,</span> <span class="token boolean">False</span><span class="token punctuation">]</span>    <span class="token punctuation">&#125;</span>    grid <span class="token operator">=</span> GridSearchCV<span class="token punctuation">(</span>LinearRegression<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> cv<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> param_grid<span class="token operator">=</span>param_grid<span class="token punctuation">,</span> n_jobs<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">)</span>    grid<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>X_train<span class="token punctuation">[</span>np<span class="token punctuation">.</span>where<span class="token punctuation">(</span>y_train <span class="token operator">!=</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span> y_train<span class="token punctuation">[</span>y_train <span class="token operator">!=</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    lr_2 <span class="token operator">=</span> grid<span class="token punctuation">.</span>best_estimator_    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Best params: </span><span class="token interpolation"><span class="token punctuation">&#123;</span>grid<span class="token punctuation">.</span>best_params_<span class="token punctuation">&#125;</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>    y_pred <span class="token operator">=</span> lr_2<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>X_valid<span class="token punctuation">[</span>np<span class="token punctuation">.</span>where<span class="token punctuation">(</span>y_valid <span class="token operator">!=</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    rmse <span class="token operator">=</span> np<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>mean_squared_error<span class="token punctuation">(</span>y_valid<span class="token punctuation">[</span>y_valid <span class="token operator">!=</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> y_pred<span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"RMSE: </span><span class="token interpolation"><span class="token punctuation">&#123;</span><span class="token builtin">round</span><span class="token punctuation">(</span>rmse<span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">&#125;</span></span><span class="token string">"</span></span><span class="token punctuation">)</span><span class="token keyword">else</span><span class="token punctuation">:</span>     best_params <span class="token operator">=</span> <span class="token punctuation">&#123;</span>        <span class="token string">'copy_X'</span><span class="token punctuation">:</span> <span class="token boolean">True</span><span class="token punctuation">,</span> <span class="token string">'fit_intercept'</span><span class="token punctuation">:</span> <span class="token boolean">True</span><span class="token punctuation">,</span> <span class="token string">'normalize'</span><span class="token punctuation">:</span> <span class="token boolean">True</span>    <span class="token punctuation">&#125;</span>    lr_2 <span class="token operator">=</span> LinearRegression<span class="token punctuation">(</span><span class="token operator">**</span>best_params<span class="token punctuation">)</span>    lr_2<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>X_train<span class="token punctuation">[</span>np<span class="token punctuation">.</span>where<span class="token punctuation">(</span>y_train <span class="token operator">!=</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span> y_train<span class="token punctuation">[</span>y_train <span class="token operator">!=</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    y_pred <span class="token operator">=</span> lr_2<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>X_valid<span class="token punctuation">[</span>np<span class="token punctuation">.</span>where<span class="token punctuation">(</span>y_valid <span class="token operator">!=</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    rmse <span class="token operator">=</span> np<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>mean_squared_error<span class="token punctuation">(</span>y_valid<span class="token punctuation">[</span>y_valid <span class="token operator">!=</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> y_pred<span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"RMSE: </span><span class="token interpolation"><span class="token punctuation">&#123;</span><span class="token builtin">round</span><span class="token punctuation">(</span>rmse<span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">&#125;</span></span><span class="token string">"</span></span><span class="token punctuation">)</span><span class="token triple-quoted-string string">"""Best params: Origin: &#123;'alpha': 1.0, 'fit_intercept': True, 'normalize': True&#125;Upsampling: &#123;'alpha': 0.1, 'fit_intercept': True, 'normalize': True&#125;Downsampling: &#123;'alpha': 1.0, 'fit_intercept': True, 'normalize': True&#125;"""</span>USE_GRID <span class="token operator">=</span> <span class="token boolean">False</span><span class="token keyword">if</span> USE_GRID<span class="token punctuation">:</span>    param_grid <span class="token operator">=</span> <span class="token punctuation">&#123;</span>        <span class="token string">'alpha'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token builtin">float</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span> <span class="token keyword">for</span> x <span class="token keyword">in</span> np<span class="token punctuation">.</span>linspace<span class="token punctuation">(</span><span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token number">1.0</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         <span class="token string">'fit_intercept'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token boolean">True</span><span class="token punctuation">,</span> <span class="token boolean">False</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         <span class="token string">'normalize'</span><span class="token punctuation">:</span><span class="token punctuation">[</span><span class="token boolean">True</span><span class="token punctuation">,</span> <span class="token boolean">False</span><span class="token punctuation">]</span>    <span class="token punctuation">&#125;</span>    grid <span class="token operator">=</span> GridSearchCV<span class="token punctuation">(</span>Ridge<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> cv<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> param_grid<span class="token operator">=</span>param_grid<span class="token punctuation">,</span> n_jobs<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">)</span>    grid<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>X_train<span class="token punctuation">[</span>np<span class="token punctuation">.</span>where<span class="token punctuation">(</span>y_train <span class="token operator">!=</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span> y_train<span class="token punctuation">[</span>y_train <span class="token operator">!=</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    rr_2 <span class="token operator">=</span> grid<span class="token punctuation">.</span>best_estimator_    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Best params: </span><span class="token interpolation"><span class="token punctuation">&#123;</span>grid<span class="token punctuation">.</span>best_params_<span class="token punctuation">&#125;</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>    y_pred <span class="token operator">=</span> rr_2<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>X_valid<span class="token punctuation">[</span>np<span class="token punctuation">.</span>where<span class="token punctuation">(</span>y_valid <span class="token operator">!=</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    rmse <span class="token operator">=</span> np<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>mean_squared_error<span class="token punctuation">(</span>y_valid<span class="token punctuation">[</span>y_valid <span class="token operator">!=</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> y_pred<span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"RMSE: </span><span class="token interpolation"><span class="token punctuation">&#123;</span><span class="token builtin">round</span><span class="token punctuation">(</span>rmse<span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">&#125;</span></span><span class="token string">"</span></span><span class="token punctuation">)</span><span class="token keyword">else</span><span class="token punctuation">:</span>     best_params <span class="token operator">=</span> <span class="token punctuation">&#123;</span>        <span class="token string">'alpha'</span><span class="token punctuation">:</span> <span class="token number">1.0</span><span class="token punctuation">,</span> <span class="token string">'fit_intercept'</span><span class="token punctuation">:</span> <span class="token boolean">True</span><span class="token punctuation">,</span> <span class="token string">'normalize'</span><span class="token punctuation">:</span> <span class="token boolean">True</span>    <span class="token punctuation">&#125;</span>    rr_2 <span class="token operator">=</span> Ridge<span class="token punctuation">(</span><span class="token operator">**</span>best_params<span class="token punctuation">)</span>    rr_2<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>X_train<span class="token punctuation">[</span>np<span class="token punctuation">.</span>where<span class="token punctuation">(</span>y_train <span class="token operator">!=</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span> y_train<span class="token punctuation">[</span>y_train <span class="token operator">!=</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    y_pred <span class="token operator">=</span> rr_2<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>X_valid<span class="token punctuation">[</span>np<span class="token punctuation">.</span>where<span class="token punctuation">(</span>y_valid <span class="token operator">!=</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    rmse <span class="token operator">=</span> np<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>mean_squared_error<span class="token punctuation">(</span>y_valid<span class="token punctuation">[</span>y_valid <span class="token operator">!=</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> y_pred<span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"RMSE: </span><span class="token interpolation"><span class="token punctuation">&#123;</span><span class="token builtin">round</span><span class="token punctuation">(</span>rmse<span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">&#125;</span></span><span class="token string">"</span></span><span class="token punctuation">)</span><span class="token triple-quoted-string string">"""Best params: Origin: &#123;'max_depth': 3, 'min_samples_split': 2, 'n_estimators': 100&#125;Upsampling: &#123;'max_depth': 5, 'min_samples_split': 2, 'n_estimators': 300&#125;Downsampling: &#123;'max_depth': 3, 'min_samples_split': 2, 'n_estimators': 300&#125;"""</span>USE_GRID <span class="token operator">=</span> <span class="token boolean">False</span><span class="token keyword">if</span> USE_GRID<span class="token punctuation">:</span>    param_grid <span class="token operator">=</span> <span class="token punctuation">&#123;</span>        <span class="token string">'n_estimators'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token builtin">int</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span> <span class="token keyword">for</span> x <span class="token keyword">in</span> np<span class="token punctuation">.</span>linspace<span class="token punctuation">(</span><span class="token number">100</span><span class="token punctuation">,</span> <span class="token number">300</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         <span class="token string">'max_depth'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token builtin">int</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span> <span class="token keyword">for</span> x <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         <span class="token string">'min_samples_split'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token comment">#     'min_samples_leaf': [1, 2, 4], </span><span class="token comment">#     'bootstrap': [True, False]</span>    <span class="token punctuation">&#125;</span>    grid <span class="token operator">=</span> GridSearchCV<span class="token punctuation">(</span>RandomForestRegressor<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> cv<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> param_grid<span class="token operator">=</span>param_grid<span class="token punctuation">,</span> n_jobs<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">)</span>    grid<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>X_train<span class="token punctuation">[</span>np<span class="token punctuation">.</span>where<span class="token punctuation">(</span>y_train <span class="token operator">!=</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span> y_train<span class="token punctuation">[</span>y_train <span class="token operator">!=</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    rfr_2 <span class="token operator">=</span> grid<span class="token punctuation">.</span>best_estimator_    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Best params: </span><span class="token interpolation"><span class="token punctuation">&#123;</span>grid<span class="token punctuation">.</span>best_params_<span class="token punctuation">&#125;</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>    y_pred <span class="token operator">=</span> rfr_2<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>X_valid<span class="token punctuation">[</span>np<span class="token punctuation">.</span>where<span class="token punctuation">(</span>y_valid <span class="token operator">!=</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    rmse <span class="token operator">=</span> np<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>mean_squared_error<span class="token punctuation">(</span>y_valid<span class="token punctuation">[</span>y_valid <span class="token operator">!=</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> y_pred<span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"RMSE: </span><span class="token interpolation"><span class="token punctuation">&#123;</span><span class="token builtin">round</span><span class="token punctuation">(</span>rmse<span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">&#125;</span></span><span class="token string">"</span></span><span class="token punctuation">)</span><span class="token keyword">else</span><span class="token punctuation">:</span>     best_params <span class="token operator">=</span> <span class="token punctuation">&#123;</span>        <span class="token string">'max_depth'</span><span class="token punctuation">:</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token string">'min_samples_split'</span><span class="token punctuation">:</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token string">'n_estimators'</span><span class="token punctuation">:</span> <span class="token number">100</span>    <span class="token punctuation">&#125;</span>    rfr_2 <span class="token operator">=</span> RandomForestRegressor<span class="token punctuation">(</span><span class="token operator">**</span>best_params<span class="token punctuation">)</span>    rfr_2<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>X_train<span class="token punctuation">[</span>np<span class="token punctuation">.</span>where<span class="token punctuation">(</span>y_train <span class="token operator">!=</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span> y_train<span class="token punctuation">[</span>y_train <span class="token operator">!=</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    y_pred <span class="token operator">=</span> rfr_2<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>X_valid<span class="token punctuation">[</span>np<span class="token punctuation">.</span>where<span class="token punctuation">(</span>y_valid <span class="token operator">!=</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    rmse <span class="token operator">=</span> np<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>mean_squared_error<span class="token punctuation">(</span>y_valid<span class="token punctuation">[</span>y_valid <span class="token operator">!=</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> y_pred<span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"RMSE: </span><span class="token interpolation"><span class="token punctuation">&#123;</span><span class="token builtin">round</span><span class="token punctuation">(</span>rmse<span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">&#125;</span></span><span class="token string">"</span></span><span class="token punctuation">)</span><span class="token triple-quoted-string string">"""Best params: Origin: &#123;'max_depth': 3, 'min_samples_split': 0.5, 'n_estimators': 200&#125;Upsampling: &#123;'max_depth': 5, 'min_samples_split': 0.1, 'n_estimators': 400&#125;Downsampling: &#123;'max_depth': 3, 'min_samples_split': 0.5, 'n_estimators': 200&#125;"""</span>USE_GRID <span class="token operator">=</span> <span class="token boolean">False</span><span class="token keyword">if</span> USE_GRID<span class="token punctuation">:</span>    param_grid <span class="token operator">=</span> <span class="token punctuation">&#123;</span>        <span class="token string">'n_estimators'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token builtin">int</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span> <span class="token keyword">for</span> x <span class="token keyword">in</span> np<span class="token punctuation">.</span>linspace<span class="token punctuation">(</span><span class="token number">200</span><span class="token punctuation">,</span> <span class="token number">400</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         <span class="token string">'max_depth'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token builtin">int</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span> <span class="token keyword">for</span> x <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         <span class="token string">"min_samples_split"</span><span class="token punctuation">:</span> np<span class="token punctuation">.</span>linspace<span class="token punctuation">(</span><span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token number">12</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token comment">#     'learning_rate': [0.1, 0.01, 0.001], </span><span class="token comment">#     'min_samples_leaf': np.linspace(0.1, 0.5, 12), </span><span class="token comment">#     "subsample": [0.5, 0.618, 0.8, 0.85, 0.9, 0.95, 1.0], </span><span class="token comment">#     'max_features': ["log2","sqrt"]</span>    <span class="token punctuation">&#125;</span>    grid <span class="token operator">=</span> GridSearchCV<span class="token punctuation">(</span>GradientBoostingRegressor<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> cv<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> param_grid<span class="token operator">=</span>param_grid<span class="token punctuation">,</span> n_jobs<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">)</span>    grid<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>X_train<span class="token punctuation">[</span>np<span class="token punctuation">.</span>where<span class="token punctuation">(</span>y_train <span class="token operator">!=</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span> y_train<span class="token punctuation">[</span>y_train <span class="token operator">!=</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    gbr_2 <span class="token operator">=</span> grid<span class="token punctuation">.</span>best_estimator_    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Best params: </span><span class="token interpolation"><span class="token punctuation">&#123;</span>grid<span class="token punctuation">.</span>best_params_<span class="token punctuation">&#125;</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>    y_pred <span class="token operator">=</span> gbr_2<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>X_valid<span class="token punctuation">[</span>np<span class="token punctuation">.</span>where<span class="token punctuation">(</span>y_valid <span class="token operator">!=</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    rmse <span class="token operator">=</span> np<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>mean_squared_error<span class="token punctuation">(</span>y_valid<span class="token punctuation">[</span>y_valid <span class="token operator">!=</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> y_pred<span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"RMSE: </span><span class="token interpolation"><span class="token punctuation">&#123;</span><span class="token builtin">round</span><span class="token punctuation">(</span>rmse<span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">&#125;</span></span><span class="token string">"</span></span><span class="token punctuation">)</span><span class="token keyword">else</span><span class="token punctuation">:</span>     best_params <span class="token operator">=</span> <span class="token punctuation">&#123;</span>        <span class="token string">'max_depth'</span><span class="token punctuation">:</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token string">'min_samples_split'</span><span class="token punctuation">:</span> <span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token string">'n_estimators'</span><span class="token punctuation">:</span> <span class="token number">200</span>    <span class="token punctuation">&#125;</span>    gbr_2 <span class="token operator">=</span> GradientBoostingRegressor<span class="token punctuation">(</span><span class="token operator">**</span>best_params<span class="token punctuation">)</span>    gbr_2<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>X_train<span class="token punctuation">[</span>np<span class="token punctuation">.</span>where<span class="token punctuation">(</span>y_train <span class="token operator">!=</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span> y_train<span class="token punctuation">[</span>y_train <span class="token operator">!=</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    y_pred <span class="token operator">=</span> gbr_2<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>X_valid<span class="token punctuation">[</span>np<span class="token punctuation">.</span>where<span class="token punctuation">(</span>y_valid <span class="token operator">!=</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    rmse <span class="token operator">=</span> np<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>mean_squared_error<span class="token punctuation">(</span>y_valid<span class="token punctuation">[</span>y_valid <span class="token operator">!=</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> y_pred<span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"RMSE: </span><span class="token interpolation"><span class="token punctuation">&#123;</span><span class="token builtin">round</span><span class="token punctuation">(</span>rmse<span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">&#125;</span></span><span class="token string">"</span></span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>Use the tandem model built from before, for predicting in the same validation data used in the beginning, and report the performance.</p><p>RandomForestClassifier + LinearRegression</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">final_prediction <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>first_prediction <span class="token operator">=</span> rfc<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>X_valid<span class="token punctuation">)</span><span class="token keyword">for</span> i<span class="token punctuation">,</span> pred <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>first_prediction<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">if</span> pred <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>        final_prediction<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>    <span class="token keyword">else</span><span class="token punctuation">:</span>        non_zero_prediction <span class="token operator">=</span> lr_2<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>X_valid<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        final_prediction<span class="token punctuation">.</span>append<span class="token punctuation">(</span>non_zero_prediction<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token builtin">round</span><span class="token punctuation">(</span>np<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>mean_squared_error<span class="token punctuation">(</span>y_valid<span class="token punctuation">,</span> final_prediction<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>RandomForestClassifier + Ridge</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">final_prediction <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>first_prediction <span class="token operator">=</span> rfc<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>X_valid<span class="token punctuation">)</span><span class="token keyword">for</span> i<span class="token punctuation">,</span> pred <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>first_prediction<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">if</span> pred <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>        final_prediction<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>    <span class="token keyword">else</span><span class="token punctuation">:</span>        non_zero_prediction <span class="token operator">=</span> rr_2<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>X_valid<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        final_prediction<span class="token punctuation">.</span>append<span class="token punctuation">(</span>non_zero_prediction<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token builtin">round</span><span class="token punctuation">(</span>np<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>mean_squared_error<span class="token punctuation">(</span>y_valid<span class="token punctuation">,</span> final_prediction<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>RandomForestClassifier + RandomForestRegressor</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">final_prediction <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>first_prediction <span class="token operator">=</span> rfc<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>X_valid<span class="token punctuation">)</span><span class="token keyword">for</span> i<span class="token punctuation">,</span> pred <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>first_prediction<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">if</span> pred <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>        final_prediction<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>    <span class="token keyword">else</span><span class="token punctuation">:</span>        non_zero_prediction <span class="token operator">=</span> rfr_2<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>X_valid<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        final_prediction<span class="token punctuation">.</span>append<span class="token punctuation">(</span>non_zero_prediction<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token builtin">round</span><span class="token punctuation">(</span>np<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>mean_squared_error<span class="token punctuation">(</span>y_valid<span class="token punctuation">,</span> final_prediction<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>RandomForestClassifier + GradientBoostinRegressor</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">final_prediction <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>first_prediction <span class="token operator">=</span> rfc<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>X_valid<span class="token punctuation">)</span><span class="token keyword">for</span> i<span class="token punctuation">,</span> pred <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>first_prediction<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">if</span> pred <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>        final_prediction<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>    <span class="token keyword">else</span><span class="token punctuation">:</span>        non_zero_prediction <span class="token operator">=</span> gbr_2<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>X_valid<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        final_prediction<span class="token punctuation">.</span>append<span class="token punctuation">(</span>non_zero_prediction<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token builtin">round</span><span class="token punctuation">(</span>np<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>mean_squared_error<span class="token punctuation">(</span>y_valid<span class="token punctuation">,</span> final_prediction<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>GradientBoostingClassifier + LinearRegression</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">final_prediction <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>first_prediction <span class="token operator">=</span> gbc<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>X_valid<span class="token punctuation">)</span><span class="token keyword">for</span> i<span class="token punctuation">,</span> pred <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>first_prediction<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">if</span> pred <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>        final_prediction<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>    <span class="token keyword">else</span><span class="token punctuation">:</span>        non_zero_prediction <span class="token operator">=</span> lr_2<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>X_valid<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        final_prediction<span class="token punctuation">.</span>append<span class="token punctuation">(</span>non_zero_prediction<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token builtin">round</span><span class="token punctuation">(</span>np<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>mean_squared_error<span class="token punctuation">(</span>y_valid<span class="token punctuation">,</span> final_prediction<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>GradientBoostingClassifier + Ridge</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">final_prediction <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>first_prediction <span class="token operator">=</span> gbc<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>X_valid<span class="token punctuation">)</span><span class="token keyword">for</span> i<span class="token punctuation">,</span> pred <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>first_prediction<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">if</span> pred <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>        final_prediction<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>    <span class="token keyword">else</span><span class="token punctuation">:</span>        non_zero_prediction <span class="token operator">=</span> rr_2<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>X_valid<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        final_prediction<span class="token punctuation">.</span>append<span class="token punctuation">(</span>non_zero_prediction<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token builtin">round</span><span class="token punctuation">(</span>np<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>mean_squared_error<span class="token punctuation">(</span>y_valid<span class="token punctuation">,</span> final_prediction<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>GradientBoostingClassifier + RandomForestRegressor</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">final_prediction <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>first_prediction <span class="token operator">=</span> gbc<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>X_valid<span class="token punctuation">)</span><span class="token keyword">for</span> i<span class="token punctuation">,</span> pred <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>first_prediction<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">if</span> pred <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>        final_prediction<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>    <span class="token keyword">else</span><span class="token punctuation">:</span>        non_zero_prediction <span class="token operator">=</span> rfr_2<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>X_valid<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        final_prediction<span class="token punctuation">.</span>append<span class="token punctuation">(</span>non_zero_prediction<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token builtin">round</span><span class="token punctuation">(</span>np<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>mean_squared_error<span class="token punctuation">(</span>y_valid<span class="token punctuation">,</span> final_prediction<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>GradientBoostingClassifier + GradientBoostingRegressor</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">final_prediction <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>first_prediction <span class="token operator">=</span> gbc<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>X_valid<span class="token punctuation">)</span><span class="token keyword">for</span> i<span class="token punctuation">,</span> pred <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>first_prediction<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">if</span> pred <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>        final_prediction<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>    <span class="token keyword">else</span><span class="token punctuation">:</span>        non_zero_prediction <span class="token operator">=</span> gbr_2<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>X_valid<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        final_prediction<span class="token punctuation">.</span>append<span class="token punctuation">(</span>non_zero_prediction<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token builtin">round</span><span class="token punctuation">(</span>np<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>mean_squared_error<span class="token punctuation">(</span>y_valid<span class="token punctuation">,</span> final_prediction<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>Finally, performance of every models come out!</p><p>Single Model:</p><img src="/Hexo-Blog/2020/11/27/2020-11-27-exploratory-data-analysis-for-predicting-insurance-claim/p1.png" class=""><p>Tandem Model:</p><img src="/Hexo-Blog/2020/11/27/2020-11-27-exploratory-data-analysis-for-predicting-insurance-claim/p2.png" class=""><p>The best model from step 2 is using <code>LinearRegression</code> without any over sampling and down sampling technique. And the best model from step 3 is using <code>RandomForest + LinearRegression</code> without any over sampling and down sampling. Mean squared error for the best model from step 2 and 3 are 214.463 and 210.0559, respectively.</p><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><ol><li>For the single regression models, up-sampling and down-sampling technique do not give advantages at all. Actually, they even have higher mean squared error for those four models. Furthermore, we can see that when using resampling methods, mean square error of tree-based models are slightly higher than linear models.</li><li>I utilised different data preprocessing approaches to encode categorical features, e.g. <code>preprocessing_baseline()</code> and <code>preprocessing_v2()</code>, such as <code>OneHotEncoder()</code> and <code>MeanEncoder()</code>. In baseline preprocessor, I used <code>OneHotEncoder()</code> only to transform the categorical feature. But I found that <code>OneHotEncoder()</code> would produce high sparsity matrix when there’s many categories. Therefore, I subclass a <code>MeanEncoder()</code> in sklearn to deal with high-cardinality categorical features. After the experiment, using <code>MeanEncoder()</code> moderately improves the performance of the models.</li><li>For the tandem models, I trained the models in two different ways. First, I built a binary classifier, and collected the prediction which is not zero. Next, I fed them into regression model to get the final predicitons. Second, I built a binary classifier as the one before, and after that I selected only non-zero labels in original dataset to feed into regression model. When I done training these two methods, the second one performs better than the first one, so I decided to use the second training method as my pipeline.</li><li>When comparing single models with tandem models, it can see that single models is better than tandem models whether or not sampling methods are used. Single models’ mean square error are less than tandem models’ by 2.85% in average. However, if we must utilise sampling method to solve imbalanced label problem, it can be seen that over sampling practically outperforms down sampling.</li></ol>]]></content>
      
      
      <categories>
          
          <category> Data Science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> ML </tag>
            
            <tag> EDA </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>QS Ranking Crawler</title>
      <link href="/Hexo-Blog/2020/04/15/2020-04-15-qs-ranking-crawler/"/>
      <url>/Hexo-Blog/2020/04/15/2020-04-15-qs-ranking-crawler/</url>
      
        <content type="html"><![CDATA[<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>This article aims to build a web scraper by using BeautifulSoup and Selenium, and scrape QS Rankings to discover the top universities from all over the world. “Uni name”, “ranking” and “location” are fetched from the table and stored as a csv file. Jupyter notebook is available as well through this <a href="https://github.com/penguinwang96825/QS_ranking_web_scraping/blob/master/QS.ipynb">link</a>.</p><h1 id="Python-Implementation"><a href="#Python-Implementation" class="headerlink" title="Python Implementation"></a>Python Implementation</h1><p>Download chrome web <a href="https://sites.google.com/a/chromium.org/chromedriver/">dirver</a> first.</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> pandas <span class="token keyword">as</span> pd<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt<span class="token keyword">import</span> time<span class="token keyword">import</span> bs4<span class="token keyword">from</span> bs4 <span class="token keyword">import</span> BeautifulSoup<span class="token keyword">from</span> selenium<span class="token punctuation">.</span>common<span class="token punctuation">.</span>exceptions <span class="token keyword">import</span> <span class="token operator">*</span><span class="token keyword">from</span> selenium <span class="token keyword">import</span> webdriver<span class="token operator">%</span>matplotlib inline<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="Main-Code"><a href="#Main-Code" class="headerlink" title="Main Code"></a>Main Code</h2><p>Only 25 universities are listed on the table per page, so I have to set the number of page I want to crawl. (max page: 40)</p><ol><li>Open the html via chrome driver. (make sure webdriver.exe is in the same directory)</li><li>Parse the html using BeautifulSoup.</li><li>Create a loop to crawl all the elements (ranking, uni name, location) in each row.</li><li>Click to the next page and start over the loop in step three.</li><li>Stop fetching the data until all pages are done.</li></ol><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">get_uni_information</span><span class="token punctuation">(</span>year<span class="token operator">=</span><span class="token number">2020</span><span class="token punctuation">,</span> unilist<span class="token punctuation">,</span> page<span class="token operator">=</span><span class="token number">40</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    url <span class="token operator">=</span> <span class="token string">r"https://www.topuniversities.com/university-rankings/world-university-rankings/&#123;&#125;"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>year<span class="token punctuation">)</span>    <span class="token comment"># Open url and get the QS Ranking html page</span>    driver_path <span class="token operator">=</span> <span class="token string">r"C:\Users\YangWang\Desktop\machineLearning\indiaNewsClassification\chromedriver.exe"</span>    driver <span class="token operator">=</span> webdriver<span class="token punctuation">.</span>Chrome<span class="token punctuation">(</span>driver_path<span class="token punctuation">)</span>    time<span class="token punctuation">.</span>sleep<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span>    driver<span class="token punctuation">.</span>get<span class="token punctuation">(</span>url<span class="token punctuation">)</span>    time<span class="token punctuation">.</span>sleep<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span>        <span class="token comment"># Crawl all the pages (max page is 40)</span>    <span class="token keyword">if</span> page <span class="token operator">&lt;=</span> <span class="token number">40</span><span class="token punctuation">:</span>         <span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token builtin">int</span><span class="token punctuation">(</span>page<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token comment"># Use BeautifulSoup to parse every page</span>            soup <span class="token operator">=</span> BeautifulSoup<span class="token punctuation">(</span>driver<span class="token punctuation">.</span>page_source<span class="token punctuation">,</span> <span class="token string">"html.parser"</span><span class="token punctuation">)</span>            <span class="token comment"># Find the table which contains the information I want</span>            x <span class="token operator">=</span> soup<span class="token punctuation">.</span>find<span class="token punctuation">(</span>name<span class="token operator">=</span><span class="token string">"table"</span><span class="token punctuation">,</span> attrs<span class="token operator">=</span><span class="token punctuation">&#123;</span><span class="token string">"class"</span><span class="token punctuation">:</span> <span class="token string">"dataTable no-footer"</span><span class="token punctuation">&#125;</span><span class="token punctuation">)</span>            <span class="token comment"># Use 'for' loop to catch every rows in the table, and append the rows into the list</span>            <span class="token keyword">for</span> tr <span class="token keyword">in</span> x<span class="token punctuation">.</span>find<span class="token punctuation">(</span>name<span class="token operator">=</span><span class="token string">"tbody"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>                <span class="token keyword">try</span><span class="token punctuation">:</span>                     tds <span class="token operator">=</span> tr<span class="token punctuation">(</span><span class="token string">'td'</span><span class="token punctuation">)</span>                    <span class="token keyword">if</span> tds<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>find<span class="token punctuation">(</span>name<span class="token operator">=</span><span class="token string">"span"</span><span class="token punctuation">)</span> <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>                        rank <span class="token operator">=</span> tds<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>find<span class="token punctuation">(</span>name<span class="token operator">=</span><span class="token string">"span"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>string                    <span class="token keyword">else</span><span class="token punctuation">:</span>                         rank <span class="token operator">=</span> <span class="token boolean">None</span>                    <span class="token keyword">if</span> tds<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>find<span class="token punctuation">(</span>name<span class="token operator">=</span><span class="token string">"a"</span><span class="token punctuation">)</span> <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>                        uni <span class="token operator">=</span> tds<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>find<span class="token punctuation">(</span>name<span class="token operator">=</span><span class="token string">"a"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>string                    <span class="token keyword">else</span><span class="token punctuation">:</span>                         uni <span class="token operator">=</span> <span class="token boolean">None</span>                    <span class="token keyword">if</span> tds<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">.</span>find<span class="token punctuation">(</span>attrs<span class="token operator">=</span><span class="token punctuation">&#123;</span><span class="token string">"class"</span><span class="token punctuation">:</span> <span class="token string">"td-wrap"</span><span class="token punctuation">&#125;</span><span class="token punctuation">)</span> <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>                        location <span class="token operator">=</span> tds<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">.</span>find<span class="token punctuation">(</span>attrs<span class="token operator">=</span><span class="token punctuation">&#123;</span><span class="token string">"class"</span><span class="token punctuation">:</span> <span class="token string">"td-wrap"</span><span class="token punctuation">&#125;</span><span class="token punctuation">)</span><span class="token punctuation">.</span>string                    <span class="token keyword">else</span><span class="token punctuation">:</span>                         location <span class="token operator">=</span> <span class="token boolean">None</span>                <span class="token keyword">except</span> <span class="token punctuation">(</span>RuntimeError<span class="token punctuation">,</span> TypeError<span class="token punctuation">,</span> NameError<span class="token punctuation">)</span><span class="token punctuation">:</span>                    <span class="token keyword">pass</span>                unilist<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token punctuation">[</span>rank<span class="token punctuation">,</span> uni<span class="token punctuation">,</span> location<span class="token punctuation">]</span><span class="token punctuation">)</span>            <span class="token comment"># Click next page button</span>            element <span class="token operator">=</span> driver<span class="token punctuation">.</span>find_element_by_xpath<span class="token punctuation">(</span><span class="token string">'//*[@id="qs-rankings_next"]'</span><span class="token punctuation">)</span>            driver<span class="token punctuation">.</span>execute_script<span class="token punctuation">(</span><span class="token string">"arguments[0].click();"</span><span class="token punctuation">,</span> element<span class="token punctuation">)</span>            time<span class="token punctuation">.</span>sleep<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span>    <span class="token keyword">else</span><span class="token punctuation">:</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Max page is 40."</span><span class="token punctuation">)</span>        driver<span class="token punctuation">.</span>quit<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> unilist<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="Get-the-DataFrame"><a href="#Get-the-DataFrame" class="headerlink" title="Get the DataFrame"></a>Get the DataFrame</h2><p>Using <code>get_uni_information()</code> function to crawl the information and then store them into a dataframe. Also do some dataframe preprocessing in order to make sure every columns are in right data types.</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">get_qs_ranking_dataframe</span><span class="token punctuation">(</span>year<span class="token operator">=</span><span class="token number">2020</span><span class="token punctuation">,</span> page<span class="token operator">=</span><span class="token number">40</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    unilist <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    unilist <span class="token operator">=</span> get_uni_information<span class="token punctuation">(</span>year<span class="token punctuation">,</span> unilist<span class="token punctuation">,</span> page<span class="token punctuation">)</span>    df <span class="token operator">=</span> pd<span class="token punctuation">.</span>DataFrame<span class="token punctuation">(</span>unilist<span class="token punctuation">)</span>    df<span class="token punctuation">.</span>columns <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">"ranking"</span><span class="token punctuation">,</span> <span class="token string">"uni"</span><span class="token punctuation">,</span> <span class="token string">"location"</span><span class="token punctuation">]</span>    df<span class="token punctuation">.</span>reset_index<span class="token punctuation">(</span>drop<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>        <span class="token comment"># Dataframe preprocessing</span>    df<span class="token punctuation">[</span><span class="token string">"ranking"</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token builtin">int</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token operator">+</span><span class="token number">1</span> <span class="token keyword">for</span> x <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>df<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">]</span>    df<span class="token punctuation">[</span><span class="token string">"uni"</span><span class="token punctuation">]</span> <span class="token operator">=</span> df<span class="token punctuation">[</span><span class="token string">"uni"</span><span class="token punctuation">]</span><span class="token punctuation">.</span><span class="token builtin">map</span><span class="token punctuation">(</span><span class="token builtin">str</span><span class="token punctuation">)</span>    df<span class="token punctuation">[</span><span class="token string">"location"</span><span class="token punctuation">]</span> <span class="token operator">=</span> df<span class="token punctuation">[</span><span class="token string">"location"</span><span class="token punctuation">]</span><span class="token punctuation">.</span><span class="token builtin">map</span><span class="token punctuation">(</span><span class="token builtin">str</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> df<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>Take a look at the dataframe.</p><h3 id="Japan"><a href="#Japan" class="headerlink" title="Japan"></a>Japan</h3><pre class="line-numbers language-python" data-language="python"><code class="language-python">qs_2020 <span class="token operator">=</span> get_qs_ranking_dataframe<span class="token punctuation">(</span>year<span class="token operator">=</span><span class="token number">2020</span><span class="token punctuation">,</span> page<span class="token operator">=</span><span class="token number">40</span><span class="token punctuation">)</span>qs_2020<span class="token punctuation">[</span><span class="token punctuation">(</span>qs_2020<span class="token punctuation">[</span><span class="token string">"location"</span><span class="token punctuation">]</span> <span class="token operator">==</span> <span class="token string">"Japan"</span><span class="token punctuation">)</span> <span class="token operator">&amp;</span> <span class="token punctuation">(</span>qs_2020<span class="token punctuation">[</span><span class="token string">"ranking"</span><span class="token punctuation">]</span> <span class="token operator">&lt;=</span> <span class="token number">100</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><table><thead><tr><th>ranking</th><th>uni</th><th>location</th></tr></thead><tbody><tr><td>23</td><td>The University of Tokyo</td><td>Japan</td></tr><tr><td>34</td><td>Kyoto University</td><td>Japan</td></tr><tr><td>59</td><td>Tokyo Institute of Technology (Tokyo Tech)</td><td>Japan</td></tr><tr><td>71</td><td>Osaka University</td><td>Japan</td></tr><tr><td>82</td><td>Tohoku University</td><td>Japan</td></tr></tbody></table><h3 id="United-States"><a href="#United-States" class="headerlink" title="United States"></a>United States</h3><pre class="line-numbers language-python" data-language="python"><code class="language-python">qs_2020<span class="token punctuation">[</span><span class="token punctuation">(</span>qs_2020<span class="token punctuation">[</span><span class="token string">"location"</span><span class="token punctuation">]</span> <span class="token operator">==</span> <span class="token string">"United States"</span><span class="token punctuation">)</span> <span class="token operator">&amp;</span> <span class="token punctuation">(</span>qs_2020<span class="token punctuation">[</span><span class="token string">"ranking"</span><span class="token punctuation">]</span> <span class="token operator">&lt;=</span> <span class="token number">100</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><table><thead><tr><th>ranking</th><th>uni</th><th>location</th></tr></thead><tbody><tr><td>1</td><td>Massachusetts Institute of Technology (MIT)</td><td>United States</td></tr><tr><td>2</td><td>Stanford University</td><td>United States</td></tr><tr><td>3</td><td>Harvard University</td><td>United States</td></tr><tr><td>5</td><td>California Institute of Technology (Caltech)</td><td>United States</td></tr><tr><td>10</td><td>University of Chicago</td><td>United States</td></tr><tr><td>13</td><td>Princeton University</td><td>United States</td></tr><tr><td>14</td><td>Cornell University</td><td>United States</td></tr><tr><td>15</td><td>University of Pennsylvania</td><td>United States</td></tr><tr><td>17</td><td>Yale University</td><td>United States</td></tr><tr><td>18</td><td>Columbia University</td><td>United States</td></tr><tr><td>21</td><td>University of Michigan-Ann Arbor</td><td>United States</td></tr><tr><td>24</td><td>Johns Hopkins University</td><td>United States</td></tr><tr><td>25</td><td>Duke University</td><td>United States</td></tr><tr><td>28</td><td>University of California, Berkeley (UCB)</td><td>United States</td></tr><tr><td>31</td><td>Northwestern University</td><td>United States</td></tr><tr><td>36</td><td>University of California, Los Angeles (UCLA)</td><td>United States</td></tr><tr><td>39</td><td>New York University (NYU)</td><td>United States</td></tr><tr><td>45</td><td>University of California, San Diego (UCSD)</td><td>United States</td></tr><tr><td>48</td><td>Carnegie Mellon University</td><td>United States</td></tr><tr><td>56</td><td>University of Wisconsin-Madison</td><td>United States</td></tr><tr><td>57</td><td>Brown University</td><td>United States</td></tr><tr><td>65</td><td>University of Texas at Austin</td><td>United States</td></tr><tr><td>68</td><td>University of Washington</td><td>United States</td></tr><tr><td>72</td><td>Georgia Institute of Technology</td><td>United States</td></tr><tr><td>74</td><td>University of Illinois at Urbana-Champaign</td><td>United States</td></tr><tr><td>86</td><td>Rice University</td><td>United States</td></tr><tr><td>90</td><td>University of North Carolina, Chapel Hill</td><td>United States</td></tr><tr><td>93</td><td>Pennsylvania State University</td><td>United States</td></tr><tr><td>98</td><td>Boston University</td><td>United States</td></tr></tbody></table><h3 id="United-Kingdom"><a href="#United-Kingdom" class="headerlink" title="United Kingdom"></a>United Kingdom</h3><pre class="line-numbers language-python" data-language="python"><code class="language-python">qs_2020<span class="token punctuation">[</span><span class="token punctuation">(</span>qs_2020<span class="token punctuation">[</span><span class="token string">"location"</span><span class="token punctuation">]</span> <span class="token operator">==</span> <span class="token string">"United Kingdom"</span><span class="token punctuation">)</span> <span class="token operator">&amp;</span> <span class="token punctuation">(</span>qs_2020<span class="token punctuation">[</span><span class="token string">"ranking"</span><span class="token punctuation">]</span> <span class="token operator">&lt;=</span> <span class="token number">100</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><table><thead><tr><th>ranking</th><th>uni</th><th>location</th></tr></thead><tbody><tr><td>4</td><td>University of Oxford</td><td>United Kingdom</td></tr><tr><td>7</td><td>University of Cambridge</td><td>United Kingdom</td></tr><tr><td>8</td><td>UCL</td><td>United Kingdom</td></tr><tr><td>9</td><td>Imperial College London</td><td>United Kingdom</td></tr><tr><td>20</td><td>The University of Edinburgh</td><td>United Kingdom</td></tr><tr><td>27</td><td>The University of Manchester</td><td>United Kingdom</td></tr><tr><td>33</td><td>King’s College London</td><td>United Kingdom</td></tr><tr><td>44</td><td>The London School of Economics and Political S…</td><td>United Kingdom</td></tr><tr><td>49</td><td>University of Bristol</td><td>United Kingdom</td></tr><tr><td>62</td><td>The University of Warwick</td><td>United Kingdom</td></tr><tr><td>67</td><td>University of Glasgow</td><td>United Kingdom</td></tr><tr><td>77</td><td>Durham University</td><td>United Kingdom</td></tr><tr><td>78</td><td>The University of Sheffield</td><td>United Kingdom</td></tr><tr><td>81</td><td>University of Birmingham</td><td>United Kingdom</td></tr><tr><td>94</td><td>University of Leeds</td><td>United Kingdom</td></tr><tr><td>96</td><td>University of Nottingham</td><td>United Kingdom</td></tr><tr><td>97</td><td>University of Southampton</td><td>United Kingdom</td></tr><tr><td>100</td><td>University of St Andrews</td><td>United Kingdom</td></tr></tbody></table><h3 id="Taiwan"><a href="#Taiwan" class="headerlink" title="Taiwan"></a>Taiwan</h3><pre class="line-numbers language-python" data-language="python"><code class="language-python">qs_2020<span class="token punctuation">[</span><span class="token punctuation">(</span>qs_2020<span class="token punctuation">[</span><span class="token string">"location"</span><span class="token punctuation">]</span> <span class="token operator">==</span> <span class="token string">"Taiwan"</span><span class="token punctuation">)</span> <span class="token operator">&amp;</span> <span class="token punctuation">(</span>qs_2020<span class="token punctuation">[</span><span class="token string">"ranking"</span><span class="token punctuation">]</span> <span class="token operator">&lt;=</span> <span class="token number">1000</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><table><thead><tr><th>ranking</th><th>uni</th><th>location</th></tr></thead><tbody><tr><td>69</td><td>National Taiwan University (NTU)</td><td>Taiwan</td></tr><tr><td>174</td><td>National Tsing Hua University</td><td>Taiwan</td></tr><tr><td>226</td><td>National Cheng Kung University (NCKU)</td><td>Taiwan</td></tr><tr><td>228</td><td>National Chiao Tung University</td><td>Taiwan</td></tr><tr><td>251</td><td>National Taiwan University of Science and Tech…</td><td>Taiwan</td></tr><tr><td>288</td><td>National Yang Ming University</td><td>Taiwan</td></tr><tr><td>333</td><td>National Taiwan Normal University</td><td>Taiwan</td></tr><tr><td>381</td><td>Taipei Medical University (TMU)</td><td>Taiwan</td></tr><tr><td>412</td><td>National Sun Yat-sen University</td><td>Taiwan</td></tr><tr><td>427</td><td>National Central University</td><td>Taiwan</td></tr><tr><td>485</td><td>Chang Gung University</td><td>Taiwan</td></tr><tr><td>514</td><td>National Taipei University of Technology</td><td>Taiwan</td></tr><tr><td>556</td><td>National Chengchi University</td><td>Taiwan</td></tr><tr><td>669</td><td>Kaohsiung Medical University</td><td>Taiwan</td></tr><tr><td>675</td><td>National Chung Hsing University</td><td>Taiwan</td></tr><tr><td>866</td><td>National Chung Cheng University</td><td>Taiwan</td></tr></tbody></table><h2 id="Visualise"><a href="#Visualise" class="headerlink" title="Visualise"></a>Visualise</h2><p>Visualise top <code>top_ranking</code> universities and show top <code>num</code> countries in the image of the certain year.</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">visualise_qs_ranking</span><span class="token punctuation">(</span>df<span class="token punctuation">,</span> year<span class="token punctuation">,</span> top_ranking<span class="token punctuation">,</span> num<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""    df: dataframe    year: year of the qs ranking    top_ranking: top # of universities to be selected    num: # of countries to be visaulised    """</span>    plt<span class="token punctuation">.</span>style<span class="token punctuation">.</span>use<span class="token punctuation">(</span><span class="token string">'seaborn-paper'</span><span class="token punctuation">)</span>    top <span class="token operator">=</span> df<span class="token punctuation">.</span>iloc<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">:</span>top_ranking<span class="token punctuation">]</span>        ax <span class="token operator">=</span> <span class="token punctuation">(</span>top<span class="token punctuation">[</span><span class="token string">'location'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>value_counts<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>head<span class="token punctuation">(</span>num<span class="token punctuation">)</span><span class="token punctuation">.</span>plot<span class="token punctuation">(</span>        kind<span class="token operator">=</span><span class="token string">'barh'</span><span class="token punctuation">,</span>         figsize<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">,</span>         color<span class="token operator">=</span><span class="token string">"tab:blue"</span><span class="token punctuation">,</span>         title<span class="token operator">=</span><span class="token string">"Number of Top &#123;&#125; Universities in QS Ranking &#123;&#125;"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>top<span class="token punctuation">[</span><span class="token string">'location'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token builtin">str</span><span class="token punctuation">(</span>year<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    ax<span class="token punctuation">.</span>set_xticks<span class="token punctuation">(</span>np<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> top<span class="token punctuation">[</span><span class="token string">'location'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>value_counts<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token operator">+</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>visualise_qs_ranking<span class="token punctuation">(</span>df<span class="token operator">=</span>qs_2020<span class="token punctuation">,</span> year<span class="token operator">=</span><span class="token number">2020</span><span class="token punctuation">,</span> top_ranking<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">,</span> num<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><img src="/Hexo-Blog/2020/04/15/2020-04-15-qs-ranking-crawler/ranking.png" class=""><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>The above is a combination of Python’s Selenium and BeautifulSoup to achieve dynamic web crawler, the complete code can be found in my <a href="https://github.com/penguinwang96825/QS-Ranking-Crawler">GitHub</a>, if you encounter problems in the process of implementation, welcome to share in the comments below.</p>]]></content>
      
      
      <categories>
          
          <category> Data Science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> Crawler </tag>
            
            <tag> Visualisation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Sentiment Analysis for KKBOX</title>
      <link href="/Hexo-Blog/2019/07/10/2019-07-10-sentiment-analysis-for-kkbox/"/>
      <url>/Hexo-Blog/2019/07/10/2019-07-10-sentiment-analysis-for-kkbox/</url>
      
        <content type="html"><![CDATA[<h1 id="Sentiment-Classification-for-UtaPass-amp-KKBOX-Reviews"><a href="#Sentiment-Classification-for-UtaPass-amp-KKBOX-Reviews" class="headerlink" title="Sentiment Classification for UtaPass &amp; KKBOX Reviews"></a>Sentiment Classification for UtaPass &amp; KKBOX Reviews</h1><p>Text classification for reviews of UtaPass &amp; KKBOX using different deep learning models.</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>This sentiment classification task is based on reviews data of UtaPass and KKBOX from Google Play platform. As a KKStreamer at KKBOX, I become more interested in Natural Language Processing, especially text classification. First, I start crawling the text data using web crawler technique, namely BeautifulSoup and Selenium. Second, I develop several different neural network architectures, including simple RNN, LSTM, GRU, and CNN, to name but a few, to detect the polarity of reviews from customers.</p><h2 id="Data-Source"><a href="#Data-Source" class="headerlink" title="Data Source"></a>Data Source</h2><ol><li><a href="https://play.google.com/store/apps/details?id=com.kddi.android.UtaPass&hl=ja&showAllReviews=true">UtaPass</a> reviews on Google Play.</li><li><a href="https://play.google.com/store/apps/details?id=com.skysoft.kkbox.android&hl=ja&showAllReviews=true">KKBOX</a> reviews on Google Play.</li></ol><h2 id="Bottleneck"><a href="#Bottleneck" class="headerlink" title="Bottleneck"></a>Bottleneck</h2><ul><li>Is text pre-processing (e.g. remove stop words, remove punctuation, remove bad characters) neccessary?</li><li>Tokenise in character-level or word-level?</li><li>Do every reviews have sentiment words or charateristic of polarity?</li><li>Does this dataset exist an imbalance problem?</li></ul><h2 id="Flow-Chart-of-Text-Classification"><a href="#Flow-Chart-of-Text-Classification" class="headerlink" title="Flow Chart of Text Classification"></a>Flow Chart of Text Classification</h2><p><img src="https://github.com/penguinwang96825/Text-Classifier-for-UtaPass-and-KKBOX/raw/master/image/flowChart.jpg" alt="Flow Chart"></p><h2 id="Workstation"><a href="#Workstation" class="headerlink" title="Workstation"></a>Workstation</h2><ul><li>Processor: Intel Core i9-9900K</li><li>Motherboard: Gigabyte Z390 AORUS MASTER</li><li>GPU: MSI RTX2080Ti Gaming X Trio 11G</li><li>RAM: Kingston 64GB DDR4-3000 HyperX Predator</li><li>CPU Cooler: MasterLiquid ML240L</li><li>Storage: PLEXTOR M9PeGN 1TB M.2 2280 PCIe SSD</li><li>Power: Antec HCG750 Gold</li><li>Case: Fractal Design R6-BKO-TG</li></ul><h1 id="Preparation"><a href="#Preparation" class="headerlink" title="Preparation"></a>Preparation</h1><ol><li>Preparing Selenium, BeautifulSoup, and Pandas.</li></ol><ul><li>Selenium: Selenium is an open source tool used for automating.</li><li>Beautiful Soup: BeautifulSoup is a Python library for parsing data out of HTML and XML files.</li><li>Pandas: Pandas is an open source data analysis tools for the Python programming language.</li></ul><ol start="2"><li>Install <a href="https://qiita.com/yukinoi/items/990b6933d9f21ba0fb43">MeCab</a> on win10.</li></ol><ul><li>Download <a href="https://github.com/ikegami-yukino/mecab/releases/download/v0.996.2/mecab-64-0.996.2.exe">MeCab</a> 64bit version first.</li><li>Run pip install <code>https://github.com/ikegami-yukino/mecab/archive/v0.996.2.tar.gz</code> in terminal.</li><li>Run <code>python -m pip install mecab</code> in terminal.</li></ul><p>There are other tools which can deal with other languages.</p><ul><li><p>English Segmentation Tools</p><ul><li><a href="https://github.com/nltk/nltk">NLTK</a></li><li><a href="https://spacy.io/api/tokenizer">spaCy</a></li><li><a href="https://github.com/google/sentencepiece">SentencePiece</a></li><li><a href="https://github.com/stanfordnlp/CoreNLP">Stanford CoreNLP</a></li></ul></li><li><p>Chinese Segmentation Tools</p><ul><li><a href="https://github.com/fxsjy/jieba">Jieba</a></li><li><a href="https://github.com/isnowfy/snownlp">SnowNLP</a></li><li><a href="https://www.ltp-cloud.com/">LTP</a></li><li><a href="https://github.com/hankcs/HanLP">HanNLP</a></li><li><a href="https://github.com/lancopku/pkuseg-python">PKUSEG</a></li></ul></li><li><p>Japanese Segmentation Tools</p><ul><li><a href="https://github.com/ikegami-yukino/mecab/releases">MeCab</a></li><li><a href="https://www.dampfkraft.com/nlp/how-to-tokenize-japanese.html">Fugashi</a></li><li><a href="https://mocobeta.github.io/janome/en/">Janome</a></li></ul></li></ul><h1 id="Main-Code-for-Crawling"><a href="#Main-Code-for-Crawling" class="headerlink" title="Main Code for Crawling"></a>Main Code for Crawling</h1><p>Data Crawling is to deal with large data-sets where you develop your crawlers (or bots) which crawl to the deepest of the web pages. In this project, I am going to crawl all the reviews related to UtaPass from Google Play.</p><h2 id="Scroll-down-Feature-and-Click-button-Feature"><a href="#Scroll-down-Feature-and-Click-button-Feature" class="headerlink" title="Scroll-down Feature and Click-button Feature"></a>Scroll-down Feature and Click-button Feature</h2><p>In Google Play, you need to scroll down pages to reach all the contents, in this project, I utilise Selenium to do so.</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">check_exists_by_xpath</span><span class="token punctuation">(</span>xpath<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">try</span><span class="token punctuation">:</span>        driver<span class="token punctuation">.</span>find_element_by_xpath<span class="token punctuation">(</span>xpath<span class="token punctuation">)</span>    <span class="token keyword">except</span> NoSuchElementException<span class="token punctuation">:</span>        <span class="token keyword">return</span> <span class="token boolean">False</span>    <span class="token keyword">return</span> <span class="token boolean">True</span><span class="token keyword">def</span> <span class="token function">scroll_ownPage</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment"># Xpath of "もっと見る" bottom</span>    button <span class="token operator">=</span> <span class="token string">'//*[@id="fcxH9b"]/div[4]/c-wiz/div/div[2]/div/div[1]/div/div/div[1]/div[2]/div[2]/div/span/span'</span>        <span class="token comment"># Keep scrolling down until to the very bottom</span>    keep_scrolling <span class="token operator">=</span> <span class="token boolean">True</span>    <span class="token keyword">while</span> keep_scrolling<span class="token punctuation">:</span>        <span class="token keyword">try</span><span class="token punctuation">:</span>             <span class="token comment"># Scroll down to the bottom</span>            <span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">:</span>                <span class="token keyword">try</span><span class="token punctuation">:</span>                     driver<span class="token punctuation">.</span>execute_script<span class="token punctuation">(</span><span class="token string">'window.scrollTo(0, document.body.scrollHeight);'</span><span class="token punctuation">)</span>                    time<span class="token punctuation">.</span>sleep<span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">+</span> random<span class="token punctuation">.</span>random<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>                <span class="token keyword">except</span><span class="token punctuation">:</span>                    <span class="token keyword">break</span>            <span class="token comment"># Click "もっと見る"</span>            <span class="token keyword">if</span> check_exists_by_xpath<span class="token punctuation">(</span>button<span class="token punctuation">)</span><span class="token punctuation">:</span>                driver<span class="token punctuation">.</span>find_element_by_xpath<span class="token punctuation">(</span>button<span class="token punctuation">)</span><span class="token punctuation">.</span>click<span class="token punctuation">(</span><span class="token punctuation">)</span>                time<span class="token punctuation">.</span>sleep<span class="token punctuation">(</span><span class="token number">2</span> <span class="token operator">+</span> random<span class="token punctuation">.</span>random<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>            <span class="token keyword">else</span><span class="token punctuation">:</span>                <span class="token comment"># Stop scrolling down</span>                keep_scrolling <span class="token operator">=</span> <span class="token boolean">False</span>        <span class="token keyword">except</span><span class="token punctuation">:</span>             <span class="token keyword">pass</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="Start-Crawling"><a href="#Start-Crawling" class="headerlink" title="Start Crawling"></a>Start Crawling</h2><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">open_google_play_reviews</span><span class="token punctuation">(</span>url<span class="token punctuation">)</span><span class="token punctuation">:</span>    driver_path <span class="token operator">=</span> <span class="token string">r"C:\Users\YangWang\Desktop\Text_Classifier_for_UtaPass_and_KKBOX\chromedriver.exe"</span>    driver <span class="token operator">=</span> webdriver<span class="token punctuation">.</span>Chrome<span class="token punctuation">(</span>driver_path<span class="token punctuation">)</span>    time<span class="token punctuation">.</span>sleep<span class="token punctuation">(</span><span class="token number">2</span> <span class="token operator">+</span> random<span class="token punctuation">.</span>random<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    driver<span class="token punctuation">.</span>get<span class="token punctuation">(</span>url<span class="token punctuation">)</span>    time<span class="token punctuation">.</span>sleep<span class="token punctuation">(</span><span class="token number">5</span> <span class="token operator">+</span> random<span class="token punctuation">.</span>random<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        scrollDownPage<span class="token punctuation">(</span><span class="token punctuation">)</span>    soup <span class="token operator">=</span> BeautifulSoup<span class="token punctuation">(</span>driver<span class="token punctuation">.</span>page_source<span class="token punctuation">,</span> <span class="token string">"html.parser"</span><span class="token punctuation">)</span>    time<span class="token punctuation">.</span>sleep<span class="token punctuation">(</span><span class="token number">3</span> <span class="token operator">+</span> random<span class="token punctuation">.</span>random<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    driver<span class="token punctuation">.</span>quit<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> soup    <span class="token keyword">def</span> <span class="token function">convert_soup_to_dataframe</span><span class="token punctuation">(</span>soup<span class="token punctuation">)</span><span class="token punctuation">:</span>    reviews <span class="token operator">=</span> soup<span class="token punctuation">.</span>find<span class="token punctuation">(</span>name<span class="token operator">=</span><span class="token string">"div"</span><span class="token punctuation">,</span> attrs<span class="token operator">=</span><span class="token punctuation">&#123;</span><span class="token string">"jsname"</span><span class="token punctuation">:</span> <span class="token string">"fk8dgd"</span><span class="token punctuation">&#125;</span><span class="token punctuation">)</span>    reviews_list <span class="token operator">=</span> reviews<span class="token punctuation">.</span>find_all<span class="token punctuation">(</span>name<span class="token operator">=</span><span class="token string">"div"</span><span class="token punctuation">,</span> attrs<span class="token operator">=</span><span class="token punctuation">&#123;</span><span class="token string">"jscontroller"</span><span class="token punctuation">:</span> <span class="token string">"H6eOGe"</span><span class="token punctuation">&#125;</span><span class="token punctuation">)</span>    reviews_all <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>reviews_list<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        name <span class="token operator">=</span> reviews_list<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>find<span class="token punctuation">(</span>name<span class="token operator">=</span><span class="token string">"span"</span><span class="token punctuation">,</span> attrs<span class="token operator">=</span><span class="token punctuation">&#123;</span><span class="token string">"class"</span><span class="token punctuation">:</span> <span class="token string">"X43Kjb"</span><span class="token punctuation">&#125;</span><span class="token punctuation">)</span><span class="token punctuation">.</span>string        date <span class="token operator">=</span> reviews_list<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>find<span class="token punctuation">(</span>name<span class="token operator">=</span><span class="token string">"span"</span><span class="token punctuation">,</span> attrs<span class="token operator">=</span><span class="token punctuation">&#123;</span><span class="token string">"class"</span><span class="token punctuation">:</span> <span class="token string">"p2TkOb"</span><span class="token punctuation">&#125;</span><span class="token punctuation">)</span><span class="token punctuation">.</span>string        rating <span class="token operator">=</span> reviews_list<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>find<span class="token punctuation">(</span>name<span class="token operator">=</span><span class="token string">"div"</span><span class="token punctuation">,</span> attrs<span class="token operator">=</span><span class="token punctuation">&#123;</span><span class="token string">"class"</span><span class="token punctuation">:</span> <span class="token string">"pf5lIe"</span><span class="token punctuation">&#125;</span><span class="token punctuation">)</span><span class="token punctuation">.</span>find<span class="token punctuation">(</span>name<span class="token operator">=</span><span class="token string">"div"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token string">"aria-label"</span><span class="token punctuation">)</span>        rating <span class="token operator">=</span> <span class="token builtin">int</span><span class="token punctuation">(</span>rating<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">"/"</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>        content <span class="token operator">=</span> reviews_list<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>find<span class="token punctuation">(</span>name<span class="token operator">=</span><span class="token string">"span"</span><span class="token punctuation">,</span> attrs<span class="token operator">=</span><span class="token punctuation">&#123;</span><span class="token string">"jsname"</span><span class="token punctuation">:</span> <span class="token string">"bN97Pc"</span><span class="token punctuation">&#125;</span><span class="token punctuation">)</span><span class="token punctuation">.</span>string        like <span class="token operator">=</span> reviews_list<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>find<span class="token punctuation">(</span>name<span class="token operator">=</span><span class="token string">"div"</span><span class="token punctuation">,</span> attrs<span class="token operator">=</span><span class="token punctuation">&#123;</span><span class="token string">"class"</span><span class="token punctuation">:</span> <span class="token string">"jUL89d y92BAb"</span><span class="token punctuation">&#125;</span><span class="token punctuation">)</span><span class="token punctuation">.</span>string        reviews_all<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token punctuation">[</span>name<span class="token punctuation">,</span> date<span class="token punctuation">,</span> rating<span class="token punctuation">,</span> content<span class="token punctuation">,</span> like<span class="token punctuation">]</span><span class="token punctuation">)</span>    df <span class="token operator">=</span> pd<span class="token punctuation">.</span>DataFrame<span class="token punctuation">(</span>reviews_all<span class="token punctuation">)</span>    df<span class="token punctuation">.</span>columns <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">"name"</span><span class="token punctuation">,</span> <span class="token string">"date"</span><span class="token punctuation">,</span> <span class="token string">"rating"</span><span class="token punctuation">,</span> <span class="token string">"content"</span><span class="token punctuation">,</span> <span class="token string">"like"</span><span class="token punctuation">]</span>    <span class="token keyword">return</span> df    <span class="token keyword">def</span> <span class="token function">crawl</span><span class="token punctuation">(</span>url<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Parsing soup from url..."</span><span class="token punctuation">)</span>    soup <span class="token operator">=</span> open_google_play_reviews<span class="token punctuation">(</span>url<span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Done parsing soup from url."</span><span class="token punctuation">)</span>    df <span class="token operator">=</span> convert_soup_to_dataframe<span class="token punctuation">(</span>soup<span class="token punctuation">)</span>    <span class="token keyword">return</span> df<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="Data-Storage"><a href="#Data-Storage" class="headerlink" title="Data Storage"></a>Data Storage</h2><p>There are 12498 reviews in total. Take a look at the dataframe.</p><table><thead><tr><th></th><th>Author Name</th><th>Review Date</th><th>Reviewer Ratings</th><th>Review Content</th></tr></thead><tbody><tr><td>195</td><td>眞也大平</td><td>2018年12月4日</td><td>1</td><td>聴い途中止まる強制終了する止まる辞め</td></tr><tr><td>13</td><td>狼音牙</td><td>2019年4月22日</td><td>1</td><td>LISMO使えなっ早くもどせうたパスLISMO使い</td></tr><tr><td>47</td><td>美能孝行</td><td>2019年3月12日</td><td>3</td><td>アルバム曲名読み方登録それ名前反映不具合かなり継続技術改善でき諦め使いあり</td></tr><tr><td>142</td><td>梅川洋子</td><td>2019年2月14日</td><td>4</td><td>いつ聴けるいい</td></tr><tr><td>45</td><td>わんたった</td><td>2019年4月27日</td><td>1</td><td>アンストアプリ残っ</td></tr></tbody></table><h1 id="Main-Code-for-Modelling"><a href="#Main-Code-for-Modelling" class="headerlink" title="Main Code for Modelling"></a>Main Code for Modelling</h1><h2 id="Import-Packages"><a href="#Import-Packages" class="headerlink" title="Import Packages"></a>Import Packages</h2><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> warnings<span class="token keyword">import</span> re<span class="token keyword">import</span> emoji<span class="token keyword">import</span> MeCab<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token keyword">import</span> pandas <span class="token keyword">as</span> pd<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt<span class="token keyword">from</span> collections <span class="token keyword">import</span> Counter<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>feature_extraction<span class="token punctuation">.</span>text <span class="token keyword">import</span> CountVectorizer<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>model_selection <span class="token keyword">import</span> train_test_split<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>metrics <span class="token keyword">import</span> accuracy_score<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>metrics <span class="token keyword">import</span> confusion_matrix<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>metrics <span class="token keyword">import</span> roc_auc_score<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>metrics <span class="token keyword">import</span> precision_score<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>metrics <span class="token keyword">import</span> recall_score<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>metrics <span class="token keyword">import</span> f1_score<span class="token keyword">import</span> tensorflow <span class="token keyword">as</span> tf<span class="token keyword">import</span> keras<span class="token punctuation">.</span>backend <span class="token keyword">as</span> K<span class="token keyword">from</span> keras <span class="token keyword">import</span> regularizers<span class="token keyword">from</span> keras<span class="token punctuation">.</span>models <span class="token keyword">import</span> Sequential<span class="token keyword">from</span> keras<span class="token punctuation">.</span>models <span class="token keyword">import</span> Model<span class="token keyword">from</span> keras<span class="token punctuation">.</span>layers <span class="token keyword">import</span> Input<span class="token keyword">from</span> keras<span class="token punctuation">.</span>layers <span class="token keyword">import</span> InputLayer<span class="token keyword">from</span> keras<span class="token punctuation">.</span>layers <span class="token keyword">import</span> Embedding<span class="token keyword">from</span> keras<span class="token punctuation">.</span>layers <span class="token keyword">import</span> Dense<span class="token keyword">from</span> keras<span class="token punctuation">.</span>layers <span class="token keyword">import</span> Add<span class="token keyword">from</span> keras<span class="token punctuation">.</span>layers <span class="token keyword">import</span> Concatenate<span class="token keyword">from</span> keras<span class="token punctuation">.</span>layers <span class="token keyword">import</span> ZeroPadding1D<span class="token keyword">from</span> keras<span class="token punctuation">.</span>layers <span class="token keyword">import</span> Dropout<span class="token keyword">from</span> keras<span class="token punctuation">.</span>layers <span class="token keyword">import</span> SpatialDropout1D<span class="token keyword">from</span> keras<span class="token punctuation">.</span>layers <span class="token keyword">import</span> Activation<span class="token keyword">from</span> keras<span class="token punctuation">.</span>layers <span class="token keyword">import</span> Conv1D<span class="token keyword">from</span> keras<span class="token punctuation">.</span>layers <span class="token keyword">import</span> MaxPooling1D<span class="token keyword">from</span> keras<span class="token punctuation">.</span>layers <span class="token keyword">import</span> GlobalAveragePooling1D<span class="token keyword">from</span> keras<span class="token punctuation">.</span>layers <span class="token keyword">import</span> GlobalMaxPool1D<span class="token keyword">from</span> keras<span class="token punctuation">.</span>layers <span class="token keyword">import</span> AveragePooling1D<span class="token keyword">from</span> keras<span class="token punctuation">.</span>layers <span class="token keyword">import</span> BatchNormalization<span class="token keyword">from</span> keras<span class="token punctuation">.</span>layers <span class="token keyword">import</span> Flatten<span class="token keyword">from</span> keras<span class="token punctuation">.</span>layers <span class="token keyword">import</span> SimpleRNN<span class="token keyword">from</span> keras<span class="token punctuation">.</span>layers <span class="token keyword">import</span> CuDNNLSTM<span class="token keyword">from</span> keras<span class="token punctuation">.</span>layers <span class="token keyword">import</span> Bidirectional<span class="token keyword">from</span> keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>recurrent <span class="token keyword">import</span> LSTM<span class="token keyword">from</span> keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>recurrent <span class="token keyword">import</span> GRU<span class="token keyword">from</span> keras<span class="token punctuation">.</span>optimizers <span class="token keyword">import</span> Adam<span class="token keyword">from</span> keras<span class="token punctuation">.</span>optimizers <span class="token keyword">import</span> SGD<span class="token keyword">from</span> keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>normalization <span class="token keyword">import</span> BatchNormalization<span class="token keyword">from</span> keras<span class="token punctuation">.</span>preprocessing <span class="token keyword">import</span> text<span class="token keyword">from</span> keras<span class="token punctuation">.</span>preprocessing <span class="token keyword">import</span> sequence<span class="token keyword">from</span> keras<span class="token punctuation">.</span>callbacks <span class="token keyword">import</span> EarlyStopping<span class="token punctuation">,</span> TensorBoard<span class="token punctuation">,</span> ModelCheckpoint<span class="token keyword">from</span> keras<span class="token punctuation">.</span>callbacks <span class="token keyword">import</span> ReduceLROnPlateau<span class="token keyword">from</span> gensim<span class="token punctuation">.</span>models<span class="token punctuation">.</span>word2vec <span class="token keyword">import</span> Word2Vec<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="Load-Data-In"><a href="#Load-Data-In" class="headerlink" title="Load Data In"></a>Load Data In</h2><p>First, split dataframe into two categories: positive and negative. Second, do some text preprocessing. For instance, if rating is lower than 3 stars, label it as negative.</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">df <span class="token operator">=</span> pd<span class="token punctuation">.</span>read_csv<span class="token punctuation">(</span><span class="token string">r"C:\Users\YangWang\Desktop\Text_Classifier_for_UtaPass_and_KKBOX\data\all_20200423.csv"</span><span class="token punctuation">)</span><span class="token comment"># create the label</span>df<span class="token punctuation">[</span><span class="token string">"label"</span><span class="token punctuation">]</span> <span class="token operator">=</span> df<span class="token punctuation">[</span><span class="token string">"rating"</span><span class="token punctuation">]</span><span class="token punctuation">.</span><span class="token builtin">apply</span><span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> <span class="token number">0</span> <span class="token keyword">if</span> <span class="token builtin">int</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span> <span class="token operator">&lt;=</span> <span class="token number">3</span> <span class="token keyword">else</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token comment"># select only relevant columns</span>df <span class="token operator">=</span> df<span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token string">"content"</span><span class="token punctuation">,</span> <span class="token string">"label"</span><span class="token punctuation">]</span><span class="token punctuation">]</span>df<span class="token punctuation">[</span><span class="token string">"content"</span><span class="token punctuation">]</span> <span class="token operator">=</span> df<span class="token punctuation">[</span><span class="token string">"content"</span><span class="token punctuation">]</span><span class="token punctuation">.</span><span class="token builtin">map</span><span class="token punctuation">(</span><span class="token builtin">str</span><span class="token punctuation">)</span>df<span class="token punctuation">.</span>head<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><table><thead><tr><th></th><th>content</th><th>label</th></tr></thead><tbody><tr><td>0</td><td>歌詞見れるいいずれ誤字あるあとお気に入りプレイリスト開くライブラリ更新リセットれるマジ入れラ…</td><td>0</td></tr><tr><td>1</td><td>通知切る方法分かりすぎる見る新たアプリ入れ通知切れ判明アプリ開発若者毎日アクティブ新曲買っ聴…</td><td>0</td></tr><tr><td>2</td><td>どうしてもLISMO比べダウンロード反映LISMO動画一覧表示パス分離とにかく使いLISMO…</td><td>0</td></tr><tr><td>3</td><td>以前購入機種だぶっダウンロードれる消す出来機種するダウンロード出来有るガラケー購入スマ出来有り</td><td>0</td></tr><tr><td>4</td><td>LISMOライブラリ開けなっ愛着あっLISMO使っ消し下らないうたパスLISMOいらついて最…</td><td>0</td></tr></tbody></table><h2 id="Data-Pre-processing"><a href="#Data-Pre-processing" class="headerlink" title="Data Pre-processing"></a>Data Pre-processing</h2><ol><li>Remove emoji.</li><li>Remove punctuation</li><li>Remove digits.</li><li>Tokenise sentence using MeCab.</li></ol><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">create_mecab_list</span><span class="token punctuation">(</span>text<span class="token punctuation">)</span><span class="token punctuation">:</span>    pos_list <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">11</span><span class="token punctuation">,</span> <span class="token number">31</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">34</span><span class="token punctuation">]</span>    pos_list<span class="token punctuation">.</span>extend<span class="token punctuation">(</span><span class="token builtin">list</span><span class="token punctuation">(</span><span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">36</span><span class="token punctuation">,</span><span class="token number">50</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    pos_list<span class="token punctuation">.</span>extend<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">59</span><span class="token punctuation">,</span> <span class="token number">60</span><span class="token punctuation">,</span> <span class="token number">62</span><span class="token punctuation">,</span> <span class="token number">67</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    mecab_list <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    mecab <span class="token operator">=</span> MeCab<span class="token punctuation">.</span>Tagger<span class="token punctuation">(</span><span class="token string">"-Owakati"</span><span class="token punctuation">)</span>    mecab<span class="token punctuation">.</span>parse<span class="token punctuation">(</span><span class="token string">""</span><span class="token punctuation">)</span>    <span class="token comment"># encoding = text.encode('utf-8')</span>    node <span class="token operator">=</span> mecab<span class="token punctuation">.</span>parseToNode<span class="token punctuation">(</span>text<span class="token punctuation">)</span>    <span class="token keyword">while</span> node<span class="token punctuation">:</span>        <span class="token keyword">if</span> <span class="token builtin">len</span><span class="token punctuation">(</span>node<span class="token punctuation">.</span>surface<span class="token punctuation">)</span> <span class="token operator">></span> <span class="token number">1</span><span class="token punctuation">:</span>            <span class="token keyword">if</span> node<span class="token punctuation">.</span>posid <span class="token keyword">in</span> pos_list<span class="token punctuation">:</span>                morpheme <span class="token operator">=</span> node<span class="token punctuation">.</span>surface                mecab_list<span class="token punctuation">.</span>append<span class="token punctuation">(</span>morpheme<span class="token punctuation">)</span>        node <span class="token operator">=</span> node<span class="token punctuation">.</span><span class="token builtin">next</span>    <span class="token keyword">return</span> mecab_list<span class="token keyword">def</span> <span class="token function">give_emoji_free_text</span><span class="token punctuation">(</span>text<span class="token punctuation">)</span><span class="token punctuation">:</span>    allchars <span class="token operator">=</span> <span class="token punctuation">[</span>string <span class="token keyword">for</span> string <span class="token keyword">in</span> text<span class="token punctuation">]</span>    emoji_list <span class="token operator">=</span> <span class="token punctuation">[</span>c <span class="token keyword">for</span> c <span class="token keyword">in</span> allchars <span class="token keyword">if</span> c <span class="token keyword">in</span> emoji<span class="token punctuation">.</span>UNICODE_EMOJI<span class="token punctuation">]</span>    cleaned_text <span class="token operator">=</span> <span class="token string">' '</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token builtin">str</span> <span class="token keyword">for</span> <span class="token builtin">str</span> <span class="token keyword">in</span> text<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">if</span> <span class="token keyword">not</span> <span class="token builtin">any</span><span class="token punctuation">(</span>i <span class="token keyword">in</span> <span class="token builtin">str</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> emoji_list<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> cleaned_text<span class="token keyword">def</span> <span class="token function">clean_text</span><span class="token punctuation">(</span>text<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment"># Remove emoji</span>    text <span class="token operator">=</span> give_emoji_free_text<span class="token punctuation">(</span>text<span class="token punctuation">)</span>    <span class="token comment"># Remove punctuation</span>    text <span class="token operator">=</span> re<span class="token punctuation">.</span>sub<span class="token punctuation">(</span><span class="token string">r'[^\w\d\s]+'</span><span class="token punctuation">,</span> <span class="token string">''</span><span class="token punctuation">,</span> text<span class="token punctuation">)</span>    <span class="token comment"># Remove digits</span>    text <span class="token operator">=</span> <span class="token string">''</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span><span class="token punctuation">[</span>i <span class="token keyword">for</span> i <span class="token keyword">in</span> text <span class="token keyword">if</span> <span class="token keyword">not</span> i<span class="token punctuation">.</span>isdigit<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>     <span class="token comment"># Tokenize the sentence</span>    tokenised_text_list <span class="token operator">=</span> create_mecab_list<span class="token punctuation">(</span>text<span class="token punctuation">)</span>    <span class="token keyword">return</span> tokenised_text_list<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="Main-Code-for-Modelling-1"><a href="#Main-Code-for-Modelling-1" class="headerlink" title="Main Code for Modelling"></a>Main Code for Modelling</h2><h3 id="Set-Parameters"><a href="#Set-Parameters" class="headerlink" title="Set Parameters"></a>Set Parameters</h3><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># Input parameters</span>config <span class="token operator">=</span> <span class="token punctuation">&#123;</span>    <span class="token comment"># Text parameters</span>    <span class="token string">"MAX_FEATURE"</span><span class="token punctuation">:</span> <span class="token number">10000</span><span class="token punctuation">,</span>     <span class="token string">"MAX_LEN"</span><span class="token punctuation">:</span> <span class="token number">64</span><span class="token punctuation">,</span>     <span class="token string">"EMBED_SIZE"</span><span class="token punctuation">:</span> <span class="token number">300</span><span class="token punctuation">,</span>     <span class="token comment"># Convolution parameters</span>    <span class="token string">"filter_length"</span><span class="token punctuation">:</span> <span class="token number">3</span><span class="token punctuation">,</span>     <span class="token string">"nb_filter"</span><span class="token punctuation">:</span> <span class="token number">150</span><span class="token punctuation">,</span>     <span class="token string">"pool_length"</span><span class="token punctuation">:</span> <span class="token number">2</span><span class="token punctuation">,</span>     <span class="token string">"cnn_activation"</span><span class="token punctuation">:</span> <span class="token string">'relu'</span><span class="token punctuation">,</span>     <span class="token string">"border_mode"</span><span class="token punctuation">:</span> <span class="token string">'same'</span><span class="token punctuation">,</span>     <span class="token comment"># RNN parameters</span>    <span class="token string">"lstm_cell"</span><span class="token punctuation">:</span> <span class="token number">128</span><span class="token punctuation">,</span>     <span class="token string">"output_size"</span><span class="token punctuation">:</span> <span class="token number">50</span><span class="token punctuation">,</span>     <span class="token string">"rnn_activation"</span><span class="token punctuation">:</span> <span class="token string">'tanh'</span><span class="token punctuation">,</span>     <span class="token string">"recurrent_activation"</span><span class="token punctuation">:</span> <span class="token string">'hard_sigmoid'</span><span class="token punctuation">,</span>         <span class="token comment"># FC and Dropout</span>    <span class="token string">"fc_cell"</span><span class="token punctuation">:</span> <span class="token number">128</span><span class="token punctuation">,</span>     <span class="token string">"dropout_rate"</span><span class="token punctuation">:</span> <span class="token number">0.25</span><span class="token punctuation">,</span>     <span class="token comment"># Compile parameters</span>    <span class="token string">"loss"</span><span class="token punctuation">:</span> <span class="token string">'binary_crossentropy'</span><span class="token punctuation">,</span>     <span class="token string">"optimizer"</span><span class="token punctuation">:</span> <span class="token string">'adam'</span><span class="token punctuation">,</span>     <span class="token comment"># Training parameters</span>    <span class="token string">"batch_size"</span><span class="token punctuation">:</span> <span class="token number">256</span><span class="token punctuation">,</span>     <span class="token string">"nb_epoch"</span><span class="token punctuation">:</span> <span class="token number">1000</span><span class="token punctuation">,</span>     <span class="token string">"validation_split"</span><span class="token punctuation">:</span> <span class="token number">0.20</span><span class="token punctuation">,</span>     <span class="token string">"shuffle"</span><span class="token punctuation">:</span> <span class="token boolean">True</span><span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="Create-Word-Index"><a href="#Create-Word-Index" class="headerlink" title="Create Word Index"></a>Create Word Index</h3><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">create_word2index_and_index2word</span><span class="token punctuation">(</span>df<span class="token punctuation">)</span><span class="token punctuation">:</span>    df<span class="token punctuation">[</span><span class="token string">"cleaned_text"</span><span class="token punctuation">]</span> <span class="token operator">=</span> df<span class="token punctuation">[</span><span class="token string">"content"</span><span class="token punctuation">]</span><span class="token punctuation">.</span><span class="token builtin">apply</span><span class="token punctuation">(</span>clean_text<span class="token punctuation">)</span>    sum_list <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    <span class="token keyword">for</span> index<span class="token punctuation">,</span> row <span class="token keyword">in</span> df<span class="token punctuation">.</span>iterrows<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        sum_list <span class="token operator">+=</span> row<span class="token punctuation">[</span><span class="token string">"cleaned_text"</span><span class="token punctuation">]</span>    word2index <span class="token operator">=</span> <span class="token builtin">dict</span><span class="token punctuation">(</span><span class="token punctuation">)</span>    index2word <span class="token operator">=</span> <span class="token builtin">dict</span><span class="token punctuation">(</span><span class="token punctuation">)</span>    num_words <span class="token operator">=</span> <span class="token number">0</span>    <span class="token keyword">for</span> word <span class="token keyword">in</span> sum_list<span class="token punctuation">:</span>        <span class="token keyword">if</span> word <span class="token keyword">not</span> <span class="token keyword">in</span> word2index<span class="token punctuation">:</span>            <span class="token comment"># First entry of word into vocabulary</span>            word2index<span class="token punctuation">[</span>word<span class="token punctuation">]</span> <span class="token operator">=</span> num_words            index2word<span class="token punctuation">[</span>num_words<span class="token punctuation">]</span> <span class="token operator">=</span> word            num_words <span class="token operator">+=</span> <span class="token number">1</span>        <span class="token keyword">return</span> word2index<span class="token punctuation">,</span> index2word<span class="token keyword">def</span> <span class="token function">convert_tokens_to_ids</span><span class="token punctuation">(</span>tokens_list<span class="token punctuation">,</span> word2index<span class="token punctuation">)</span><span class="token punctuation">:</span>    ids_list <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    <span class="token keyword">for</span> token <span class="token keyword">in</span> tokens_list<span class="token punctuation">:</span>        <span class="token keyword">if</span> word2index<span class="token punctuation">.</span>get<span class="token punctuation">(</span>token<span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">)</span> <span class="token operator">!=</span> <span class="token boolean">None</span><span class="token punctuation">:</span>            ids_list<span class="token punctuation">.</span>append<span class="token punctuation">(</span>word2index<span class="token punctuation">[</span>token<span class="token punctuation">]</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> ids_list<span class="token keyword">def</span> <span class="token function">remove_empty_ids_rows</span><span class="token punctuation">(</span>df<span class="token punctuation">)</span><span class="token punctuation">:</span>    empty <span class="token operator">=</span> <span class="token punctuation">(</span>df<span class="token punctuation">[</span><span class="token string">'ids'</span><span class="token punctuation">]</span><span class="token punctuation">.</span><span class="token builtin">map</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> df<span class="token punctuation">[</span><span class="token operator">~</span>empty<span class="token punctuation">]</span>word2index<span class="token punctuation">,</span> index2word <span class="token operator">=</span> create_word2index_and_index2word<span class="token punctuation">(</span>df<span class="token punctuation">)</span>df<span class="token punctuation">[</span><span class="token string">"ids"</span><span class="token punctuation">]</span> <span class="token operator">=</span> df<span class="token punctuation">[</span><span class="token string">"content"</span><span class="token punctuation">]</span><span class="token punctuation">.</span><span class="token builtin">apply</span><span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> convert_tokens_to_ids<span class="token punctuation">(</span>clean_text<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">,</span> word2index<span class="token punctuation">)</span><span class="token punctuation">)</span>df <span class="token operator">=</span> remove_empty_ids_rows<span class="token punctuation">(</span>df<span class="token punctuation">)</span>df<span class="token punctuation">.</span>head<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="Split-Data"><a href="#Split-Data" class="headerlink" title="Split Data"></a>Split Data</h3><p>Split the data into training data (80%) and testing data (20%).</p><ul><li>Training set: a subset to train a model</li><li>Testing set: a subset to test the trained model</li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python">x <span class="token operator">=</span> df<span class="token punctuation">[</span><span class="token string">"ids"</span><span class="token punctuation">]</span><span class="token punctuation">.</span><span class="token builtin">map</span><span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>x <span class="token operator">=</span> sequence<span class="token punctuation">.</span>pad_sequences<span class="token punctuation">(</span>x<span class="token punctuation">,</span> maxlen<span class="token operator">=</span>config<span class="token punctuation">[</span><span class="token string">"MAX_LEN"</span><span class="token punctuation">]</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">"post"</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Features: \n"</span><span class="token punctuation">,</span> x<span class="token punctuation">)</span>y <span class="token operator">=</span> df<span class="token punctuation">[</span><span class="token string">"label"</span><span class="token punctuation">]</span><span class="token punctuation">.</span>values<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Labels: \n"</span><span class="token punctuation">,</span> y<span class="token punctuation">)</span>x_train<span class="token punctuation">,</span> x_test<span class="token punctuation">,</span> y_train<span class="token punctuation">,</span> y_test <span class="token operator">=</span> train_test_split<span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">,</span> test_size<span class="token operator">=</span><span class="token number">0.20</span><span class="token punctuation">,</span> random_state<span class="token operator">=</span><span class="token number">17</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="Word-Embedding"><a href="#Word-Embedding" class="headerlink" title="Word Embedding"></a>Word Embedding</h3><p>Build the word2vec model to do word embedding.</p><p>Reference:</p><ul><li><a href="https://github.com/philipperemy/japanese-words-to-vectors/blob/master/README.md">https://github.com/philipperemy/japanese-words-to-vectors/blob/master/README.md</a>)</li><li><a href="http://jalammar.github.io/illustrated-word2vec/">http://jalammar.github.io/illustrated-word2vec/</a></li></ul><p>Training a Japanese Wikipedia Word2Vec Model by Gensim and Mecab:</p><ul><li>Kyubyong Park’s <a href="https://github.com/Kyubyong/wordvectors">GitHub</a></li><li>Omuram’s <a href="https://qiita.com/omuram/items/6570973c090c6f0cb060">Qiita</a></li><li>TextMiner’s <a href="https://textminingonline.com/training-a-japanese-wikipedia-word2vec-model-by-gensim-and-mecab">Website</a></li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">get_embedding_index</span><span class="token punctuation">(</span>model_path<span class="token punctuation">)</span><span class="token punctuation">:</span>    w2v <span class="token operator">=</span> Word2Vec<span class="token punctuation">.</span>load<span class="token punctuation">(</span>model_path<span class="token punctuation">)</span>    embedding_index <span class="token operator">=</span> <span class="token punctuation">&#123;</span><span class="token punctuation">&#125;</span>    <span class="token keyword">for</span> word <span class="token keyword">in</span> w2v<span class="token punctuation">.</span>wv<span class="token punctuation">.</span>vocab<span class="token punctuation">:</span>        embedding_index<span class="token punctuation">[</span>word<span class="token punctuation">]</span> <span class="token operator">=</span> w2v<span class="token punctuation">.</span>wv<span class="token punctuation">.</span>word_vec<span class="token punctuation">(</span>word<span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'Loaded &#123;&#125; word vectors.'</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>embedding_index<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> embedding_index<span class="token keyword">def</span> <span class="token function">get_embedding_matrix</span><span class="token punctuation">(</span>word2index<span class="token punctuation">,</span> embeddings_index<span class="token punctuation">,</span> embed_size<span class="token punctuation">)</span><span class="token punctuation">:</span>    embedding_matrix <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>word2index<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">,</span> embed_size<span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">for</span> word<span class="token punctuation">,</span> i <span class="token keyword">in</span> word2index<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        embedding_vector <span class="token operator">=</span> embeddings_index<span class="token punctuation">.</span>get<span class="token punctuation">(</span>word<span class="token punctuation">)</span>        <span class="token keyword">if</span> embedding_vector <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>            <span class="token comment"># words found in embedding index will be pretrained vectors.</span>            embedding_matrix<span class="token punctuation">[</span>i<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">=</span> embedding_vector        <span class="token keyword">else</span><span class="token punctuation">:</span>            <span class="token comment"># words not found in embedding index will be random vectors with certain mean&amp;std.</span>            embedding_matrix<span class="token punctuation">[</span>i<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>normal<span class="token punctuation">(</span><span class="token number">0.053</span><span class="token punctuation">,</span> <span class="token number">0.3146</span><span class="token punctuation">,</span> size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> embed_size<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>    <span class="token comment"># save embedding matrix</span>    <span class="token comment"># embed_df = pd.DataFrame(embedding_matrix)</span>    <span class="token comment"># embed_df.to_csv(self.path_embedding_matrix, header=None, sep=' ')</span>    <span class="token keyword">return</span> embedding_matrixembedding_index <span class="token operator">=</span> get_embedding_index<span class="token punctuation">(</span>    <span class="token string">r'C:\Users\YangWang\Desktop\Text_Classifier_for_UtaPass_and_KKBOX\word2vec\ja.bin'</span><span class="token punctuation">)</span>embedding_matrix <span class="token operator">=</span> get_embedding_matrix<span class="token punctuation">(</span>word2index<span class="token punctuation">,</span> embedding_index<span class="token punctuation">,</span> embed_size<span class="token operator">=</span><span class="token number">300</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="Build-Model"><a href="#Build-Model" class="headerlink" title="Build Model"></a>Build Model</h3><p>Construct neural network architectures.</p><p>Reference:</p><ul><li>Asanilta Fahda’s <a href="https://github.com/asanilta/amazon-sentiment-keras-experiment">GitHub</a></li><li>teratsyk’s <a href="https://github.com/teratsyk/bokete-ai">GitHub</a></li></ul><p>Build a customised metrics function to record f1 value after each epoch.</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">get_f1</span><span class="token punctuation">(</span>y_true<span class="token punctuation">,</span> y_pred<span class="token punctuation">)</span><span class="token punctuation">:</span>    true_positives <span class="token operator">=</span> K<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>K<span class="token punctuation">.</span><span class="token builtin">round</span><span class="token punctuation">(</span>K<span class="token punctuation">.</span>clip<span class="token punctuation">(</span>y_true <span class="token operator">*</span> y_pred<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    possible_positives <span class="token operator">=</span> K<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>K<span class="token punctuation">.</span><span class="token builtin">round</span><span class="token punctuation">(</span>K<span class="token punctuation">.</span>clip<span class="token punctuation">(</span>y_true<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    predicted_positives <span class="token operator">=</span> K<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>K<span class="token punctuation">.</span><span class="token builtin">round</span><span class="token punctuation">(</span>K<span class="token punctuation">.</span>clip<span class="token punctuation">(</span>y_pred<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    precision <span class="token operator">=</span> true_positives <span class="token operator">/</span> <span class="token punctuation">(</span>predicted_positives <span class="token operator">+</span> K<span class="token punctuation">.</span>epsilon<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    recall <span class="token operator">=</span> true_positives <span class="token operator">/</span> <span class="token punctuation">(</span>possible_positives <span class="token operator">+</span> K<span class="token punctuation">.</span>epsilon<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    f1_val <span class="token operator">=</span> <span class="token number">2</span><span class="token operator">*</span><span class="token punctuation">(</span>precision<span class="token operator">*</span>recall<span class="token punctuation">)</span><span class="token operator">/</span><span class="token punctuation">(</span>precision<span class="token operator">+</span>recall<span class="token operator">+</span>K<span class="token punctuation">.</span>epsilon<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> f1_val<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>When training a neural network, f1 score is an important metric to evaluate the performance of classification models, especially for unbalanced classes where the binary accuracy is useless. So I biuld two helper functions <code>get_f1</code> and <code>predict</code>.</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">predict</span><span class="token punctuation">(</span>sentences<span class="token punctuation">,</span> model<span class="token punctuation">)</span><span class="token punctuation">:</span>    y_prob <span class="token operator">=</span> model<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>x_test<span class="token punctuation">)</span>    y_prob <span class="token operator">=</span> y_prob<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token punctuation">)</span>    y_pred <span class="token operator">=</span> <span class="token punctuation">(</span>y_prob <span class="token operator">></span> <span class="token number">0.5</span><span class="token punctuation">)</span>     <span class="token keyword">return</span> y_pred<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>Set optimisers to update gradient.</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">adagrad <span class="token operator">=</span> Adagrad<span class="token punctuation">(</span>learning_rate<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">)</span>adadelta <span class="token operator">=</span> Adadelta<span class="token punctuation">(</span>learning_rate<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">,</span> rho<span class="token operator">=</span><span class="token number">0.95</span><span class="token punctuation">)</span>rmsprop <span class="token operator">=</span> RMSprop<span class="token punctuation">(</span>learning_rate<span class="token operator">=</span><span class="token number">0.001</span><span class="token punctuation">,</span> rho<span class="token operator">=</span><span class="token number">0.9</span><span class="token punctuation">)</span>nadam <span class="token operator">=</span> Nadam<span class="token punctuation">(</span>learning_rate<span class="token operator">=</span><span class="token number">0.001</span><span class="token punctuation">,</span> beta_1<span class="token operator">=</span><span class="token number">0.9</span><span class="token punctuation">,</span> beta_2<span class="token operator">=</span><span class="token number">0.999</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><h4 id="Simple-RNN"><a href="#Simple-RNN" class="headerlink" title="Simple RNN"></a>Simple RNN</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">train_simple_rnn</span><span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> y_train<span class="token punctuation">,</span> wv_matrix<span class="token punctuation">,</span> verbose<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> load_weights<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>backend<span class="token punctuation">.</span>clear_session<span class="token punctuation">(</span><span class="token punctuation">)</span>        x_input <span class="token operator">=</span> Input<span class="token punctuation">(</span>shape<span class="token operator">=</span><span class="token punctuation">(</span>config<span class="token punctuation">[</span><span class="token string">"MAX_LEN"</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">)</span><span class="token punctuation">)</span>    x <span class="token operator">=</span> Embedding<span class="token punctuation">(</span>wv_matrix<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> wv_matrix<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> weights<span class="token operator">=</span><span class="token punctuation">[</span>wv_matrix<span class="token punctuation">]</span><span class="token punctuation">,</span> trainable<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x_input<span class="token punctuation">)</span>    x <span class="token operator">=</span> SpatialDropout1D<span class="token punctuation">(</span>config<span class="token punctuation">[</span><span class="token string">'dropout_rate'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>    x <span class="token operator">=</span> SimpleRNN<span class="token punctuation">(</span>        units<span class="token operator">=</span>config<span class="token punctuation">[</span><span class="token string">"output_size"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         activation<span class="token operator">=</span>config<span class="token punctuation">[</span><span class="token string">"rnn_activation"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         kernel_regularizer<span class="token operator">=</span>regularizers<span class="token punctuation">.</span>l2<span class="token punctuation">(</span><span class="token number">0.3</span><span class="token punctuation">)</span><span class="token punctuation">,</span>         return_sequences<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>    x <span class="token operator">=</span> SimpleRNN<span class="token punctuation">(</span>        units<span class="token operator">=</span>config<span class="token punctuation">[</span><span class="token string">"output_size"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         activation<span class="token operator">=</span>config<span class="token punctuation">[</span><span class="token string">"rnn_activation"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         kernel_regularizer<span class="token operator">=</span>regularizers<span class="token punctuation">.</span>l2<span class="token punctuation">(</span><span class="token number">0.3</span><span class="token punctuation">)</span><span class="token punctuation">,</span>         return_sequences<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>    x <span class="token operator">=</span> Dropout<span class="token punctuation">(</span>config<span class="token punctuation">[</span><span class="token string">'dropout_rate'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>    x <span class="token operator">=</span> Dense<span class="token punctuation">(</span>units<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>    x <span class="token operator">=</span> Activation<span class="token punctuation">(</span><span class="token string">'sigmoid'</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>    model <span class="token operator">=</span> Model<span class="token punctuation">(</span>inputs<span class="token operator">=</span>x_input<span class="token punctuation">,</span> outputs<span class="token operator">=</span>x<span class="token punctuation">)</span>    <span class="token keyword">if</span> <span class="token keyword">not</span> load_weights<span class="token punctuation">:</span>         model<span class="token punctuation">.</span><span class="token builtin">compile</span><span class="token punctuation">(</span>loss<span class="token operator">=</span>config<span class="token punctuation">[</span><span class="token string">"loss"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>                       optimizer<span class="token operator">=</span>adagrad<span class="token punctuation">,</span>                       metrics<span class="token operator">=</span><span class="token punctuation">[</span>get_f1<span class="token punctuation">]</span><span class="token punctuation">)</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"="</span><span class="token operator">*</span><span class="token number">20</span><span class="token punctuation">,</span> <span class="token string">"Start Training Simple RNN"</span><span class="token punctuation">,</span> <span class="token string">"="</span><span class="token operator">*</span><span class="token number">20</span><span class="token punctuation">)</span>        path <span class="token operator">=</span> <span class="token string">r'C:\Users\YangWang\Desktop\UtaPass_KKBOX_Classifier\weights\rnn_weights.hdf5'</span>        model_checkpoint <span class="token operator">=</span> ModelCheckpoint<span class="token punctuation">(</span>path<span class="token punctuation">,</span> monitor<span class="token operator">=</span><span class="token string">'val_loss'</span><span class="token punctuation">,</span> verbose<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> save_best_only<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> mode<span class="token operator">=</span><span class="token string">'auto'</span><span class="token punctuation">)</span>        early_stopping <span class="token operator">=</span> EarlyStopping<span class="token punctuation">(</span>monitor<span class="token operator">=</span><span class="token string">'val_loss'</span><span class="token punctuation">,</span> patience<span class="token operator">=</span><span class="token number">20</span><span class="token punctuation">,</span> verbose<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> mode<span class="token operator">=</span><span class="token string">'auto'</span><span class="token punctuation">)</span>        reduce_lr <span class="token operator">=</span> ReduceLROnPlateau<span class="token punctuation">(</span>monitor<span class="token operator">=</span><span class="token string">'val_loss'</span><span class="token punctuation">,</span> factor<span class="token operator">=</span><span class="token number">0.2</span><span class="token punctuation">,</span> patience<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">,</span> min_lr<span class="token operator">=</span><span class="token number">0.001</span><span class="token punctuation">)</span>        history <span class="token operator">=</span> model<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>            x_train<span class="token punctuation">,</span> y_train<span class="token punctuation">,</span>             batch_size<span class="token operator">=</span>config<span class="token punctuation">[</span><span class="token string">'batch_size'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>             epochs<span class="token operator">=</span>config<span class="token punctuation">[</span><span class="token string">'nb_epoch'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>             validation_split<span class="token operator">=</span>config<span class="token punctuation">[</span><span class="token string">'validation_split'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>             shuffle<span class="token operator">=</span>config<span class="token punctuation">[</span><span class="token string">'shuffle'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>             verbose<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span>             callbacks<span class="token operator">=</span><span class="token punctuation">[</span>early_stopping<span class="token punctuation">,</span> reduce_lr<span class="token punctuation">,</span> model_checkpoint<span class="token punctuation">]</span><span class="token punctuation">)</span>    <span class="token keyword">if</span> load_weights<span class="token punctuation">:</span>        history <span class="token operator">=</span> <span class="token boolean">None</span>        path <span class="token operator">=</span> <span class="token string">r'C:\Users\YangWang\Desktop\UtaPass_KKBOX_Classifier\weights\rnn_weights.hdf5'</span>        model<span class="token punctuation">.</span>load_weights<span class="token punctuation">(</span>path<span class="token punctuation">)</span>        <span class="token keyword">return</span> history<span class="token punctuation">,</span> model<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="GRU"><a href="#GRU" class="headerlink" title="GRU"></a>GRU</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">train_gru</span><span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> y_train<span class="token punctuation">,</span> wv_matrix<span class="token punctuation">,</span> verbose<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> load_weights<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>backend<span class="token punctuation">.</span>clear_session<span class="token punctuation">(</span><span class="token punctuation">)</span>        x_input <span class="token operator">=</span> Input<span class="token punctuation">(</span>shape<span class="token operator">=</span><span class="token punctuation">(</span>config<span class="token punctuation">[</span><span class="token string">"MAX_LEN"</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">)</span><span class="token punctuation">)</span>    x <span class="token operator">=</span> Embedding<span class="token punctuation">(</span>wv_matrix<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> wv_matrix<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> weights<span class="token operator">=</span><span class="token punctuation">[</span>wv_matrix<span class="token punctuation">]</span><span class="token punctuation">,</span> trainable<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x_input<span class="token punctuation">)</span>    x <span class="token operator">=</span> SpatialDropout1D<span class="token punctuation">(</span>config<span class="token punctuation">[</span><span class="token string">'dropout_rate'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>    x <span class="token operator">=</span> GRU<span class="token punctuation">(</span>units<span class="token operator">=</span>config<span class="token punctuation">[</span><span class="token string">"output_size"</span><span class="token punctuation">]</span><span class="token punctuation">,</span> return_sequences<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> dropout<span class="token operator">=</span>config<span class="token punctuation">[</span><span class="token string">'dropout_rate'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>    x <span class="token operator">=</span> GRU<span class="token punctuation">(</span>units<span class="token operator">=</span>config<span class="token punctuation">[</span><span class="token string">"output_size"</span><span class="token punctuation">]</span><span class="token punctuation">,</span> return_sequences<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> dropout<span class="token operator">=</span>config<span class="token punctuation">[</span><span class="token string">'dropout_rate'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>    x <span class="token operator">=</span> Dropout<span class="token punctuation">(</span>config<span class="token punctuation">[</span><span class="token string">'dropout_rate'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>    x <span class="token operator">=</span> Dense<span class="token punctuation">(</span>units<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>    x <span class="token operator">=</span> Activation<span class="token punctuation">(</span><span class="token string">'sigmoid'</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>    model <span class="token operator">=</span> Model<span class="token punctuation">(</span>inputs<span class="token operator">=</span>x_input<span class="token punctuation">,</span> outputs<span class="token operator">=</span>x<span class="token punctuation">)</span>    <span class="token keyword">if</span> <span class="token keyword">not</span> load_weights<span class="token punctuation">:</span>        model<span class="token punctuation">.</span><span class="token builtin">compile</span><span class="token punctuation">(</span>loss<span class="token operator">=</span>config<span class="token punctuation">[</span><span class="token string">"loss"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>                       optimizer<span class="token operator">=</span>rmsprop<span class="token punctuation">,</span>                       metrics<span class="token operator">=</span><span class="token punctuation">[</span>get_f1<span class="token punctuation">]</span><span class="token punctuation">)</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"="</span><span class="token operator">*</span><span class="token number">20</span><span class="token punctuation">,</span> <span class="token string">"Start Training GRU"</span><span class="token punctuation">,</span> <span class="token string">"="</span><span class="token operator">*</span><span class="token number">20</span><span class="token punctuation">)</span>        path <span class="token operator">=</span> <span class="token string">r'C:\Users\YangWang\Desktop\UtaPass_KKBOX_Classifier\weights\gru_weights.hdf5'</span>        model_checkpoint <span class="token operator">=</span> ModelCheckpoint<span class="token punctuation">(</span>path<span class="token punctuation">,</span> monitor<span class="token operator">=</span><span class="token string">'val_loss'</span><span class="token punctuation">,</span> verbose<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> save_best_only<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> mode<span class="token operator">=</span><span class="token string">'auto'</span><span class="token punctuation">)</span>        early_stopping <span class="token operator">=</span> EarlyStopping<span class="token punctuation">(</span>monitor<span class="token operator">=</span><span class="token string">'val_loss'</span><span class="token punctuation">,</span> patience<span class="token operator">=</span><span class="token number">20</span><span class="token punctuation">,</span> verbose<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> mode<span class="token operator">=</span><span class="token string">'auto'</span><span class="token punctuation">)</span>        reduce_lr <span class="token operator">=</span> ReduceLROnPlateau<span class="token punctuation">(</span>monitor<span class="token operator">=</span><span class="token string">'val_loss'</span><span class="token punctuation">,</span> factor<span class="token operator">=</span><span class="token number">0.2</span><span class="token punctuation">,</span> patience<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">,</span> min_lr<span class="token operator">=</span><span class="token number">0.001</span><span class="token punctuation">)</span>        history <span class="token operator">=</span> model<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>            x_train<span class="token punctuation">,</span> y_train<span class="token punctuation">,</span>             batch_size<span class="token operator">=</span>config<span class="token punctuation">[</span><span class="token string">'batch_size'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>             epochs<span class="token operator">=</span>config<span class="token punctuation">[</span><span class="token string">'nb_epoch'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>             validation_split<span class="token operator">=</span>config<span class="token punctuation">[</span><span class="token string">'validation_split'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>             shuffle<span class="token operator">=</span>config<span class="token punctuation">[</span><span class="token string">'shuffle'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>             verbose<span class="token operator">=</span>verbose<span class="token punctuation">,</span>             callbacks<span class="token operator">=</span><span class="token punctuation">[</span>early_stopping<span class="token punctuation">,</span> reduce_lr<span class="token punctuation">,</span> model_checkpoint<span class="token punctuation">]</span><span class="token punctuation">)</span>    <span class="token keyword">if</span> load_weights<span class="token punctuation">:</span>        history <span class="token operator">=</span> <span class="token boolean">None</span>        path <span class="token operator">=</span> <span class="token string">r'C:\Users\YangWang\Desktop\UtaPass_KKBOX_Classifier\weights\gru_weights.hdf5'</span>        model<span class="token punctuation">.</span>load_weights<span class="token punctuation">(</span>path<span class="token punctuation">)</span>        <span class="token keyword">return</span> history<span class="token punctuation">,</span> model<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">train_lstm</span><span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> y_train<span class="token punctuation">,</span> wv_matrix<span class="token punctuation">,</span> verbose<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> load_weights<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>backend<span class="token punctuation">.</span>clear_session<span class="token punctuation">(</span><span class="token punctuation">)</span>        x_input <span class="token operator">=</span> Input<span class="token punctuation">(</span>shape<span class="token operator">=</span><span class="token punctuation">(</span>config<span class="token punctuation">[</span><span class="token string">"MAX_LEN"</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">)</span><span class="token punctuation">)</span>    x <span class="token operator">=</span> Embedding<span class="token punctuation">(</span>wv_matrix<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> wv_matrix<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> weights<span class="token operator">=</span><span class="token punctuation">[</span>wv_matrix<span class="token punctuation">]</span><span class="token punctuation">,</span> trainable<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x_input<span class="token punctuation">)</span>    x <span class="token operator">=</span> SpatialDropout1D<span class="token punctuation">(</span>config<span class="token punctuation">[</span><span class="token string">'dropout_rate'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>    x <span class="token operator">=</span> LSTM<span class="token punctuation">(</span>units<span class="token operator">=</span>config<span class="token punctuation">[</span><span class="token string">'lstm_cell'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> return_sequences<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> kernel_regularizer<span class="token operator">=</span>regularizers<span class="token punctuation">.</span>l2<span class="token punctuation">(</span><span class="token number">0.3</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>    x <span class="token operator">=</span> LSTM<span class="token punctuation">(</span>units<span class="token operator">=</span>config<span class="token punctuation">[</span><span class="token string">'lstm_cell'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> return_sequences<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> kernel_regularizer<span class="token operator">=</span>regularizers<span class="token punctuation">.</span>l2<span class="token punctuation">(</span><span class="token number">0.3</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>    x <span class="token operator">=</span> Dropout<span class="token punctuation">(</span>config<span class="token punctuation">[</span><span class="token string">'dropout_rate'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>    x <span class="token operator">=</span> Dense<span class="token punctuation">(</span>units<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>    x <span class="token operator">=</span> Activation<span class="token punctuation">(</span><span class="token string">'sigmoid'</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>    model <span class="token operator">=</span> Model<span class="token punctuation">(</span>inputs<span class="token operator">=</span>x_input<span class="token punctuation">,</span> outputs<span class="token operator">=</span>x<span class="token punctuation">)</span>    <span class="token keyword">if</span> <span class="token keyword">not</span> load_weights<span class="token punctuation">:</span>         model<span class="token punctuation">.</span><span class="token builtin">compile</span><span class="token punctuation">(</span>loss<span class="token operator">=</span>config<span class="token punctuation">[</span><span class="token string">"loss"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>                       optimizer<span class="token operator">=</span>adagrad<span class="token punctuation">,</span>                       metrics<span class="token operator">=</span><span class="token punctuation">[</span>get_f1<span class="token punctuation">]</span><span class="token punctuation">)</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"="</span><span class="token operator">*</span><span class="token number">20</span><span class="token punctuation">,</span> <span class="token string">"Start Training LSTM"</span><span class="token punctuation">,</span> <span class="token string">"="</span><span class="token operator">*</span><span class="token number">20</span><span class="token punctuation">)</span>        path <span class="token operator">=</span> <span class="token string">r'C:\Users\YangWang\Desktop\Text_Classifier_for_UtaPass_and_KKBOX\weights\lstm_weights.hdf5'</span>        model_checkpoint <span class="token operator">=</span> ModelCheckpoint<span class="token punctuation">(</span>path<span class="token punctuation">,</span> monitor<span class="token operator">=</span><span class="token string">'val_loss'</span><span class="token punctuation">,</span> verbose<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> save_best_only<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> mode<span class="token operator">=</span><span class="token string">'auto'</span><span class="token punctuation">)</span>        early_stopping <span class="token operator">=</span> EarlyStopping<span class="token punctuation">(</span>monitor<span class="token operator">=</span><span class="token string">'val_loss'</span><span class="token punctuation">,</span> patience<span class="token operator">=</span><span class="token number">20</span><span class="token punctuation">,</span> verbose<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> mode<span class="token operator">=</span><span class="token string">'auto'</span><span class="token punctuation">)</span>        reduce_lr <span class="token operator">=</span> ReduceLROnPlateau<span class="token punctuation">(</span>monitor<span class="token operator">=</span><span class="token string">'val_loss'</span><span class="token punctuation">,</span> factor<span class="token operator">=</span><span class="token number">0.2</span><span class="token punctuation">,</span> patience<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">,</span> min_lr<span class="token operator">=</span><span class="token number">0.001</span><span class="token punctuation">)</span>        history <span class="token operator">=</span> model<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>            x_train<span class="token punctuation">,</span> y_train<span class="token punctuation">,</span>             batch_size<span class="token operator">=</span>config<span class="token punctuation">[</span><span class="token string">'batch_size'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>             epochs<span class="token operator">=</span>config<span class="token punctuation">[</span><span class="token string">'nb_epoch'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>             validation_split<span class="token operator">=</span>config<span class="token punctuation">[</span><span class="token string">'validation_split'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>             shuffle<span class="token operator">=</span>config<span class="token punctuation">[</span><span class="token string">'shuffle'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>             verbose<span class="token operator">=</span>verbose<span class="token punctuation">,</span>             callbacks<span class="token operator">=</span><span class="token punctuation">[</span>early_stopping<span class="token punctuation">,</span> reduce_lr<span class="token punctuation">,</span> model_checkpoint<span class="token punctuation">]</span><span class="token punctuation">)</span>    <span class="token keyword">if</span> load_weights<span class="token punctuation">:</span>        history <span class="token operator">=</span> <span class="token boolean">None</span>        path <span class="token operator">=</span> <span class="token string">r'C:\Users\YangWang\Desktop\Text_Classifier_for_UtaPass_and_KKBOX\weights\lstm_weights.hdf5'</span>        model<span class="token punctuation">.</span>load_weights<span class="token punctuation">(</span>path<span class="token punctuation">)</span>        <span class="token keyword">return</span> history<span class="token punctuation">,</span> model<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="BiLSTM"><a href="#BiLSTM" class="headerlink" title="BiLSTM"></a>BiLSTM</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">train_bilstm</span><span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> y_train<span class="token punctuation">,</span> wv_matrix<span class="token punctuation">,</span> verbose<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> load_weights<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>backend<span class="token punctuation">.</span>clear_session<span class="token punctuation">(</span><span class="token punctuation">)</span>        x_input <span class="token operator">=</span> Input<span class="token punctuation">(</span>shape<span class="token operator">=</span><span class="token punctuation">(</span>config<span class="token punctuation">[</span><span class="token string">"MAX_LEN"</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">)</span><span class="token punctuation">)</span>    x <span class="token operator">=</span> Embedding<span class="token punctuation">(</span>wv_matrix<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> wv_matrix<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> weights<span class="token operator">=</span><span class="token punctuation">[</span>wv_matrix<span class="token punctuation">]</span><span class="token punctuation">,</span> trainable<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x_input<span class="token punctuation">)</span>    x <span class="token operator">=</span> SpatialDropout1D<span class="token punctuation">(</span>config<span class="token punctuation">[</span><span class="token string">'dropout_rate'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>    x <span class="token operator">=</span> Bidirectional<span class="token punctuation">(</span>LSTM<span class="token punctuation">(</span>        units<span class="token operator">=</span>config<span class="token punctuation">[</span><span class="token string">'lstm_cell'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         return_sequences<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>         dropout<span class="token operator">=</span>config<span class="token punctuation">[</span><span class="token string">'dropout_rate'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         kernel_regularizer<span class="token operator">=</span>regularizers<span class="token punctuation">.</span>l2<span class="token punctuation">(</span><span class="token number">0.3</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>    x <span class="token operator">=</span> Bidirectional<span class="token punctuation">(</span>LSTM<span class="token punctuation">(</span>        units<span class="token operator">=</span>config<span class="token punctuation">[</span><span class="token string">'lstm_cell'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         return_sequences<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>         kernel_regularizer<span class="token operator">=</span>regularizers<span class="token punctuation">.</span>l2<span class="token punctuation">(</span><span class="token number">0.3</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>    x <span class="token operator">=</span> Dropout<span class="token punctuation">(</span>config<span class="token punctuation">[</span><span class="token string">'dropout_rate'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>    x <span class="token operator">=</span> Dense<span class="token punctuation">(</span>config<span class="token punctuation">[</span><span class="token string">'fc_cell'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>    x <span class="token operator">=</span> Dropout<span class="token punctuation">(</span>config<span class="token punctuation">[</span><span class="token string">'dropout_rate'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>    x <span class="token operator">=</span> Dense<span class="token punctuation">(</span>units<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>    x <span class="token operator">=</span> Activation<span class="token punctuation">(</span><span class="token string">'sigmoid'</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>    model <span class="token operator">=</span> Model<span class="token punctuation">(</span>inputs<span class="token operator">=</span>x_input<span class="token punctuation">,</span> outputs<span class="token operator">=</span>x<span class="token punctuation">)</span>    <span class="token keyword">if</span> <span class="token keyword">not</span> load_weights<span class="token punctuation">:</span>        model<span class="token punctuation">.</span><span class="token builtin">compile</span><span class="token punctuation">(</span>loss<span class="token operator">=</span>config<span class="token punctuation">[</span><span class="token string">"loss"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>                       optimizer<span class="token operator">=</span>adagrad<span class="token punctuation">,</span>                       metrics<span class="token operator">=</span><span class="token punctuation">[</span>get_f1<span class="token punctuation">]</span><span class="token punctuation">)</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"="</span><span class="token operator">*</span><span class="token number">20</span><span class="token punctuation">,</span> <span class="token string">"Start Training BiLSTM"</span><span class="token punctuation">,</span> <span class="token string">"="</span><span class="token operator">*</span><span class="token number">20</span><span class="token punctuation">)</span>        path <span class="token operator">=</span> <span class="token string">r'C:\Users\YangWang\Desktop\Text_Classifier_for_UtaPass_and_KKBOX\weights\bilstm_weights.hdf5'</span>        model_checkpoint <span class="token operator">=</span> ModelCheckpoint<span class="token punctuation">(</span>path<span class="token punctuation">,</span> monitor<span class="token operator">=</span><span class="token string">'val_loss'</span><span class="token punctuation">,</span> verbose<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> save_best_only<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> mode<span class="token operator">=</span><span class="token string">'auto'</span><span class="token punctuation">)</span>        early_stopping <span class="token operator">=</span> EarlyStopping<span class="token punctuation">(</span>monitor<span class="token operator">=</span><span class="token string">'val_loss'</span><span class="token punctuation">,</span> patience<span class="token operator">=</span><span class="token number">20</span><span class="token punctuation">,</span> verbose<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> mode<span class="token operator">=</span><span class="token string">'auto'</span><span class="token punctuation">)</span>        reduce_lr <span class="token operator">=</span> ReduceLROnPlateau<span class="token punctuation">(</span>monitor<span class="token operator">=</span><span class="token string">'val_loss'</span><span class="token punctuation">,</span> factor<span class="token operator">=</span><span class="token number">0.2</span><span class="token punctuation">,</span> patience<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">,</span> min_lr<span class="token operator">=</span><span class="token number">0.001</span><span class="token punctuation">)</span>        history <span class="token operator">=</span> model<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>            x_train<span class="token punctuation">,</span> y_train<span class="token punctuation">,</span>             batch_size<span class="token operator">=</span>config<span class="token punctuation">[</span><span class="token string">'batch_size'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>             epochs<span class="token operator">=</span>config<span class="token punctuation">[</span><span class="token string">'nb_epoch'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>             validation_split<span class="token operator">=</span>config<span class="token punctuation">[</span><span class="token string">'validation_split'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>             shuffle<span class="token operator">=</span>config<span class="token punctuation">[</span><span class="token string">'shuffle'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>             verbose<span class="token operator">=</span>verbose<span class="token punctuation">,</span>             callbacks<span class="token operator">=</span><span class="token punctuation">[</span>early_stopping<span class="token punctuation">,</span> reduce_lr<span class="token punctuation">,</span> model_checkpoint<span class="token punctuation">]</span><span class="token punctuation">)</span>    <span class="token keyword">if</span> load_weights<span class="token punctuation">:</span>        history <span class="token operator">=</span> <span class="token boolean">None</span>        path <span class="token operator">=</span> <span class="token string">r'C:\Users\YangWang\Desktop\Text_Classifier_for_UtaPass_and_KKBOX\weights\bilstm_weights.hdf5'</span>        model<span class="token punctuation">.</span>load_weights<span class="token punctuation">(</span>path<span class="token punctuation">)</span>        <span class="token keyword">return</span> history<span class="token punctuation">,</span> model<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">train_attention</span><span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> y_train<span class="token punctuation">,</span> wv_matrix<span class="token punctuation">,</span> verbose<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> load_weights<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>backend<span class="token punctuation">.</span>clear_session<span class="token punctuation">(</span><span class="token punctuation">)</span>        x_input <span class="token operator">=</span> Input<span class="token punctuation">(</span>shape<span class="token operator">=</span><span class="token punctuation">(</span>config<span class="token punctuation">[</span><span class="token string">"MAX_LEN"</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">)</span><span class="token punctuation">)</span>    x <span class="token operator">=</span> Embedding<span class="token punctuation">(</span>wv_matrix<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> wv_matrix<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> weights<span class="token operator">=</span><span class="token punctuation">[</span>wv_matrix<span class="token punctuation">]</span><span class="token punctuation">,</span> trainable<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x_input<span class="token punctuation">)</span>    x <span class="token operator">=</span> SpatialDropout1D<span class="token punctuation">(</span>config<span class="token punctuation">[</span><span class="token string">'dropout_rate'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>    x <span class="token operator">=</span> SeqSelfAttention<span class="token punctuation">(</span>units<span class="token operator">=</span><span class="token number">128</span><span class="token punctuation">,</span> kernel_regularizer<span class="token operator">=</span>regularizers<span class="token punctuation">.</span>l2<span class="token punctuation">(</span><span class="token number">0.1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>    x <span class="token operator">=</span> SeqSelfAttention<span class="token punctuation">(</span>units<span class="token operator">=</span><span class="token number">128</span><span class="token punctuation">,</span> kernel_regularizer<span class="token operator">=</span>regularizers<span class="token punctuation">.</span>l2<span class="token punctuation">(</span><span class="token number">0.1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>    x <span class="token operator">=</span> SeqSelfAttention<span class="token punctuation">(</span>units<span class="token operator">=</span><span class="token number">128</span><span class="token punctuation">,</span> kernel_regularizer<span class="token operator">=</span>regularizers<span class="token punctuation">.</span>l2<span class="token punctuation">(</span><span class="token number">0.1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>    x <span class="token operator">=</span> SeqSelfAttention<span class="token punctuation">(</span>units<span class="token operator">=</span><span class="token number">128</span><span class="token punctuation">,</span> kernel_regularizer<span class="token operator">=</span>regularizers<span class="token punctuation">.</span>l2<span class="token punctuation">(</span><span class="token number">0.1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>    x <span class="token operator">=</span> SeqSelfAttention<span class="token punctuation">(</span>units<span class="token operator">=</span><span class="token number">128</span><span class="token punctuation">,</span> kernel_regularizer<span class="token operator">=</span>regularizers<span class="token punctuation">.</span>l2<span class="token punctuation">(</span><span class="token number">0.1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>    x <span class="token operator">=</span> SeqSelfAttention<span class="token punctuation">(</span>units<span class="token operator">=</span><span class="token number">128</span><span class="token punctuation">,</span> kernel_regularizer<span class="token operator">=</span>regularizers<span class="token punctuation">.</span>l2<span class="token punctuation">(</span><span class="token number">0.1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>    x <span class="token operator">=</span> SeqSelfAttention<span class="token punctuation">(</span>units<span class="token operator">=</span><span class="token number">128</span><span class="token punctuation">,</span> kernel_regularizer<span class="token operator">=</span>regularizers<span class="token punctuation">.</span>l2<span class="token punctuation">(</span><span class="token number">0.1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>    x <span class="token operator">=</span> SeqSelfAttention<span class="token punctuation">(</span>units<span class="token operator">=</span><span class="token number">128</span><span class="token punctuation">,</span> kernel_regularizer<span class="token operator">=</span>regularizers<span class="token punctuation">.</span>l2<span class="token punctuation">(</span><span class="token number">0.1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>    x <span class="token operator">=</span> Flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>    x <span class="token operator">=</span> Dropout<span class="token punctuation">(</span>config<span class="token punctuation">[</span><span class="token string">'dropout_rate'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>    x <span class="token operator">=</span> Dense<span class="token punctuation">(</span>units<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>    x <span class="token operator">=</span> Activation<span class="token punctuation">(</span><span class="token string">'sigmoid'</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>    model <span class="token operator">=</span> Model<span class="token punctuation">(</span>inputs<span class="token operator">=</span>x_input<span class="token punctuation">,</span> outputs<span class="token operator">=</span>x<span class="token punctuation">)</span>    <span class="token keyword">if</span> <span class="token keyword">not</span> load_weights<span class="token punctuation">:</span>        model<span class="token punctuation">.</span><span class="token builtin">compile</span><span class="token punctuation">(</span>loss<span class="token operator">=</span>config<span class="token punctuation">[</span><span class="token string">"loss"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>                       optimizer<span class="token operator">=</span>adagrad<span class="token punctuation">,</span>                       metrics<span class="token operator">=</span><span class="token punctuation">[</span>get_f1<span class="token punctuation">]</span><span class="token punctuation">)</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"="</span><span class="token operator">*</span><span class="token number">20</span><span class="token punctuation">,</span> <span class="token string">"Start Training Attention"</span><span class="token punctuation">,</span> <span class="token string">"="</span><span class="token operator">*</span><span class="token number">20</span><span class="token punctuation">)</span>        path <span class="token operator">=</span> <span class="token string">r'C:\Users\YangWang\Desktop\Text_Classifier_for_UtaPass_and_KKBOX\weights\attention_weights.hdf5'</span>        model_checkpoint <span class="token operator">=</span> ModelCheckpoint<span class="token punctuation">(</span>path<span class="token punctuation">,</span> monitor<span class="token operator">=</span><span class="token string">'val_loss'</span><span class="token punctuation">,</span> verbose<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> save_best_only<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> mode<span class="token operator">=</span><span class="token string">'auto'</span><span class="token punctuation">)</span>        early_stopping <span class="token operator">=</span> EarlyStopping<span class="token punctuation">(</span>monitor<span class="token operator">=</span><span class="token string">'val_loss'</span><span class="token punctuation">,</span> patience<span class="token operator">=</span><span class="token number">20</span><span class="token punctuation">,</span> verbose<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> mode<span class="token operator">=</span><span class="token string">'auto'</span><span class="token punctuation">)</span>        reduce_lr <span class="token operator">=</span> ReduceLROnPlateau<span class="token punctuation">(</span>monitor<span class="token operator">=</span><span class="token string">'val_loss'</span><span class="token punctuation">,</span> factor<span class="token operator">=</span><span class="token number">0.2</span><span class="token punctuation">,</span> patience<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">,</span> min_lr<span class="token operator">=</span><span class="token number">0.001</span><span class="token punctuation">,</span> verbose<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>        history <span class="token operator">=</span> model<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>            x_train<span class="token punctuation">,</span> y_train<span class="token punctuation">,</span>             batch_size<span class="token operator">=</span>config<span class="token punctuation">[</span><span class="token string">'batch_size'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>             epochs<span class="token operator">=</span>config<span class="token punctuation">[</span><span class="token string">'nb_epoch'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>             validation_split<span class="token operator">=</span>config<span class="token punctuation">[</span><span class="token string">'validation_split'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>             shuffle<span class="token operator">=</span>config<span class="token punctuation">[</span><span class="token string">'shuffle'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>             verbose<span class="token operator">=</span>verbose<span class="token punctuation">,</span>             callbacks<span class="token operator">=</span><span class="token punctuation">[</span>early_stopping<span class="token punctuation">,</span> reduce_lr<span class="token punctuation">,</span> model_checkpoint<span class="token punctuation">]</span><span class="token punctuation">)</span>    <span class="token keyword">if</span> load_weights<span class="token punctuation">:</span>        history <span class="token operator">=</span> <span class="token boolean">None</span>        path <span class="token operator">=</span> <span class="token string">r'C:\Users\YangWang\Desktop\Text_Classifier_for_UtaPass_and_KKBOX\weights\attention_weights.hdf5'</span>        model<span class="token punctuation">.</span>load_weights<span class="token punctuation">(</span>path<span class="token punctuation">)</span>        <span class="token keyword">return</span> history<span class="token punctuation">,</span> model<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="CNN-LSTM"><a href="#CNN-LSTM" class="headerlink" title="CNN-LSTM"></a>CNN-LSTM</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">train_cnn_lstm</span><span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> y_train<span class="token punctuation">,</span> wv_matrix<span class="token punctuation">,</span> verbose<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> load_weights<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>backend<span class="token punctuation">.</span>clear_session<span class="token punctuation">(</span><span class="token punctuation">)</span>        x_input <span class="token operator">=</span> Input<span class="token punctuation">(</span>shape<span class="token operator">=</span><span class="token punctuation">(</span>config<span class="token punctuation">[</span><span class="token string">"MAX_LEN"</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">)</span><span class="token punctuation">)</span>    x <span class="token operator">=</span> Embedding<span class="token punctuation">(</span>wv_matrix<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> wv_matrix<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> weights<span class="token operator">=</span><span class="token punctuation">[</span>wv_matrix<span class="token punctuation">]</span><span class="token punctuation">,</span> trainable<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x_input<span class="token punctuation">)</span>    x <span class="token operator">=</span> SpatialDropout1D<span class="token punctuation">(</span>config<span class="token punctuation">[</span><span class="token string">'dropout_rate'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>        x <span class="token operator">=</span> Conv1D<span class="token punctuation">(</span>filters<span class="token operator">=</span>config<span class="token punctuation">[</span><span class="token string">'nb_filter'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span>config<span class="token punctuation">[</span><span class="token string">'filter_length'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> padding<span class="token operator">=</span>config<span class="token punctuation">[</span><span class="token string">'border_mode'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>    x <span class="token operator">=</span> BatchNormalization<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>    x <span class="token operator">=</span> Activation<span class="token punctuation">(</span><span class="token string">"relu"</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>    x <span class="token operator">=</span> MaxPooling1D<span class="token punctuation">(</span>pool_size<span class="token operator">=</span>config<span class="token punctuation">[</span><span class="token string">'pool_length'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>        x <span class="token operator">=</span> Conv1D<span class="token punctuation">(</span>filters<span class="token operator">=</span>config<span class="token punctuation">[</span><span class="token string">'nb_filter'</span><span class="token punctuation">]</span><span class="token operator">*</span><span class="token number">2</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span>config<span class="token punctuation">[</span><span class="token string">'filter_length'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> padding<span class="token operator">=</span>config<span class="token punctuation">[</span><span class="token string">'border_mode'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>    x <span class="token operator">=</span> BatchNormalization<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>    x <span class="token operator">=</span> Activation<span class="token punctuation">(</span><span class="token string">"relu"</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>    x <span class="token operator">=</span> MaxPooling1D<span class="token punctuation">(</span>pool_size<span class="token operator">=</span>config<span class="token punctuation">[</span><span class="token string">'pool_length'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>        x <span class="token operator">=</span> Conv1D<span class="token punctuation">(</span>filters<span class="token operator">=</span>config<span class="token punctuation">[</span><span class="token string">'nb_filter'</span><span class="token punctuation">]</span><span class="token operator">*</span><span class="token number">4</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span>config<span class="token punctuation">[</span><span class="token string">'filter_length'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> padding<span class="token operator">=</span>config<span class="token punctuation">[</span><span class="token string">'border_mode'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>    x <span class="token operator">=</span> BatchNormalization<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>    x <span class="token operator">=</span> Activation<span class="token punctuation">(</span><span class="token string">"relu"</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>    x <span class="token operator">=</span> MaxPooling1D<span class="token punctuation">(</span>pool_size<span class="token operator">=</span>config<span class="token punctuation">[</span><span class="token string">'pool_length'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>        x <span class="token operator">=</span> LSTM<span class="token punctuation">(</span>units<span class="token operator">=</span>config<span class="token punctuation">[</span><span class="token string">'lstm_cell'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> return_sequences<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> kernel_regularizer<span class="token operator">=</span>regularizers<span class="token punctuation">.</span>l2<span class="token punctuation">(</span><span class="token number">0.3</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>    x <span class="token operator">=</span> SeqSelfAttention<span class="token punctuation">(</span>units<span class="token operator">=</span>config<span class="token punctuation">[</span><span class="token string">'lstm_cell'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> kernel_regularizer<span class="token operator">=</span>regularizers<span class="token punctuation">.</span>l2<span class="token punctuation">(</span><span class="token number">0.1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>        x <span class="token operator">=</span> Flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>    x <span class="token operator">=</span> Dense<span class="token punctuation">(</span>config<span class="token punctuation">[</span><span class="token string">'fc_cell'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>    x <span class="token operator">=</span> Dropout<span class="token punctuation">(</span>config<span class="token punctuation">[</span><span class="token string">'dropout_rate'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>    x <span class="token operator">=</span> Dense<span class="token punctuation">(</span>units<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>    x <span class="token operator">=</span> Activation<span class="token punctuation">(</span><span class="token string">'sigmoid'</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>    model <span class="token operator">=</span> Model<span class="token punctuation">(</span>inputs<span class="token operator">=</span>x_input<span class="token punctuation">,</span> outputs<span class="token operator">=</span>x<span class="token punctuation">)</span>    <span class="token keyword">if</span> <span class="token keyword">not</span> load_weights<span class="token punctuation">:</span>        model<span class="token punctuation">.</span><span class="token builtin">compile</span><span class="token punctuation">(</span>loss<span class="token operator">=</span>config<span class="token punctuation">[</span><span class="token string">'loss'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>                       optimizer<span class="token operator">=</span>adadelta<span class="token punctuation">,</span>                       metrics<span class="token operator">=</span><span class="token punctuation">[</span>get_f1<span class="token punctuation">]</span><span class="token punctuation">)</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"="</span><span class="token operator">*</span><span class="token number">20</span><span class="token punctuation">,</span> <span class="token string">"Start Training CNN-LSTM"</span><span class="token punctuation">,</span> <span class="token string">"="</span><span class="token operator">*</span><span class="token number">20</span><span class="token punctuation">)</span>        path <span class="token operator">=</span> <span class="token string">r'C:\Users\YangWang\Desktop\UtaPass_KKBOX_Classifier\weights\&#123;&#125;_weights.hdf5'</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span><span class="token string">"cnn_lstm"</span><span class="token punctuation">)</span>        model_checkpoint <span class="token operator">=</span> ModelCheckpoint<span class="token punctuation">(</span>path<span class="token punctuation">,</span> monitor<span class="token operator">=</span><span class="token string">'val_loss'</span><span class="token punctuation">,</span> verbose<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> save_best_only<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> mode<span class="token operator">=</span><span class="token string">'auto'</span><span class="token punctuation">)</span>        early_stopping <span class="token operator">=</span> EarlyStopping<span class="token punctuation">(</span>monitor<span class="token operator">=</span><span class="token string">'val_loss'</span><span class="token punctuation">,</span> patience<span class="token operator">=</span><span class="token number">20</span><span class="token punctuation">,</span> verbose<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> mode<span class="token operator">=</span><span class="token string">'auto'</span><span class="token punctuation">)</span>        reduce_lr <span class="token operator">=</span> ReduceLROnPlateau<span class="token punctuation">(</span>monitor<span class="token operator">=</span><span class="token string">'val_loss'</span><span class="token punctuation">,</span> factor<span class="token operator">=</span><span class="token number">0.2</span><span class="token punctuation">,</span> patience<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">,</span> min_lr<span class="token operator">=</span><span class="token number">0.001</span><span class="token punctuation">)</span>        history <span class="token operator">=</span> model<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>            x_train<span class="token punctuation">,</span> y_train<span class="token punctuation">,</span>             batch_size<span class="token operator">=</span>config<span class="token punctuation">[</span><span class="token string">'batch_size'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>             epochs<span class="token operator">=</span>config<span class="token punctuation">[</span><span class="token string">'nb_epoch'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>             validation_split<span class="token operator">=</span>config<span class="token punctuation">[</span><span class="token string">'validation_split'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>             shuffle<span class="token operator">=</span>config<span class="token punctuation">[</span><span class="token string">'shuffle'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>             verbose<span class="token operator">=</span>verbose<span class="token punctuation">,</span>             callbacks<span class="token operator">=</span><span class="token punctuation">[</span>early_stopping<span class="token punctuation">,</span> reduce_lr<span class="token punctuation">,</span> model_checkpoint<span class="token punctuation">]</span><span class="token punctuation">)</span>    <span class="token keyword">if</span> load_weights<span class="token punctuation">:</span>         history <span class="token operator">=</span> <span class="token boolean">None</span>        path <span class="token operator">=</span> <span class="token string">r'C:\Users\YangWang\Desktop\UtaPass_KKBOX_Classifier\weights\&#123;&#125;_weights.hdf5'</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span><span class="token string">"cnn_lstm"</span><span class="token punctuation">)</span>        model<span class="token punctuation">.</span>load_weights<span class="token punctuation">(</span>path<span class="token punctuation">)</span>        <span class="token keyword">return</span> history<span class="token punctuation">,</span> model<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="CNN-static"><a href="#CNN-static" class="headerlink" title="CNN-static"></a>CNN-static</h4><p>Based on “Convolutional Neural Networks for Sentence Classification” written by Yoon Kim [<a href="http://arxiv.org/pdf/1408.5882v2.pdf">paper link</a>]</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># Yoon Kim's paper: Convolutional Neural Networks for Sentence Classification</span><span class="token comment"># Reference from https://www.aclweb.org/anthology/D14-1181.pdf</span><span class="token keyword">def</span> <span class="token function">train_cnn_static</span><span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> y_train<span class="token punctuation">,</span> wv_matrix<span class="token punctuation">,</span> trainable<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> verbose<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> load_weights<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>backend<span class="token punctuation">.</span>clear_session<span class="token punctuation">(</span><span class="token punctuation">)</span>        x_input <span class="token operator">=</span> Input<span class="token punctuation">(</span>shape<span class="token operator">=</span><span class="token punctuation">(</span>config<span class="token punctuation">[</span><span class="token string">"MAX_LEN"</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">)</span><span class="token punctuation">)</span>    x <span class="token operator">=</span> Embedding<span class="token punctuation">(</span>wv_matrix<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> wv_matrix<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> weights<span class="token operator">=</span><span class="token punctuation">[</span>wv_matrix<span class="token punctuation">]</span><span class="token punctuation">,</span> trainable<span class="token operator">=</span>trainable<span class="token punctuation">)</span><span class="token punctuation">(</span>x_input<span class="token punctuation">)</span>        x_conv_1 <span class="token operator">=</span> Conv1D<span class="token punctuation">(</span>filters<span class="token operator">=</span>config<span class="token punctuation">[</span><span class="token string">'nb_filter'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>                       kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span>                       padding<span class="token operator">=</span>config<span class="token punctuation">[</span><span class="token string">'border_mode'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>                       kernel_regularizer<span class="token operator">=</span>regularizers<span class="token punctuation">.</span>l2<span class="token punctuation">(</span><span class="token number">0.3</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>    x_conv_1 <span class="token operator">=</span> BatchNormalization<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x_conv_1<span class="token punctuation">)</span>    x_conv_1 <span class="token operator">=</span> Activation<span class="token punctuation">(</span><span class="token string">"relu"</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x_conv_1<span class="token punctuation">)</span>    x_conv_1 <span class="token operator">=</span> MaxPooling1D<span class="token punctuation">(</span>pool_size<span class="token operator">=</span><span class="token punctuation">(</span>config<span class="token punctuation">[</span><span class="token string">"MAX_LEN"</span><span class="token punctuation">]</span><span class="token operator">-</span><span class="token number">3</span><span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> strides<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">"valid"</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x_conv_1<span class="token punctuation">)</span>        x_conv_2 <span class="token operator">=</span> Conv1D<span class="token punctuation">(</span>filters<span class="token operator">=</span>config<span class="token punctuation">[</span><span class="token string">'nb_filter'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>                       kernel_size<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">,</span>                       padding<span class="token operator">=</span>config<span class="token punctuation">[</span><span class="token string">'border_mode'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>                       kernel_regularizer<span class="token operator">=</span>regularizers<span class="token punctuation">.</span>l2<span class="token punctuation">(</span><span class="token number">0.3</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>    x_conv_2 <span class="token operator">=</span> BatchNormalization<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x_conv_2<span class="token punctuation">)</span>    x_conv_2 <span class="token operator">=</span> Activation<span class="token punctuation">(</span><span class="token string">"relu"</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x_conv_2<span class="token punctuation">)</span>    x_conv_2 <span class="token operator">=</span> MaxPooling1D<span class="token punctuation">(</span>pool_size<span class="token operator">=</span><span class="token punctuation">(</span>config<span class="token punctuation">[</span><span class="token string">"MAX_LEN"</span><span class="token punctuation">]</span><span class="token operator">-</span><span class="token number">4</span><span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> strides<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">"valid"</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x_conv_2<span class="token punctuation">)</span>        x_conv_3 <span class="token operator">=</span> Conv1D<span class="token punctuation">(</span>filters<span class="token operator">=</span>config<span class="token punctuation">[</span><span class="token string">'nb_filter'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>                       kernel_size<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">,</span>                       padding<span class="token operator">=</span>config<span class="token punctuation">[</span><span class="token string">'border_mode'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>                       kernel_regularizer<span class="token operator">=</span>regularizers<span class="token punctuation">.</span>l2<span class="token punctuation">(</span><span class="token number">0.3</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>    x_conv_3 <span class="token operator">=</span> BatchNormalization<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x_conv_3<span class="token punctuation">)</span>    x_conv_3 <span class="token operator">=</span> Activation<span class="token punctuation">(</span><span class="token string">"relu"</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x_conv_3<span class="token punctuation">)</span>    x_conv_3 <span class="token operator">=</span> MaxPooling1D<span class="token punctuation">(</span>pool_size<span class="token operator">=</span><span class="token punctuation">(</span>config<span class="token punctuation">[</span><span class="token string">"MAX_LEN"</span><span class="token punctuation">]</span><span class="token operator">-</span><span class="token number">5</span><span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> strides<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">"valid"</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x_conv_3<span class="token punctuation">)</span>        main <span class="token operator">=</span> Concatenate<span class="token punctuation">(</span>axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">(</span><span class="token punctuation">[</span>x_conv_1<span class="token punctuation">,</span> x_conv_2<span class="token punctuation">,</span> x_conv_3<span class="token punctuation">]</span><span class="token punctuation">)</span>    main <span class="token operator">=</span> Flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">(</span>main<span class="token punctuation">)</span>    main <span class="token operator">=</span> Dropout<span class="token punctuation">(</span>config<span class="token punctuation">[</span><span class="token string">'dropout_rate'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">(</span>main<span class="token punctuation">)</span>    main <span class="token operator">=</span> Dense<span class="token punctuation">(</span>units<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">(</span>main<span class="token punctuation">)</span>    main <span class="token operator">=</span> Activation<span class="token punctuation">(</span><span class="token string">'sigmoid'</span><span class="token punctuation">)</span><span class="token punctuation">(</span>main<span class="token punctuation">)</span>    model <span class="token operator">=</span> Model<span class="token punctuation">(</span>inputs<span class="token operator">=</span>x_input<span class="token punctuation">,</span> outputs<span class="token operator">=</span>main<span class="token punctuation">)</span>    <span class="token keyword">if</span> <span class="token keyword">not</span> load_weights<span class="token punctuation">:</span>        model<span class="token punctuation">.</span><span class="token builtin">compile</span><span class="token punctuation">(</span>loss<span class="token operator">=</span>config<span class="token punctuation">[</span><span class="token string">"loss"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>                       optimizer<span class="token operator">=</span>adagrad<span class="token punctuation">,</span>                       metrics<span class="token operator">=</span><span class="token punctuation">[</span>get_f1<span class="token punctuation">]</span><span class="token punctuation">)</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"="</span><span class="token operator">*</span><span class="token number">20</span><span class="token punctuation">,</span> <span class="token string">"Start Training CNN-static"</span><span class="token punctuation">,</span> <span class="token string">"="</span><span class="token operator">*</span><span class="token number">20</span><span class="token punctuation">)</span>        path <span class="token operator">=</span> <span class="token string">r'C:\Users\YangWang\Desktop\Text_Classifier_for_UtaPass_and_KKBOX\weights\cnn_static_weights.hdf5'</span>        model_checkpoint <span class="token operator">=</span> ModelCheckpoint<span class="token punctuation">(</span>path<span class="token punctuation">,</span> monitor<span class="token operator">=</span><span class="token string">'val_loss'</span><span class="token punctuation">,</span> verbose<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> save_best_only<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> mode<span class="token operator">=</span><span class="token string">'auto'</span><span class="token punctuation">)</span>        early_stopping <span class="token operator">=</span> EarlyStopping<span class="token punctuation">(</span>monitor<span class="token operator">=</span><span class="token string">'val_loss'</span><span class="token punctuation">,</span> patience<span class="token operator">=</span><span class="token number">20</span><span class="token punctuation">,</span> verbose<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> mode<span class="token operator">=</span><span class="token string">'auto'</span><span class="token punctuation">)</span>        reduce_lr <span class="token operator">=</span> ReduceLROnPlateau<span class="token punctuation">(</span>monitor<span class="token operator">=</span><span class="token string">'val_loss'</span><span class="token punctuation">,</span> factor<span class="token operator">=</span><span class="token number">0.2</span><span class="token punctuation">,</span> patience<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">,</span> min_lr<span class="token operator">=</span><span class="token number">0.001</span><span class="token punctuation">)</span>        history <span class="token operator">=</span> model<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>            x_train<span class="token punctuation">,</span> y_train<span class="token punctuation">,</span>             batch_size<span class="token operator">=</span>config<span class="token punctuation">[</span><span class="token string">'batch_size'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>             epochs<span class="token operator">=</span>config<span class="token punctuation">[</span><span class="token string">'nb_epoch'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>             validation_split<span class="token operator">=</span>config<span class="token punctuation">[</span><span class="token string">'validation_split'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>             shuffle<span class="token operator">=</span>config<span class="token punctuation">[</span><span class="token string">'shuffle'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>             verbose<span class="token operator">=</span>verbose<span class="token punctuation">,</span>             callbacks<span class="token operator">=</span><span class="token punctuation">[</span>early_stopping<span class="token punctuation">,</span> reduce_lr<span class="token punctuation">,</span> model_checkpoint<span class="token punctuation">]</span><span class="token punctuation">)</span>    <span class="token keyword">if</span> load_weights<span class="token punctuation">:</span>        history <span class="token operator">=</span> <span class="token boolean">None</span>        path <span class="token operator">=</span> <span class="token string">r'C:\Users\YangWang\Desktop\Text_Classifier_for_UtaPass_and_KKBOX\weights\cnn_static_weights.hdf5'</span>        model<span class="token punctuation">.</span>load_weights<span class="token punctuation">(</span>path<span class="token punctuation">)</span>        <span class="token keyword">return</span> history<span class="token punctuation">,</span> model<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="CNN-multichannel"><a href="#CNN-multichannel" class="headerlink" title="CNN-multichannel"></a>CNN-multichannel</h4><p>Based on “Convolutional Neural Networks for Sentence Classification” written by Yoon Kim [<a href="http://arxiv.org/pdf/1408.5882v2.pdf">paper link</a>]</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">train_cnn_multichannel</span><span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> y_train<span class="token punctuation">,</span> wv_matrix<span class="token punctuation">,</span> trainable<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> verbose<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> load_weights<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>backend<span class="token punctuation">.</span>clear_session<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment"># Channel 1</span>    x_input_1 <span class="token operator">=</span> Input<span class="token punctuation">(</span>shape<span class="token operator">=</span><span class="token punctuation">(</span>config<span class="token punctuation">[</span><span class="token string">"MAX_LEN"</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">)</span><span class="token punctuation">)</span>    embedding_1 <span class="token operator">=</span> Embedding<span class="token punctuation">(</span>        wv_matrix<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         wv_matrix<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         weights<span class="token operator">=</span><span class="token punctuation">[</span>wv_matrix<span class="token punctuation">]</span><span class="token punctuation">,</span>         trainable<span class="token operator">=</span>trainable<span class="token punctuation">)</span><span class="token punctuation">(</span>x_input_1<span class="token punctuation">)</span>    x_conv_1 <span class="token operator">=</span> Conv1D<span class="token punctuation">(</span>        filters<span class="token operator">=</span>config<span class="token punctuation">[</span><span class="token string">'nb_filter'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span>         padding<span class="token operator">=</span>config<span class="token punctuation">[</span><span class="token string">'border_mode'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         kernel_regularizer<span class="token operator">=</span>regularizers<span class="token punctuation">.</span>l2<span class="token punctuation">(</span><span class="token number">0.3</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">(</span>embedding_1<span class="token punctuation">)</span>    x_conv_1 <span class="token operator">=</span> BatchNormalization<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x_conv_1<span class="token punctuation">)</span>    x_conv_1 <span class="token operator">=</span> Activation<span class="token punctuation">(</span><span class="token string">"relu"</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x_conv_1<span class="token punctuation">)</span>    x_conv_1 <span class="token operator">=</span> MaxPooling1D<span class="token punctuation">(</span>pool_size<span class="token operator">=</span><span class="token punctuation">(</span>config<span class="token punctuation">[</span><span class="token string">"MAX_LEN"</span><span class="token punctuation">]</span><span class="token operator">-</span><span class="token number">3</span><span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> strides<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">"valid"</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x_conv_1<span class="token punctuation">)</span>    flat_1 <span class="token operator">=</span> Flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x_conv_1<span class="token punctuation">)</span>        <span class="token comment"># Channel 2</span>    x_input_2 <span class="token operator">=</span> Input<span class="token punctuation">(</span>shape<span class="token operator">=</span><span class="token punctuation">(</span>config<span class="token punctuation">[</span><span class="token string">"MAX_LEN"</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">)</span><span class="token punctuation">)</span>    embedding_2 <span class="token operator">=</span> Embedding<span class="token punctuation">(</span>        wv_matrix<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         wv_matrix<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         weights<span class="token operator">=</span><span class="token punctuation">[</span>wv_matrix<span class="token punctuation">]</span><span class="token punctuation">,</span>         trainable<span class="token operator">=</span>trainable<span class="token punctuation">)</span><span class="token punctuation">(</span>x_input_2<span class="token punctuation">)</span>    x_conv_2 <span class="token operator">=</span> Conv1D<span class="token punctuation">(</span>        filters<span class="token operator">=</span>config<span class="token punctuation">[</span><span class="token string">'nb_filter'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span>         padding<span class="token operator">=</span>config<span class="token punctuation">[</span><span class="token string">'border_mode'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         kernel_regularizer<span class="token operator">=</span>regularizers<span class="token punctuation">.</span>l2<span class="token punctuation">(</span><span class="token number">0.3</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">(</span>embedding_2<span class="token punctuation">)</span>    x_conv_2 <span class="token operator">=</span> BatchNormalization<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x_conv_2<span class="token punctuation">)</span>    x_conv_2 <span class="token operator">=</span> Activation<span class="token punctuation">(</span><span class="token string">"relu"</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x_conv_2<span class="token punctuation">)</span>    x_conv_2 <span class="token operator">=</span> MaxPooling1D<span class="token punctuation">(</span>pool_size<span class="token operator">=</span><span class="token punctuation">(</span>config<span class="token punctuation">[</span><span class="token string">"MAX_LEN"</span><span class="token punctuation">]</span><span class="token operator">-</span><span class="token number">3</span><span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> strides<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">"valid"</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x_conv_2<span class="token punctuation">)</span>    flat_2 <span class="token operator">=</span> Flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x_conv_2<span class="token punctuation">)</span>        <span class="token comment"># Channel 1</span>    x_input_3 <span class="token operator">=</span> Input<span class="token punctuation">(</span>shape<span class="token operator">=</span><span class="token punctuation">(</span>config<span class="token punctuation">[</span><span class="token string">"MAX_LEN"</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">)</span><span class="token punctuation">)</span>    embedding_3 <span class="token operator">=</span> Embedding<span class="token punctuation">(</span>        wv_matrix<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         wv_matrix<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         weights<span class="token operator">=</span><span class="token punctuation">[</span>wv_matrix<span class="token punctuation">]</span><span class="token punctuation">,</span>         trainable<span class="token operator">=</span>trainable<span class="token punctuation">)</span><span class="token punctuation">(</span>x_input_3<span class="token punctuation">)</span>    x_conv_3 <span class="token operator">=</span> Conv1D<span class="token punctuation">(</span>        filters<span class="token operator">=</span>config<span class="token punctuation">[</span><span class="token string">'nb_filter'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span>         padding<span class="token operator">=</span>config<span class="token punctuation">[</span><span class="token string">'border_mode'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         kernel_regularizer<span class="token operator">=</span>regularizers<span class="token punctuation">.</span>l2<span class="token punctuation">(</span><span class="token number">0.3</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">(</span>embedding_3<span class="token punctuation">)</span>    x_conv_3 <span class="token operator">=</span> BatchNormalization<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x_conv_3<span class="token punctuation">)</span>    x_conv_3 <span class="token operator">=</span> Activation<span class="token punctuation">(</span><span class="token string">"relu"</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x_conv_3<span class="token punctuation">)</span>    x_conv_3 <span class="token operator">=</span> MaxPooling1D<span class="token punctuation">(</span>pool_size<span class="token operator">=</span><span class="token punctuation">(</span>config<span class="token punctuation">[</span><span class="token string">"MAX_LEN"</span><span class="token punctuation">]</span><span class="token operator">-</span><span class="token number">3</span><span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> strides<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">"valid"</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x_conv_3<span class="token punctuation">)</span>    flat_3 <span class="token operator">=</span> Flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x_conv_3<span class="token punctuation">)</span>        main <span class="token operator">=</span> Concatenate<span class="token punctuation">(</span>axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">(</span><span class="token punctuation">[</span>flat_1<span class="token punctuation">,</span> flat_2<span class="token punctuation">,</span> flat_3<span class="token punctuation">]</span><span class="token punctuation">)</span>    main <span class="token operator">=</span> Dense<span class="token punctuation">(</span>units<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">)</span><span class="token punctuation">(</span>main<span class="token punctuation">)</span>    main <span class="token operator">=</span> Activation<span class="token punctuation">(</span><span class="token string">'relu'</span><span class="token punctuation">)</span><span class="token punctuation">(</span>main<span class="token punctuation">)</span>    main <span class="token operator">=</span> Dropout<span class="token punctuation">(</span>config<span class="token punctuation">[</span><span class="token string">'dropout_rate'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">(</span>main<span class="token punctuation">)</span>    main <span class="token operator">=</span> Dense<span class="token punctuation">(</span>units<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">(</span>main<span class="token punctuation">)</span>    main <span class="token operator">=</span> Activation<span class="token punctuation">(</span><span class="token string">'sigmoid'</span><span class="token punctuation">)</span><span class="token punctuation">(</span>main<span class="token punctuation">)</span>    model <span class="token operator">=</span> Model<span class="token punctuation">(</span>inputs<span class="token operator">=</span><span class="token punctuation">[</span>x_input_1<span class="token punctuation">,</span> x_input_2<span class="token punctuation">,</span> x_input_3<span class="token punctuation">]</span><span class="token punctuation">,</span> outputs<span class="token operator">=</span>main<span class="token punctuation">)</span>    <span class="token keyword">if</span> <span class="token keyword">not</span> load_weights<span class="token punctuation">:</span>        model<span class="token punctuation">.</span><span class="token builtin">compile</span><span class="token punctuation">(</span>loss<span class="token operator">=</span>config<span class="token punctuation">[</span><span class="token string">"loss"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>                       optimizer<span class="token operator">=</span>adagrad<span class="token punctuation">,</span>                       metrics<span class="token operator">=</span><span class="token punctuation">[</span>get_f1<span class="token punctuation">]</span><span class="token punctuation">)</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"="</span><span class="token operator">*</span><span class="token number">20</span><span class="token punctuation">,</span> <span class="token string">"Start Training CNN-multichannel"</span><span class="token punctuation">,</span> <span class="token string">"="</span><span class="token operator">*</span><span class="token number">20</span><span class="token punctuation">)</span>        path <span class="token operator">=</span> <span class="token string">r'C:\Users\YangWang\Desktop\Text_Classifier_for_UtaPass_and_KKBOX\weights\&#123;&#125;_weights.hdf5'</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span><span class="token string">"cnn_multi"</span><span class="token punctuation">)</span>        model_checkpoint <span class="token operator">=</span> ModelCheckpoint<span class="token punctuation">(</span>path<span class="token punctuation">,</span> monitor<span class="token operator">=</span><span class="token string">'val_loss'</span><span class="token punctuation">,</span> verbose<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> save_best_only<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> mode<span class="token operator">=</span><span class="token string">'auto'</span><span class="token punctuation">)</span>        early_stopping <span class="token operator">=</span> EarlyStopping<span class="token punctuation">(</span>monitor<span class="token operator">=</span><span class="token string">'val_loss'</span><span class="token punctuation">,</span> patience<span class="token operator">=</span><span class="token number">20</span><span class="token punctuation">,</span> verbose<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> mode<span class="token operator">=</span><span class="token string">'auto'</span><span class="token punctuation">)</span>        reduce_lr <span class="token operator">=</span> ReduceLROnPlateau<span class="token punctuation">(</span>monitor<span class="token operator">=</span><span class="token string">'val_loss'</span><span class="token punctuation">,</span> factor<span class="token operator">=</span><span class="token number">0.2</span><span class="token punctuation">,</span> patience<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">,</span> min_lr<span class="token operator">=</span><span class="token number">0.001</span><span class="token punctuation">)</span>        history <span class="token operator">=</span> model<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>            <span class="token punctuation">[</span>x_train<span class="token punctuation">,</span> x_train<span class="token punctuation">,</span> x_train<span class="token punctuation">]</span><span class="token punctuation">,</span> y_train<span class="token punctuation">,</span>             batch_size<span class="token operator">=</span>config<span class="token punctuation">[</span><span class="token string">'batch_size'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>             epochs<span class="token operator">=</span>config<span class="token punctuation">[</span><span class="token string">'nb_epoch'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>             validation_split<span class="token operator">=</span>config<span class="token punctuation">[</span><span class="token string">'validation_split'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>             shuffle<span class="token operator">=</span>config<span class="token punctuation">[</span><span class="token string">'shuffle'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>             verbose<span class="token operator">=</span>verbose<span class="token punctuation">,</span>             callbacks<span class="token operator">=</span><span class="token punctuation">[</span>early_stopping<span class="token punctuation">,</span> reduce_lr<span class="token punctuation">,</span> model_checkpoint<span class="token punctuation">]</span><span class="token punctuation">)</span>    <span class="token keyword">if</span> load_weights<span class="token punctuation">:</span>        history <span class="token operator">=</span> <span class="token boolean">None</span>        path <span class="token operator">=</span> <span class="token string">r'C:\Users\YangWang\Desktop\Text_Classifier_for_UtaPass_and_KKBOX\weights\&#123;&#125;_weights.hdf5'</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span><span class="token string">"cnn_multi"</span><span class="token punctuation">)</span>        model<span class="token punctuation">.</span>load_weights<span class="token punctuation">(</span>path<span class="token punctuation">)</span>        <span class="token keyword">return</span> history<span class="token punctuation">,</span> model<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="Text-ResNet"><a href="#Text-ResNet" class="headerlink" title="Text-ResNet"></a>Text-ResNet</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">identity_resnet_block</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> nb_filter<span class="token punctuation">)</span><span class="token punctuation">:</span>    x_shortcut <span class="token operator">=</span> x    <span class="token comment"># First component of main path</span>    res_x <span class="token operator">=</span> Conv1D<span class="token punctuation">(</span>        filters<span class="token operator">=</span>nb_filter<span class="token punctuation">,</span>         kernel_size<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">,</span>         strides<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>         padding<span class="token operator">=</span><span class="token string">'same'</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>    res_x <span class="token operator">=</span> BatchNormalization<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">(</span>res_x<span class="token punctuation">)</span>    res_x <span class="token operator">=</span> Activation<span class="token punctuation">(</span><span class="token string">"relu"</span><span class="token punctuation">)</span><span class="token punctuation">(</span>res_x<span class="token punctuation">)</span>    <span class="token comment"># Second component of main path</span>    res_x <span class="token operator">=</span> Conv1D<span class="token punctuation">(</span>        filters<span class="token operator">=</span>nb_filter<span class="token punctuation">,</span>         kernel_size<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">,</span>         strides<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>         padding<span class="token operator">=</span><span class="token string">'same'</span><span class="token punctuation">)</span><span class="token punctuation">(</span>res_x<span class="token punctuation">)</span>    res_x <span class="token operator">=</span> BatchNormalization<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">(</span>res_x<span class="token punctuation">)</span>    res_x <span class="token operator">=</span> Activation<span class="token punctuation">(</span><span class="token string">"relu"</span><span class="token punctuation">)</span><span class="token punctuation">(</span>res_x<span class="token punctuation">)</span>    <span class="token comment"># Third component of main path</span>    res_x <span class="token operator">=</span> Conv1D<span class="token punctuation">(</span>        filters<span class="token operator">=</span>nb_filter<span class="token punctuation">,</span>         kernel_size<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">,</span>         strides<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>         padding<span class="token operator">=</span><span class="token string">'same'</span><span class="token punctuation">)</span><span class="token punctuation">(</span>res_x<span class="token punctuation">)</span>    res_x <span class="token operator">=</span> BatchNormalization<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">(</span>res_x<span class="token punctuation">)</span>    <span class="token comment"># Final Step: add shortcut value to the main path</span>    x <span class="token operator">=</span> Add<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">(</span><span class="token punctuation">[</span>x_shortcut<span class="token punctuation">,</span> res_x<span class="token punctuation">]</span><span class="token punctuation">)</span>    output <span class="token operator">=</span> Activation<span class="token punctuation">(</span><span class="token string">'relu'</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>    <span class="token keyword">return</span> output<span class="token keyword">def</span> <span class="token function">convolutional_resnet_block</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> nb_filter<span class="token punctuation">)</span><span class="token punctuation">:</span>    x_shortcut <span class="token operator">=</span> x    <span class="token comment"># First component of main path</span>    res_x <span class="token operator">=</span> Conv1D<span class="token punctuation">(</span>        filters<span class="token operator">=</span>nb_filter<span class="token punctuation">,</span>         kernel_size<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">,</span>         strides<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>         padding<span class="token operator">=</span><span class="token string">'same'</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>    res_x <span class="token operator">=</span> BatchNormalization<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">(</span>res_x<span class="token punctuation">)</span>    res_x <span class="token operator">=</span> Activation<span class="token punctuation">(</span><span class="token string">"relu"</span><span class="token punctuation">)</span><span class="token punctuation">(</span>res_x<span class="token punctuation">)</span>    <span class="token comment"># Second component of main path</span>    res_x <span class="token operator">=</span> Conv1D<span class="token punctuation">(</span>        filters<span class="token operator">=</span>nb_filter<span class="token punctuation">,</span>         kernel_size<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">,</span>         strides<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>         padding<span class="token operator">=</span><span class="token string">'same'</span><span class="token punctuation">)</span><span class="token punctuation">(</span>res_x<span class="token punctuation">)</span>    res_x <span class="token operator">=</span> BatchNormalization<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">(</span>res_x<span class="token punctuation">)</span>    res_x <span class="token operator">=</span> Activation<span class="token punctuation">(</span><span class="token string">"relu"</span><span class="token punctuation">)</span><span class="token punctuation">(</span>res_x<span class="token punctuation">)</span>    <span class="token comment"># Third component of main path</span>    res_x <span class="token operator">=</span> Conv1D<span class="token punctuation">(</span>        filters<span class="token operator">=</span>nb_filter<span class="token punctuation">,</span>         kernel_size<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">,</span>         strides<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>         padding<span class="token operator">=</span><span class="token string">'same'</span><span class="token punctuation">)</span><span class="token punctuation">(</span>res_x<span class="token punctuation">)</span>    res_x <span class="token operator">=</span> BatchNormalization<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">(</span>res_x<span class="token punctuation">)</span>    <span class="token comment"># Shortcut path</span>    x_shortcut <span class="token operator">=</span> Conv1D<span class="token punctuation">(</span>        filters<span class="token operator">=</span>nb_filter<span class="token punctuation">,</span>         kernel_size<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">,</span>         strides<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>         padding<span class="token operator">=</span><span class="token string">'same'</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x_shortcut<span class="token punctuation">)</span>    x_shortcut <span class="token operator">=</span> BatchNormalization<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x_shortcut<span class="token punctuation">)</span>    <span class="token comment"># Final Step: add shortcut value to the main path</span>    x <span class="token operator">=</span> Add<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">(</span><span class="token punctuation">[</span>x_shortcut<span class="token punctuation">,</span> res_x<span class="token punctuation">]</span><span class="token punctuation">)</span>    output <span class="token operator">=</span> Activation<span class="token punctuation">(</span><span class="token string">'relu'</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>    <span class="token keyword">return</span> output<span class="token keyword">def</span> <span class="token function">train_text_resnet</span><span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> y_train<span class="token punctuation">,</span> wv_matrix<span class="token punctuation">,</span> verbose<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> load_weights<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>backend<span class="token punctuation">.</span>clear_session<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token comment"># Resnet for reviews of UtaPass and KKBOX</span>    x_input <span class="token operator">=</span> Input<span class="token punctuation">(</span>shape<span class="token operator">=</span><span class="token punctuation">(</span>config<span class="token punctuation">[</span><span class="token string">"MAX_LEN"</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">)</span><span class="token punctuation">)</span>    x <span class="token operator">=</span> Embedding<span class="token punctuation">(</span>wv_matrix<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> wv_matrix<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> weights<span class="token operator">=</span><span class="token punctuation">[</span>wv_matrix<span class="token punctuation">]</span><span class="token punctuation">,</span> trainable<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x_input<span class="token punctuation">)</span>    x <span class="token operator">=</span> SpatialDropout1D<span class="token punctuation">(</span><span class="token number">0.25</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>    <span class="token comment"># Stage 1</span>    x <span class="token operator">=</span> Conv1D<span class="token punctuation">(</span>filters<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> strides<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>    x <span class="token operator">=</span> BatchNormalization<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>    x <span class="token operator">=</span> Activation<span class="token punctuation">(</span><span class="token string">"relu"</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>    x <span class="token operator">=</span> MaxPooling1D<span class="token punctuation">(</span>pool_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>    <span class="token comment"># Stage 2</span>    x <span class="token operator">=</span> convolutional_resnet_block<span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">)</span>    x <span class="token operator">=</span> identity_resnet_block<span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">)</span>    x <span class="token operator">=</span> identity_resnet_block<span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">)</span>    <span class="token comment"># Stage 3</span>    x <span class="token operator">=</span> convolutional_resnet_block<span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">)</span>    x <span class="token operator">=</span> identity_resnet_block<span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">)</span>    x <span class="token operator">=</span> identity_resnet_block<span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">)</span>    x <span class="token operator">=</span> identity_resnet_block<span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">)</span>    <span class="token comment"># Stage 4</span>    x <span class="token operator">=</span> convolutional_resnet_block<span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">)</span>    x <span class="token operator">=</span> identity_resnet_block<span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">)</span>    x <span class="token operator">=</span> identity_resnet_block<span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">)</span>    x <span class="token operator">=</span> identity_resnet_block<span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">)</span>    x <span class="token operator">=</span> identity_resnet_block<span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">)</span>    x <span class="token operator">=</span> identity_resnet_block<span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">)</span>    <span class="token comment"># Stage 5</span>    x <span class="token operator">=</span> convolutional_resnet_block<span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token number">512</span><span class="token punctuation">)</span>    x <span class="token operator">=</span> identity_resnet_block<span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token number">512</span><span class="token punctuation">)</span>    x <span class="token operator">=</span> identity_resnet_block<span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token number">512</span><span class="token punctuation">)</span>    <span class="token comment"># Average pool</span>    x <span class="token operator">=</span> AveragePooling1D<span class="token punctuation">(</span>pool_size <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>    <span class="token comment"># Output layer</span>    x <span class="token operator">=</span> Flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>    x <span class="token operator">=</span> Dense<span class="token punctuation">(</span>units<span class="token operator">=</span><span class="token number">1024</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'relu'</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>    x <span class="token operator">=</span> Dropout<span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>    x <span class="token operator">=</span> Dense<span class="token punctuation">(</span>units<span class="token operator">=</span><span class="token number">1024</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'relu'</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>    x <span class="token operator">=</span> Dense<span class="token punctuation">(</span>units<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>    x <span class="token operator">=</span> BatchNormalization<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>    x <span class="token operator">=</span> Activation<span class="token punctuation">(</span><span class="token string">'sigmoid'</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>    model <span class="token operator">=</span> Model<span class="token punctuation">(</span>inputs<span class="token operator">=</span>x_input<span class="token punctuation">,</span> outputs<span class="token operator">=</span>x<span class="token punctuation">)</span>        <span class="token keyword">if</span> <span class="token keyword">not</span> load_weights<span class="token punctuation">:</span>         model<span class="token punctuation">.</span><span class="token builtin">compile</span><span class="token punctuation">(</span>loss<span class="token operator">=</span>config<span class="token punctuation">[</span><span class="token string">"loss"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>                      optimizer<span class="token operator">=</span>Nadam<span class="token punctuation">(</span>learning_rate<span class="token operator">=</span><span class="token number">0.001</span><span class="token punctuation">,</span> beta_1<span class="token operator">=</span><span class="token number">0.9</span><span class="token punctuation">,</span> beta_2<span class="token operator">=</span><span class="token number">0.999</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                      metrics<span class="token operator">=</span><span class="token punctuation">[</span>get_f1<span class="token punctuation">]</span><span class="token punctuation">)</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"="</span><span class="token operator">*</span><span class="token number">20</span><span class="token punctuation">,</span> <span class="token string">"Start Training Text-ResNet"</span><span class="token punctuation">,</span> <span class="token string">"="</span><span class="token operator">*</span><span class="token number">20</span><span class="token punctuation">)</span>        path <span class="token operator">=</span> <span class="token string">r'C:\Users\YangWang\Desktop\Text_Classifier_for_UtaPass_and_KKBOX\weights\text_resnet_weights.hdf5'</span>        model_checkpoint <span class="token operator">=</span> ModelCheckpoint<span class="token punctuation">(</span>path<span class="token punctuation">,</span> monitor<span class="token operator">=</span><span class="token string">'val_loss'</span><span class="token punctuation">,</span> verbose<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> save_best_only<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> mode<span class="token operator">=</span><span class="token string">'auto'</span><span class="token punctuation">)</span>        early_stopping <span class="token operator">=</span> EarlyStopping<span class="token punctuation">(</span>monitor<span class="token operator">=</span><span class="token string">'val_loss'</span><span class="token punctuation">,</span> patience<span class="token operator">=</span><span class="token number">20</span><span class="token punctuation">,</span> verbose<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> mode<span class="token operator">=</span><span class="token string">'auto'</span><span class="token punctuation">)</span>        reduce_lr <span class="token operator">=</span> ReduceLROnPlateau<span class="token punctuation">(</span>monitor<span class="token operator">=</span><span class="token string">'val_loss'</span><span class="token punctuation">,</span> factor<span class="token operator">=</span><span class="token number">0.2</span><span class="token punctuation">,</span> patience<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">,</span> min_lr<span class="token operator">=</span><span class="token number">0.001</span><span class="token punctuation">)</span>        history <span class="token operator">=</span> model<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>            x_train<span class="token punctuation">,</span> y_train<span class="token punctuation">,</span>             batch_size<span class="token operator">=</span>config<span class="token punctuation">[</span><span class="token string">"batch_size"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>             epochs<span class="token operator">=</span>config<span class="token punctuation">[</span><span class="token string">"nb_epoch"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>             validation_split<span class="token operator">=</span>config<span class="token punctuation">[</span><span class="token string">"validation_split"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>             shuffle<span class="token operator">=</span>config<span class="token punctuation">[</span><span class="token string">"shuffle"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>             verbose<span class="token operator">=</span>verbose<span class="token punctuation">,</span>             callbacks<span class="token operator">=</span><span class="token punctuation">[</span>early_stopping<span class="token punctuation">,</span> reduce_lr<span class="token punctuation">,</span> model_checkpoint<span class="token punctuation">]</span><span class="token punctuation">)</span>    <span class="token keyword">if</span> load_weights<span class="token punctuation">:</span>        history <span class="token operator">=</span> <span class="token boolean">None</span>        path <span class="token operator">=</span> <span class="token string">r'C:\Users\YangWang\Desktop\Text_Classifier_for_UtaPass_and_KKBOX\weights\text_resnet_weights.hdf5'</span>        model<span class="token punctuation">.</span>load_weights<span class="token punctuation">(</span>path<span class="token punctuation">)</span>        <span class="token keyword">return</span> history<span class="token punctuation">,</span> model<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="Start-Training"><a href="#Start-Training" class="headerlink" title="Start Training"></a>Start Training</h3><ol><li>Train each model and get history and model architecure.</li><li>Load the checkpoint from weights folder we saved during training.</li><li>Predict sentiment probability from testing set.</li><li>Compute accuracy score, f1 score, and confusion matrix.</li><li>Plot train and val history (loss and f1) and visualise confusion matrix.</li></ol><h4 id="Visualisation"><a href="#Visualisation" class="headerlink" title="Visualisation"></a>Visualisation</h4><p>Define two ploting functions to visualise the history of accuracy and loss.</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> pltplt<span class="token punctuation">.</span>style<span class="token punctuation">.</span>use<span class="token punctuation">(</span><span class="token string">'ggplot'</span><span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">plot_history_ggplot</span><span class="token punctuation">(</span>history<span class="token punctuation">)</span><span class="token punctuation">:</span>    acc <span class="token operator">=</span> history<span class="token punctuation">.</span>history<span class="token punctuation">[</span><span class="token string">'acc'</span><span class="token punctuation">]</span>    val_acc <span class="token operator">=</span> history<span class="token punctuation">.</span>history<span class="token punctuation">[</span><span class="token string">'val_acc'</span><span class="token punctuation">]</span>    loss <span class="token operator">=</span> history<span class="token punctuation">.</span>history<span class="token punctuation">[</span><span class="token string">'loss'</span><span class="token punctuation">]</span>    val_loss <span class="token operator">=</span> history<span class="token punctuation">.</span>history<span class="token punctuation">[</span><span class="token string">'val_loss'</span><span class="token punctuation">]</span>    x <span class="token operator">=</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>acc<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span>    plt<span class="token punctuation">.</span>figure<span class="token punctuation">(</span>figsize<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">12</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    plt<span class="token punctuation">.</span>subplot<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>    plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>x<span class="token punctuation">,</span> acc<span class="token punctuation">,</span> <span class="token string">'b'</span><span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">'Training acc'</span><span class="token punctuation">)</span>    plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>x<span class="token punctuation">,</span> val_acc<span class="token punctuation">,</span> <span class="token string">'r'</span><span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">'Validation acc'</span><span class="token punctuation">)</span>    plt<span class="token punctuation">.</span>title<span class="token punctuation">(</span><span class="token string">'Training and validation accuracy'</span><span class="token punctuation">)</span>    plt<span class="token punctuation">.</span>legend<span class="token punctuation">(</span><span class="token punctuation">)</span>    plt<span class="token punctuation">.</span>subplot<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>    plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>x<span class="token punctuation">,</span> loss<span class="token punctuation">,</span> <span class="token string">'b'</span><span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">'Training loss'</span><span class="token punctuation">)</span>    plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>x<span class="token punctuation">,</span> val_loss<span class="token punctuation">,</span> <span class="token string">'r'</span><span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">'Validation loss'</span><span class="token punctuation">)</span>    plt<span class="token punctuation">.</span>title<span class="token punctuation">(</span><span class="token string">'Training and validation loss'</span><span class="token punctuation">)</span>    plt<span class="token punctuation">.</span>legend<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">plot_history</span><span class="token punctuation">(</span>history<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment"># plot results</span>    loss <span class="token operator">=</span> history<span class="token punctuation">.</span>history<span class="token punctuation">[</span><span class="token string">'loss'</span><span class="token punctuation">]</span>    val_loss <span class="token operator">=</span> history<span class="token punctuation">.</span>history<span class="token punctuation">[</span><span class="token string">'val_loss'</span><span class="token punctuation">]</span>    f1 <span class="token operator">=</span> history<span class="token punctuation">.</span>history<span class="token punctuation">[</span><span class="token string">'acc'</span><span class="token punctuation">]</span>    val_f1 <span class="token operator">=</span> history<span class="token punctuation">.</span>history<span class="token punctuation">[</span><span class="token string">'val_acc'</span><span class="token punctuation">]</span>    plt<span class="token punctuation">.</span>figure<span class="token punctuation">(</span>figsize<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    plt<span class="token punctuation">.</span>subplot<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>    plt<span class="token punctuation">.</span>title<span class="token punctuation">(</span><span class="token string">'Loss'</span><span class="token punctuation">)</span>    epochs <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>loss<span class="token punctuation">)</span>    plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span><span class="token builtin">range</span><span class="token punctuation">(</span>epochs<span class="token punctuation">)</span><span class="token punctuation">,</span> loss<span class="token punctuation">,</span> marker<span class="token operator">=</span><span class="token string">'.'</span><span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">'loss'</span><span class="token punctuation">)</span>    plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span><span class="token builtin">range</span><span class="token punctuation">(</span>epochs<span class="token punctuation">)</span><span class="token punctuation">,</span> val_loss<span class="token punctuation">,</span> marker<span class="token operator">=</span><span class="token string">'.'</span><span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">'val_loss'</span><span class="token punctuation">)</span>    plt<span class="token punctuation">.</span>legend<span class="token punctuation">(</span>loc<span class="token operator">=</span><span class="token string">'best'</span><span class="token punctuation">)</span>    ax <span class="token operator">=</span> plt<span class="token punctuation">.</span>gca<span class="token punctuation">(</span><span class="token punctuation">)</span>    ax<span class="token punctuation">.</span>spines<span class="token punctuation">[</span><span class="token string">'bottom'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_linewidth<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span>    ax<span class="token punctuation">.</span>spines<span class="token punctuation">[</span><span class="token string">'top'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_linewidth<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span>    ax<span class="token punctuation">.</span>spines<span class="token punctuation">[</span><span class="token string">'right'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_linewidth<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span>    ax<span class="token punctuation">.</span>spines<span class="token punctuation">[</span><span class="token string">'left'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_linewidth<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span>    ax<span class="token punctuation">.</span>set_facecolor<span class="token punctuation">(</span><span class="token string">'snow'</span><span class="token punctuation">)</span>    plt<span class="token punctuation">.</span>grid<span class="token punctuation">(</span>color<span class="token operator">=</span><span class="token string">'lightgray'</span><span class="token punctuation">,</span> linestyle<span class="token operator">=</span><span class="token string">'-'</span><span class="token punctuation">,</span> linewidth<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>    plt<span class="token punctuation">.</span>xlabel<span class="token punctuation">(</span><span class="token string">'epoch'</span><span class="token punctuation">)</span>    plt<span class="token punctuation">.</span>ylabel<span class="token punctuation">(</span><span class="token string">'loss'</span><span class="token punctuation">)</span>    plt<span class="token punctuation">.</span>subplot<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>    plt<span class="token punctuation">.</span>title<span class="token punctuation">(</span><span class="token string">'Accuracy'</span><span class="token punctuation">)</span>    plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span><span class="token builtin">range</span><span class="token punctuation">(</span>epochs<span class="token punctuation">)</span><span class="token punctuation">,</span> f1<span class="token punctuation">,</span> marker<span class="token operator">=</span><span class="token string">'.'</span><span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">'acc'</span><span class="token punctuation">)</span>    plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span><span class="token builtin">range</span><span class="token punctuation">(</span>epochs<span class="token punctuation">)</span><span class="token punctuation">,</span> val_f1<span class="token punctuation">,</span> marker<span class="token operator">=</span><span class="token string">'.'</span><span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">'val_acc'</span><span class="token punctuation">)</span>    plt<span class="token punctuation">.</span>legend<span class="token punctuation">(</span>loc<span class="token operator">=</span><span class="token string">'best'</span><span class="token punctuation">)</span>    ax <span class="token operator">=</span> plt<span class="token punctuation">.</span>gca<span class="token punctuation">(</span><span class="token punctuation">)</span>    ax<span class="token punctuation">.</span>spines<span class="token punctuation">[</span><span class="token string">'bottom'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_linewidth<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span>    ax<span class="token punctuation">.</span>spines<span class="token punctuation">[</span><span class="token string">'top'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_linewidth<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span>    ax<span class="token punctuation">.</span>spines<span class="token punctuation">[</span><span class="token string">'right'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_linewidth<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span>    ax<span class="token punctuation">.</span>spines<span class="token punctuation">[</span><span class="token string">'left'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_linewidth<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span>    ax<span class="token punctuation">.</span>set_facecolor<span class="token punctuation">(</span><span class="token string">'snow'</span><span class="token punctuation">)</span>    plt<span class="token punctuation">.</span>grid<span class="token punctuation">(</span>color<span class="token operator">=</span><span class="token string">'lightgray'</span><span class="token punctuation">,</span> linestyle<span class="token operator">=</span><span class="token string">'-'</span><span class="token punctuation">,</span> linewidth<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>    plt<span class="token punctuation">.</span>xlabel<span class="token punctuation">(</span><span class="token string">'epoch'</span><span class="token punctuation">)</span>    plt<span class="token punctuation">.</span>ylabel<span class="token punctuation">(</span><span class="token string">'acc'</span><span class="token punctuation">)</span>        plt<span class="token punctuation">.</span>tight_layout<span class="token punctuation">(</span><span class="token punctuation">)</span>    plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>Plot confusion matrix heatmap.</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">plot_confusion_matrix</span><span class="token punctuation">(</span>cm<span class="token punctuation">)</span><span class="token punctuation">:</span>    df_cm <span class="token operator">=</span> pd<span class="token punctuation">.</span>DataFrame<span class="token punctuation">(</span>cm<span class="token punctuation">)</span>    plt<span class="token punctuation">.</span>figure<span class="token punctuation">(</span>figsize<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    hmap <span class="token operator">=</span> sns<span class="token punctuation">.</span>heatmap<span class="token punctuation">(</span>cm<span class="token punctuation">,</span> annot<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> fmt<span class="token operator">=</span><span class="token string">"d"</span><span class="token punctuation">,</span> cmap<span class="token operator">=</span><span class="token string">"Blues"</span><span class="token punctuation">)</span>    hmap<span class="token punctuation">.</span>yaxis<span class="token punctuation">.</span>set_ticklabels<span class="token punctuation">(</span>hmap<span class="token punctuation">.</span>yaxis<span class="token punctuation">.</span>get_ticklabels<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> rotation<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> ha<span class="token operator">=</span><span class="token string">"right"</span><span class="token punctuation">)</span>    hmap<span class="token punctuation">.</span>xaxis<span class="token punctuation">.</span>set_ticklabels<span class="token punctuation">(</span>hmap<span class="token punctuation">.</span>xaxis<span class="token punctuation">.</span>get_ticklabels<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> rotation<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> ha<span class="token operator">=</span><span class="token string">"right"</span><span class="token punctuation">)</span>    plt<span class="token punctuation">.</span>xlabel<span class="token punctuation">(</span><span class="token string">"Predicted Sentiment"</span><span class="token punctuation">)</span>    plt<span class="token punctuation">.</span>ylabel<span class="token punctuation">(</span><span class="token string">"Target Sentiment"</span><span class="token punctuation">)</span>    plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="Accuracy-F1-Score-and-Confusion-Matrix"><a href="#Accuracy-F1-Score-and-Confusion-Matrix" class="headerlink" title="Accuracy, F1 Score, and Confusion Matrix"></a>Accuracy, F1 Score, and Confusion Matrix</h3><p>Compare the performance among several deep learning models.</p><h4 id="Simple-RNN-1"><a href="#Simple-RNN-1" class="headerlink" title="Simple RNN"></a>Simple RNN</h4><p>Accuracy:  0.7419<br>F1 Score:  0.7773</p><details><summary>Simple RNN Model Architecture</summary><pre><code>Model: "model_1"_________________________________________________________________Layer (type)                 Output Shape              Param #   =================================================================input_1 (InputLayer)         (None, 64)                0         _________________________________________________________________embedding_1 (Embedding)      (None, 64, 300)           2251500   _________________________________________________________________spatial_dropout1d_1 (Spatial (None, 64, 300)           0         _________________________________________________________________simple_rnn_1 (SimpleRNN)     (None, 64, 50)            17550     _________________________________________________________________simple_rnn_2 (SimpleRNN)     (None, 50)                5050      _________________________________________________________________dropout_1 (Dropout)          (None, 50)                0         _________________________________________________________________dense_1 (Dense)              (None, 1)                 51        _________________________________________________________________activation_1 (Activation)    (None, 1)                 0         =================================================================Total params: 2,274,151Trainable params: 22,651Non-trainable params: 2,251,500_________________________________________________________________</code></pre></details><h4 id="GRU-1"><a href="#GRU-1" class="headerlink" title="GRU"></a>GRU</h4><p>Accuracy:  0.7821<br>F1 Score:  0.8216</p><details><summary>GRU Model Architecture</summary><pre><code>Model: "model_1"_________________________________________________________________Layer (type)                 Output Shape              Param #   =================================================================input_1 (InputLayer)         (None, 64)                0         _________________________________________________________________embedding_1 (Embedding)      (None, 64, 300)           2251500   _________________________________________________________________spatial_dropout1d_1 (Spatial (None, 64, 300)           0         _________________________________________________________________gru_1 (GRU)                  (None, 64, 50)            52650     _________________________________________________________________gru_2 (GRU)                  (None, 50)                15150     _________________________________________________________________dropout_1 (Dropout)          (None, 50)                0         _________________________________________________________________dense_1 (Dense)              (None, 1)                 51        _________________________________________________________________activation_1 (Activation)    (None, 1)                 0         =================================================================Total params: 2,319,351Trainable params: 67,851Non-trainable params: 2,251,500_________________________________________________________________</code></pre></details><h4 id="LSTM-1"><a href="#LSTM-1" class="headerlink" title="LSTM"></a>LSTM</h4><p>Accuracy:  0.7697<br>F1 Score:  0.7945</p><details><summary>LSTM Model Architecture</summary><pre><code>Model: "model_1"_________________________________________________________________Layer (type)                 Output Shape              Param #   =================================================================input_1 (InputLayer)         (None, 64)                0         _________________________________________________________________embedding_1 (Embedding)      (None, 64, 300)           2251500   _________________________________________________________________spatial_dropout1d_1 (Spatial (None, 64, 300)           0         _________________________________________________________________lstm_1 (LSTM)                (None, 64, 64)            93440     _________________________________________________________________lstm_2 (LSTM)                (None, 64)                33024     _________________________________________________________________dropout_1 (Dropout)          (None, 64)                0         _________________________________________________________________dense_1 (Dense)              (None, 1)                 65        _________________________________________________________________activation_1 (Activation)    (None, 1)                 0         =================================================================Total params: 2,378,029Trainable params: 126,529Non-trainable params: 2,251,500_________________________________________________________________</code></pre></details><h4 id="BiLSTM-1"><a href="#BiLSTM-1" class="headerlink" title="BiLSTM"></a>BiLSTM</h4><p>Accuracy:  0.7430<br>F1 Score:  0.7696</p><details><summary>BiLSTM Model Architecture</summary><pre><code>Model: "model_1"_________________________________________________________________Layer (type)                 Output Shape              Param #   =================================================================input_1 (InputLayer)         (None, 64)                0         _________________________________________________________________embedding_1 (Embedding)      (None, 64, 300)           2251500   _________________________________________________________________spatial_dropout1d_1 (Spatial (None, 64, 300)           0         _________________________________________________________________bidirectional_1 (Bidirection (None, 64, 128)           186880    _________________________________________________________________bidirectional_2 (Bidirection (None, 128)               98816     _________________________________________________________________dropout_1 (Dropout)          (None, 128)               0         _________________________________________________________________dense_1 (Dense)              (None, 128)               16512     _________________________________________________________________dropout_2 (Dropout)          (None, 128)               0         _________________________________________________________________dense_2 (Dense)              (None, 1)                 129       _________________________________________________________________activation_1 (Activation)    (None, 1)                 0         =================================================================Total params: 2,553,837Trainable params: 302,337Non-trainable params: 2,251,500_________________________________________________________________</code></pre></details><h4 id="Attention-1"><a href="#Attention-1" class="headerlink" title="Attention"></a>Attention</h4><p>Accuracy:  0.7496<br>F1 Score:  0.7857</p><details><summary>Attention Model Architecture</summary><pre><code>Model: "model_1"_________________________________________________________________Layer (type)                 Output Shape              Param #   =================================================================input_1 (InputLayer)         (None, 64)                0         _________________________________________________________________embedding_1 (Embedding)      (None, 64, 300)           2251500   _________________________________________________________________spatial_dropout1d_1 (Spatial (None, 64, 300)           0         _________________________________________________________________seq_self_attention_1 (SeqSel (None, 64, 300)           77057     _________________________________________________________________seq_self_attention_2 (SeqSel (None, 64, 300)           77057     _________________________________________________________________seq_self_attention_3 (SeqSel (None, 64, 300)           77057     _________________________________________________________________flatten_1 (Flatten)          (None, 19200)             0         _________________________________________________________________dropout_1 (Dropout)          (None, 19200)             0         _________________________________________________________________dense_1 (Dense)              (None, 1)                 19201     _________________________________________________________________activation_1 (Activation)    (None, 1)                 0         =================================================================Total params: 2,501,872Trainable params: 250,372Non-trainable params: 2,251,500_________________________________________________________________</code></pre></details><h4 id="CNN-Static"><a href="#CNN-Static" class="headerlink" title="CNN-Static"></a>CNN-Static</h4><p>Accuracy:  0.7736<br>F1 Score:  0.8031</p><details><summary>CNN-Static Model Architecture</summary><pre><code>Model: "model_1"__________________________________________________________________________________________________Layer (type)                    Output Shape         Param #     Connected to                     ==================================================================================================input_1 (InputLayer)            (None, 64)           0                                            __________________________________________________________________________________________________embedding_1 (Embedding)         (None, 64, 300)      2251500     input_1[0][0]                    __________________________________________________________________________________________________conv1d_1 (Conv1D)               (None, 64, 150)      135150      embedding_1[0][0]                __________________________________________________________________________________________________conv1d_2 (Conv1D)               (None, 64, 150)      180150      embedding_1[0][0]                __________________________________________________________________________________________________conv1d_3 (Conv1D)               (None, 64, 150)      225150      embedding_1[0][0]                __________________________________________________________________________________________________batch_normalization_1 (BatchNor (None, 64, 150)      600         conv1d_1[0][0]                   __________________________________________________________________________________________________batch_normalization_2 (BatchNor (None, 64, 150)      600         conv1d_2[0][0]                   __________________________________________________________________________________________________batch_normalization_3 (BatchNor (None, 64, 150)      600         conv1d_3[0][0]                   __________________________________________________________________________________________________activation_1 (Activation)       (None, 64, 150)      0           batch_normalization_1[0][0]      __________________________________________________________________________________________________activation_2 (Activation)       (None, 64, 150)      0           batch_normalization_2[0][0]      __________________________________________________________________________________________________activation_3 (Activation)       (None, 64, 150)      0           batch_normalization_3[0][0]      __________________________________________________________________________________________________max_pooling1d_1 (MaxPooling1D)  (None, 3, 150)       0           activation_1[0][0]               __________________________________________________________________________________________________max_pooling1d_2 (MaxPooling1D)  (None, 4, 150)       0           activation_2[0][0]               __________________________________________________________________________________________________max_pooling1d_3 (MaxPooling1D)  (None, 5, 150)       0           activation_3[0][0]               __________________________________________________________________________________________________concatenate_1 (Concatenate)     (None, 12, 150)      0           max_pooling1d_1[0][0]                                                                             max_pooling1d_2[0][0]                                                                             max_pooling1d_3[0][0]            __________________________________________________________________________________________________flatten_1 (Flatten)             (None, 1800)         0           concatenate_1[0][0]              __________________________________________________________________________________________________dropout_1 (Dropout)             (None, 1800)         0           flatten_1[0][0]                  __________________________________________________________________________________________________dense_1 (Dense)                 (None, 1)            1801        dropout_1[0][0]                  __________________________________________________________________________________________________activation_4 (Activation)       (None, 1)            0           dense_1[0][0]                    ==================================================================================================Total params: 2,795,551Trainable params: 543,151Non-trainable params: 2,252,400__________________________________________________________________________________________________</code></pre></details><h4 id="CNN-MultiChannel"><a href="#CNN-MultiChannel" class="headerlink" title="CNN-MultiChannel"></a>CNN-MultiChannel</h4><p>Accuracy:  0.7744<br>F1 Score:  0.8073</p><details><summary>CNN-MultiChannel Model Architecture</summary><pre><code>Model: "model_1"__________________________________________________________________________________________________Layer (type)                    Output Shape         Param #     Connected to                     ==================================================================================================input_1 (InputLayer)            (None, 64)           0                                            __________________________________________________________________________________________________input_2 (InputLayer)            (None, 64)           0                                            __________________________________________________________________________________________________input_3 (InputLayer)            (None, 64)           0                                            __________________________________________________________________________________________________embedding_1 (Embedding)         (None, 64, 300)      2251500     input_1[0][0]                    __________________________________________________________________________________________________embedding_2 (Embedding)         (None, 64, 300)      2251500     input_2[0][0]                    __________________________________________________________________________________________________embedding_3 (Embedding)         (None, 64, 300)      2251500     input_3[0][0]                    __________________________________________________________________________________________________conv1d_1 (Conv1D)               (None, 64, 150)      135150      embedding_1[0][0]                __________________________________________________________________________________________________conv1d_2 (Conv1D)               (None, 64, 150)      135150      embedding_2[0][0]                __________________________________________________________________________________________________conv1d_3 (Conv1D)               (None, 64, 150)      135150      embedding_3[0][0]                __________________________________________________________________________________________________batch_normalization_1 (BatchNor (None, 64, 150)      600         conv1d_1[0][0]                   __________________________________________________________________________________________________batch_normalization_2 (BatchNor (None, 64, 150)      600         conv1d_2[0][0]                   __________________________________________________________________________________________________batch_normalization_3 (BatchNor (None, 64, 150)      600         conv1d_3[0][0]                   __________________________________________________________________________________________________activation_1 (Activation)       (None, 64, 150)      0           batch_normalization_1[0][0]      __________________________________________________________________________________________________activation_2 (Activation)       (None, 64, 150)      0           batch_normalization_2[0][0]      __________________________________________________________________________________________________activation_3 (Activation)       (None, 64, 150)      0           batch_normalization_3[0][0]      __________________________________________________________________________________________________max_pooling1d_1 (MaxPooling1D)  (None, 3, 150)       0           activation_1[0][0]               __________________________________________________________________________________________________max_pooling1d_2 (MaxPooling1D)  (None, 3, 150)       0           activation_2[0][0]               __________________________________________________________________________________________________max_pooling1d_3 (MaxPooling1D)  (None, 3, 150)       0           activation_3[0][0]               __________________________________________________________________________________________________flatten_1 (Flatten)             (None, 450)          0           max_pooling1d_1[0][0]            __________________________________________________________________________________________________flatten_2 (Flatten)             (None, 450)          0           max_pooling1d_2[0][0]            __________________________________________________________________________________________________flatten_3 (Flatten)             (None, 450)          0           max_pooling1d_3[0][0]            __________________________________________________________________________________________________concatenate_1 (Concatenate)     (None, 1350)         0           flatten_1[0][0]                                                                                   flatten_2[0][0]                                                                                   flatten_3[0][0]                  __________________________________________________________________________________________________dense_1 (Dense)                 (None, 100)          135100      concatenate_1[0][0]              __________________________________________________________________________________________________activation_4 (Activation)       (None, 100)          0           dense_1[0][0]                    __________________________________________________________________________________________________dropout_1 (Dropout)             (None, 100)          0           activation_4[0][0]               __________________________________________________________________________________________________dense_2 (Dense)                 (None, 1)            101         dropout_1[0][0]                  __________________________________________________________________________________________________activation_5 (Activation)       (None, 1)            0           dense_2[0][0]                    ==================================================================================================Total params: 7,296,951Trainable params: 541,551Non-trainable params: 6,755,400__________________________________________________________________________________________________</code></pre></details><h4 id="CNN-LSTM-1"><a href="#CNN-LSTM-1" class="headerlink" title="CNN-LSTM"></a>CNN-LSTM</h4><p>Accuracy:  0.7380<br>F1 Score:  0.7785</p><details><summary>CNN-LSTM Model Architecture</summary><pre><code>Model: "model_1"_________________________________________________________________Layer (type)                 Output Shape              Param #   =================================================================input_1 (InputLayer)         (None, 64)                0         _________________________________________________________________embedding_1 (Embedding)      (None, 64, 300)           2251500   _________________________________________________________________spatial_dropout1d_1 (Spatial (None, 64, 300)           0         _________________________________________________________________conv1d_1 (Conv1D)            (None, 64, 150)           135150    _________________________________________________________________batch_normalization_1 (Batch (None, 64, 150)           600       _________________________________________________________________activation_1 (Activation)    (None, 64, 150)           0         _________________________________________________________________max_pooling1d_1 (MaxPooling1 (None, 32, 150)           0         _________________________________________________________________conv1d_2 (Conv1D)            (None, 32, 300)           135300    _________________________________________________________________batch_normalization_2 (Batch (None, 32, 300)           1200      _________________________________________________________________activation_2 (Activation)    (None, 32, 300)           0         _________________________________________________________________max_pooling1d_2 (MaxPooling1 (None, 16, 300)           0         _________________________________________________________________conv1d_3 (Conv1D)            (None, 16, 600)           540600    _________________________________________________________________batch_normalization_3 (Batch (None, 16, 600)           2400      _________________________________________________________________activation_3 (Activation)    (None, 16, 600)           0         _________________________________________________________________max_pooling1d_3 (MaxPooling1 (None, 8, 600)            0         _________________________________________________________________lstm_1 (LSTM)                (None, 8, 64)             170240    _________________________________________________________________seq_self_attention_1 (SeqSel (None, 8, 64)             8321      _________________________________________________________________flatten_1 (Flatten)          (None, 512)               0         _________________________________________________________________dense_1 (Dense)              (None, 128)               65664     _________________________________________________________________dropout_1 (Dropout)          (None, 128)               0         _________________________________________________________________dense_2 (Dense)              (None, 1)                 129       _________________________________________________________________activation_4 (Activation)    (None, 1)                 0         =================================================================Total params: 3,311,104Trainable params: 1,057,504Non-trainable params: 2,253,600_________________________________________________________________</code></pre></details><h4 id="Text-ResNet-1"><a href="#Text-ResNet-1" class="headerlink" title="Text-ResNet"></a>Text-ResNet</h4><p>Accuracy:  0.7283<br>F1 Score:  0.7535</p><details><summary>Text-ResNet Model Architecture</summary><pre><code>Model: "model_1"__________________________________________________________________________________________________Layer (type)                    Output Shape         Param #     Connected to                     ==================================================================================================input_1 (InputLayer)            (None, 64)           0                                            __________________________________________________________________________________________________embedding_1 (Embedding)         (None, 64, 300)      2251500     input_1[0][0]                    __________________________________________________________________________________________________spatial_dropout1d_1 (SpatialDro (None, 64, 300)      0           embedding_1[0][0]                __________________________________________________________________________________________________conv1d_1 (Conv1D)               (None, 31, 64)       57664       spatial_dropout1d_1[0][0]        __________________________________________________________________________________________________batch_normalization_1 (BatchNor (None, 31, 64)       256         conv1d_1[0][0]                   __________________________________________________________________________________________________activation_1 (Activation)       (None, 31, 64)       0           batch_normalization_1[0][0]      __________________________________________________________________________________________________max_pooling1d_1 (MaxPooling1D)  (None, 15, 64)       0           activation_1[0][0]               __________________________________________________________________________________________________conv1d_2 (Conv1D)               (None, 15, 64)       20544       max_pooling1d_1[0][0]            __________________________________________________________________________________________________batch_normalization_2 (BatchNor (None, 15, 64)       256         conv1d_2[0][0]                   __________________________________________________________________________________________________activation_2 (Activation)       (None, 15, 64)       0           batch_normalization_2[0][0]      __________________________________________________________________________________________________conv1d_3 (Conv1D)               (None, 15, 64)       20544       activation_2[0][0]               __________________________________________________________________________________________________batch_normalization_3 (BatchNor (None, 15, 64)       256         conv1d_3[0][0]                   __________________________________________________________________________________________________activation_3 (Activation)       (None, 15, 64)       0           batch_normalization_3[0][0]      __________________________________________________________________________________________________conv1d_5 (Conv1D)               (None, 15, 64)       20544       max_pooling1d_1[0][0]            __________________________________________________________________________________________________conv1d_4 (Conv1D)               (None, 15, 64)       20544       activation_3[0][0]               __________________________________________________________________________________________________batch_normalization_5 (BatchNor (None, 15, 64)       256         conv1d_5[0][0]                   __________________________________________________________________________________________________batch_normalization_4 (BatchNor (None, 15, 64)       256         conv1d_4[0][0]                   __________________________________________________________________________________________________add_1 (Add)                     (None, 15, 64)       0           batch_normalization_5[0][0]                                                                       batch_normalization_4[0][0]      __________________________________________________________________________________________________activation_4 (Activation)       (None, 15, 64)       0           add_1[0][0]                      __________________________________________________________________________________________________conv1d_6 (Conv1D)               (None, 15, 64)       20544       activation_4[0][0]               __________________________________________________________________________________________________batch_normalization_6 (BatchNor (None, 15, 64)       256         conv1d_6[0][0]                   __________________________________________________________________________________________________activation_5 (Activation)       (None, 15, 64)       0           batch_normalization_6[0][0]      __________________________________________________________________________________________________conv1d_7 (Conv1D)               (None, 15, 64)       20544       activation_5[0][0]               __________________________________________________________________________________________________batch_normalization_7 (BatchNor (None, 15, 64)       256         conv1d_7[0][0]                   __________________________________________________________________________________________________activation_6 (Activation)       (None, 15, 64)       0           batch_normalization_7[0][0]      __________________________________________________________________________________________________conv1d_8 (Conv1D)               (None, 15, 64)       20544       activation_6[0][0]               __________________________________________________________________________________________________batch_normalization_8 (BatchNor (None, 15, 64)       256         conv1d_8[0][0]                   __________________________________________________________________________________________________add_2 (Add)                     (None, 15, 64)       0           activation_4[0][0]                                                                                batch_normalization_8[0][0]      __________________________________________________________________________________________________activation_7 (Activation)       (None, 15, 64)       0           add_2[0][0]                      __________________________________________________________________________________________________conv1d_9 (Conv1D)               (None, 15, 64)       20544       activation_7[0][0]               __________________________________________________________________________________________________batch_normalization_9 (BatchNor (None, 15, 64)       256         conv1d_9[0][0]                   __________________________________________________________________________________________________activation_8 (Activation)       (None, 15, 64)       0           batch_normalization_9[0][0]      __________________________________________________________________________________________________conv1d_10 (Conv1D)              (None, 15, 64)       20544       activation_8[0][0]               __________________________________________________________________________________________________batch_normalization_10 (BatchNo (None, 15, 64)       256         conv1d_10[0][0]                  __________________________________________________________________________________________________activation_9 (Activation)       (None, 15, 64)       0           batch_normalization_10[0][0]     __________________________________________________________________________________________________conv1d_11 (Conv1D)              (None, 15, 64)       20544       activation_9[0][0]               __________________________________________________________________________________________________batch_normalization_11 (BatchNo (None, 15, 64)       256         conv1d_11[0][0]                  __________________________________________________________________________________________________add_3 (Add)                     (None, 15, 64)       0           activation_7[0][0]                                                                                batch_normalization_11[0][0]     __________________________________________________________________________________________________activation_10 (Activation)      (None, 15, 64)       0           add_3[0][0]                      __________________________________________________________________________________________________conv1d_12 (Conv1D)              (None, 15, 128)      41088       activation_10[0][0]              __________________________________________________________________________________________________batch_normalization_12 (BatchNo (None, 15, 128)      512         conv1d_12[0][0]                  __________________________________________________________________________________________________activation_11 (Activation)      (None, 15, 128)      0           batch_normalization_12[0][0]     __________________________________________________________________________________________________conv1d_13 (Conv1D)              (None, 15, 128)      82048       activation_11[0][0]              __________________________________________________________________________________________________batch_normalization_13 (BatchNo (None, 15, 128)      512         conv1d_13[0][0]                  __________________________________________________________________________________________________activation_12 (Activation)      (None, 15, 128)      0           batch_normalization_13[0][0]     __________________________________________________________________________________________________conv1d_15 (Conv1D)              (None, 15, 128)      41088       activation_10[0][0]              __________________________________________________________________________________________________conv1d_14 (Conv1D)              (None, 15, 128)      82048       activation_12[0][0]              __________________________________________________________________________________________________batch_normalization_15 (BatchNo (None, 15, 128)      512         conv1d_15[0][0]                  __________________________________________________________________________________________________batch_normalization_14 (BatchNo (None, 15, 128)      512         conv1d_14[0][0]                  __________________________________________________________________________________________________add_4 (Add)                     (None, 15, 128)      0           batch_normalization_15[0][0]                                                                      batch_normalization_14[0][0]     __________________________________________________________________________________________________activation_13 (Activation)      (None, 15, 128)      0           add_4[0][0]                      __________________________________________________________________________________________________conv1d_16 (Conv1D)              (None, 15, 128)      82048       activation_13[0][0]              __________________________________________________________________________________________________batch_normalization_16 (BatchNo (None, 15, 128)      512         conv1d_16[0][0]                  __________________________________________________________________________________________________activation_14 (Activation)      (None, 15, 128)      0           batch_normalization_16[0][0]     __________________________________________________________________________________________________conv1d_17 (Conv1D)              (None, 15, 128)      82048       activation_14[0][0]              __________________________________________________________________________________________________batch_normalization_17 (BatchNo (None, 15, 128)      512         conv1d_17[0][0]                  __________________________________________________________________________________________________activation_15 (Activation)      (None, 15, 128)      0           batch_normalization_17[0][0]     __________________________________________________________________________________________________conv1d_18 (Conv1D)              (None, 15, 128)      82048       activation_15[0][0]              __________________________________________________________________________________________________batch_normalization_18 (BatchNo (None, 15, 128)      512         conv1d_18[0][0]                  __________________________________________________________________________________________________add_5 (Add)                     (None, 15, 128)      0           activation_13[0][0]                                                                               batch_normalization_18[0][0]     __________________________________________________________________________________________________activation_16 (Activation)      (None, 15, 128)      0           add_5[0][0]                      __________________________________________________________________________________________________conv1d_19 (Conv1D)              (None, 15, 128)      82048       activation_16[0][0]              __________________________________________________________________________________________________batch_normalization_19 (BatchNo (None, 15, 128)      512         conv1d_19[0][0]                  __________________________________________________________________________________________________activation_17 (Activation)      (None, 15, 128)      0           batch_normalization_19[0][0]     __________________________________________________________________________________________________conv1d_20 (Conv1D)              (None, 15, 128)      82048       activation_17[0][0]              __________________________________________________________________________________________________batch_normalization_20 (BatchNo (None, 15, 128)      512         conv1d_20[0][0]                  __________________________________________________________________________________________________activation_18 (Activation)      (None, 15, 128)      0           batch_normalization_20[0][0]     __________________________________________________________________________________________________conv1d_21 (Conv1D)              (None, 15, 128)      82048       activation_18[0][0]              __________________________________________________________________________________________________batch_normalization_21 (BatchNo (None, 15, 128)      512         conv1d_21[0][0]                  __________________________________________________________________________________________________add_6 (Add)                     (None, 15, 128)      0           activation_16[0][0]                                                                               batch_normalization_21[0][0]     __________________________________________________________________________________________________activation_19 (Activation)      (None, 15, 128)      0           add_6[0][0]                      __________________________________________________________________________________________________conv1d_22 (Conv1D)              (None, 15, 128)      82048       activation_19[0][0]              __________________________________________________________________________________________________batch_normalization_22 (BatchNo (None, 15, 128)      512         conv1d_22[0][0]                  __________________________________________________________________________________________________activation_20 (Activation)      (None, 15, 128)      0           batch_normalization_22[0][0]     __________________________________________________________________________________________________conv1d_23 (Conv1D)              (None, 15, 128)      82048       activation_20[0][0]              __________________________________________________________________________________________________batch_normalization_23 (BatchNo (None, 15, 128)      512         conv1d_23[0][0]                  __________________________________________________________________________________________________activation_21 (Activation)      (None, 15, 128)      0           batch_normalization_23[0][0]     __________________________________________________________________________________________________conv1d_24 (Conv1D)              (None, 15, 128)      82048       activation_21[0][0]              __________________________________________________________________________________________________batch_normalization_24 (BatchNo (None, 15, 128)      512         conv1d_24[0][0]                  __________________________________________________________________________________________________add_7 (Add)                     (None, 15, 128)      0           activation_19[0][0]                                                                               batch_normalization_24[0][0]     __________________________________________________________________________________________________activation_22 (Activation)      (None, 15, 128)      0           add_7[0][0]                      __________________________________________________________________________________________________conv1d_25 (Conv1D)              (None, 15, 256)      164096      activation_22[0][0]              __________________________________________________________________________________________________batch_normalization_25 (BatchNo (None, 15, 256)      1024        conv1d_25[0][0]                  __________________________________________________________________________________________________activation_23 (Activation)      (None, 15, 256)      0           batch_normalization_25[0][0]     __________________________________________________________________________________________________conv1d_26 (Conv1D)              (None, 15, 256)      327936      activation_23[0][0]              __________________________________________________________________________________________________batch_normalization_26 (BatchNo (None, 15, 256)      1024        conv1d_26[0][0]                  __________________________________________________________________________________________________activation_24 (Activation)      (None, 15, 256)      0           batch_normalization_26[0][0]     __________________________________________________________________________________________________conv1d_28 (Conv1D)              (None, 15, 256)      164096      activation_22[0][0]              __________________________________________________________________________________________________conv1d_27 (Conv1D)              (None, 15, 256)      327936      activation_24[0][0]              __________________________________________________________________________________________________batch_normalization_28 (BatchNo (None, 15, 256)      1024        conv1d_28[0][0]                  __________________________________________________________________________________________________batch_normalization_27 (BatchNo (None, 15, 256)      1024        conv1d_27[0][0]                  __________________________________________________________________________________________________add_8 (Add)                     (None, 15, 256)      0           batch_normalization_28[0][0]                                                                      batch_normalization_27[0][0]     __________________________________________________________________________________________________activation_25 (Activation)      (None, 15, 256)      0           add_8[0][0]                      __________________________________________________________________________________________________conv1d_29 (Conv1D)              (None, 15, 256)      327936      activation_25[0][0]              __________________________________________________________________________________________________batch_normalization_29 (BatchNo (None, 15, 256)      1024        conv1d_29[0][0]                  __________________________________________________________________________________________________activation_26 (Activation)      (None, 15, 256)      0           batch_normalization_29[0][0]     __________________________________________________________________________________________________conv1d_30 (Conv1D)              (None, 15, 256)      327936      activation_26[0][0]              __________________________________________________________________________________________________batch_normalization_30 (BatchNo (None, 15, 256)      1024        conv1d_30[0][0]                  __________________________________________________________________________________________________activation_27 (Activation)      (None, 15, 256)      0           batch_normalization_30[0][0]     __________________________________________________________________________________________________conv1d_31 (Conv1D)              (None, 15, 256)      327936      activation_27[0][0]              __________________________________________________________________________________________________batch_normalization_31 (BatchNo (None, 15, 256)      1024        conv1d_31[0][0]                  __________________________________________________________________________________________________add_9 (Add)                     (None, 15, 256)      0           activation_25[0][0]                                                                               batch_normalization_31[0][0]     __________________________________________________________________________________________________activation_28 (Activation)      (None, 15, 256)      0           add_9[0][0]                      __________________________________________________________________________________________________conv1d_32 (Conv1D)              (None, 15, 256)      327936      activation_28[0][0]              __________________________________________________________________________________________________batch_normalization_32 (BatchNo (None, 15, 256)      1024        conv1d_32[0][0]                  __________________________________________________________________________________________________activation_29 (Activation)      (None, 15, 256)      0           batch_normalization_32[0][0]     __________________________________________________________________________________________________conv1d_33 (Conv1D)              (None, 15, 256)      327936      activation_29[0][0]              __________________________________________________________________________________________________batch_normalization_33 (BatchNo (None, 15, 256)      1024        conv1d_33[0][0]                  __________________________________________________________________________________________________activation_30 (Activation)      (None, 15, 256)      0           batch_normalization_33[0][0]     __________________________________________________________________________________________________conv1d_34 (Conv1D)              (None, 15, 256)      327936      activation_30[0][0]              __________________________________________________________________________________________________batch_normalization_34 (BatchNo (None, 15, 256)      1024        conv1d_34[0][0]                  __________________________________________________________________________________________________add_10 (Add)                    (None, 15, 256)      0           activation_28[0][0]                                                                               batch_normalization_34[0][0]     __________________________________________________________________________________________________activation_31 (Activation)      (None, 15, 256)      0           add_10[0][0]                     __________________________________________________________________________________________________conv1d_35 (Conv1D)              (None, 15, 256)      327936      activation_31[0][0]              __________________________________________________________________________________________________batch_normalization_35 (BatchNo (None, 15, 256)      1024        conv1d_35[0][0]                  __________________________________________________________________________________________________activation_32 (Activation)      (None, 15, 256)      0           batch_normalization_35[0][0]     __________________________________________________________________________________________________conv1d_36 (Conv1D)              (None, 15, 256)      327936      activation_32[0][0]              __________________________________________________________________________________________________batch_normalization_36 (BatchNo (None, 15, 256)      1024        conv1d_36[0][0]                  __________________________________________________________________________________________________activation_33 (Activation)      (None, 15, 256)      0           batch_normalization_36[0][0]     __________________________________________________________________________________________________conv1d_37 (Conv1D)              (None, 15, 256)      327936      activation_33[0][0]              __________________________________________________________________________________________________batch_normalization_37 (BatchNo (None, 15, 256)      1024        conv1d_37[0][0]                  __________________________________________________________________________________________________add_11 (Add)                    (None, 15, 256)      0           activation_31[0][0]                                                                               batch_normalization_37[0][0]     __________________________________________________________________________________________________activation_34 (Activation)      (None, 15, 256)      0           add_11[0][0]                     __________________________________________________________________________________________________conv1d_38 (Conv1D)              (None, 15, 256)      327936      activation_34[0][0]              __________________________________________________________________________________________________batch_normalization_38 (BatchNo (None, 15, 256)      1024        conv1d_38[0][0]                  __________________________________________________________________________________________________activation_35 (Activation)      (None, 15, 256)      0           batch_normalization_38[0][0]     __________________________________________________________________________________________________conv1d_39 (Conv1D)              (None, 15, 256)      327936      activation_35[0][0]              __________________________________________________________________________________________________batch_normalization_39 (BatchNo (None, 15, 256)      1024        conv1d_39[0][0]                  __________________________________________________________________________________________________activation_36 (Activation)      (None, 15, 256)      0           batch_normalization_39[0][0]     __________________________________________________________________________________________________conv1d_40 (Conv1D)              (None, 15, 256)      327936      activation_36[0][0]              __________________________________________________________________________________________________batch_normalization_40 (BatchNo (None, 15, 256)      1024        conv1d_40[0][0]                  __________________________________________________________________________________________________add_12 (Add)                    (None, 15, 256)      0           activation_34[0][0]                                                                               batch_normalization_40[0][0]     __________________________________________________________________________________________________activation_37 (Activation)      (None, 15, 256)      0           add_12[0][0]                     __________________________________________________________________________________________________conv1d_41 (Conv1D)              (None, 15, 256)      327936      activation_37[0][0]              __________________________________________________________________________________________________batch_normalization_41 (BatchNo (None, 15, 256)      1024        conv1d_41[0][0]                  __________________________________________________________________________________________________activation_38 (Activation)      (None, 15, 256)      0           batch_normalization_41[0][0]     __________________________________________________________________________________________________conv1d_42 (Conv1D)              (None, 15, 256)      327936      activation_38[0][0]              __________________________________________________________________________________________________batch_normalization_42 (BatchNo (None, 15, 256)      1024        conv1d_42[0][0]                  __________________________________________________________________________________________________activation_39 (Activation)      (None, 15, 256)      0           batch_normalization_42[0][0]     __________________________________________________________________________________________________conv1d_43 (Conv1D)              (None, 15, 256)      327936      activation_39[0][0]              __________________________________________________________________________________________________batch_normalization_43 (BatchNo (None, 15, 256)      1024        conv1d_43[0][0]                  __________________________________________________________________________________________________add_13 (Add)                    (None, 15, 256)      0           activation_37[0][0]                                                                               batch_normalization_43[0][0]     __________________________________________________________________________________________________activation_40 (Activation)      (None, 15, 256)      0           add_13[0][0]                     __________________________________________________________________________________________________conv1d_44 (Conv1D)              (None, 15, 512)      655872      activation_40[0][0]              __________________________________________________________________________________________________batch_normalization_44 (BatchNo (None, 15, 512)      2048        conv1d_44[0][0]                  __________________________________________________________________________________________________activation_41 (Activation)      (None, 15, 512)      0           batch_normalization_44[0][0]     __________________________________________________________________________________________________conv1d_45 (Conv1D)              (None, 15, 512)      1311232     activation_41[0][0]              __________________________________________________________________________________________________batch_normalization_45 (BatchNo (None, 15, 512)      2048        conv1d_45[0][0]                  __________________________________________________________________________________________________activation_42 (Activation)      (None, 15, 512)      0           batch_normalization_45[0][0]     __________________________________________________________________________________________________conv1d_47 (Conv1D)              (None, 15, 512)      655872      activation_40[0][0]              __________________________________________________________________________________________________conv1d_46 (Conv1D)              (None, 15, 512)      1311232     activation_42[0][0]              __________________________________________________________________________________________________batch_normalization_47 (BatchNo (None, 15, 512)      2048        conv1d_47[0][0]                  __________________________________________________________________________________________________batch_normalization_46 (BatchNo (None, 15, 512)      2048        conv1d_46[0][0]                  __________________________________________________________________________________________________add_14 (Add)                    (None, 15, 512)      0           batch_normalization_47[0][0]                                                                      batch_normalization_46[0][0]     __________________________________________________________________________________________________activation_43 (Activation)      (None, 15, 512)      0           add_14[0][0]                     __________________________________________________________________________________________________conv1d_48 (Conv1D)              (None, 15, 512)      1311232     activation_43[0][0]              __________________________________________________________________________________________________batch_normalization_48 (BatchNo (None, 15, 512)      2048        conv1d_48[0][0]                  __________________________________________________________________________________________________activation_44 (Activation)      (None, 15, 512)      0           batch_normalization_48[0][0]     __________________________________________________________________________________________________conv1d_49 (Conv1D)              (None, 15, 512)      1311232     activation_44[0][0]              __________________________________________________________________________________________________batch_normalization_49 (BatchNo (None, 15, 512)      2048        conv1d_49[0][0]                  __________________________________________________________________________________________________activation_45 (Activation)      (None, 15, 512)      0           batch_normalization_49[0][0]     __________________________________________________________________________________________________conv1d_50 (Conv1D)              (None, 15, 512)      1311232     activation_45[0][0]              __________________________________________________________________________________________________batch_normalization_50 (BatchNo (None, 15, 512)      2048        conv1d_50[0][0]                  __________________________________________________________________________________________________add_15 (Add)                    (None, 15, 512)      0           activation_43[0][0]                                                                               batch_normalization_50[0][0]     __________________________________________________________________________________________________activation_46 (Activation)      (None, 15, 512)      0           add_15[0][0]                     __________________________________________________________________________________________________conv1d_51 (Conv1D)              (None, 15, 512)      1311232     activation_46[0][0]              __________________________________________________________________________________________________batch_normalization_51 (BatchNo (None, 15, 512)      2048        conv1d_51[0][0]                  __________________________________________________________________________________________________activation_47 (Activation)      (None, 15, 512)      0           batch_normalization_51[0][0]     __________________________________________________________________________________________________conv1d_52 (Conv1D)              (None, 15, 512)      1311232     activation_47[0][0]              __________________________________________________________________________________________________batch_normalization_52 (BatchNo (None, 15, 512)      2048        conv1d_52[0][0]                  __________________________________________________________________________________________________activation_48 (Activation)      (None, 15, 512)      0           batch_normalization_52[0][0]     __________________________________________________________________________________________________conv1d_53 (Conv1D)              (None, 15, 512)      1311232     activation_48[0][0]              __________________________________________________________________________________________________batch_normalization_53 (BatchNo (None, 15, 512)      2048        conv1d_53[0][0]                  __________________________________________________________________________________________________add_16 (Add)                    (None, 15, 512)      0           activation_46[0][0]                                                                               batch_normalization_53[0][0]     __________________________________________________________________________________________________activation_49 (Activation)      (None, 15, 512)      0           add_16[0][0]                     __________________________________________________________________________________________________average_pooling1d_1 (AveragePoo (None, 15, 512)      0           activation_49[0][0]              __________________________________________________________________________________________________flatten_1 (Flatten)             (None, 7680)         0           average_pooling1d_1[0][0]        __________________________________________________________________________________________________dense_1 (Dense)                 (None, 1024)         7865344     flatten_1[0][0]                  __________________________________________________________________________________________________dropout_1 (Dropout)             (None, 1024)         0           dense_1[0][0]                    __________________________________________________________________________________________________dense_2 (Dense)                 (None, 1024)         1049600     dropout_1[0][0]                  __________________________________________________________________________________________________dense_3 (Dense)                 (None, 1)            1025        dense_2[0][0]                    __________________________________________________________________________________________________batch_normalization_54 (BatchNo (None, 1)            4           dense_3[0][0]                    __________________________________________________________________________________________________activation_50 (Activation)      (None, 1)            0           batch_normalization_54[0][0]     ==================================================================================================Total params: 30,169,393Trainable params: 27,893,187Non-trainable params: 2,276,206__________________________________________________________________________________________________</code></pre></details><h2 id="Performance"><a href="#Performance" class="headerlink" title="Performance"></a>Performance</h2><h3 id="Proposed-Model"><a href="#Proposed-Model" class="headerlink" title="Proposed Model"></a>Proposed Model</h3><h4 id="Statistical-Model"><a href="#Statistical-Model" class="headerlink" title="Statistical Model"></a>Statistical Model</h4><ol><li>Feature extracted by <code>CountVectorizer</code></li></ol><table><thead><tr><th></th><th>Naive Bayes</th><th>Gaussian Bayes</th><th>Bernoulli Bayes</th></tr></thead><tbody><tr><td>Accuracy</td><td><strong>0.7904</strong></td><td>0.6606</td><td>0.7540</td></tr><tr><td>F1 Score</td><td><strong>0.8357</strong></td><td>0.7689</td><td>0.8172</td></tr></tbody></table><ol start="2"><li>Feature extracted by <code>TfidfVectorizer</code></li></ol><table><thead><tr><th></th><th>Naive Bayes</th><th>Gaussian Bayes</th><th>Bernoulli Bayes</th></tr></thead><tbody><tr><td>Accuracy</td><td><strong>0.7879</strong></td><td>0.6703</td><td>0.7540</td></tr><tr><td>F1 Score</td><td><strong>0.8362</strong></td><td>0.7708</td><td>0.8172</td></tr></tbody></table><h4 id="Deep-Learning-Model"><a href="#Deep-Learning-Model" class="headerlink" title="Deep Learning Model"></a>Deep Learning Model</h4><p>Feature extracted by word2vec.</p><table><thead><tr><th></th><th>Simple-RNN</th><th>GRU</th><th>LSTM</th><th>BiLSTM</th><th>Attention</th><th>CNN-Static</th><th>CNN-MultiChannel</th><th>CNN-LSTM</th><th>Text-ResNet</th></tr></thead><tbody><tr><td>Accuracy</td><td>0.7419</td><td><strong>0.7821</strong></td><td>0.7697</td><td>0.7430</td><td>0.7496</td><td>0.7736</td><td>0.7744</td><td>0.7380</td><td>0.7283</td></tr><tr><td>F1 Score</td><td>0.7773</td><td><strong>0.8216</strong></td><td>0.7945</td><td>0.7696</td><td>0.7857</td><td>0.8031</td><td>0.8073</td><td>0.7785</td><td>0.7535</td></tr><tr><td>Total params (M)</td><td>2.27</td><td>2.31</td><td>2.37</td><td>2.55</td><td>2.50</td><td>2.79</td><td>7.29</td><td>3.31</td><td>30.16</td></tr><tr><td>Trainable params (M)</td><td>0.02</td><td>0.06</td><td>0.12</td><td>0.30</td><td>0.25</td><td>0.54</td><td>0.54</td><td>1.05</td><td>27.89</td></tr><tr><td>Non-trainable params (M)</td><td>2.25</td><td>2.25</td><td>2.25</td><td>2.25</td><td>2.25</td><td>2.25</td><td>6.75</td><td>2.25</td><td>2.27</td></tr></tbody></table><h4 id="Pre-trained-Language-Model"><a href="#Pre-trained-Language-Model" class="headerlink" title="Pre-trained Language Model"></a>Pre-trained Language Model</h4><table><thead><tr><th></th><th>BERT</th><th>ALBERT</th><th>DISTILBERT</th></tr></thead><tbody><tr><td>Accuracy</td><td><strong>0.8543</strong></td><td>0.8005</td><td>0.8528</td></tr><tr><td>F1 Score</td><td>0.8806</td><td>0.8410</td><td><strong>0.8815</strong></td></tr></tbody></table><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li>[<a href="https://arxiv.org/pdf/1606.01781.pdf">link</a>] Alexis Conneau, Very Deep Convolutional Networks for Text Classification </li><li>[<a href="https://anlp.jp/proceedings/annual_meeting/2018/pdf_dir/P12-2.pdf">link</a>] Lasguido Nio, Japanese Sentiment Classification Using Bidirectional Long Short-Term Memory Recurrent Neural Network </li><li>[<a href="https://www.scitepress.org/Papers/2017/61934/61934.pdf">link</a>] Minato Sato, Japanese Text Classification by Character-level Deep ConvNets and Transfer Learning </li><li>[<a href="https://www.aclweb.org/anthology/D14-1181.pdf">link</a>] Yoon Kim, Convolutional Neural Networks for Sentence Classification </li><li>[<a href="https://arxiv.org/pdf/1810.04805.pdf">link</a>] Jacob Devlin, BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding </li><li>[<a href="https://arxiv.org/pdf/1909.11942.pdf">link</a>] Zhenzhong Lan, ALBERT: A Lite BERT for Self-supervised Learning of Language Representations </li><li>[<a href="https://arxiv.org/pdf/1910.01108.pdf">link</a>] Victor Sanh, DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter </li><li>[<a href="https://www.bioinf.jku.at/publications/older/2604.pdf">link</a>] Sepp Hochreiter, Long Short-Term Memory </li><li>[<a href="https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf">link</a>] Tomas Mikolov, Distributed representations of words and phrases and their compositionality </li><li>[<a href="https://www.aclweb.org/anthology/E17-1096.pdf">link</a>] Amr El-Desoky Mousa, Contextual bidirectional long short-term memory recurrent neural network language models: A generative approach to sentiment analysis </li><li>[<a href="https://dl.acm.org/doi/pdf/10.1145/2766462.2767830">link</a>] Aliaksei Severyn, Twitter sentiment analysis with deep convolutional neural networks </li><li>[<a href="http://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf">link</a>] Nitish Srivastava, Dropout: A Simple Way to Prevent Neural Networks from Overfitting </li><li>[<a href="https://arxiv.org/pdf/1502.03167.pdf">link</a>] Sergey Ioffe, Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift </li><li>[<a href="https://arxiv.org/pdf/1602.07868.pdf">link</a>] Tim Salimans, Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks </li><li>[<a href="https://arxiv.org/pdf/1706.03762.pdf">link</a>] Ashish Vaswani, Attention Is All You Need </li><li>[<a href="https://arxiv.org/pdf/1906.08237.pdf">link</a>] Zhilin Yang, XLNet: Generalized Autoregressive Pretraining for Language Understanding </li><li>[<a href="https://arxiv.org/pdf/1904.08398.pdf">link</a>] Ashutosh Adhikari, DocBERT: BERT for Document Classification </li></ol><h2 id="Future-Roadmap"><a href="#Future-Roadmap" class="headerlink" title="Future Roadmap"></a>Future Roadmap</h2><p>It is completely possible to use only raw text as input for making predictions. The most important thing is to automatically extract the relevant features from this raw source of reviews data. Although the models don’t perform well and need more improvement, I have done a practise with a full harvest.</p><p>Text classifier is a meat-and-potatoes issue for most sentiment analysis task, and there are still many things can be done on this task. In future works, I might construct a multi-class text classifier to separate customers’ reviews into different issue types. (e.g. Function, UI, Crash, Truncate, Subscription, Server, Enhancement, etc), in order to tackle each consumer’s problem more efficiently and effectively.</p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> NLP </tag>
            
            <tag> KKBOX </tag>
            
            <tag> UtaPass </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Categorising Song Genre by Analysing Lyrics</title>
      <link href="/Hexo-Blog/2019/06/11/2019-06-11-categorising-song-genre-by-analysing-lyrics/"/>
      <url>/Hexo-Blog/2019/06/11/2019-06-11-categorising-song-genre-by-analysing-lyrics/</url>
      
        <content type="html"><![CDATA[<p>The ability to classify music in an automated manner has become increasingly more important with the advent of musical streaming services allowing greater access to music. Spotify alone hit 100 million users in 2016, with other services provided by companies such as Apple, Soundcloud and YouTube. In addition, there are huge numbers of professional musicians, approximately 53,000 in the USA alone, as well as amateurs who are producing music which needs to be classified. With this quantity of music, it is unfeasible to classify genres without an automated method.</p><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>The aim of this project is to try to develop a classifier for song genres using only its lyrics. Firstly, a dataset of song lyrics and their associated genres needs to be produced. Therefore, I build a crawler to get the dataset, which I will not demonstate in this article. Secondly, a review of the potential classification models needs to be undertaken to determine which is most likely to be successful in this task. I will compare conventional machine learning models to state-of-the-art deep learning models. Thirdly, a final result should be produced with the optimised model. This will then be reviewed with comparison to both ML and DL models to determine what areas are working successfully and where there are remaining issues, which still need to be overcome.</p><h1 id="Exploratory-Data-Analysis"><a href="#Exploratory-Data-Analysis" class="headerlink" title="Exploratory Data Analysis"></a>Exploratory Data Analysis</h1><h2 id="Import-Libraries"><a href="#Import-Libraries" class="headerlink" title="Import Libraries"></a>Import Libraries</h2><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> re<span class="token keyword">import</span> warnings<span class="token keyword">import</span> pandas <span class="token keyword">as</span> pd<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token keyword">from</span> tqdm <span class="token keyword">import</span> tqdmwarnings<span class="token punctuation">.</span>filterwarnings<span class="token punctuation">(</span><span class="token string">"ignore"</span><span class="token punctuation">)</span><span class="token keyword">import</span> sys<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt<span class="token keyword">import</span> seaborn <span class="token keyword">as</span> sns<span class="token keyword">from</span> collections <span class="token keyword">import</span> Counter<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>model_selection <span class="token keyword">import</span> train_test_split<span class="token keyword">from</span> collections <span class="token keyword">import</span> defaultdict<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>base <span class="token keyword">import</span> BaseEstimator<span class="token punctuation">,</span> TransformerMixin<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>feature_extraction<span class="token punctuation">.</span>text <span class="token keyword">import</span> TfidfVectorizer<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>decomposition <span class="token keyword">import</span> TruncatedSVD<span class="token keyword">from</span> gensim<span class="token punctuation">.</span>models <span class="token keyword">import</span> KeyedVectors<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>linear_model <span class="token keyword">import</span> LogisticRegression<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>ensemble <span class="token keyword">import</span> RandomForestClassifier<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>svm <span class="token keyword">import</span> LinearSVC<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>model_selection <span class="token keyword">import</span> cross_val_score<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><p>Load the data.</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">data <span class="token operator">=</span> pd<span class="token punctuation">.</span>read_csv<span class="token punctuation">(</span><span class="token string">"./lyrics.csv"</span><span class="token punctuation">,</span> delimiter<span class="token operator">=</span><span class="token string">","</span><span class="token punctuation">)</span>data<span class="token punctuation">.</span>head<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><img src="/Hexo-Blog/2019/06/11/2019-06-11-categorising-song-genre-by-analysing-lyrics/data.png" class=""><h2 id="Data-Processing"><a href="#Data-Processing" class="headerlink" title="Data Processing"></a>Data Processing</h2><p>There are lots of techniques for NLP data processing, such as noise removal (remove stopwords), lexicon normalisation (stemming, lemmatisation), object standardisation (acronyms, hash tags, colloquial slangs), etc. However, these are out of the scope in this article, instead, I will only do tokenisation. I wrote an <a href="/Hexo-Blog/2020/11/27/2020-11-27-exploratory-data-analysis-for-predicting-insurance-claim/" title="[article]">[article]</a> that teachs you how to do a proper EDA on your data.</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">tokenization</span><span class="token punctuation">(</span>text<span class="token punctuation">)</span><span class="token punctuation">:</span>    text <span class="token operator">=</span> re<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">'\W+'</span><span class="token punctuation">,</span> text<span class="token punctuation">)</span>    <span class="token keyword">return</span> textdata<span class="token punctuation">[</span><span class="token string">"lyrics_tokenised"</span><span class="token punctuation">]</span> <span class="token operator">=</span> data<span class="token punctuation">[</span><span class="token string">"lyrics"</span><span class="token punctuation">]</span><span class="token punctuation">.</span><span class="token builtin">apply</span><span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> tokenization<span class="token punctuation">(</span><span class="token builtin">str</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">.</span>lower<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>Segregated data into training (40%), validation (20%), and testing (20%) dataset.</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">lyrics <span class="token operator">=</span> data<span class="token punctuation">[</span><span class="token string">'lyrics_tokenised'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>valuesgenres <span class="token operator">=</span> data<span class="token punctuation">[</span><span class="token string">'genre'</span><span class="token punctuation">]</span><span class="token punctuation">.</span><span class="token builtin">apply</span><span class="token punctuation">(</span><span class="token builtin">str</span><span class="token punctuation">)</span><span class="token punctuation">.</span>valuesX_train<span class="token punctuation">,</span> X_test<span class="token punctuation">,</span> y_train<span class="token punctuation">,</span> y_test <span class="token operator">=</span> train_test_split<span class="token punctuation">(</span>    lyrics<span class="token punctuation">,</span> genres<span class="token punctuation">,</span> test_size<span class="token operator">=</span><span class="token number">0.4</span><span class="token punctuation">,</span> random_state<span class="token operator">=</span><span class="token number">914</span><span class="token punctuation">,</span> stratify<span class="token operator">=</span>genres<span class="token punctuation">)</span>X_valid<span class="token punctuation">,</span> X_test<span class="token punctuation">,</span> y_valid<span class="token punctuation">,</span> y_test <span class="token operator">=</span> train_test_split<span class="token punctuation">(</span>    X_test<span class="token punctuation">,</span> y_test<span class="token punctuation">,</span> test_size<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">,</span> random_state<span class="token operator">=</span><span class="token number">914</span><span class="token punctuation">,</span> stratify<span class="token operator">=</span>y_test<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>There are 217342, 72447, 72447 samples of training, validation, and testing dataset, respectively.</p><h1 id="Word-Embedding"><a href="#Word-Embedding" class="headerlink" title="Word Embedding"></a>Word Embedding</h1><p>I utilised three word embedding vectorisers <code>MeanEmbeddingVectorizer()</code>, <code>TfidfEmbeddingVectorizer()</code>, and <code>SifEmbeddingVectorizer()</code> from one of my <a href="/Hexo-Blog/2021/01/25/2021-01-25-weighted-word-embedding/" title="[post]">[post]</a>.</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># Word2Vec</span>vectoriser_w2v_mean <span class="token operator">=</span> MeanEmbeddingVectorizer<span class="token punctuation">(</span>word2vec<span class="token operator">=</span>w2v_model<span class="token punctuation">)</span>feature_train_w2v_mean <span class="token operator">=</span> vectoriser_w2v_mean<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>X_train<span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">)</span>vectoriser_w2v_tfidf <span class="token operator">=</span> TfidfEmbeddingVectorizer<span class="token punctuation">(</span>word2vec<span class="token operator">=</span>w2v_model<span class="token punctuation">)</span>feature_train_w2v_tfidf <span class="token operator">=</span> vectoriser_w2v_tfidf<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>X_train<span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">)</span>vectoriser_w2v_sif <span class="token operator">=</span> SifEmbeddingVectorizer<span class="token punctuation">(</span>word2vec<span class="token operator">=</span>w2v_model<span class="token punctuation">)</span>feature_train_w2v_sif <span class="token operator">=</span> vectoriser_w2v_sif<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>X_train<span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">)</span><span class="token comment"># GloVe</span>vectoriser_glove_mean <span class="token operator">=</span> MeanEmbeddingVectorizer<span class="token punctuation">(</span>word2vec<span class="token operator">=</span>glove_model<span class="token punctuation">)</span>feature_train_glove_mean <span class="token operator">=</span> vectoriser_glove_mean<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>X_train<span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">)</span>vectoriser_glove_tfidf <span class="token operator">=</span> TfidfEmbeddingVectorizer<span class="token punctuation">(</span>word2vec<span class="token operator">=</span>glove_model<span class="token punctuation">)</span>feature_train_glove_tfidf <span class="token operator">=</span> vectoriser_glove_tfidf<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>X_train<span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">)</span>vectoriser_glove_sif <span class="token operator">=</span> SifEmbeddingVectorizer<span class="token punctuation">(</span>word2vec<span class="token operator">=</span>glove_model<span class="token punctuation">)</span>feature_train_glove_sif <span class="token operator">=</span> vectoriser_glove_sif<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>X_train<span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">)</span><span class="token comment"># FastText</span>vectoriser_ft_mean <span class="token operator">=</span> MeanEmbeddingVectorizer<span class="token punctuation">(</span>word2vec<span class="token operator">=</span>ft_model<span class="token punctuation">)</span>feature_train_ft_mean <span class="token operator">=</span> vectoriser_ft_mean<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>X_train<span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">)</span>vectoriser_ft_tfidf <span class="token operator">=</span> TfidfEmbeddingVectorizer<span class="token punctuation">(</span>word2vec<span class="token operator">=</span>ft_model<span class="token punctuation">)</span>feature_train_ft_tfidf <span class="token operator">=</span> vectoriser_ft_tfidf<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>X_train<span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">)</span>vectoriser_ft_sif <span class="token operator">=</span> SifEmbeddingVectorizer<span class="token punctuation">(</span>word2vec<span class="token operator">=</span>ft_model<span class="token punctuation">)</span>feature_train_ft_sif <span class="token operator">=</span> vectoriser_ft_sif<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>X_train<span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>After a long period of time, finally got the embedding vectors! Let’s put them into dictionaries <code>vectorisers_dcit</code> and <code>features_train_dict</code> for later use.</p><h1 id="Modelling"><a href="#Modelling" class="headerlink" title="Modelling"></a>Modelling</h1><h2 id="Introduction-1"><a href="#Introduction-1" class="headerlink" title="Introduction"></a>Introduction</h2><p>This is a supervised text classification problem, and our goal is to investigate which supervised machine learning methods are best suited to solve it. Given a new lyrics comes in, we want to assign it to one of the twelve categories. This is a multi-class text classification task. </p><h2 id="Imbalanced-Classes"><a href="#Imbalanced-Classes" class="headerlink" title="Imbalanced Classes"></a>Imbalanced Classes</h2><p>Let’s take a look at the distribution of label in training dataset.</p><img src="/Hexo-Blog/2019/06/11/2019-06-11-categorising-song-genre-by-analysing-lyrics/distribution.png" class=""><p>We can see that the number of genres per song is imbalanced. Genres of the songs are more biased towards “Rock” music. When we encounter such problems, we are bound to have difficulties solving them with standard algorithms, Conventional algorithms are often biased towards the majority classes, not taking the data distribution into account. In the worst case, minority classes are considered as outliers or being ignored. For some cases, such as fraud detection or cancer prediction, we would need to carefully configure our model or artificially balance the dataset, for instance, using resampling technique (under-sampling, over-sampling), Tomek Links, SMOTE (Synthetic Minority Oversampling Technique), class weights in the models, or changing your evaluation metrics.</p><p>Various other methods might work depending on your use case and the problem you are trying to solve. </p><ol><li>Collect more data</li><li>Treat the problem as anomaly detection (e.g. isolation forests, autoencoders, …)</li><li>Model-based approach (boosting models, …)</li></ol><p>However, in our case, I will not operate any of the techniques mentioned above, I will leave it as it is.</p><h2 id="Baseline"><a href="#Baseline" class="headerlink" title="Baseline"></a>Baseline</h2><p>We are now ready to experiment with different machine learning models, evaluate their accuracy and find the source of any potential issues.</p><p>We will benchmark the following three models:</p><ul><li>Random Forest</li><li>Linear Support Vector Machine</li><li>Logistic Regression</li></ul><p>I also built same models for 9 different weighted embedding method: word2vec-mean, word2vec-tfidf, word2vec-sif, glove-mean, glove-tfidf, glove-sif, fasttext-mean, fasttext-tfidf, and fasttext-sif.</p><p>After calculating for all the cv dataframe, you will get something like the following:</p><img src="/Hexo-Blog/2019/06/11/2019-06-11-categorising-song-genre-by-analysing-lyrics/cvtable.png" class=""><p>Give this some plots: (if you want to visulise them by yourself to capture some interesting point, I’ll put the csv file over <a href="https://penguinwang96825.github.io/Hexo-Blog/download/song-genre-classification-performace.csv">here</a>)</p><img src="/Hexo-Blog/2019/06/11/2019-06-11-categorising-song-genre-by-analysing-lyrics/perf1.png" class=""><img src="/Hexo-Blog/2019/06/11/2019-06-11-categorising-song-genre-by-analysing-lyrics/perf2.png" class=""><img src="/Hexo-Blog/2019/06/11/2019-06-11-categorising-song-genre-by-analysing-lyrics/perf3.png" class=""><img src="/Hexo-Blog/2019/06/11/2019-06-11-categorising-song-genre-by-analysing-lyrics/perf4.png" class=""><img src="/Hexo-Blog/2019/06/11/2019-06-11-categorising-song-genre-by-analysing-lyrics/perf-embedding.png" class=""><img src="/Hexo-Blog/2019/06/11/2019-06-11-categorising-song-genre-by-analysing-lyrics/perf-weight.png" class=""><p>The figures tell us some intriguing points:</p><ol><li>LinearSVC (mean acc: 0.4070) and LogisticRegression (mean acc: 0.3979) perform slightly better than RandomForest (mean acc: 0.3826).</li><li>SIF usually has better accuracy and has lower variance comparing to TF-IDF and averaging method.</li><li>Integrating GloVe embedding model with SIF weight seems to be the best choice to this task. Top 1, top 2, and top 3 model are all embedding with GloVe model, having a mean accuracy around 42%. </li></ol><div style="display: flex; justify-content: center;">    <table class="styled-table">        <thead>            <tr>                <th>Model Name</th>                <th>Weight</th>                <th>Embedding</th>                <th>Accuracy</th>            </tr>        </thead>        <tbody>            <tr>                <td rowspan=9>LinearSVC</td>                <td rowspan=3>mean</td>                <td>fasttext</td>                <td>0.425256</td>            </tr>            <tr class="active-row">                <td>glove</td>                <td>🥈 0.431067</td>            </tr>            <tr>                <td>word2vec</td>                <td>0.421520</td>            </tr>            <tr>                <td rowspan="3">sif</td>                <td>fasttext</td>                <td>0.421695</td>            </tr>            <tr class="active-row">                <td>glove</td>                <td>🥇 0.440918</td>            </tr>            <tr>                <td>word2vec</td>                <td>0.426650</td>            </tr>            <tr>                <td rowspan="3">tfidf</td>                <td>fasttext</td>                <td>0.366289</td>            </tr>            <tr>                <td>glove</td>                <td>0.365272</td>            </tr>            <tr>                <td>word2vec</td>                <td>0.364651</td>            </tr>            <tr>                <td rowspan=9>LogisticRegression</td>                <td rowspan=3>mean</td>                <td>fasttext</td>                <td>0.415023</td>            </tr>            <tr>                <td>glove</td>                <td>0.426816</td>            </tr>            <tr>                <td>word2vec</td>                <td>0.416371</td>            </tr>            <tr>                <td rowspan="3">sif</td>                <td>fasttext</td>                <td>0.397213</td>            </tr>            <tr class="active-row">                <td>glove</td>                <td>🥉 0.429250</td>            </tr>            <tr>                <td>word2vec</td>                <td>0.406907</td>            </tr>            <tr>                <td rowspan="3">tfidf</td>                <td>fasttext</td>                <td>0.363473</td>            </tr>            <tr>                <td>glove</td>                <td>0.363828</td>            </tr>            <tr>                <td>word2vec</td>                <td>0.362917</td>            </tr>            <tr>                <td rowspan=9>RandomForestClassifier</td>                <td rowspan=3>mean</td>                <td>fasttext</td>                <td>0.362760</td>            </tr>            <tr>                <td>glove</td>                <td>0.371686</td>            </tr>            <tr>                <td>word2vec</td>                <td>0.362682</td>            </tr>            <tr>                <td rowspan="3">sif</td>                <td>fasttext</td>                <td>0.400079</td>            </tr>            <tr>                <td>glove</td>                <td>0.403953</td>            </tr>            <tr>                <td>word2vec</td>                <td>0.397659</td>            </tr>            <tr>                <td rowspan="3">tfidf</td>                <td>fasttext</td>                <td>0.379264</td>            </tr>            <tr>                <td>glove</td>                <td>0.384008</td>            </tr>            <tr>                <td>word2vec</td>                <td>0.382075</td>            </tr>        </tbody>    </table></div><h2 id="Deep-Learning-Model"><a href="#Deep-Learning-Model" class="headerlink" title="Deep Learning Model"></a>Deep Learning Model</h2><p>In this project, I used <code>tez</code>, a simple PyTorch wrapper, to design our deep learning model structure. This library keeps things super simple and customisable. </p><h3 id="Import-Libraries-1"><a href="#Import-Libraries-1" class="headerlink" title="Import Libraries"></a>Import Libraries</h3><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> tez<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn<span class="token keyword">import</span> pandas <span class="token keyword">as</span> pd<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token keyword">import</span> torchimport transformers<span class="token keyword">from</span> collections <span class="token keyword">import</span> Counter<span class="token keyword">from</span> sklearn <span class="token keyword">import</span> metrics<span class="token punctuation">,</span> model_selection<span class="token punctuation">,</span> preprocessing<span class="token keyword">from</span> transformers <span class="token keyword">import</span> AdamW<span class="token punctuation">,</span> get_linear_schedule_with_warmup<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="Build-Dataset"><a href="#Build-Dataset" class="headerlink" title="Build Dataset"></a>Build Dataset</h3><p>PyTorch provides many tools to make data loading easy and hopefully, to make your code more readable. In this section, we will see how to load and preprocess data from a custom dataset. In this Dataset class, I tokenize the lyrics, and break them up into word and subwords in the format DistilBERT is comfortable with.</p><p>Before we can hand our lyrics to <code>SongGenreDistilbertClassifier()</code>, we need to do some minimal processing to put them in the format it requires.</p><ol><li>Tokenise: break them up into word and subwords.</li><li>Padding: pad all lists to the same size.</li><li>Masking: ignore (mask) the padding we’ve added when it’s processing its input.</li></ol><img src="/Hexo-Blog/2019/06/11/2019-06-11-categorising-song-genre-by-analysing-lyrics/tokenisation.png" class=""><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">DistilbertDataset</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> text<span class="token punctuation">,</span> target<span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>text <span class="token operator">=</span> text        self<span class="token punctuation">.</span>target <span class="token operator">=</span> target        self<span class="token punctuation">.</span>tokenizer <span class="token operator">=</span> transformers<span class="token punctuation">.</span>DistilBertTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>            <span class="token string">"distilbert-base-uncased"</span><span class="token punctuation">,</span> do_lower_case<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> use_fast<span class="token operator">=</span><span class="token boolean">False</span>        <span class="token punctuation">)</span>        self<span class="token punctuation">.</span>max_len <span class="token operator">=</span> <span class="token number">64</span>    <span class="token keyword">def</span> <span class="token function">__len__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> <span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>text<span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">__getitem__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> item<span class="token punctuation">)</span><span class="token punctuation">:</span>        text <span class="token operator">=</span> <span class="token builtin">str</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>text<span class="token punctuation">[</span>item<span class="token punctuation">]</span><span class="token punctuation">)</span>        text <span class="token operator">=</span> <span class="token string">" "</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>text<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        inputs <span class="token operator">=</span> self<span class="token punctuation">.</span>tokenizer<span class="token punctuation">.</span>encode_plus<span class="token punctuation">(</span>            text<span class="token punctuation">,</span>            <span class="token boolean">None</span><span class="token punctuation">,</span>            add_special_tokens<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>            max_length<span class="token operator">=</span>self<span class="token punctuation">.</span>max_len<span class="token punctuation">,</span>            padding<span class="token operator">=</span><span class="token string">"max_length"</span><span class="token punctuation">,</span>            truncation<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>        <span class="token punctuation">)</span>        ids <span class="token operator">=</span> inputs<span class="token punctuation">[</span><span class="token string">"input_ids"</span><span class="token punctuation">]</span>        mask <span class="token operator">=</span> inputs<span class="token punctuation">[</span><span class="token string">"attention_mask"</span><span class="token punctuation">]</span>        <span class="token keyword">return</span> <span class="token punctuation">&#123;</span>            <span class="token string">"ids"</span><span class="token punctuation">:</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>ids<span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">long</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            <span class="token string">"mask"</span><span class="token punctuation">:</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>mask<span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">long</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            <span class="token string">"targets"</span><span class="token punctuation">:</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>self<span class="token punctuation">.</span>target<span class="token punctuation">[</span>item<span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">long</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        <span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="Build-Model"><a href="#Build-Model" class="headerlink" title="Build Model"></a>Build Model</h3><p>A typical training procedure for a neural network is as follows:</p><ol><li>Define the neural network that has some learnable parameters (or weights)</li><li>Iterate over a dataset of inputs</li><li>Process input through the network</li><li>Compute the loss (how far is the output from being correct)</li><li>Propagate gradients back into the network’s parameters</li><li>Update the weights of the network, typically using a simple update rule: <code>weight = weight - learning_rate * gradient</code></li></ol><h4 id="BERT"><a href="#BERT" class="headerlink" title="BERT"></a>BERT</h4><p>BERT is a new language representation model, which stands for Bidirectional Encoder from Transformer, published by researchers at Google AL Language. In this work, I will talk about DistilBERT, which is a smaller, faster, cheaper and lighter version of BERT. DistilBERT uses a technique called <code>distillation</code>, which approximates the BERT, the larger neural network by a smaller one. The idea is that once a large neural network has been trained, its full output distributions can be approximated using a smaller network. However, the basic structure of DistilBERT almost remain the same as BERT, and it retains 95% performance but using only half the number of parameters.</p><p>The <code>forward()</code> function runs our lyrics through DistilBERT. The results of the processing will be returned into <code>last_hidden_states</code>. Let’s slice only the part of the output that we need. That is the output corresponding the first token of each sentence. The way BERT does sentence classification, is that it adds a token called [CLS] at the beginning of every sentence. The output corresponding to that token can be thought of as an embedding for the entire sentence. The shape of <code>last_hidden_states[0]</code> sequentially contains <code>lyrics</code>, <code>position of every tokens</code>, <code>hidden unit outputs</code>. We’ll then save those in the features variable, as they’ll serve as the features to our fully connection layer.</p><img src="/Hexo-Blog/2019/06/11/2019-06-11-categorising-song-genre-by-analysing-lyrics/distilbert.png" class=""><p>Let’s define the network <code>SongGenreDistilbertClassifier()</code> using pertrained model from <code>HuggingFace</code>:</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">SongGenreDistilbertClassifier</span><span class="token punctuation">(</span>tez<span class="token punctuation">.</span>Model<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> num_train_steps<span class="token punctuation">,</span> num_classes<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>tokenizer <span class="token operator">=</span> transformers<span class="token punctuation">.</span>DistilBertTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>            <span class="token string">"distilbert-base-uncased"</span><span class="token punctuation">,</span> do_lower_case<span class="token operator">=</span><span class="token boolean">True</span>        <span class="token punctuation">)</span>        self<span class="token punctuation">.</span>bert <span class="token operator">=</span> transformers<span class="token punctuation">.</span>DistilBertModel<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>            <span class="token string">"distilbert-base-uncased"</span><span class="token punctuation">,</span>             return_dict<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>bert_drop <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span><span class="token number">0.3</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>out <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>bert<span class="token punctuation">.</span>config<span class="token punctuation">.</span>dim<span class="token punctuation">,</span> num_classes<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>num_train_steps <span class="token operator">=</span> num_train_steps        self<span class="token punctuation">.</span>step_scheduler_after <span class="token operator">=</span> <span class="token string">"batch"</span>    <span class="token keyword">def</span> <span class="token function">fetch_optimizer</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        param_optimizer <span class="token operator">=</span> <span class="token builtin">list</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>named_parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        no_decay <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">"bias"</span><span class="token punctuation">,</span> <span class="token string">"LayerNorm.bias"</span><span class="token punctuation">]</span>        optimizer_parameters <span class="token operator">=</span> <span class="token punctuation">[</span>            <span class="token punctuation">&#123;</span>                <span class="token string">"params"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span>                    p <span class="token keyword">for</span> n<span class="token punctuation">,</span> p <span class="token keyword">in</span> param_optimizer <span class="token keyword">if</span> <span class="token keyword">not</span> <span class="token builtin">any</span><span class="token punctuation">(</span>nd <span class="token keyword">in</span> n <span class="token keyword">for</span> nd <span class="token keyword">in</span> no_decay<span class="token punctuation">)</span>                <span class="token punctuation">]</span><span class="token punctuation">,</span>                <span class="token string">"weight_decay"</span><span class="token punctuation">:</span> <span class="token number">0.001</span><span class="token punctuation">,</span>            <span class="token punctuation">&#125;</span><span class="token punctuation">,</span>            <span class="token punctuation">&#123;</span>                <span class="token string">"params"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span>                    p <span class="token keyword">for</span> n<span class="token punctuation">,</span> p <span class="token keyword">in</span> param_optimizer <span class="token keyword">if</span> <span class="token builtin">any</span><span class="token punctuation">(</span>nd <span class="token keyword">in</span> n <span class="token keyword">for</span> nd <span class="token keyword">in</span> no_decay<span class="token punctuation">)</span>                <span class="token punctuation">]</span><span class="token punctuation">,</span>                <span class="token string">"weight_decay"</span><span class="token punctuation">:</span> <span class="token number">0.0</span><span class="token punctuation">,</span>            <span class="token punctuation">&#125;</span><span class="token punctuation">,</span>        <span class="token punctuation">]</span>        opt <span class="token operator">=</span> AdamW<span class="token punctuation">(</span>optimizer_parameters<span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">3e</span><span class="token operator">-</span><span class="token number">5</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> opt    <span class="token keyword">def</span> <span class="token function">fetch_scheduler</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        sch <span class="token operator">=</span> get_linear_schedule_with_warmup<span class="token punctuation">(</span>            self<span class="token punctuation">.</span>optimizer<span class="token punctuation">,</span> num_warmup_steps<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> num_training_steps<span class="token operator">=</span>self<span class="token punctuation">.</span>num_train_steps        <span class="token punctuation">)</span>        <span class="token keyword">return</span> sch    <span class="token keyword">def</span> <span class="token function">loss</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> outputs<span class="token punctuation">,</span> targets<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">if</span> targets <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>            <span class="token keyword">return</span> <span class="token boolean">None</span>        <span class="token keyword">return</span> nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">(</span>outputs<span class="token punctuation">,</span> targets<span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">monitor_metrics</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> outputs<span class="token punctuation">,</span> targets<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">if</span> targets <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>            <span class="token keyword">return</span> <span class="token punctuation">&#123;</span><span class="token punctuation">&#125;</span>        outputs <span class="token operator">=</span> torch<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>outputs<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span>        targets <span class="token operator">=</span> targets<span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span>        accuracy <span class="token operator">=</span> metrics<span class="token punctuation">.</span>accuracy_score<span class="token punctuation">(</span>targets<span class="token punctuation">,</span> outputs<span class="token punctuation">)</span>        f1_score <span class="token operator">=</span> metrics<span class="token punctuation">.</span>f1_score<span class="token punctuation">(</span>targets<span class="token punctuation">,</span> outputs<span class="token punctuation">,</span> average<span class="token operator">=</span><span class="token string">'weighted'</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> <span class="token punctuation">&#123;</span><span class="token string">"accuracy"</span><span class="token punctuation">:</span> accuracy<span class="token punctuation">,</span> <span class="token string">"f1"</span><span class="token punctuation">:</span> f1_score<span class="token punctuation">&#125;</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> ids<span class="token punctuation">,</span> mask<span class="token punctuation">,</span> targets<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        last_hidden_states <span class="token operator">=</span> self<span class="token punctuation">.</span>bert<span class="token punctuation">(</span>ids<span class="token punctuation">,</span> attention_mask<span class="token operator">=</span>mask<span class="token punctuation">)</span>        b_o <span class="token operator">=</span> self<span class="token punctuation">.</span>bert_drop<span class="token punctuation">(</span>last_hidden_states<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span>        output <span class="token operator">=</span> self<span class="token punctuation">.</span>out<span class="token punctuation">(</span>b_o<span class="token punctuation">)</span>        loss <span class="token operator">=</span> self<span class="token punctuation">.</span>loss<span class="token punctuation">(</span>output<span class="token punctuation">,</span> targets<span class="token punctuation">)</span>        acc <span class="token operator">=</span> self<span class="token punctuation">.</span>monitor_metrics<span class="token punctuation">(</span>output<span class="token punctuation">,</span> targets<span class="token punctuation">)</span>        <span class="token keyword">return</span> output<span class="token punctuation">,</span> loss<span class="token punctuation">,</span> acc    <span class="token keyword">def</span> <span class="token function">score</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> valid_dataset<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">,</span> n_jobs<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        preds <span class="token operator">=</span> self<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>valid_dataset<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">,</span> n_jobs<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>        preds <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token builtin">list</span><span class="token punctuation">(</span>flatten<span class="token punctuation">(</span><span class="token builtin">list</span><span class="token punctuation">(</span>preds<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        preds <span class="token operator">=</span> preds<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>valid_dataset<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">12</span><span class="token punctuation">)</span>        preds <span class="token operator">=</span> np<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>preds<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>        targets <span class="token operator">=</span> valid_dataset<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">"targets"</span><span class="token punctuation">]</span><span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span>        acc <span class="token operator">=</span> metrics<span class="token punctuation">.</span>accuracy_score<span class="token punctuation">(</span>targets<span class="token punctuation">,</span> preds<span class="token punctuation">)</span>        f1 <span class="token operator">=</span> metrics<span class="token punctuation">.</span>f1_score<span class="token punctuation">(</span>targets<span class="token punctuation">,</span> preds<span class="token punctuation">,</span> average<span class="token operator">=</span><span class="token string">'weighted'</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> acc<span class="token punctuation">,</span> f1<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="Preparing-the-Dataset"><a href="#Preparing-the-Dataset" class="headerlink" title="Preparing the Dataset"></a>Preparing the Dataset</h3><pre class="line-numbers language-python" data-language="python"><code class="language-python">data <span class="token operator">=</span> pd<span class="token punctuation">.</span>read_csv<span class="token punctuation">(</span><span class="token string">"./lyrics.csv"</span><span class="token punctuation">,</span> delimiter<span class="token operator">=</span><span class="token string">","</span><span class="token punctuation">)</span>data <span class="token operator">=</span> data<span class="token punctuation">.</span>dropna<span class="token punctuation">(</span>subset<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">"lyrics"</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>reset_index<span class="token punctuation">(</span>drop<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>lbl_enc <span class="token operator">=</span> preprocessing<span class="token punctuation">.</span>LabelEncoder<span class="token punctuation">(</span><span class="token punctuation">)</span>data<span class="token punctuation">.</span>genre <span class="token operator">=</span> lbl_enc<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>data<span class="token punctuation">.</span>genre<span class="token punctuation">.</span>values<span class="token punctuation">)</span>df_train<span class="token punctuation">,</span> df_valid <span class="token operator">=</span> model_selection<span class="token punctuation">.</span>train_test_split<span class="token punctuation">(</span>    data<span class="token punctuation">,</span> test_size<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">,</span> random_state<span class="token operator">=</span><span class="token number">42</span><span class="token punctuation">,</span> stratify<span class="token operator">=</span>data<span class="token punctuation">.</span>genre<span class="token punctuation">.</span>values<span class="token punctuation">)</span>df_train <span class="token operator">=</span> df_train<span class="token punctuation">.</span>reset_index<span class="token punctuation">(</span>drop<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>df_valid <span class="token operator">=</span> df_valid<span class="token punctuation">.</span>reset_index<span class="token punctuation">(</span>drop<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>train_dataset <span class="token operator">=</span> DistilbertDataset<span class="token punctuation">(</span>    text<span class="token operator">=</span>df_train<span class="token punctuation">.</span>lyrics<span class="token punctuation">.</span>values<span class="token punctuation">,</span> target<span class="token operator">=</span>df_train<span class="token punctuation">.</span>genre<span class="token punctuation">.</span>values<span class="token punctuation">)</span>valid_dataset <span class="token operator">=</span> DistilbertDataset<span class="token punctuation">(</span>    text<span class="token operator">=</span>df_valid<span class="token punctuation">.</span>lyrics<span class="token punctuation">.</span>values<span class="token punctuation">,</span> target<span class="token operator">=</span>df_valid<span class="token punctuation">.</span>genre<span class="token punctuation">.</span>values<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="Start-Training"><a href="#Start-Training" class="headerlink" title="Start Training"></a>Start Training</h3><pre class="line-numbers language-python" data-language="python"><code class="language-python">n_train_steps <span class="token operator">=</span> <span class="token builtin">int</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>df_train<span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token number">32</span> <span class="token operator">*</span> <span class="token number">10</span><span class="token punctuation">)</span>model <span class="token operator">=</span> SongGenreDistilbertClassifier<span class="token punctuation">(</span>    num_train_steps<span class="token operator">=</span>n_train_steps<span class="token punctuation">,</span> num_classes<span class="token operator">=</span><span class="token builtin">len</span><span class="token punctuation">(</span>Counter<span class="token punctuation">(</span>data<span class="token punctuation">.</span>genre<span class="token punctuation">)</span><span class="token punctuation">.</span>keys<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>tb_logger <span class="token operator">=</span> tez<span class="token punctuation">.</span>callbacks<span class="token punctuation">.</span>TensorBoardLogger<span class="token punctuation">(</span>log_dir<span class="token operator">=</span><span class="token string">"./logs/"</span><span class="token punctuation">)</span>es <span class="token operator">=</span> tez<span class="token punctuation">.</span>callbacks<span class="token punctuation">.</span>EarlyStopping<span class="token punctuation">(</span>monitor<span class="token operator">=</span><span class="token string">"valid_loss"</span><span class="token punctuation">,</span>                                  model_path<span class="token operator">=</span><span class="token string">"./output/diltilbert.bin"</span><span class="token punctuation">,</span>                                  patience<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">,</span>                                  mode<span class="token operator">=</span><span class="token string">"max"</span><span class="token punctuation">,</span><span class="token punctuation">)</span>model<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>    train_dataset<span class="token punctuation">,</span>    valid_dataset<span class="token operator">=</span>valid_dataset<span class="token punctuation">,</span>    train_bs<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">,</span>    device<span class="token operator">=</span><span class="token string">"cuda"</span><span class="token punctuation">,</span>    epochs<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">,</span>    callbacks<span class="token operator">=</span><span class="token punctuation">[</span>tb_logger<span class="token punctuation">,</span> es<span class="token punctuation">]</span><span class="token punctuation">,</span>    fp16<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span><span class="token punctuation">)</span>model<span class="token punctuation">.</span>save<span class="token punctuation">(</span><span class="token string">"./output/diltilbert.bin"</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="Evaluate-the-Model"><a href="#Evaluate-the-Model" class="headerlink" title="Evaluate the Model"></a>Evaluate the Model</h3><pre class="line-numbers language-python" data-language="python"><code class="language-python">model<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">"output/diltilbert.bin"</span><span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token string">"cuda"</span><span class="token punctuation">)</span>acc<span class="token punctuation">,</span> f1 <span class="token operator">=</span> model<span class="token punctuation">.</span>score<span class="token punctuation">(</span>valid_dataset<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">,</span> n_jobs<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h3 id="Performance"><a href="#Performance" class="headerlink" title="Performance"></a>Performance</h3><p>Without doing text preprocessing step or standard tokenisation technique, pre-trained models leads to a big performance increase, making it competitive with other conventional machine learning models.</p><div style="display: flex; justify-content: center;">    <table class="styled-table">        <thead>            <tr>                <th>Model Name</th>                <th>Accuracy</th>                <th>F1-score</th>            </tr>        </thead>        <tbody>            <tr>                <td>BERT</td>                <td>0.55998</td>                <td>🥇 0.53611</td>            </tr>            <tr>                <td>DistilBERT</td>                <td>0.56167</td>                <td>🥈 0.53433</td>            </tr>            <tr>                <td>ALBERT</td>                <td>0.50131</td>                <td>0.42038</td>            </tr>            <tr>                <td>ELECTRA</td>                <td>0.54820</td>                <td>0.50825</td>            </tr>            <tr>                <td>XLNet</td>                <td>0.55214</td>                <td>🥉 0.52457</td>            </tr>        </tbody>    </table></div><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>In this article, you’ve learned how you can train BERT, DistilBERT, ALBERT, ELECTRA, and XLNet using Huggingface <a href="https://github.com/huggingface/transformers">Transformers</a> library on your dataset. Note that, you can also use other transformer models, such as GPT-2 with GPT2ForSequenceClassification, RoBERTa with GPT2ForSequenceClassification, and much more.</p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ol><li><a href="https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html">https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html</a></li><li><a href="https://github.com/abhishekkrthakur/tez">https://github.com/abhishekkrthakur/tez</a></li><li><a href="https://www.kdnuggets.com/2019/09/bert-roberta-distilbert-xlnet-one-use.html">https://www.kdnuggets.com/2019/09/bert-roberta-distilbert-xlnet-one-use.html</a></li><li><a href="https://www.kaggle.com/atulanandjha/distillbert-extensive-tutorial-starter-kernel">https://www.kaggle.com/atulanandjha/distillbert-extensive-tutorial-starter-kernel</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Speech </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> NLP </tag>
            
            <tag> Embedding </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>This is how the Journey Begins</title>
      <link href="/Hexo-Blog/2018/12/03/2018-12-03-this-is-how-the-journey-begins/"/>
      <url>/Hexo-Blog/2018/12/03/2018-12-03-this-is-how-the-journey-begins/</url>
      
        <content type="html"><![CDATA[<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Graduation is not the end; it’s the beginning to start a new journey after graduation. This is how the journey begins!</p><h2 id="As-a-Mathematics-Major-and-Engineering-Life-in-KKBOX"><a href="#As-a-Mathematics-Major-and-Engineering-Life-in-KKBOX" class="headerlink" title="As a Mathematics Major and Engineering Life in KKBOX"></a>As a Mathematics Major and Engineering Life in KKBOX</h2><p>I attended my BS Degree in Applied Mathematics from National Chung Hsing University in Taiwan. After I graduated last year, I have totally no idea what I should do and what I could do in the future. I then started to apply for jobs in both Taipei and Taichung.</p><img src="/Hexo-Blog/2018/12/03/2018-12-03-this-is-how-the-journey-begins/kkbox.jpg" class=""><p>Luckily, I sought a full-time position as a Software Quality Assurance Assistant Engineer with KKBOX Group. During the first two months, I learned a lot of testing skills via Jenkins, Confluence, TestRail, etc. The period in KKBOX was so great due to free beverage and snack on a daily basis. It is wonderful to have such great colleagues and supervisors though, however, I reckoned that I couldn’t apply my mathematics knowledge to anything in my work. As a result, this job became much more boring and dull.</p><img src="/Hexo-Blog/2018/12/03/2018-12-03-this-is-how-the-journey-begins/environment.jpg" class=""><p>Accordingly, I then asked my supervisor whether I could conduct a NLP-related project which I was interested in. Equipped with programming knowledge in Matlab, Python and R, my supervisor duly allowed me to undertake this project. Therefore, I created a polarity text classifier for the reviews of UtaPass, a music streaming app, and divided the massive amounts of reviews into positive and negative areas, which allowed me to resolve user’s issues both effectively and efficiently.</p><img src="/Hexo-Blog/2018/12/03/2018-12-03-this-is-how-the-journey-begins/vendor.jpg" class=""><p>Aside from academic knowledge and working experience, extracurricular competitions have figured highly in my life. One notable contest occurred at Kaggle Featured Code Competition, wherein I was able to detect the toxicity of conversations with Jigsaw Unintended Bias in Toxicity Classification dataset. At the same time, another competition at TeraSoft allowed me to construct a deep learning-based merchandise detector for a self-service store.</p><img src="/Hexo-Blog/2018/12/03/2018-12-03-this-is-how-the-journey-begins/mac-kk.jpg" class=""><p>During these project and competitions, I realised that possessing a BS degree is simply inadequate to get a better job. In consequence, I decide to learn more about machine learning upon a postgraduate degree course, and as the field of computer science focuses on the study of NLP. I believe a degree in this discipline will allow me to pursue a variety of careers.</p><h2 id="Apply-for-NAIST"><a href="#Apply-for-NAIST" class="headerlink" title="Apply for NAIST"></a>Apply for NAIST</h2><p>In May 2019, I wrote a letter to Professor Nakamura in NAIST (Nara Institute of Science and Technology). His current research interests include speech-to-speech translation, speech recognition, speech synthesis, spoken dialog systems, multi-modal communication, and brain activity sensing in linguistics, which gave me lots of attention.</p><img src="/Hexo-Blog/2018/12/03/2018-12-03-this-is-how-the-journey-begins/naist-letter.jpg" class=""><p>When I received the letter from Professor in the morning, my legs felt like jelly, not because of fear, but on the cloud nine. It means so much to me. Three days after, I made a leave and took a flight to Nara, Japan immediately. I was so nervous but exciting to visit the laboratory at NAIST.</p><img src="/Hexo-Blog/2018/12/03/2018-12-03-this-is-how-the-journey-begins/solo-trip.jpg" class=""><p>After having a great conversation with Prof. Nakamura and Prof. Yoshino, I decided to apply for 2019 fall admission. On top of that, I also got a letter of acceptance from Prof. Nakamura.</p><img src="/Hexo-Blog/2018/12/03/2018-12-03-this-is-how-the-journey-begins/naist-lab.jpg" class=""><p>Applying for Master’s course, I prepared the document below,</p><ol><li>Application form</li><li>Examination voucher</li><li>Receipt form for examination fee payment</li><li>Graduation/Expected Graduation certificate</li><li>Transcript</li><li>Research proposal</li><li>Document verifying English Proficiency</li><li>Address label</li><li>Letter of Acceptance</li></ol><p>On the examination day, I took mathematics test first, which was an oral examination on questions of mathematics (Linear Algebra and Calculus) written out in English.</p><p>The second exam was questions regarding the research proposal and areas of information science relevant to the examinees’ area of specialization. Examinees are required to give a self intro of no more than three minutes (in English) without equipment or handouts, which should be prepared in advance. The questions asked by Prof. Nakamura was the following,</p><ol><li>Please explain your research proposal.</li><li>Have you done reading lots of papers regarding to your area of specialization? Please make some examples.</li><li>How to utilize the model into your research proposal?</li><li>What else could you improve on your model?</li><li>Why could mathematics profession be your strength?</li><li>What is the connection between Question-Answering and your research proposal?</li><li>Could you implement your chat-bot with the state-of-the-art models right now? Please illustrate it.</li></ol><img src="/Hexo-Blog/2018/12/03/2018-12-03-this-is-how-the-journey-begins/exam.jpg" class=""><p>I didn’t perform well on the last question; however, thanks for Prof. Nakamura, I did realise how to improve my skills in some respect. After exam, I just wanted to have fun anyway, and relax in Japan in the next few days.</p><img src="/Hexo-Blog/2018/12/03/2018-12-03-this-is-how-the-journey-begins/okonomiyaki.jpg" class=""><p>Announcements of the results was really fast, it was on July 19th 2019. As expected, I did not pass the exam.</p><img src="/Hexo-Blog/2018/12/03/2018-12-03-this-is-how-the-journey-begins/score-report.jpg" class=""><h2 id="Apply-for-Sheffield"><a href="#Apply-for-Sheffield" class="headerlink" title="Apply for Sheffield"></a>Apply for Sheffield</h2><p>In order to get a better chance, I need to gain more experience. I then decided to quit my job at KKBOX and apply for Universities in the U.K. In the whole September, I did a lot of research on the universities in the U.K. Next, I started to prepare for IELTS test on October 26th 2019.</p><p>Finally, I got my IELTS transcript two weeks later. I submitted it to the universities in the U.K. Surprisingly, Aston University gave me Unconditional Offer in two days; University of Essex gave me Unconditional Offers in three days; University of Swansea gave me Unconditional Offer in five days.</p><p>The most incredible thing is that I received offer from The University of Sheffield on 16th December 2019. Computer Science with Speech and Language Processing is my most favorite discipline.</p><img src="/Hexo-Blog/2018/12/03/2018-12-03-this-is-how-the-journey-begins/offer.jpg" class=""><p>So I retook IELTS on 4th January 2020. Unfortunately, I got a touch of flu in the middle of December. This was a hard time for me to prepare IELTS test, cause I could not focus on my preparation.<br>Unexpectedly, I got a band 6.5! To be honest, it indeed is unbelievable!</p><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>I am definitely super excited and really looking forward to attending Sheffield this year.<br>Here I am, Sheffield.</p>]]></content>
      
      
      <categories>
          
          <category> MSc </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NAIST </tag>
            
            <tag> Sheffield </tag>
            
            <tag> CSSLP </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
